[
  {
    "question": "How do I undo the most recent local commits in Git?",
    "answer": "Undo a commit & redo\n$ git commit -m \"Something terribly misguided\" # (0: Your Accident)\n$ git reset HEAD~                              # (1)\n# === If you just want to undo the commit, stop here! ===\n[ edit files as necessary ]                    # (2)\n$ git add .                                    # (3)\n$ git commit -c ORIG_HEAD                      # (4)\n\n\ngit reset is the command responsible for the undo. It will undo your last commit while leaving your working tree (the state of your files on disk) untouched. You'll need to add them again before you can commit them again.\nMake corrections to working tree files.\ngit add anything that you want to include in your new commit.\nCommit the changes, reusing the old commit message. reset copied the old head to .git/ORIG_HEAD; commit with -c ORIG_HEAD will open an editor, which initially contains the log message from the old commit and allows you to edit it. If you do not need to edit the message, you could use the -C option.\n\nAlternatively, to edit the previous commit (or just its commit message), commit --amend will add changes within the current index to the previous commit.\nTo remove (not revert) a commit that has been pushed to the server, rewriting history with git push origin main --force[-with-lease] is necessary. It's almost always a bad idea to use --force; prefer --force-with-lease instead, and as noted in the git manual:\n\nYou should understand the implications of rewriting history if you amend a commit that has already been published.\n\n\nFurther Reading\nYou can use git reflog to determine the SHA-1 for the commit to which you wish to revert. Once you have this value, use the sequence of commands as explained above.\n\nHEAD~ is the same as HEAD~1. The article What is the HEAD in git? is helpful if you want to uncommit multiple commits.",
    "tag": "git"
  },
  {
    "question": "How do I delete a Git branch locally and remotely?",
    "answer": "Executive Summary\ngit push -d <remote_name> <branchname>   # Delete remote\ngit branch -d <branchname>               # Delete local\n\nNote: In most cases, <remote_name> will be origin.\nDelete Local Branch\nTo delete the local branch, use one of the following:\ngit branch -d <branch_name>\ngit branch -D <branch_name>\n\n\nThe -d option is an alias for --delete, which only deletes the branch if it has already been fully merged in its upstream branch.\nThe -D option is an alias for --delete --force, which deletes the branch \"irrespective of its merged status.\" [Source: man git-branch]\nAs of Git v2.3, git branch -d (delete) learned to honor the -f (force) flag.\nYou will receive an error if you try to delete the currently selected branch.\n\nDelete Remote Branch\nAs of Git v1.7.0, you can delete a remote branch using\n$ git push <remote_name> --delete <branch_name>\n\nwhich might be easier to remember than\n$ git push <remote_name> :<branch_name>\n\nwhich was added in Git v1.5.0 \"to delete a remote branch or a tag.\"\nStarting with Git v2.8.0, you can also use git push with the -d option as an alias for --delete. Therefore, the version of Git you have installed will dictate whether you need to use the easier or harder syntax.\nDelete Remote Branch [Original Answer from 5-Jan-2010]\nFrom Chapter 3 of Pro Git by Scott Chacon:\n\nDeleting Remote Branches\nSuppose you’re done with a remote branch — say, you and your collaborators are finished with a feature and have merged it into your remote’s main branch (or whatever branch your stable code-line is in). You can delete a remote branch using the rather obtuse syntax git push [remotename] :[branch]. If you want to delete your serverfix branch from the server, you run the following:\n$ git push origin :serverfix\nTo git@github.com:schacon/simplegit.git\n - [deleted]         serverfix\n\nBoom. No more branches on your server. You may want to dog-ear this page, because you’ll need that command, and you’ll likely forget the syntax. A way to remember this command is by recalling the git push [remotename] [localbranch]:[remotebranch] syntax that we went over a bit earlier. If you leave off the [localbranch] portion, then you’re basically saying, “Take nothing on my side and make it be [remotebranch].”\n\nI ran git push origin :bugfix, and it worked beautifully. Scott Chacon was right—I will want to dog-ear that page (or virtually dog ear-by answering this on Stack Overflow).\nFetch changes\nFinally, execute the following on other machines to propagate changes:\n# Fetch changes from all remotes and locally delete \n# remote deleted branches/tags etc\n# --prune will do the job :-;\ngit fetch --all --prune",
    "tag": "git"
  },
  {
    "question": "What is the difference between 'git pull' and 'git fetch'?",
    "answer": "In the simplest terms, git pull does a git fetch followed by a git merge.\n\ngit fetch updates your remote-tracking branches under refs/remotes/<remote>/. This operation is safe to run at any time since it never changes any of your local branches under refs/heads.\ngit pull brings a local branch up-to-date with its remote version, while also updating your other remote-tracking branches.\nFrom the Git documentation for git pull:\n\ngit pull runs git fetch with the given parameters and then depending on configuration options or command line flags, will call either git rebase or git merge to reconcile diverging branches.",
    "tag": "git"
  },
  {
    "question": "How can I rename a local Git branch?",
    "answer": "To rename the current branch:\ngit branch -m <newname>\n\nTo rename a branch while pointed to any branch:\ngit branch -m <oldname> <newname>\n\n-m is short for --move.\n\nTo push the  local branch and reset the upstream branch:\ngit push origin -u <newname>\n\nTo delete the  remote branch:\ngit push origin --delete <oldname>\n\n\nTo create a git rename alias:\ngit config --global alias.rename 'branch -m'\n\n\nOn Windows or another case-insensitive filesystem, use -M if there are only capitalization changes in the name. Otherwise, Git will throw a \"branch already exists\" error.\ngit branch -M <newname>",
    "tag": "git"
  },
  {
    "question": "How do I undo 'git add' before commit?",
    "answer": "To unstage a specific file\ngit reset <file>\n\nThat will remove the file from the current index (the \"about to be committed\" list) without changing anything else.\nTo unstage all files from the current change set:\ngit reset\n\n\nIn old versions of Git, the above commands are equivalent to git reset HEAD <file> and git reset HEAD respectively, and will fail if HEAD is undefined (because you haven't yet made any commits in your repository) or ambiguous (because you created a branch called HEAD, which is a stupid thing that you shouldn't do). This was changed in Git 1.8.2, though, so in modern versions of Git you can use the commands above even prior to making your first commit:\n\n\"git reset\" (without options or parameters) used to error out when\nyou do not have any commits in your history, but it now gives you\nan empty index (to match non-existent commit you are not even on).\n\nDocumentation: git reset",
    "tag": "git"
  },
  {
    "question": "How do I force \"git pull\" to overwrite local files?",
    "answer": "⚠ Warning:\nAny uncommitted local change to tracked files will be lost, even if staged.\nBut any local file that's not tracked by Git will not be affected.\n\n\nFirst, update all origin/<branch> refs to latest:\ngit fetch --all\n\nBackup your current branch (e.g. main):\ngit branch backup-main\n\nJump to the latest commit on origin/main and checkout those files:\ngit reset --hard origin/main\n\nExplanation:\ngit fetch downloads the latest from remote without trying to merge or rebase anything.\ngit reset resets the master branch to what you just fetched. The --hard option changes all the files in your working tree to match the files in origin/main.\n\nMaintain current local commits\n[*]: It's worth noting that it is possible to maintain current local commits by creating a branch from main before resetting:\ngit checkout main\ngit branch new-branch-to-save-current-commits\ngit fetch --all\ngit reset --hard origin/main\n\nAfter this, all of the old commits will be kept in new-branch-to-save-current-commits.\nUncommitted changes\nUncommitted changes, even if staged (with git add), will be lost. Make sure to stash or commit anything you need. For example, run the following:\ngit stash\n\nAnd later (after git reset), reapply these uncommitted changes:\ngit stash pop\n\n\nWhich may create merge conflicts.",
    "tag": "git"
  },
  {
    "question": "How to check out a remote Git branch?",
    "answer": "The answer has been split depending on whether there is one remote repository configured or multiple. The reason for this is that for the single remote case, some of the commands can be simplified as there is less ambiguity.\nUpdated for Git 2.23: For older versions, see the section at the end.\nWith One Remote\nIn both cases, start by fetching from the remote repository to make sure you have all the latest changes downloaded.\n$ git fetch\n\nThis will fetch all of the remote branches for you. You can see the branches available for checkout with:\n$ git branch -v -a\n\n...\nremotes/origin/test\n\nThe branches that start with remotes/* can be thought of as read only copies of the remote branches. To work on a branch you need to create a local branch from it. This is done with the Git command switch (since Git 2.23) by giving it the name of the remote branch (minus the remote name):\n$ git switch test\n\nIn this case Git is guessing (can be disabled with --no-guess) that you are trying to checkout and track the remote branch with the same name.\nWith Multiple Remotes\nIn the case where multiple remote repositories exist, the remote repository needs to be explicitly named.\nAs before, start by fetching the latest remote changes:\n$ git fetch origin\n\nThis will fetch all of the remote branches for you. You can see the branches available for checkout with:\n$ git branch -v -a\n\nWith the remote branches in hand, you now need to check out the branch you are interested in with -c to create a new local branch:\n$ git switch -c test origin/test\n\nFor more information about using git switch:\n$ man git-switch\n\nPrior to Git 2.23\ngit switch was added in Git 2.23, prior to this git checkout was used to switch branches.\nTo checkout out with only a single remote repository:\ngit checkout test\n\nif there are multiple remote repositories configured then it becomes a bit longer\ngit checkout -b test <name of remote>/test",
    "tag": "git"
  },
  {
    "question": "How do I make Git forget about a file that was tracked, but is now in .gitignore?",
    "answer": ".gitignore will prevent untracked files from being added (without an add -f) to the set of files tracked by Git. However, Git will continue to track any files that are already being tracked.\nUpdated Answer in 2024\nDo NOT use git rm --cached <file> if you ever want to see that file again. It will remove it from git, and also your local machine.\nIf you want to keep the file locally, but remove it from git tracking, use the answer by Konstantin. In short, use the following instead of git rm:\ngit update-index --skip-worktree <file>\nHowever, according to the official git documentation:\n\nUsers often try to use the assume-unchanged and skip-worktree bits to tell Git to ignore changes to files that are tracked. This does not work as expected, since Git may still check working tree files against the index when performing certain operations. In general, Git does not provide a way to ignore changes to tracked files, so alternate solutions are recommended.\n\nTherefore, you should still consider using the original answer below.\nOriginal Answer\nWARNING: This will remove the physical file from your local machine and other developers' machines on your or their next git pull.\nTo stop tracking a file, we must remove it from the index:\ngit rm --cached <file>\n\nTo remove a folder and all files in the folder recursively:\ngit rm -r --cached <folder>\n\nThe removal of the file from the head revision will happen on the next commit.",
    "tag": "git"
  },
  {
    "question": "How do I remove local (untracked) files from the current Git working tree?",
    "answer": "git-clean - Remove untracked files from the working tree\nSynopsis\ngit clean [-d] [-f] [-i] [-n] [-q] [-e <pattern>] [-x | -X] [--] <path>…​\n\nDescription\nCleans the working tree by recursively removing files that are not under version control, starting from the current directory.\nNormally, only files unknown to Git are removed, but if the -x option is specified, ignored files are also removed. This can, for example, be useful to remove all build products.\nIf any optional <path>... arguments are given, only those paths are affected.\n\n\nStep 1 is to show what will be deleted by using the -n option:\n# Print out the list of files and directories which will be removed (dry run)\ngit clean -n -d\n\nClean Step - beware: this will delete files:\n# Delete the files from the repository\ngit clean -f\n\n\nTo remove directories, run git clean -f -d or git clean -fd\nTo remove ignored files, run git clean -f -X or git clean -fX\nTo remove ignored and non-ignored files, run git clean -f -x or git clean -fx\n\nNote the case difference on the X for the two latter commands.\nIf clean.requireForce is set to \"true\" (the default) in your configuration, one needs to specify -f otherwise nothing will actually happen.\nAgain see the git-clean docs for more information.\n\n\nOptions\n-f, --force\nIf the Git configuration variable clean.requireForce is not set to\nfalse, git clean will refuse to run unless given -f, -n or -i.\n-x\nDon’t use the standard ignore rules read from .gitignore (per\ndirectory) and $GIT_DIR/info/exclude, but do still use the ignore\nrules given with -e options. This allows removing all untracked files,\nincluding build products. This can be used (possibly in conjunction\nwith git reset) to create a pristine working directory to test a clean\nbuild.\n-X\nRemove only files ignored by Git. This may be useful to rebuild\neverything from scratch, but keep manually created files.\n-n, --dry-run\nDon’t actually remove anything, just show what would be done.\n-d\nRemove untracked directories in addition to untracked files. If an\nuntracked directory is managed by a different Git repository, it is\nnot removed by default. Use -f option twice if you really want to\nremove such a directory.",
    "tag": "git"
  },
  {
    "question": "How to modify existing, unpushed commit messages?",
    "answer": "Amending the most recent commit message\ngit commit --amend\n\nwill open your editor, allowing you to change the commit message of the most recent commit. Additionally, you can set the commit message directly in the command line with:\ngit commit --amend -m \"New commit message\"\n\n…however, this can make multi-line commit messages or small corrections more cumbersome to enter.\nMake sure you don't have any working copy changes staged before doing this or they will get committed too. (Unstaged changes will not get committed.)\nChanging the message of a commit that you've already pushed to your remote branch\nIf you've already pushed your commit up to your remote branch, then - after amending your commit locally (as described above) - you'll also need to force push the commit with:\ngit push <remote> <branch> --force\n# Or\ngit push <remote> <branch> -f\n\nWarning: force-pushing will overwrite the remote branch with the state of your local one. If there are commits on the remote branch that you don't have in your local branch, you will lose those commits.\nWarning: be cautious about amending commits that you have already shared with other people. Amending commits essentially rewrites them to have different SHA IDs, which poses a problem if other people have copies of the old commit that you've rewritten. Anyone who has a copy of the old commit will need to synchronize their work with your newly re-written commit, which can sometimes be difficult, so make sure you coordinate with others when attempting to rewrite shared commit history, or just avoid rewriting shared commits altogether.\n\nPerform an interactive rebase\nAnother option is to use interactive rebase.\nThis allows you to edit any message you want to update even if it's not the latest message.\nIn order to do a Git squash, follow these steps:\n// n is the number of commits up to the last commit you want to be able to edit\ngit rebase -i HEAD~n\n\nOnce you squash your commits - choose the e/r for editing the message:\n\nImportant note about interactive rebase\nWhen you use git rebase -i HEAD~n there can be more than n commits. Git will \"collect\" all the commits in the last n commits, and if there was a merge somewhere in between that range you will see all the commits as well, so the outcome will be n + .\nGood tip:\nIf you have to do it for more than a single branch and you might face conflicts when amending the content, set up git rerere and let Git resolve those conflicts automatically for you.\n\nDocumentation\n\ngit-commit(1) Manual Page\n\ngit-rebase(1) Manual Page\n\ngit-push(1) Manual Page",
    "tag": "git"
  },
  {
    "question": "How do I revert a Git repository to a previous commit?",
    "answer": "This depends a lot on what you mean by \"revert\".\nTemporarily switch to a different commit\nIf you want to temporarily go back to it, fool around, then come back to where you are, all you have to do is check out the desired commit:\n# This will detach your HEAD, that is, leave you with no branch checked out:\ngit checkout 0d1d7fc32\n\nOr if you want to make commits while you're there, go ahead and make a new branch while you're at it:\ngit checkout -b old-state 0d1d7fc32\n\nTo go back to where you were, just check out the branch you were on again. (If you've made changes, as always when switching branches, you'll have to deal with them as appropriate. You could reset to throw them away; you could stash, checkout, stash pop to take them with you; you could commit them to a branch there if you want a branch there.)\nHard delete unpublished commits\nIf, on the other hand, you want to really get rid of everything you've done since then, there are two possibilities. One, if you haven't published any of these commits, simply reset:\n# This will destroy any local modifications.\n# Don't do it if you have uncommitted work you want to keep.\ngit reset --hard 0d1d7fc32\n\n# Alternatively, if there's work to keep:\ngit stash\ngit reset --hard 0d1d7fc32\ngit stash pop\n# This saves the modifications, then reapplies that patch after resetting.\n# You could get merge conflicts, if you've modified things which were\n# changed since the commit you reset to.\n\nIf you mess up, you've already thrown away your local changes, but you can at least get back to where you were before by resetting again.\nUndo published commits with new commits\nOn the other hand, if you've published the work, you probably don't want to reset the branch, since that's effectively rewriting history. In that case, you could indeed revert the commits. In many enterprise organisations, the concept of \"protected\" branches will even prevent history from being rewritten on some major branches. In this case, reverting is your only option.\nWith Git, revert has a very specific meaning: create a commit with the reverse patch to cancel it out. This way you don't rewrite any history.\nFirst figure out what commits to revert. Depending on the technique chosen below, you want to either revert only the merge commits, or only the non-merge commits.\n# This lists all merge commits between 0d1d7fc and HEAD:\ngit log --merges --pretty=format:\"%h\" 0d1d7fc..HEAD | tr '\\n' ' '\n\n# This lists all non merge commits between 0d1d7fc and HEAD:\ngit log --no-merges --pretty=format:\"%h\" 0d1d7fc..HEAD | tr '\\n' ' '\n\nNote: if you revert multiple commits, the order matters. Start with the most recent commit.\n# This will create three separate revert commits, use non merge commits only:\ngit revert a867b4af 25eee4ca 0766c053\n\n# It also takes ranges. This will revert the last two commits:\ngit revert HEAD~2..HEAD\n\n# Similarly, you can revert a range of commits using commit hashes (non inclusive of first hash):\ngit revert 0d1d7fc..a867b4a\n\n# Reverting a merge commit. You can also use a range of merge commits here.\ngit revert -m 1 <merge_commit_sha>\n\n# To get just one, you could use `rebase -i` to squash them afterwards\n# Or, you could do it manually (be sure to do this at top level of the repo)\n# get your index and work tree into the desired state, without changing HEAD:\ngit checkout 0d1d7fc32 .\n\n# Then commit. Be sure and write a good message describing what you just did\ngit commit\n\nThe git-revert manpage actually covers a lot of this in its description. Another useful link is this git-scm.com section discussing git-revert.\nIf you decide you didn't want to revert after all, you can revert the revert (as described here) or reset back to before the revert (see the previous section).\nYou may also find this answer helpful in this case:\nHow can I move HEAD back to a previous location? (Detached head) & Undo commits",
    "tag": "git"
  },
  {
    "question": "How do I change the URI (URL) for a remote Git repository?",
    "answer": "First, view the existing remotes to verify which URL is currently set:\ngit remote -v\n\nThen, you can set it with:\ngit remote set-url origin <NEW_GIT_URL_HERE>\n\nSee git help remote. You also can edit .git/config and change the URLs there.\nYou're not in any danger of losing history unless you do something very silly (and if you're worried, just make a copy of your repo, since your repo is your history.)",
    "tag": "git"
  },
  {
    "question": "Move the most recent commit(s) to a new branch with Git",
    "answer": "WARNING: You need to store uncommitted edits to your stash before doing this, using git stash. Once complete, you can retrieve the stashed uncommitted edits with git stash pop. git reset hard command will remove all changes!\nMoving to an existing branch\nIf you want to move your commits to an existing branch, it will look like this:\ngit checkout existingbranch\ngit merge branchToMoveCommitFrom\ngit checkout branchToMoveCommitFrom\ngit reset --hard HEAD~3 # Go back 3 commits. You *will* lose uncommitted work.\ngit checkout existingbranch\n\nMoving to a new branch\nWARNING: This method works because you are creating a new branch with the first command: git branch newbranch. If you want to move commits to an existing branch you need to merge your changes into the existing branch before executing git reset --hard HEAD~3 (see Moving to an existing branch above). If you don't merge your changes first, they will be lost.\nUnless there are other circumstances involved, this can be easily done by branching and rolling back.\n# Note: Any changes not committed will be lost.\ngit branch newbranch      # Create a new branch, saving the desired commits\ngit checkout master       # checkout master, this is the place you want to go back\ngit reset --hard HEAD~3   # Move master back by 3 commits (Make sure you know how many commits you need to go back)\ngit checkout newbranch    # Go to the new branch that still has the desired commits\n\nBut do make sure how many commits to go back. Alternatively, you can instead of HEAD~3, simply provide the hash of the commit (or the reference like origin/master) you want to \"revert back to\" on the master (/current) branch, e.g:\ngit reset --hard a1b2c3d4\n\nNote: You will only be \"losing\" commits from the master branch, but don't worry, you'll have those commits in newbranch! An easy way to check that, after completing the 4 step sequence of commands above, is by looking at git log -n4 which will show the history of newbranch actually retained the 3 commits (and the reason is that newbranch was created at the time those changes were already commited on master!). They have only been removed from master, as git reset only affected the branch that was checked out at the time of its execution, i.e. master (see git reset description: Reset current HEAD to the specified state). git status however will not show any checkouts on the newbranch, which might be surprising at first but that is actually expected.\nLastly, you may need to force push your latest changes to main repo:\ngit push origin master --force\n\nWARNING: With Git version 2.0 and later, if you later git rebase the new branch upon the original (master) branch, you may need an explicit --no-fork-point option during the rebase to avoid losing the carried-over commits.  Having branch.autosetuprebase always set makes this more likely.  See John Mellor's answer for details.",
    "tag": "git"
  },
  {
    "question": "How do I discard unstaged changes in Git?",
    "answer": "For all unstaged files in current working directory use:\ngit restore .\n\nFor a specific file use:\ngit restore path/to/file/to/revert\n\nThat together with git switch replaces the overloaded git checkout (see here), and thus removes the argument disambiguation.\nIf a file has both staged and unstaged changes, only the unstaged changes shown in git diff are reverted. Changes shown in git diff --staged stay intact.\nBefore Git 2.23\nFor all unstaged files in current working directory:\ngit checkout -- .\n\nFor a specific file:\ngit checkout -- path/to/file/to/revert\n\n-- here to remove ambiguity (this is known as  argument disambiguation).",
    "tag": "git"
  },
  {
    "question": "Reset local repository branch to be just like remote repository HEAD",
    "answer": "Setting your branch to exactly match the remote branch can be done in two steps:\ngit fetch origin\ngit reset --hard origin/master\n\nIf you want to save your current branch's state before doing this (just in case), you can do:\ngit commit -a -m \"Saving my work, just in case\"\ngit branch my-saved-work\n\nNow your work is saved on the branch \"my-saved-work\" in case you decide you want it back (or want to look at it later or diff it against your updated branch).\nNote: the first example assumes that the remote repo's name is origin and that the branch named master in the remote repo matches the currently checked-out branch in your local repo, since that is in line with the example given in the question. If you are trying to reset to the default branch in a more recent repository, it is likely that it will be main.\nBTW, this situation that you're in looks an awful lot like a common case where a push has been done into the currently checked out branch of a non-bare repository. Did you recently push into your local repo? If not, then no worries -- something else must have caused these files to unexpectedly end up modified. Otherwise, you should be aware that it's not recommended to push into a non-bare repository (and not into the currently checked-out branch, in particular).",
    "tag": "git"
  },
  {
    "question": "How can I reset or revert a file to a specific revision?",
    "answer": "Assuming the hash of the commit you want is c5f567:\ngit checkout c5f567 -- file1/to/restore file2/to/restore\n\nThe git checkout man page gives more information.\nIf you want to revert to the commit before c5f567, append ~1 (where 1 is the number of commits you want to go back, it can be anything):\ngit checkout c5f567~1 -- file1/to/restore file2/to/restore\n\nAs a side note, I've always been uncomfortable with this command because it's used for both ordinary things (changing between branches) and unusual, destructive things (discarding changes in the working directory).\nFor the meaning of -- in the command, refer to In Git, what does -- (dash dash) mean?\n\nThere is also a new git restore command that is specifically designed for restoring working copy files that have been modified. If your git is new enough you can use this command, but the documentation comes with a warning:\n\nTHIS COMMAND IS EXPERIMENTAL. THE BEHAVIOR MAY CHANGE.\n\nBecause git restore is experimental, it should not yet be promoted as the primary answer to this question. When the command is no longer marked as \"experimental\", then this answer can be amended to promote the use of git restore. [At the time of writing, the git restore command has been marked as \"experimental\" for at least four years.]",
    "tag": "git"
  },
  {
    "question": "How do I push a new local branch to a remote Git repository and track it too?",
    "answer": "In Git 1.7.0 and later, you can checkout a new branch:\ngit checkout -b <branch>\n\nEdit files, add and commit. Then push with the -u (short for --set-upstream) option:\ngit push -u origin <branch>\n\nGit will set up the tracking information during the push.",
    "tag": "git"
  },
  {
    "question": "How do I squash my last N commits together?",
    "answer": "You can do this fairly easily without git rebase or git merge --squash. In this example, we'll squash the last 3 commits.\nIf you want to write the new commit message from scratch, this suffices:\ngit reset --soft HEAD~3\ngit commit\n\nIf you want to start editing the new commit message with a concatenation of the existing commit messages (i.e. similar to what a pick/squash/squash/…/squash git rebase -i instruction list would start you with), then you need to extract those messages and pass them to git commit:\ngit reset --soft HEAD~3 && \ngit commit --edit -m\"$(git log --format=%B --reverse HEAD..HEAD@{1})\"\n\nBoth of those methods squash the last three commits into a single new commit in the same way. The soft reset just re-points HEAD to the last commit that you do not want to squash. Neither the index nor the working tree are touched by the soft reset, leaving the index in the desired state for your new commit (i.e. it already has all the changes from the commits that you are about to “throw away”).\nEdit Based on Comments\nBecause git reset and git rebase rewrite history, you must use the --force flag to push the modified branch to the remote. This is what the --force flag is meant for. You can also be extra careful and fully define your target:\ngit push --force-with-lease origin <branch-name>",
    "tag": "git"
  },
  {
    "question": "How to determine the URL that a local Git repository was originally cloned from",
    "answer": "To obtain only the remote URL:\ngit config --get remote.origin.url\n\nIf you require full output, and you are on a network that can reach the remote repo where the origin resides:\ngit remote show origin\n\nWhen using git clone (from GitHub, or any source repository for that matter) the default name for the source of the clone is \"origin\". Using git remote show will display the information about this remote name. The first few lines should show:\nC:\\Users\\jaredpar\\VsVim> git remote show origin\n* remote origin\n  Fetch URL: git@github.com:jaredpar/VsVim.git\n  Push  URL: git@github.com:jaredpar/VsVim.git\n  HEAD branch: master\n  Remote branches:\n\nIf you want to use the value in a script, you would use the first command listed in this answer.",
    "tag": "git"
  },
  {
    "question": "How do I add an empty directory to a Git repository?",
    "answer": "Another way to make a directory stay (almost) empty (in the repository) is to create a .gitignore file inside that directory that contains these four lines:\n# Ignore everything in this directory\n*\n# Except this file\n!.gitignore\n\nThen you don't have to get the order right the way that you have to do in m104's solution.\nThis also gives the benefit that files in that directory won't show up as \"untracked\" when you do a git status.\nMaking @GreenAsJade's comment persistent:\n\nI think it's worth noting that this solution does precisely what the question asked for, but is not perhaps what many people looking at this question will have been looking for. This solution guarantees that the directory remains empty. It says \"I truly never want files checked in here\". As opposed to \"I don't have any files to check in here, yet, but I need the directory here, files may be coming later\".",
    "tag": "git"
  },
  {
    "question": "How do I resolve merge conflicts in a Git repository?",
    "answer": "Try:\ngit mergetool\n\nIt opens a GUI that steps you through each conflict, and you get to choose how to merge.  Sometimes it requires a bit of hand editing afterwards, but usually it's enough by itself.  It is much better than doing the whole thing by hand certainly.\n\nAs per Josh Glover's comment:\n\n[This command]\ndoesn't necessarily open a GUI unless you install one. Running git mergetool for me resulted in vimdiff being used. You can install\none of the following tools to use it instead: meld, opendiff,\nkdiff3, tkdiff, xxdiff, tortoisemerge, gvimdiff, diffuse,\necmerge, p4merge, araxis, vimdiff, emerge.\n\n\nBelow is a sample procedure using vimdiff to resolve merge conflicts, based on this link.\n\nRun the following commands in your terminal\ngit config merge.tool vimdiff\ngit config merge.conflictstyle diff3\ngit config mergetool.prompt false\n\nThis will set vimdiff as the default merge tool.\n\nRun the following command in your terminal\ngit mergetool\n\n\nYou will see a vimdiff display in the following format:\n  ╔═══════╦══════╦════════╗\n  ║       ║      ║        ║\n  ║ LOCAL ║ BASE ║ REMOTE ║\n  ║       ║      ║        ║\n  ╠═══════╩══════╩════════╣\n  ║                       ║\n  ║        MERGED         ║\n  ║                       ║\n  ╚═══════════════════════╝\n\nThese 4 views are\n\nLOCAL: this is the file from the current branch\nBASE: the common ancestor, how this file looked before both changes\nREMOTE: the file you are merging into your branch\nMERGED: the merge result; this is what gets saved in the merge commit and used in the future\n\nYou can navigate among these views using ctrl+w. You can directly reach the MERGED view using ctrl+w followed by j.\nMore information about vimdiff navigation is here and here.\n\nYou can edit the MERGED view like this:\n\nIf you want to get changes from REMOTE\n:diffg RE\n\n\nIf you want to get changes from BASE\n:diffg BA\n\n\nIf you want to get changes from LOCAL\n:diffg LO\n\n\n\n\nSave, Exit, Commit, and Clean up\n:wqa save and exit from vi\ngit commit -m \"message\"\ngit clean Remove extra files (e.g. *.orig). Warning: It will remove all untracked files, if you won't pass any arguments.",
    "tag": "git"
  },
  {
    "question": "How can I delete a remote tag?",
    "answer": "You can push an 'empty' reference to the remote tag name:\ngit push origin :tagname\n\nOr, more expressively, use the --delete option (or -d if your git version is older than 1.8.0):\ngit push --delete origin tagname\n\nNote that git has tag namespace and branch namespace so you may use the same name for a branch and for a tag. If you want to make sure that you cannot accidentally remove the branch instead of the tag, you can specify full ref which will never delete a branch:\ngit push origin :refs/tags/tagname\n\nIf you also need to delete the local tag, use:\ngit tag --delete tagname\n\nor\ngit tag -d tagname\n\n\nBackground\nPushing a branch, tag, or other ref to a remote repository involves specifying \"which repo, what source, what destination?\"\ngit push remote-repo source-ref:destination-ref\n\nA real world example where you push your master branch to the origin's master branch is:\ngit push origin refs/heads/master:refs/heads/master\n\nWhich because of default paths, can be shortened to:\ngit push origin master:master\n\nTags work the same way:\ngit push origin refs/tags/release-1.0:refs/tags/release-1.0\n\nWhich can also be shortened to:\ngit push origin release-1.0:release-1.0\n\nBy omitting the source ref (the part before the colon), you push 'nothing' to the destination, deleting the ref on the remote end.",
    "tag": "git"
  },
  {
    "question": "Undo a Git merge that hasn't been pushed yet",
    "answer": "With git reflog check which commit is one prior the merge (git reflog will be a better option than git log). Then you can reset it using:\ngit reset --hard commit_sha\n\nThere's also another way:\ngit reset --hard HEAD~1\n\nIt will get you back 1 commit.\nBe aware that any modified and uncommitted/unstashed files will be reset to their unmodified state. To keep them either stash changes away or see --merge option below.  \n\nAs @Velmont suggested below in his answer, in this direct case using:\ngit reset --hard ORIG_HEAD\n\nmight yield better results, as it should preserve your changes. ORIG_HEAD will point to a commit directly before merge has occurred, so you don't have to hunt for it yourself.\n\nA further tip is to use the --merge switch instead of --hard since it doesn't reset files unnecessarily:\ngit reset --merge ORIG_HEAD\n\n\n--merge\nResets the index and updates the files in the working tree that are different between <commit> and HEAD, but keeps those which are different between the index and working tree (i.e. which have changes which have not been added).",
    "tag": "git"
  },
  {
    "question": "How do I clone all remote branches?",
    "answer": "First, clone a remote Git repository and cd into it:\n$ git clone git://example.com/myproject\n$ cd myproject\n\nNext, look at the local branches in your repository:\n$ git branch\n* master\n\nBut there are other branches hiding in your repository! See these using the -a flag:\n$ git branch -a\n* master\n  remotes/origin/HEAD\n  remotes/origin/master\n  remotes/origin/v1.0-stable\n  remotes/origin/experimental\n\nTo take a quick peek at an upstream branch, check it out directly:\n$ git checkout origin/experimental\n\nTo work on that branch, create a local tracking branch, which is done automatically by:\n$ git checkout experimental\n\nBranch experimental set up to track remote branch experimental from origin.\nSwitched to a new branch 'experimental'\n\nHere, \"new branch\" simply means that the branch is taken from the index and created locally for you.  As the previous line tells you, the branch is being set up to track the remote branch, which usually means the origin/branch_name branch.\nYour local branches should now show:\n$ git branch\n* experimental\n  master\n\nYou can track more than one remote repository using git remote:\n$ git remote add win32 git://example.com/users/joe/myproject-win32-port\n$ git branch -a\n* master\n  remotes/origin/HEAD\n  remotes/origin/master\n  remotes/origin/v1.0-stable\n  remotes/origin/experimental\n  remotes/win32/master\n  remotes/win32/new-widgets\n\nAt this point, things are getting pretty crazy, so run gitk to see what's going on:\n$ gitk --all &",
    "tag": "git"
  },
  {
    "question": "How do I update or sync a forked repository on GitHub?",
    "answer": "In your local clone of your forked repository, you can add the original GitHub repository as a \"remote\".  (\"Remotes\" are like nicknames for the URLs of repositories - origin is one, for example.)  Then you can fetch all the branches from that upstream repository, and rebase your work to continue working on the upstream version.  In terms of commands that might look like:\n# Add the remote, call it \"upstream\":\n\ngit remote add upstream https://github.com/whoever/whatever.git\n\n# Fetch all the branches of that remote into remote-tracking branches\n\ngit fetch upstream\n\n# Make sure that you're on your main branch:\n\ngit checkout main\n\n# Rewrite your main branch so that any commits of yours that\n# aren't already in upstream/main are replayed on top of that\n# other branch:\n\ngit rebase upstream/main\n\nIf you don't want to rewrite the history of your main branch, (for example because other people may have cloned it) then you should replace the last command with git merge upstream/main.  However, for making further pull requests that are as clean as possible, it's probably better to rebase.\n\nIf you've rebased your branch onto upstream/main you may need to force the push in order to push it to your own forked repository on GitHub.  You'd do that with:\ngit push -f origin main\n\nYou only need to use the -f the first time after you've rebased.",
    "tag": "git"
  },
  {
    "question": "How do I remove a submodule?",
    "answer": "In modern git (I'm writing this in 2022, with an updated git installation), this has become quite a bit simpler:\n\nRun git rm <path-to-submodule>, and commit.\n\nThis removes the filetree at <path-to-submodule>, and the submodule's entry in the .gitmodules file. I.e. all traces of the submodule in your repository proper are removed.\nAs the docs note however, the .git dir of the submodule is kept around (in the modules/ directory of the main project's .git dir), \"to make it possible to checkout past commits without requiring fetching from another repository\".\nIf you nonetheless want to remove this info, manually delete the submodule's directory in .git/modules/, and remove the submodule's entry in the file .git/config. These steps can be automated using the commands\n\nrm -rf .git/modules/<path-to-submodule>, and\ngit config --remove-section submodule.<path-to-submodule>.\n\n\n\nCaution: content below is older community wiki instructions.  Ignore if you have a modern git:\nVia the page Git Submodule Tutorial:\nTo remove a submodule you need to:\n\nDelete the relevant section from the .gitmodules file.\nStage the .gitmodules changes:git add .gitmodules\nDelete the relevant section from .git/config.\nRemove the submodule files from the working tree and index:git rm --cached path_to_submodule (no trailing slash).\nRemove the submodule's .git directory:rm -rf .git/modules/path_to_submodule\nCommit the changes:git commit -m \"Removed submodule <name>\"\nDelete the now untracked submodule files:rm -rf path_to_submodule\n\nSee also: alternative steps below.",
    "tag": "git"
  },
  {
    "question": "How do I delete a commit from a branch?",
    "answer": "Careful: git reset --hard WILL DELETE YOUR WORKING DIRECTORY CHANGES.\nBe sure to stash any local changes you want to keep before running this command.\nAssuming you are sitting on that commit, then this command will wack it...\ngit reset --hard HEAD~1\n\nThe HEAD~1 means the commit before head.\nOr, you could look at the output of git log, find the commit id of the commit you want to back up to, and then do this:\ngit reset --hard <sha1-commit-id>\n\n\nIf you already pushed it, you will need to do a force push to get rid of it...\ngit push origin HEAD --force\n\nHowever, if others may have pulled it, then you would be better off starting a new branch.  Because when they pull, it will just merge it into their work, and you will get it pushed back up again.\nIf you already pushed, it may be better to use git revert, to create a \"mirror image\" commit that will undo the changes.  However, both commits will be in the log.\n\nFYI: git reset --hard HEAD is great if you want to get rid of WORK IN PROGRESS.It will reset you back to the most recent commit, and erase all the changes in your working tree and index.\ngit stash does the same except you can restore it later if you need, versus permanently delete with reset hard mode. Check your stashes by using git stash list and git stash show 'stash@123'\n\nLastly, if you need to find a commit that you \"deleted\", it is typically present in git reflog unless you have garbage collected your repository.",
    "tag": "git"
  },
  {
    "question": "Undoing a git rebase",
    "answer": "The easiest way would be to find the head commit of the branch as it was immediately before the rebase started in the reflog...\ngit reflog\n\nand to reset the current branch to it.\nSuppose the old commit was HEAD@{2} in the ref log:\ngit reset --soft \"HEAD@{2}\"\n\n(If you do not want to retain the working copy changes, you can use --hard instead of --soft)\nYou can check the history of the candidate old head by just doing a git log \"HEAD@{2}\".\nIf you've not disabled per branch reflogs you should be able to simply do git reflog \"branchname@{1}\" as a rebase detaches the branch head before reattaching to the final head. I would double-check this behavior, though, as I haven't verified it recently.\nPer default, all reflogs are activated for non-bare repositories:\n[core]\n    logAllRefUpdates = true",
    "tag": "git"
  },
  {
    "question": "How do I get the current branch name in Git?",
    "answer": "To display only the name of the current branch you're on:\ngit rev-parse --abbrev-ref HEAD\n\nReference: Show just the current branch in Git",
    "tag": "git"
  },
  {
    "question": "Message 'src refspec master does not match any' when pushing commits in Git",
    "answer": "Maybe you just need to commit. I ran into this when I did:\nmkdir repo && cd repo\ngit init\ngit remote add origin /path/to/origin.git\ngit add .\n\nOops! Never committed!\ngit push -u origin master\nerror: src refspec master does not match any.\n\nAll I had to do was:\ngit commit -m \"initial commit\"\ngit push origin main\n\nSuccess!",
    "tag": "git"
  },
  {
    "question": "How do I clone a specific Git branch?",
    "answer": "git clone -b <branch> <remote_repo>\n\nExample:\ngit clone -b my-branch git@github.com:user/myproject.git\n\nWith Git 1.7.10 and later, add --single-branch to prevent fetching of all branches. Example, with OpenCV 2.4 branch:\ngit clone -b opencv-2.4 --single-branch https://github.com/Itseez/opencv.git",
    "tag": "git"
  },
  {
    "question": "Make an existing Git branch track a remote branch?",
    "answer": "Given a branch foo and a remote upstream:\nAs of Git 1.8.0:\ngit branch -u upstream/foo\n\nOr, if local branch foo is not the current branch:\ngit branch -u upstream/foo foo\n\nOr, if you like to type longer commands, these are equivalent to the above two:\ngit branch --set-upstream-to=upstream/foo\n\ngit branch --set-upstream-to=upstream/foo foo\n\nAs of Git 1.7.0 (before 1.8.0):\ngit branch --set-upstream foo upstream/foo\n\nNotes:\n\nAll of the above commands will cause local branch foo to track remote branch foo from remote upstream.\nThe old (1.7.x) syntax is deprecated in favor of the new (1.8+) syntax.  The new syntax is intended to be more intuitive and easier to remember.\nDefining an upstream branch will fail when run against newly-created remotes that have not already been fetched. In that case, run git fetch upstream beforehand.\n\n\nSee also: Why do I need to do `--set-upstream` all the time?",
    "tag": "git"
  },
  {
    "question": "Remove a file from a Git repository without deleting it from the local filesystem",
    "answer": "The git rm documentation states:\n\nWhen --cached is given, the staged content has to match either the tip of the branch or the file on disk, allowing the file to be removed from just the index.\n\nSo, for a single file:\ngit rm --cached file_to_remove.txt\n\nand for a single directory:\ngit rm --cached -r directory_to_remove",
    "tag": "git"
  },
  {
    "question": "Move existing, uncommitted work to a new branch in Git",
    "answer": "Update 2020 / Git 2.23\nGit 2.23 adds the new switch subcommand in an attempt to clear some of the confusion that comes from the overloaded usage of checkout (switching branches, restoring files, detaching HEAD, etc.)\nStarting with this version of Git, replace the checkout command with:\ngit switch -c <new-branch>\n\nThe behavior is identical and remains unchanged.\n\nBefore Update 2020 / Git 2.23\nUse the following:\ngit checkout -b <new-branch>\n\nThis will leave your current branch as it is, create and checkout a new branch and keep all your changes. You can then stage changes in files to commit with:\ngit add <files>\n\nand commit to your new branch with:\ngit commit -m \"<Brief description of this commit>\"\n\nThe changes in the working directory and changes staged in index do not belong to any branch yet. This changes the branch where those modifications would end in.\nYou don't reset your original branch, it stays as it is. The last commit on <old-branch> will still be the same. Therefore you checkout -b and then commit.",
    "tag": "git"
  },
  {
    "question": "View the change history of a file using Git versioning",
    "answer": "This lets Git generate the patches for each log entry:\ngit log -p -- <filename>\n\n-p in git generates patch text\nSee git help log for more options — it can actually do a lot of nice things. :)\n\nTo get just the diff for a specific commit, use\ngit show <revision> -- <filename>\n\nor specify any other revision by identifier.\n\nTo browse the changes visually:\ngitk -- <filename>",
    "tag": "git"
  },
  {
    "question": "How do I stash only one file out of multiple files that have changed?",
    "answer": "git stash push -p -m \"my commit message\"\n\n-p let's you select the hunks that should be stashed; whole files can be selected as well.\nYou'll be prompted with a few actions for each hunk:\n   y - stash this hunk\n   n - do not stash this hunk\n   q - quit; do not stash this hunk or any of the remaining ones\n   a - stash this hunk and all later hunks in the file\n   d - do not stash this hunk or any of the later hunks in the file\n   g - select a hunk to go to\n   / - search for a hunk matching the given regex\n   j - leave this hunk undecided, see next undecided hunk\n   J - leave this hunk undecided, see next hunk\n   k - leave this hunk undecided, see previous undecided hunk\n   K - leave this hunk undecided, see previous hunk\n   s - split the current hunk into smaller hunks\n   e - manually edit the current hunk\n   ? - print help",
    "tag": "git"
  },
  {
    "question": "Git refusing to merge unrelated histories on rebase",
    "answer": "You can use --allow-unrelated-histories to force the merge to happen.\nThe reason behind this is that default behavior has changed since Git 2.9:\n\n\"git merge\" used to allow merging two branches that have no common\nbase by default, which led to a brand new history of an existing\nproject created and then get pulled by an unsuspecting maintainer,\nwhich allowed an unnecessary parallel history merged into the\nexisting project. The command has been taught not to allow this by\ndefault, with an escape hatch --allow-unrelated-histories option\nto be used in a rare event that merges histories of two projects\nthat started their lives independently.\n\nSee the Git release changelog for more information.\nMore information can be found in this answer.",
    "tag": "git"
  },
  {
    "question": "How do I create a remote Git branch?",
    "answer": "First, create a new local branch and check it out:\ngit checkout -b <branch-name>\n\nThe remote branch is automatically created when you push it to the remote server:\ngit push <remote-name> <branch-name> \n\n<remote-name> is typically origin, which is the name which git gives to the remote you cloned from. Your colleagues may then simply pull that branch.\nNote however that formally, the format is:\ngit push <remote-name> <local-branch-name>:<remote-branch-name>\n\nBut when you omit one, it assumes both branch names are the same. Having said this, as a word of caution, do not make the critical mistake of specifying only :<remote-branch-name> (with the colon), or the remote branch will be deleted!\n\nSo that a subsequent git pull will know what to do, you might instead want to use:\ngit push --set-upstream <remote-name> <local-branch-name> \n\nAs described below, the --set-upstream option sets up an upstream branch:\n\nFor every branch that is up to date or\nsuccessfully pushed, add upstream\n(tracking) reference, used by\nargument-less git-pull(1) and other\ncommands.",
    "tag": "git"
  },
  {
    "question": "Commit only part of a file's changes in Git",
    "answer": "You can use:\ngit add --patch <filename>\n\nor for short:\ngit add -p <filename>\n\nGit will break down your file into what it thinks are sensible \"hunks\" (portions of the file). It will then prompt you with this question:\nStage this hunk [y,n,q,a,d,/,j,J,g,s,e,?]?\n\nHere is a description of each option:\n\ny stage this hunk for the next commit\nn do not stage this hunk for the next commit\nq quit; do not stage this hunk or any of the remaining hunks\na stage this hunk and all later hunks in the file\nd do not stage this hunk or any of the later hunks in the file\ng select a hunk to go to\n/ search for a hunk matching the given regex\nj leave this hunk undecided, see next undecided hunk\nJ leave this hunk undecided, see next hunk\nk leave this hunk undecided, see previous undecided hunk\nK leave this hunk undecided, see previous hunk\ns split the current hunk into smaller hunks\ne manually edit the current hunk\n\nYou can then edit the hunk manually by replacing +/- by # (thanks veksen)\n\n\n? print hunk help\n\nIf the file is not in the repository yet, you can first do git add -N <filename>. Afterwards you can go on with git add -p <filename>.\nAfterwards, you can use:\n\ngit diff --staged to check that you staged the correct changes\ngit reset -p to unstage mistakenly added hunks\ngit commit -v to view your commit while you edit the commit message.\n\nNote this is far different than the git format-patch command, whose purpose is to parse commit data into a .patch files.\nReference for future: Git Tools - Interactive Staging",
    "tag": "git"
  },
  {
    "question": "How do I list all the files in a commit?",
    "answer": "Preferred Way (because it's a plumbing command; meant to be programmatic):\n$ git diff-tree --no-commit-id --name-only bd61ad98 -r\nindex.html\njavascript/application.js\njavascript/ie6.js\n\nAnother Way (less preferred for scripts, because it's a porcelain command; meant to be user-facing)\n$ git show --pretty=\"\" --name-only bd61ad98    \nindex.html\njavascript/application.js\njavascript/ie6.js\n\n\n\nThe --no-commit-id suppresses the commit ID output.\nThe --pretty argument specifies an empty format string to avoid the cruft at the beginning.\nThe --name-only argument shows only the file names that were affected (Thanks Hank). Use --name-status instead, if you want to see what happened to each file (Deleted, Modified, Added)\nThe -r argument is to recurse into sub-trees",
    "tag": "git"
  },
  {
    "question": "How do you push a tag to a remote repository using Git?",
    "answer": "To push a single tag:\ngit push origin tag <tag_name>\n\nAnd the following command should push all tags (not recommended):\n# not recommended\ngit push --tags",
    "tag": "git"
  },
  {
    "question": "Git is not working after macOS update (\"xcrun: error: invalid active developer path (/Library/Developer/CommandLineTools\")",
    "answer": "The problem is that Xcode Command-line Tools needs to be updated due to a MacOs update.\n\nDid not run into this on Sonoma.\n\nMaybe Apple fixed the process?\n\n\nUpdated for Ventura\n\nAfter opening the terminal after restarting, I tried to go to my code, and do a git status, and I got an error and prompt for command line software agreement:\nSo press space until you get to the [agree, print, cancel] option, so careful hit space to scroll down to the end, if you blow past It you have to run a command to get it back. Use sudo xcodebuild -license to get to it again.\nJust be careful on scrolling down and enter agree and press return and it will launch into an update.\n\nThen I tried to use git after the install, and it prompted me to install Xcode tools again.\nI followed my own advice from previous years (see below), and went to https://developer.apple.com/download/all and downloaded\n\"Command Line Tools for Xcode 14\" (You have to log in with your Apple ID and enter MFA code, so have all the devices you need for that handy. Then select \"Command Line Tools for Xcode 14\", or if you want to get into the alphas or betas, that's up to you. But stable releases are probably the best choice for software developers.\n\nYou have to either download the tools from CLI or the developer page and before you can use git, you need to reboot!!! Or you will get stuck in a loop of prompt & downloading\nRebooting will break the loop and complete the installation of your CLI tools including git so that you can get back to work\nSolutions for previous years, these may or may not be valid these days as the downloads page has changed significantly:\nPREVIOUS YEARS SOLUTIONS, probably #2 is most helpful.\n*** Solution #1:\nGo back to your terminal and enter:\nxcode-select --install\n\nYou'll then receive the following output:\nxcode-select: note: install requested for command line developer tools\n\nYou will then be prompted in a window to update Xcode Command Line tools. (which could take a while)\nOpen a new terminal window and your development tools should be returned.\nAddition: With any major or semi-major update you'll need to update the command line tools in order to get them functioning properly again. Check Xcode with any update. This goes beyond Mojave...\nAfter that restart your terminal\nAlternatively, IF that fails, and it might.... you'll get a pop-up box saying \"Software not found on server\", proceed to solution 2.\n*** Solution #2: (Preferred method)\nIf you hit xcode-select --install and it doesn't find the software, log into Apple Developer, and install it via webpage.\nLog in or sign up here:\nhttps://developer.apple.com/download/more/\nLook for: \"Command Line Tools for Xcode 14.x\" in the list of downloads\nThen click the dmg and download. (See previous image above) either way, you will probably wind up at an apple downloads webpage.",
    "tag": "git"
  },
  {
    "question": "Difference between \"git add -A\" and \"git add .\"",
    "answer": "This answer only applies to Git version 1.x. For Git version 2.x, see other answers.\n\nSummary:\n\ngit add -A stages all changes\n\ngit add . stages new files and modifications, without deletions (on the current directory and its subdirectories).\n\ngit add -u stages modifications and deletions, without new files\n\n\n\nDetail:\ngit add -A is equivalent to  git add .; git add -u.\nThe important point about git add . is that it looks at the working tree and adds all those paths to the staged changes if they are either changed or are new and not ignored, it does not stage any 'rm' actions.\ngit add -u looks at all the already tracked files and stages the changes to those files if they are different or if they have been removed. It does not add any new files, it only stages changes to already tracked files.\ngit add -A is a handy shortcut for doing both of those.\nYou can test the differences out with something like this (note that for Git version 2.x your output for git add . git status will be different):\ngit init\necho Change me > change-me\necho Delete me > delete-me\ngit add change-me delete-me\ngit commit -m initial\n\necho OK >> change-me\nrm delete-me\necho Add me > add-me\n\ngit status\n# Changed but not updated:\n#   modified:   change-me\n#   deleted:    delete-me\n# Untracked files:\n#   add-me\n\ngit add .\ngit status\n\n# Changes to be committed:\n#   new file:   add-me\n#   modified:   change-me\n# Changed but not updated:\n#   deleted:    delete-me\n\ngit reset\n\ngit add -u\ngit status\n\n# Changes to be committed:\n#   modified:   change-me\n#   deleted:    delete-me\n# Untracked files:\n#   add-me\n\ngit reset\n\ngit add -A\ngit status\n\n# Changes to be committed:\n#   new file:   add-me\n#   modified:   change-me\n#   deleted:    delete-me",
    "tag": "git"
  },
  {
    "question": "How do I make git use the editor of my choice for editing commit messages?",
    "answer": "Setting the default editor for Git\nPick one:\n\nSet core.editor in your Git config:\ngit config --global core.editor \"vim\"\n\n\nSet the GIT_EDITOR environment variable:\nexport GIT_EDITOR=vim\n\n\n\n\nSetting the default editor for all programs\nSet the standardized VISUAL and EDITOR environment variables*:\nexport VISUAL=vim\nexport EDITOR=\"$VISUAL\"\n\nNOTE: Setting both is not necessarily needed, but some programs may not use the more-correct VISUAL. See VISUAL vs. EDITOR.\n\nFixing compatibility issues\nSome editors require a --wait flag, or they will open a blank page. For example:\n\nSublime Text (if correctly set up; or use the full path to the executable in place of subl):\nexport VISUAL=\"subl --wait\"\n\n\nVS Code (after adding the shell command):\nexport VISUAL=\"code --wait\"",
    "tag": "git"
  },
  {
    "question": "What does cherry-picking a commit with Git mean?",
    "answer": "Cherry-picking in Git means choosing a commit from one branch and applying it to another.\nThis contrasts with other ways such as merge and rebase which normally apply many commits to another branch.\nIt's also possible to cherry-pick multiple commits but merge is the preferred way over cherry-picking.\n\nMake sure you are on the branch you want to apply the commit to.\ngit switch master\n\n\nExecute the following:\ngit cherry-pick <commit-hash>\n\n\n\nN.B.:\n\nIf you cherry-pick from a public branch, you should consider using\ngit cherry-pick -x <commit-hash>\n\nThis will generate a standardized commit message. This way, you (and your co-workers) can still keep track of the origin of the commit and may avoid merge conflicts in the future.\n\nIf you have notes attached to the commit they do not follow the cherry-pick. To bring them over as well, You have to use:\ngit notes copy <from> <to>\n\n\n\nAdditional links:\n\ngit official guide page\ngit cherry-pick guide",
    "tag": "git"
  },
  {
    "question": "How do I modify a specific commit?",
    "answer": "Use git rebase. For example, to modify commit bbc643cd, run:\ngit rebase --interactive bbc643cd~\n\nPlease note the tilde ~ at the end of the command, because you need to reapply commits on top of the previous commit of bbc643cd (i.e. bbc643cd~).\nIn the default editor, modify pick to edit in the line mentioning bbc643cd.\nSave the file and exit. git will interpret and automatically execute the commands in the file. You will find yourself in the previous situation in which you just had created commit bbc643cd.\nAt this point, bbc643cd is your last commit and you can easily amend it. Make your changes and then commit them with the command:\ngit commit --all --amend --no-edit\n\nAfter that, return back to the previous HEAD commit using:\ngit rebase --continue\n\nWARNING: Note that this will change the SHA-1 of that commit as well as all children -- in other words, this rewrites the history from that point forward. You can break repos doing this if you push using the command git push --force.",
    "tag": "git"
  },
  {
    "question": "How can I see the differences between two branches?",
    "answer": "Use git diff.\ngit diff [<options>] <commit>..​<commit> [--] [<path>…​]\n\n<commit> is a branch name, a commit hash, or a shorthand symbolic reference.\nExamples:\ngit diff abc123..def567\ngit diff HEAD..origin/master\nThat will produce the diff between the tips of the two branches. If you'd prefer to find the changes that occurred in the common ancestor since the branch was started off, you can use three dots instead of two:\ngit diff <commit>...<commit>\n\nTo check which files differ, not how the content differs, use --name-only:\ngit diff --name-only <commit>..​<commit>\n\nNote that in the <commit>..<commit> (two dot) syntax, the dots are optional; the following is synonymous:\ngit diff commit1 commit2",
    "tag": "git"
  },
  {
    "question": "How can I change the commit author for a single commit?",
    "answer": "Interactive rebase off of a point earlier in the history than the commit you need to modify (git rebase -i <earliercommit>). In the list of commits being rebased, change the text from pick to edit next to the hash of the one you want to modify. Then when git prompts you to change the commit, use this:\ngit commit --amend --author=\"Author Name <email@address.com>\" --no-edit\n\n\nFor example, if your commit history is A-B-C-D-E-F with F as HEAD, and you want to change the author of C and D, then you would...\n\nSpecify git rebase -i B (here is an example of what you will see after executing the git rebase -i B command)\n\n\nif you need to edit A, use git rebase -i --root\n\nChange the lines for both C and D from pick to edit\nExit the editor (for vim, this would be pressing Esc and then typing :wq).\nOnce the rebase started, it would first pause at C\nYou would git commit --amend --author=\"Author Name <email@address.com>\"\nThen git rebase --continue\nIt would pause again at D\nThen you would git commit --amend --author=\"Author Name <email@address.com>\" again\ngit rebase --continue\nThe rebase would complete.\nUse git push -f to update your origin with the updated commits.",
    "tag": "git"
  },
  {
    "question": "I ran into a merge conflict. How do I abort the merge?",
    "answer": "Since your pull was unsuccessful then HEAD (not HEAD^) is the last \"valid\" commit on your branch:\ngit reset --hard HEAD\n\nThe other piece you want is to let their changes over-ride your changes.  \nOlder versions of git allowed you to use the \"theirs\" merge strategy:\ngit pull --strategy=theirs remote_branch\n\nBut this has since been removed, as explained in this message by Junio Hamano (the Git maintainer).  As noted in the link, instead you would do this:\ngit fetch origin\ngit reset --hard origin",
    "tag": "git"
  },
  {
    "question": "How do I find and restore a deleted file in a Git repository?",
    "answer": "Find the last commit that affected the given path. As the file isn't in the HEAD commit, that previous commit must have deleted it.\ngit rev-list -n 1 HEAD -- <file_path>\n\nThen checkout the version at the commit before, using the caret (^) symbol:\ngit checkout <deleting_commit>^ -- <file_path>\n\nOr in one command, if $file is the file in question.\ngit checkout $(git rev-list -n 1 HEAD -- \"$file\")^ -- \"$file\"\n\n\nIf you are using zsh and have the EXTENDED_GLOB option enabled, the caret symbol won't work. You can use ~1 instead.\ngit checkout $(git rev-list -n 1 HEAD -- \"$file\")~1 -- \"$file\"",
    "tag": "git"
  },
  {
    "question": "`git fetch` a remote branch",
    "answer": "Update: Using Git Switch\nAll of the information written below was accurate, but a new command, git switch has been added that simplifies the effort.\nIf daves_branch exists on the remote repository, but not on your local branch, you can simply type:\ngit switch daves_branch\n\nSince you do not have the branch locally, this will automatically make switch look on the remote repo.  It will then also automatically set up remote branch tracking.\nNote that if daves_branch doesn't exist locally you'll need to git fetch first before using switch.\n\nOriginal Post\nYou need to create a local branch that tracks a remote branch. The following command will create a local branch named daves_branch, tracking the remote branch origin/daves_branch. When you push your changes the remote branch will be updated.\nFor most recent versions of Git:\ngit checkout --track origin/daves_branch\n\n--track is shorthand for git checkout -b [branch] [remotename]/[branch] where [remotename] is origin in this case and [branch] is twice the same, daves_branch in this case.\nFor Git 1.5.6.5 you needed this:\ngit checkout --track -b daves_branch origin/daves_branch\n\nFor Git 1.7.2.3 and higher, this is enough (it might have started earlier, but this is the earliest confirmation I could find quickly):\ngit checkout daves_branch\n\nNote that with recent Git versions, this command will not create a local branch and will put you in a 'detached HEAD' state. If you want a local branch, use the --track option.\nFull details are here: 3.5 Git Branching - Remote Branches, Tracking Branches",
    "tag": "git"
  },
  {
    "question": "How do I change the author and committer name/email for multiple commits?",
    "answer": "NOTE: This answer changes SHA1s, so take care when using it on a branch that has already been pushed. If you only want to fix the spelling of a name or update an old email, Git lets you do this without rewriting history using .mailmap. See my other answer.\nUsing Rebase\nFirst, if you haven't already done so, you will likely want to fix your name in git-config:\ngit config --global user.name \"New Author Name\"\ngit config --global user.email \"<email@address.example>\"\n\nThis is optional, but it will also make sure to reset the committer name, too, assuming that's what you need.\nTo rewrite metadata for a range of commits using a rebase, do\ngit rebase -r <some commit before all of your bad commits> \\\n    --exec 'git commit --amend --no-edit --reset-author'\n\n--exec will run the git commit step after each commit is rewritten (as if you ran git commit && git rebase --continue repeatedly).\nIf you also want to change your first commit (also called the 'root' commit), you will have to add --root to the rebase call.\nThis will change both the committer and the author to your user.name/user.email configuration. If you did not want to change that config, you can use --author \"New Author Name <email@address.example>\" instead of --reset-author. Note that doing so will not update the committer -- just the author.\nSingle Commit\nIf you just want to change the most recent commit, a rebase is not necessary. Just amend the commit:\ngit commit --amend --no-edit --reset-author\n\nEntire project history\ngit rebase -r --root --exec \"git commit --amend --no-edit --reset-author\"\n\nFor older Git clients (pre-July 2020)\n-r,--rebase-merges may not exist for you. As a replacement, you can use -p. Note that -p has serious issues and is now deprecated.",
    "tag": "git"
  },
  {
    "question": "How do I clone a Git repository into a specific folder?",
    "answer": "Option A:\ngit clone git@github.com:whatever folder-name\n\nErgo, for right here use:\ngit clone git@github.com:whatever .\n\nOption B:\nMove the .git folder, too. Note that the .git folder is hidden in most graphical file explorers, so be sure to show hidden files.\nmv /where/it/is/right/now/* /where/I/want/it/\nmv /where/it/is/right/now/.* /where/I/want/it/\n\nThe first line grabs all normal files, the second line grabs dot-files. It is also possibe to do it in one line by enabling dotglob (i.e. shopt -s dotglob) but that is probably a bad solution if you are asking the question this answer answers.\nBetter yet:\nKeep your working copy somewhere else, and create a symbolic link. Like this:\nln -s /where/it/is/right/now /the/path/I/want/to/use\n\nFor your case this would be something like:\nln -sfn /opt/projectA/prod/public /httpdocs/public\n\nWhich easily could be changed to test if you wanted it, i.e.:\nln -sfn /opt/projectA/test/public /httpdocs/public\n\nwithout moving files around. Added -fn in case someone is copying these lines (-f is force,  -n avoid some often unwanted interactions with already and non-existing links).\nIf you just want it to work, use Option A, if someone else is going to look at what you have done, use Option C.",
    "tag": "git"
  },
  {
    "question": "How do I make Git ignore file mode (chmod) changes?",
    "answer": "Try:\ngit config core.fileMode false\n\nFrom git-config(1):\n\ncore.fileMode\n    Tells Git if the executable bit of files in the working tree\n    is to be honored.\n\n    Some filesystems lose the executable bit when a file that is\n    marked as executable is checked out, or checks out a\n    non-executable file with executable bit on. git-clone(1)\n    or git-init(1) probe the filesystem to see if it handles the \n    executable bit correctly and this variable is automatically\n    set as necessary.\n\n    A repository, however, may be on a filesystem that handles\n    the filemode correctly, and this variable is set to true when\n    created, but later may be made accessible from another\n    environment that loses the filemode (e.g. exporting ext4\n    via CIFS mount, visiting a Cygwin created repository with Git\n    for Windows or Eclipse). In such a case it may be necessary\n    to set this variable to false. See git-update-index(1).\n\n    The default is true (when core.filemode is not specified\n    in the config file).\n\n\nThe -c flag can be used to set this option for one-off commands:\ngit -c core.fileMode=false diff\n\nTyping the -c core.fileMode=false can be bothersome and so you can set this flag for all git repos or just for one git repo:\n# this will set your the flag for your user for all git repos (modifies `$HOME/.gitconfig`)\n# WARNING: this will be override by local config, fileMode value is automatically selected with latest version of git.\n# This mean that if git detect your current filesystem is compatible it will set local core.fileMode to true when you clone or init a repository.\n# Tool like cygwin emulation will be detected as compatible and so your local setting WILL BE SET to true no matter what you set in global setting.\ngit config --global core.fileMode false\n\n# this will set the flag for one git repo (modifies `$current_git_repo/.git/config`)\ngit config core.fileMode false\n\nAdditionally, git clone and git init explicitly set core.fileMode to true in the repo config as discussed in Git global core.fileMode false overridden locally on clone\nWarning\ncore.fileMode is not the best practice and should be used carefully. This setting only covers the executable bit of mode and never the read/write bits. In many cases you think you need this setting because you did something like chmod -R 777, making all your files executable. But in most projects most files don't need and should not be executable for security reasons.\nThe proper way to solve this kind of situation is to handle folder and file permission separately, with something like:\nfind . -type d -exec chmod a+rwx {} \\; # Make folders traversable and read/write\nfind . -type f -exec chmod a+rw {} \\;  # Make files read/write\n\nIf you do that, you'll never need to use core.fileMode, except in very rare environment.",
    "tag": "git"
  },
  {
    "question": "Ignore files that have already been committed to a Git repository",
    "answer": "To untrack a single file that has already been added/initialized to your repository, i.e., stop tracking the file but not delete it from your system use: git rm --cached filename\nTo untrack every file that is now in your .gitignore:\nFirst commit any outstanding code changes, and then, run this command:\ngit rm -r --cached .\n\nThis removes any changed files from the index(staging area), then just run:\ngit add .\n\nCommit it:\ngit commit -m \".gitignore is now working\"\n\n\nTo undo git rm --cached filename, use git add filename.\n\nMake sure to commit all your important changes before running git add .\nOtherwise, you will lose any changes to other files.\n\n\nPlease be careful, when you push this to a repository and pull from somewhere else into a state where those files are still tracked, the files will be DELETED\n\n\nTo remove all files that do not match the paths listed in some path spec file (something that has the same format as a .gitignore file):\ngit rm --cached --ignore-unmatch -r --pathspec-from-file .ignorelist \n\nNote: unlike .gitignore, which can have new lines, .ignorelist cannot have new lines because git will complain about empty patterns not being valid path spec items.\nExplanation:\n\n-r: to allow recursive removal when directories are involved\n--pathspec-from-file: path to file containing the path specifications for files to be removed\n--ignore-unmatch: tell git to ignore any path specifications that do not have a matching cache hit (otherwise it will abort with an error on that path spec)",
    "tag": "git"
  },
  {
    "question": "See what's in a stash without applying it",
    "answer": "From man git-stash (which can also be obtained via git help stash):\n\nThe modifications stashed away by this command can be listed with git stash list, inspected with git stash show, and ...\n\nshow [<stash>]\n    Show the changes recorded in the stash as a diff between the stashed\n    state and its original parent. When no <stash> is given, shows the\n    latest one. By default, the command shows the diffstat, but it will\n    accept any format known to git diff (e.g., git stash show -p stash@{1}\n    to view the second most recent stash in patch form).\n\nNote: the -p option generates a patch, as per git-diff documentation.\nList the stashes:\ngit stash list\n\nShow the files in the most recent stash:\ngit stash show\n\nShow the changes of the most recent stash:\ngit stash show -p\n\nShow the changes of the named stash:\ngit stash show -p stash@{1}\n\nOr in short:\ngit stash show -p 1 \n\nIf you want to view changes of only the last stash:\ngit stash show -p 0",
    "tag": "git"
  },
  {
    "question": "How do I get the hash for the current commit in Git?",
    "answer": "To turn any extended object reference into a hash, use git-rev-parse:\ngit rev-parse HEAD\n\nor\ngit rev-parse --verify HEAD\n\nAs noted by Alexander's answer, you can also retrieve the short hash:\ngit rev-parse --short HEAD\n\nTo turn references (e.g. branches and tags) into hashes, use git show-ref and git for-each-ref.",
    "tag": "git"
  },
  {
    "question": "How can I save username and password in Git?",
    "answer": "Attention:\nThis method saves the credentials in plaintext on your PC's disk. Everyone on your computer can access it, e.g. malicious NPM modules.\nRun:\ngit config --global credential.helper store\n\nthen:\ngit pull\n\nprovide a username and password and those details will then be remembered later. The credentials are stored in a file on the disk, with the disk permissions of \"just user readable/writable\" but still in plaintext.\nIf you want to change the password later:\ngit pull\n\nWill fail, because the password is incorrect, git then removes the offending user+password from the ~/.git-credentials file, so now re-run:\ngit pull\n\nto provide a new password so it works as earlier.",
    "tag": "git"
  },
  {
    "question": "How do I \"git clone\" a repo, including its submodules?",
    "answer": "With version 2.13 of Git and later, --recurse-submodules can be used instead of --recursive:\ngit clone --recurse-submodules -j8 git://github.com/foo/bar.git\ncd bar\n\nEditor’s note: -j8 is an optional performance optimization that became available in version 2.8, and fetches up to 8 submodules at a time in parallel — see man git-clone.\nWith version 1.9 of Git up until version 2.12 (-j flag only available in version 2.8+):\ngit clone --recursive -j8 git://github.com/foo/bar.git\ncd bar\n\nWith version 1.6.5 of Git and later, you can use:\ngit clone --recursive git://github.com/foo/bar.git\ncd bar\n\nFor already cloned repos, or older Git versions, use:\ngit clone git://github.com/foo/bar.git\ncd bar\ngit submodule update --init --recursive",
    "tag": "git"
  },
  {
    "question": "How to list only the names of files that changed between two commits",
    "answer": "git diff --name-only SHA1 SHA2\n\nwhere you only need to include enough of the SHA hash to identify the commits. The order of the SHAs does not matter. The output (which includes the relative path, not just the file name) follows this format:\n dir 1/dir 2/filename.ext\n dir 3/dir 4/other filename.ext\n\nYou can also do, for example\ngit diff --name-only HEAD~10 HEAD~5\n\nto see the differences between the tenth latest commit and the fifth latest (or so).",
    "tag": "git"
  },
  {
    "question": "Pull latest changes for all git submodules",
    "answer": "If it's the first time you check-out a repo you need to use --init first:\ngit submodule update --init --recursive\n\nFor git 1.8.2 or above, the option --remote was added to support updating to latest tips of remote branches:\ngit submodule update --recursive --remote\n\nThis has the added benefit of respecting any \"non default\" branches specified in the .gitmodules or .git/config files (if you happen to have any, default is origin/master, in which case some of the other answers here would work as well).\nFor git 1.7.3 or above you can use (but the below gotchas around what update does still apply):\ngit submodule update --recursive\n\nor:\ngit pull --recurse-submodules\n\nif you want to pull your submodules to latest commits instead of the current commit the repo points to.\nSee git-submodule(1) for details",
    "tag": "git"
  },
  {
    "question": "How do I delete all Git branches which have been merged?",
    "answer": "NOTE: You can add other branches to exclude like master and dev if your workflow has those as a possible ancestor. Usually I branch off of a \"sprint-start\" tag and master, dev and qa are not ancestors.\n\nFirst, list locally-tracking branches that were merged in remote (consider using -r flag to list all remote-tracking branches).\ngit branch --merged\n\nYou might see a few branches you don't want to remove. We can add arguments to skip important branches that we don't want to delete like master or a develop. The following command will skip the master/main branch and anything that has 'dev' in it.\ngit branch --merged | grep -Ev \"(^\\*|^\\+|master|main|dev)\"\n\nThe first part (^\\*|^+) excludes the current branch and any branch checked out in another worktree.\nIf you want to skip a branch, you can add it to the grep command as below. The branch skip_branch_name will not be deleted.\ngit branch --merged | grep -Ev \"(^\\*|^\\+|master|main|dev|skip_branch_name)\"\n\nTo delete all local branches that are already merged into the currently checked out branch:\ngit branch --merged | grep -Ev \"(^\\*|^\\+|master|main|dev)\" | xargs --no-run-if-empty git branch -d\n\nYou can see that master and dev are excluded in case they are an ancestor.\n\nYou can delete a merged local branch with:\ngit branch -d branchname\n\nTo force deletion of an unmerged branch, use:\ngit branch -D branchname\n\nTo delete it from the remote use:\ngit push --delete origin branchname\n\ngit push origin :branchname    # for really old git\n\nOnce you delete the branch from the remote, you can prune to get rid of remote tracking branches with:\ngit remote prune origin\n\nor prune individual remote tracking branches, as the other answer suggests, with:\ngit branch -dr branchname",
    "tag": "git"
  },
  {
    "question": "Make .gitignore ignore everything except a few files",
    "answer": "An optional prefix ! which negates the pattern; any matching file excluded by\na previous pattern will become included again. If a negated pattern matches,\nthis will override lower precedence patterns sources.\n\n# Ignore everything\n*\n\n# But not these files...\n!.gitignore\n!script.pl\n!template.latex\n# etc...\n\n# ...even if they are in subdirectories\n!*/",
    "tag": "git"
  },
  {
    "question": "How do I revert all local changes in Git managed project to previous state?",
    "answer": "To revert changes made to your working copy, do this:\ngit checkout .\n\nOr equivalently, for git version >= 2.23:\ngit restore .\n\nTo revert changes made to the index (i.e., that you have added), do this. Warning this will reset all of your unpushed commits to master!:\ngit reset\n\nTo revert a change that you have committed:\ngit revert <commit 1> <commit 2>\n\nTo remove untracked files (e.g., new files, generated files):\ngit clean -f\n\nOr untracked directories (e.g., new or automatically generated directories):\ngit clean -fd",
    "tag": "git"
  },
  {
    "question": "How to branch from a previous commit",
    "answer": "Create the branch using a commit hash:\ngit branch branch_name <commit-hash>\n\nOr by using a symbolic reference:\ngit branch branch_name HEAD~3\n\nTo checkout the branch while creating it, use:\ngit checkout -b branch_name <commit-hash or HEAD~3>",
    "tag": "git"
  },
  {
    "question": "How do I safely merge a Git branch into master?",
    "answer": "How I would do this\ngit checkout master\ngit pull origin master\ngit merge test\ngit push origin master\n\nIf I have a local branch from a remote one, I don't feel comfortable with merging other branches than this one with the remote. Also I would not push my changes, until I'm happy with what I want to push and also I wouldn't push things at all, that are only for me and my local repository. In your description it seems, that test is only for you? So no reason to publish it.\ngit always tries to respect yours and others changes, and so will --rebase. I don't think I can explain it appropriately, so have a look at the Git book - Rebasing or git-ready: Intro into rebasing for a little description. It's a quite cool feature",
    "tag": "git"
  },
  {
    "question": "What are the differences between .gitignore and .gitkeep?",
    "answer": ".gitkeep isn’t documented, because it’s not a feature of Git.\nGit cannot add a completely empty directory. People who want to track empty directories in Git have created the convention of putting files called .gitkeep in these directories. The file could be called anything; Git assigns no special significance to this name.\nThere is a competing convention of adding a .gitignore file to the empty directories to get them tracked, but some people see this as confusing since the goal is to keep the empty directories, not ignore them; .gitignore is also used to list files that should be ignored by Git when looking for untracked files.",
    "tag": "git"
  },
  {
    "question": "How do I show the changes which have been staged?",
    "answer": "It should just be:\ngit diff --cached\n\n--cached means show the changes in the cache/index (i.e. staged changes) against the current HEAD. --staged is a synonym for --cached.\n--staged and --cached does not point to HEAD, just difference with respect to HEAD. If you cherry pick what to commit using git add --patch (or git add -p), --staged will return what is staged.",
    "tag": "git"
  },
  {
    "question": "How do I recover a dropped stash in Git?",
    "answer": "Once you know the hash of the stash commit you dropped, you can apply it as a stash:\ngit stash apply $stash_hash\n\nOr, you can create a separate branch for it with\ngit branch recovered $stash_hash\n\nAfter that, you can do whatever you want with all the normal tools. When you’re done, just blow the branch away.\nFinding the hash\nIf you have only just popped it and the terminal is still open, you will still have the hash value printed by git stash pop on screen (thanks, Dolda).\nOtherwise, you can find this way in Linux, Unix or Git Bash for Windows:\ngit fsck --no-reflog | awk '/dangling commit/ {print $NF}'\n\n… or in PowerShell for Windows:\ngit fsck --no-reflog | select-string 'dangling commit' | foreach { $_.ToString().Split(\" \")[-1] }\n\nThis will show you all the commits at the tips of your commit graph which are no longer referenced from any branch or tag – every lost commit, including every stash commit you’ve ever created, will be somewhere in that graph.\nThe easiest way to find the stash commit you want is probably to pass that list straight to gitk:\ngitk --all $( git fsck --no-reflog | awk '/dangling commit/ {print $NF}' )\n\n… or in PowerShell for Windows:\ngitk --all $( git fsck --no-reflog | select-string 'dangling commit' | foreach { $_.ToString().Split(\" \")[-1] } )\n\nThis will launch a repository browser showing you every single commit in the repository ever, regardless of whether it is reachable or not.\nYou can replace gitk there with something like git log --graph --oneline --decorate if you prefer a nice graph on the console over a separate GUI app.\nTo spot stash commits, look for commit messages of this form:\n        WIP on somebranch: commithash Some old commit message\nNote: The commit message will only be in this form (starting with \"WIP on\") if you did not supply a message when you did git stash.",
    "tag": "git"
  },
  {
    "question": "How do I delete a file from a Git repository?",
    "answer": "Use git rm.\nIf you want to remove the file from the Git repository and the filesystem, use:\ngit rm file1.txt\ngit commit -m \"remove file1.txt\"\n\nBut if you want to remove the file only from the Git repository and not remove it from the filesystem, use:  \ngit rm --cached file1.txt\ngit commit -m \"remove file1.txt\"\n\nAnd to push changes to remote repo\ngit push origin branch_name",
    "tag": "git"
  },
  {
    "question": "Do a \"git export\" (like \"svn export\")?",
    "answer": "Probably the simplest way to achieve this is with git archive. If you really need just the expanded tree you can do something like this.\ngit archive master | tar -x -C /somewhere/else\n\nMost of the time that I need to 'export' something from git, I want a compressed archive in any case so I do something like this.\ngit archive master | bzip2 >source-tree.tar.bz2\n\nZIP archive:\ngit archive --format zip --output /full/path/to/zipfile.zip master \n\ngit help archive for more details, it's quite flexible.\n\nBe aware that even though the archive will not contain the .git directory, it will, however, contain other hidden git-specific files like .gitignore, .gitattributes, etc. If you don't want them in the archive, make sure you use the export-ignore attribute in a .gitattributes file and commit this before doing your archive. Read more...\n\nNote: If you are interested in exporting the index, the command is\ngit checkout-index -a -f --prefix=/destination/path/\n\n(See Greg's answer for more details)\n\nHere's a real-world example using libchrony on Linux:\nmkdir $HOME/dev\ncd $HOME/dev\npushd /tmp\ngit clone https://gitlab.com/chrony/libchrony.git\ncd libchrony\nBRANCH=$(git rev-parse --abbrev-ref HEAD)\ngit archive -o ../libchrony.zip --prefix=\"libchrony/\" $BRANCH\npopd\nunzip /tmp/libchrony.zip\n\nThose commands produce a zip file and extract it into $HOME/dev/libchrony. We can peek into the archive using:\n$ unzip -v /tmp/libchrony\nArchive:  /tmp/libchrony.zip\ne0a3807f770b56f6b0e9833254baa7c4fc13564b\n Length   Method    Size  Cmpr    Date    Time   CRC-32   Name\n--------  ------  ------- ---- ---------- ----- --------  ----\n       0  Stored        0   0% 2023-07-20 09:37 00000000  libchrony/\n      49  Defl:N       47   4% 2023-07-20 09:37 37c3f2e2  libchrony/.gitignore\n   26530  Defl:N     9350  65% 2023-07-20 09:37 5622583e  libchrony/COPYING\n     961  Defl:N      467  51% 2023-07-20 09:37 da9221e3  libchrony/Makefile\n     475  Defl:N      304  36% 2023-07-20 09:37 cae27f70  libchrony/README.adoc\n    3313  Defl:N     1119  66% 2023-07-20 09:37 37eb110f  libchrony/chrony.h\n    7673  Defl:N     2261  71% 2023-07-20 09:37 5d455a52  libchrony/client.c\n    6190  Defl:N     2093  66% 2023-07-20 09:37 7ea9d81b  libchrony/example-reports.c\n   16348  Defl:N     3855  76% 2023-07-20 09:37 e82f5fe3  libchrony/message.c\n    2946  Defl:N     1099  63% 2023-07-20 09:37 945ee82b  libchrony/message.h\n--------          -------  ---                            -------\n   64485            20595  68%                            10 files",
    "tag": "git"
  },
  {
    "question": "How do I name and retrieve a Git stash by name?",
    "answer": "To save a stash with a message:\ngit stash push -m \"my_stash_name\"\n\nAlternatively (deprecated since v2.16):\ngit stash save \"my_stash_name\"\n\n\nTo list stashes:\ngit stash list\n\nAll the stashes are stored in a stack.\n\nTo pop (i.e. apply and drop) the nth stash:\ngit stash pop stash@{n}\n\nTo pop (i.e. apply and drop) a stash by name is not possible with git stash pop (see footnote-1).\n\nTo apply the nth stash:\ngit stash apply stash@{n}\n\nTo apply a stash by name:\ngit stash apply stash^{/my_stash_name}\n\n\nfootnote-1:\n\nSee the man git-stash section regarding apply:\n\nUnlike pop,  may be any commit that looks like a commit created by stash push or stash create.\n\n\nPossible workaround (tested on git version 2.27 and 2.31):\ngit stash pop $(git stash list --pretty='%gd %s'|grep \"my_stash_name\"|head -1|gawk '{print $1}')",
    "tag": "git"
  },
  {
    "question": "How can I git stash a specific file?",
    "answer": "Since git 2.13, there is a command to save a specific path to the stash: git stash push <path>. For example:\ngit stash push -m welcome_cart app/views/cart/welcome.thtml\n\n\nWith earlier versions:\nYou can do that using git stash --patch (or git stash -p) -- you'll enter interactive mode where you'll be presented with each hunk that was changed. Use n to skip the files that you don't want to stash, y when you encounter the one that you want to stash, and q to quit and leave the remaining hunks unstashed.  a will stash the shown hunk and the rest of the hunks in that file.\nNot the most user-friendly approach, but it gets the work done if you really need it.",
    "tag": "git"
  },
  {
    "question": "Do I commit the package-lock.json file created by npm 5?",
    "answer": "Yes, package-lock.json is intended to be checked into source control. If you're using npm 5+, you may see this notice on the command line: created a lockfile as package-lock.json. You should commit this file. According to npm help package-lock.json:\n\npackage-lock.json is automatically generated for any operations where npm\nmodifies either the node_modules tree, or package.json. It describes the\nexact tree that was generated, such that subsequent installs are able to\ngenerate identical trees, regardless of intermediate dependency updates.\nThis file is intended to be committed into source repositories, and serves\nvarious purposes:\n\nDescribe a single representation of a dependency tree such that teammates, deployments, and continuous integration are guaranteed to install exactly the same dependencies.\n\nProvide a facility for users to \"time-travel\" to previous states of node_modules without having to commit the directory itself.\n\nTo facilitate greater visibility of tree changes through readable source control diffs.\n\nAnd optimize the installation process by allowing npm to skip repeated metadata resolutions for previously-installed packages.\n\n\nOne key detail about package-lock.json is that it cannot be published, and it\nwill be ignored if found in any place other than the toplevel package. It shares\na format with npm-shrinkwrap.json, which is essentially the same file, but\nallows publication. This is not recommended unless deploying a CLI tool or\notherwise using the publication process for producing production packages.\nIf both package-lock.json and npm-shrinkwrap.json are present in the root of\na package, package-lock.json will be completely ignored.",
    "tag": "git"
  },
  {
    "question": "When do you use Git rebase instead of Git merge?",
    "answer": "Short Version\n\nMerge takes all the changes in one branch and merges them into another branch in one commit.\nRebase says I want the point at which I branched to move to a new starting point\n\nSo when do you use either one?\nMerge\n\nLet's say you have created a branch for the purpose of developing a single feature.  When you want to bring those changes back to master, you probably want merge.\n\nRebase\n\nA second scenario would be if you started doing some development and then another developer made an unrelated change. You probably want to pull and then rebase to base your changes from the current version from the repository.\n\nSquashing: All commits are preserved in both cases (for example: \"add feature\", then \"typo\", then \"oops typo again\"...). Commits can be combined into a single commits by squashing. Squashing can be done as part of a merge or rebase operation (--squash flag), in which case it's often called a squash-merge or a squash-rebase.\nPull Requests: Popular git servers (Bitbucket, GitLab, GitHub, etc...) allow to configure how pull requests are merged on a per-repo basis. the UI may show a \"Merge\" button by convention but the button can do any operations with any flags (keywords: merge, rebase, squash, fast-forward).",
    "tag": "git"
  },
  {
    "question": "How do you stash an untracked file?",
    "answer": "To stash your working directory including untracked files (especially those that are in the .gitignore) then you probably want to use this cmd:\ngit stash --include-untracked\n\nAlternatively, you can use the shorthand -u instead of --include-untracked, or simply git stash --all (see warning below for this one) which stashes all files, including untracked and ignored files. This behaviour changed in 2018, so make sure your git is up to date.\n\nWarning: there seems to be (or have been) situations in which contents of ignored directories could be deleted permanently. See this archived website for more information.",
    "tag": "git"
  },
  {
    "question": "Throw away local commits in Git",
    "answer": "If your excess commits are only visible to you, you can just do \ngit reset --hard origin/<branch_name> \nto move back to where the origin is. This will reset the state of the repository to the previous commit, and it will discard all local changes.\nDoing a git revert makes new commits to remove old commits in a way that keeps everyone's history sane.",
    "tag": "git"
  },
  {
    "question": "How do I fetch all Git branches?",
    "answer": "TL;DR answer\ngit branch -r \\\n  | grep -v '\\->' \\\n  | sed \"s,\\x1B\\[[0-9;]*[a-zA-Z],,g\" \\\n  | while read remote; do \\\n      git branch --track \"${remote#origin/}\" \"$remote\"; \\\n    done\ngit fetch --all\ngit pull --all\n\n(grep -v matches the inverse of given string; sed removes control sequences: \\x1B matches esc)\n(It seems that pull fetches all branches from all remotes, but I always fetch first just to be sure.)\nRun the first command only if there are remote branches on the server that aren't tracked by your local branches.\nComplete answer\nYou can fetch all branches from all remotes like this:\ngit fetch --all\n\nIt's basically a power move.\nfetch updates local copies of remote branches so this is always safe for your local branches BUT:\n\nfetch will not update local branches (which track remote branches); if you want to update your local branches you still need to pull every branch.\n\nfetch will not create local branches (which track remote branches), you have to do this manually. If you want to list all remote branches:\ngit branch -a\n\n\nTo update local branches which track remote branches:\ngit pull --all\n\nHowever, this can be still insufficient. It will work only for your local branches which track remote branches. To track all remote branches execute this oneliner BEFORE git pull --all:\ngit branch -r | grep -v '\\->' | sed \"s,\\x1B\\[[0-9;]*[a-zA-Z],,g\" | while read remote; do git branch --track \"${remote#origin/}\" \"$remote\"; done\n\nP.S. AFAIK git fetch --all and git remote update are equivalent.\n\n\nKamil Szot's comment, which folks have found useful.\n\nI had to use:\nfor remote in `git branch -r`; do git branch --track ${remote#origin/} $remote; done\n\nbecause your code created local branches named origin/branchname and\nI was getting \"refname 'origin/branchname' is ambiguous whenever I\nreferred to it.",
    "tag": "git"
  },
  {
    "question": "How can I see the changes in a Git commit?",
    "answer": "To see the diff for a particular COMMIT hash, where COMMIT is the hash of the commit:\ngit diff COMMIT~ COMMIT will show you the difference between that COMMIT's ancestor and the COMMIT. See the man pages for git diff for details about the command and gitrevisions about the ~ notation and its friends.\nAlternatively, git show COMMIT will do something very similar. (The commit's data, including its diff - but not for merge commits.) See the git show manpage.\n(also git diff COMMIT will show you the difference between that COMMIT and the head.)",
    "tag": "git"
  },
  {
    "question": "Showing which files have changed between two revisions",
    "answer": "To compare the current branch against main branch:\n$ git diff --name-status main\n\nTo compare any two branches:\n$ git diff --name-status firstbranch..yourBranchName\n\nThere are more options to git diff in the official documentation (and specifically the --name-status option).",
    "tag": "git"
  },
  {
    "question": "How do I get the directory where a Bash script is located from within the script itself?",
    "answer": "#!/usr/bin/env bash\n\nSCRIPT_DIR=$( cd -- \"$( dirname -- \"${BASH_SOURCE[0]}\" )\" &> /dev/null && pwd )\n\nis a useful one-liner which will give you the full directory name of the script no matter where it is being called from.\nIt will work as long as the last component of the path used to find the script is not a symlink (directory links are OK).  If you also want to resolve any links to the script itself, you need a multi-line solution:\n#!/usr/bin/env bash\n\nget_script_dir()\n{\n    local SOURCE_PATH=\"${BASH_SOURCE[0]}\"\n    local SYMLINK_DIR\n    local SCRIPT_DIR\n    # Resolve symlinks recursively\n    while [ -L \"$SOURCE_PATH\" ]; do\n        # Get symlink directory\n        SYMLINK_DIR=\"$( cd -P \"$( dirname \"$SOURCE_PATH\" )\" >/dev/null 2>&1 && pwd )\"\n        # Resolve symlink target (relative or absolute)\n        SOURCE_PATH=\"$(readlink \"$SOURCE_PATH\")\"\n        # Check if candidate path is relative or absolute\n        if [[ $SOURCE_PATH != /* ]]; then\n            # Candidate path is relative, resolve to full path\n            SOURCE_PATH=$SYMLINK_DIR/$SOURCE_PATH\n        fi\n    done\n    # Get final script directory path from fully resolved source path\n    SCRIPT_DIR=\"$(cd -P \"$( dirname \"$SOURCE_PATH\" )\" >/dev/null 2>&1 && pwd)\"\n    echo \"$SCRIPT_DIR\"\n}\n\necho \"get_script_dir: $(get_script_dir)\"\n\nThis last one will work with any combination of aliases, source, bash -c, symlinks, etc.\nBeware: if you cd to a different directory before running this snippet, the result may be incorrect!\nAlso, watch out for $CDPATH gotchas, and stderr output side effects if the user has smartly overridden cd to redirect output to stderr instead (including escape sequences, such as when calling update_terminal_cwd >&2 on Mac). Adding >/dev/null 2>&1 at the end of your cd command will take care of both possibilities.\nTo understand how it works, try running this more verbose form:\n#!/usr/bin/env bash\n\nSOURCE=${BASH_SOURCE[0]}\nwhile [ -L \"$SOURCE\" ]; do # resolve $SOURCE until the file is no longer a symlink\n  TARGET=$(readlink \"$SOURCE\")\n  if [[ $TARGET == /* ]]; then\n    echo \"SOURCE '$SOURCE' is an absolute symlink to '$TARGET'\"\n    SOURCE=$TARGET\n  else\n    DIR=$( dirname \"$SOURCE\" )\n    echo \"SOURCE '$SOURCE' is a relative symlink to '$TARGET' (relative to '$DIR')\"\n    SOURCE=$DIR/$TARGET # if $SOURCE was a relative symlink, we need to resolve it relative to the path where the symlink file was located\n  fi\ndone\necho \"SOURCE is '$SOURCE'\"\nRDIR=$( dirname \"$SOURCE\" )\nDIR=$( cd -P \"$( dirname \"$SOURCE\" )\" >/dev/null 2>&1 && pwd )\nif [ \"$DIR\" != \"$RDIR\" ]; then\n  echo \"DIR '$RDIR' resolves to '$DIR'\"\nfi\necho \"DIR is '$DIR'\"\n\nAnd it will print something like:\nSOURCE './scriptdir.sh' is a relative symlink to 'sym2/scriptdir.sh' (relative to '.')\nSOURCE is './sym2/scriptdir.sh'\nDIR './sym2' resolves to '/home/ubuntu/dotfiles/fo fo/real/real1/real2'\nDIR is '/home/ubuntu/dotfiles/fo fo/real/real1/real2'",
    "tag": "bash"
  },
  {
    "question": "How do I check if a directory exists or not in a Bash shell script?",
    "answer": "To check if a directory exists:\nif [ -d \"$DIRECTORY\" ]; then\n  echo \"$DIRECTORY does exist.\"\nfi\n\nTo check if a directory does not exist:\nif [ ! -d \"$DIRECTORY\" ]; then\n  echo \"$DIRECTORY does not exist.\"\nfi\n\n\nHowever, as Jon Ericson points out, subsequent commands may not work as intended if you do not take into account that a symbolic link to a directory will also pass this check.\nE.g. running this:\nln -s \"$ACTUAL_DIR\" \"$SYMLINK\"\nif [ -d \"$SYMLINK\" ]; then \n  rmdir \"$SYMLINK\" \nfi\n\nWill produce the error message:\nrmdir: failed to remove `symlink': Not a directory\n\nSo symbolic links may have to be treated differently, if subsequent commands expect directories:\nif [ -d \"$LINK_OR_DIR\" ]; then \n  if [ -L \"$LINK_OR_DIR\" ]; then\n    # It is a symlink!\n    # Symbolic link specific commands go here.\n    rm \"$LINK_OR_DIR\"\n  else\n    # It's a directory!\n    # Directory command goes here.\n    rmdir \"$LINK_OR_DIR\"\n  fi\nfi\n\n\nTake particular note of the double-quotes used to wrap the variables. The reason for this is explained by 8jean in another answer.\nIf the variables contain spaces or other unusual characters it will probably cause the script to fail.",
    "tag": "bash"
  },
  {
    "question": "How do I tell if a file does not exist in Bash?",
    "answer": "The test command (written as [ here) has a \"not\" logical operator, ! (exclamation mark):\nif [ ! -f /tmp/foo.txt ]; then\n    echo \"File not found!\"\nfi",
    "tag": "bash"
  },
  {
    "question": "Echo newline in Bash prints literal \\n",
    "answer": "Use printf instead:\nprintf \"hello\\nworld\\n\"\n\nprintf behaves more consistently across different environments than echo.",
    "tag": "bash"
  },
  {
    "question": "How to check if a string contains a substring in Bash",
    "answer": "You can use Marcus's answer (* wildcards) outside a case statement, too, if you use double brackets:\nstring='My long string'\nif [[ $string == *\"My long\"* ]]; then\n  echo \"It's there!\"\nfi\n\nNote that spaces in the needle string need to be placed between double quotes, and the * wildcards should be outside. Also note that a simple comparison operator is used (i.e. ==), not the regex operator =~.",
    "tag": "bash"
  },
  {
    "question": "How to concatenate string variables in Bash",
    "answer": "foo=\"Hello\"\nfoo=\"${foo} World\"\necho \"${foo}\"\n> Hello World\n\nIn general to concatenate two variables you can just write them one after another:\na='Hello'\nb='World'\nc=\"${a} ${b}\"\necho \"${c}\"\n> Hello World",
    "tag": "bash"
  },
  {
    "question": "What does \" 2>&1 \" mean?",
    "answer": "File descriptor 1 is the standard output (stdout).\nFile descriptor 2 is the standard error (stderr).\nAt first, 2>1 may look like a good way to redirect stderr to stdout. However, it will actually be interpreted as \"redirect stderr to a file named 1\".\n& indicates that what follows and precedes is a file descriptor, and not a filename. Thus, we use 2>&1. Consider >& to be a redirect merger operator.",
    "tag": "bash"
  },
  {
    "question": "How can I check if a program exists from a Bash script?",
    "answer": "Answer\nPOSIX compatible:\ncommand -v <the_command>\n\nExample use:\nif ! command -v <the_command> >/dev/null 2>&1\nthen\n    echo \"<the_command> could not be found\"\n    exit 1\nfi\n\nFor Bash specific environments:\nhash <the_command> # For regular commands. Or...\ntype <the_command> # To check built-ins and keywords\n\nExplanation\nAvoid which. Not only is it an external process you're launching for doing very little (meaning builtins like hash, type or command are way cheaper), you can also rely on the builtins to actually do what you want, while the effects of external commands can easily vary from system to system.\nWhy care?\n\nMany operating systems have a which that doesn't even set an exit status, meaning the if which foo won't even work there and will always report that foo exists, even if it doesn't (note that some POSIX shells appear to do this for hash too).\nMany operating systems make which do custom and evil stuff like change the output or even hook into the package manager.\n\nSo, don't use which. Instead use one of these:\ncommand -v foo >/dev/null 2>&1 || { echo >&2 \"I require foo but it's not installed.  Aborting.\"; exit 1; }\n\ntype foo >/dev/null 2>&1 || { echo >&2 \"I require foo but it's not installed.  Aborting.\"; exit 1; }\n\nhash foo 2>/dev/null || { echo >&2 \"I require foo but it's not installed.  Aborting.\"; exit 1; }\n\n(Minor side-note: some will suggest 2>&- is the same 2>/dev/null but shorter – this is untrue.  2>&- closes FD 2 which causes an error in the program when it tries to write to stderr, which is very different from successfully writing to it and discarding the output (and dangerous!))\n(Additional minor side-note: some will suggest &>/dev/null, but this is not POSIX compliant)\nIf your hash bang is /bin/sh then you should care about what POSIX says. type and hash's exit codes aren't terribly well defined by POSIX, and hash is seen to exit successfully when the command doesn't exist (haven't seen this with type yet).  command's exit status is well defined by POSIX, so that one is probably the safest to use.\nIf your script uses bash though, POSIX rules don't really matter anymore and both type and hash become perfectly safe to use. type now has a -P to search just the PATH and hash has the side-effect that the command's location will be hashed (for faster lookup next time you use it), which is usually a good thing since you probably check for its existence in order to actually use it.\nAs a simple example, here's a function that runs gdate if it exists, otherwise date:\ngnudate() {\n    if hash gdate 2>/dev/null; then\n        gdate \"$@\"\n    else\n        date \"$@\"\n    fi\n}",
    "tag": "bash"
  },
  {
    "question": "How do I split a string on a delimiter in Bash?",
    "answer": "You can set the internal field separator (IFS) variable, and then let it parse into an array. When this happens in a command, then the assignment to IFS only takes place to that single command's environment (to read ). It then parses the input according to the IFS variable value into an array, which we can then iterate over.\nThis example will parse one line of items separated by ;, pushing it into an array:\nIFS=';' read -ra ADDR <<< \"$IN\"\nfor i in \"${ADDR[@]}\"; do\n  # process \"$i\"\ndone\n\nThis other example is for processing the whole content of $IN, each time one line of input separated by ;:\nwhile IFS=';' read -ra ADDR; do\n  for i in \"${ADDR[@]}\"; do\n    # process \"$i\"\n  done\ndone <<< \"$IN\"",
    "tag": "bash"
  },
  {
    "question": "Extract filename and extension in Bash",
    "answer": "First, get file name without the path:\nfilename=$(basename -- \"$fullfile\")\nextension=\"${filename##*.}\"\nfilename=\"${filename%.*}\"\n\nAlternatively, you can focus on the last '/' of the path instead of the '.' which should work even if you have unpredictable file extensions:\nfilename=\"${fullfile##*/}\"\n\nYou may want to check the documentation :\n\nOn the web at section \"3.5.3 Shell Parameter Expansion\"\nIn the bash manpage at section called \"Parameter Expansion\"",
    "tag": "bash"
  },
  {
    "question": "How to change the output color of echo in Linux",
    "answer": "You can use these ANSI escape codes:\nBlack        0;30     Dark Gray     1;30\nRed          0;31     Light Red     1;31\nGreen        0;32     Light Green   1;32\nBrown/Orange 0;33     Yellow        1;33\nBlue         0;34     Light Blue    1;34\nPurple       0;35     Light Purple  1;35\nCyan         0;36     Light Cyan    1;36\nLight Gray   0;37     White         1;37\n\nAnd then use them like this in your script:\n#    .---------- constant part!\n#    vvvv vvvv-- the code from above\nRED='\\033[0;31m'\nNC='\\033[0m' # No Color\nprintf \"I ${RED}love${NC} Stack Overflow\\n\"\n\nwhich prints love in red.\nFrom @james-lim's comment, if you are using the echo command, be sure to use the -e flag to allow backslash escapes.\n#    .---------- constant part!\n#    vvvv vvvv-- the code from above\nRED='\\033[0;31m'\nNC='\\033[0m' # No Color\necho -e \"I ${RED}love${NC} Stack Overflow\"\n\nNote: Don't add \"\\n\" when using echo unless you want to add an additional empty line.",
    "tag": "bash"
  },
  {
    "question": "How do I parse command line arguments in Bash?",
    "answer": "Bash Space-Separated (e.g., --option argument)\ncat >/tmp/demo-space-separated.sh <<'EOF'\n#!/bin/bash\n\nPOSITIONAL_ARGS=()\n\nwhile [[ $# -gt 0 ]]; do\n  case $1 in\n    -e|--extension)\n      EXTENSION=\"$2\"\n      shift # past argument\n      shift # past value\n      ;;\n    -s|--searchpath)\n      SEARCHPATH=\"$2\"\n      shift # past argument\n      shift # past value\n      ;;\n    --default)\n      DEFAULT=YES\n      shift # past argument\n      ;;\n    -*|--*)\n      echo \"Unknown option $1\"\n      exit 1\n      ;;\n    *)\n      POSITIONAL_ARGS+=(\"$1\") # save positional arg\n      shift # past argument\n      ;;\n  esac\ndone\n\nset -- \"${POSITIONAL_ARGS[@]}\" # restore positional parameters\n\necho \"FILE EXTENSION  = ${EXTENSION}\"\necho \"SEARCH PATH     = ${SEARCHPATH}\"\necho \"DEFAULT         = ${DEFAULT}\"\necho \"Number files in SEARCH PATH with EXTENSION:\" $(ls -1 \"${SEARCHPATH}\"/*.\"${EXTENSION}\" | wc -l)\n\nif [[ -n $1 ]]; then\n    echo \"Last line of file specified as non-opt/last argument:\"\n    tail -1 \"$1\"\nfi\nEOF\n\nchmod +x /tmp/demo-space-separated.sh\n\n/tmp/demo-space-separated.sh -e conf -s /etc /etc/hosts\n\nOutput from copy-pasting the block above\nFILE EXTENSION  = conf\nSEARCH PATH     = /etc\nDEFAULT         =\nNumber files in SEARCH PATH with EXTENSION: 14\nLast line of file specified as non-opt/last argument:\n#93.184.216.34    example.com\n\nUsage\ndemo-space-separated.sh -e conf -s /etc /etc/hosts\n\n\nBash Equals-Separated (e.g., --option=argument)\ncat >/tmp/demo-equals-separated.sh <<'EOF'\n#!/bin/bash\n\nfor i in \"$@\"; do\n  case $i in\n    -e=*|--extension=*)\n      EXTENSION=\"${i#*=}\"\n      shift # past argument=value\n      ;;\n    -s=*|--searchpath=*)\n      SEARCHPATH=\"${i#*=}\"\n      shift # past argument=value\n      ;;\n    --default)\n      DEFAULT=YES\n      shift # past argument with no value\n      ;;\n    -*|--*)\n      echo \"Unknown option $i\"\n      exit 1\n      ;;\n    *)\n      ;;\n  esac\ndone\n\necho \"FILE EXTENSION  = ${EXTENSION}\"\necho \"SEARCH PATH     = ${SEARCHPATH}\"\necho \"DEFAULT         = ${DEFAULT}\"\necho \"Number files in SEARCH PATH with EXTENSION:\" $(ls -1 \"${SEARCHPATH}\"/*.\"${EXTENSION}\" | wc -l)\n\nif [[ -n $1 ]]; then\n    echo \"Last line of file specified as non-opt/last argument:\"\n    tail -1 $1\nfi\nEOF\n\nchmod +x /tmp/demo-equals-separated.sh\n\n/tmp/demo-equals-separated.sh -e=conf -s=/etc /etc/hosts\n\nOutput from copy-pasting the block above\nFILE EXTENSION  = conf\nSEARCH PATH     = /etc\nDEFAULT         =\nNumber files in SEARCH PATH with EXTENSION: 14\nLast line of file specified as non-opt/last argument:\n#93.184.216.34    example.com\n\nUsage\ndemo-equals-separated.sh -e=conf -s=/etc /etc/hosts\n\n\nTo better understand ${i#*=} search for \"Substring Removal\" in this guide. It is functionally equivalent to `sed 's/[^=]*=//' <<< \"$i\"` which calls a needless subprocess or `echo \"$i\" | sed 's/[^=]*=//'` which calls two needless subprocesses.\n\nUsing bash with getopt[s]\ngetopt(1) limitations (older, relatively-recent getopt versions):\n\ncan't handle arguments that are empty strings\ncan't handle arguments with embedded whitespace\n\nMore recent getopt versions don't have these limitations. For more information, see these docs.\n\nPOSIX getopts\nAdditionally, the POSIX shell and others offer getopts which doen't have these limitations. I've included a simplistic getopts example.\ncat >/tmp/demo-getopts.sh <<'EOF'\n#!/bin/sh\n\n# A POSIX variable\nOPTIND=1         # Reset in case getopts has been used previously in the shell.\n\n# Initialize our own variables:\noutput_file=\"\"\nverbose=0\n\nwhile getopts \"h?vf:\" opt; do\n  case \"$opt\" in\n    h|\\?)\n      show_help\n      exit 0\n      ;;\n    v)  verbose=1\n      ;;\n    f)  output_file=$OPTARG\n      ;;\n  esac\ndone\n\nshift $((OPTIND-1))\n\n[ \"${1:-}\" = \"--\" ] && shift\n\necho \"verbose=$verbose, output_file='$output_file', Leftovers: $@\"\nEOF\n\nchmod +x /tmp/demo-getopts.sh\n\n/tmp/demo-getopts.sh -vf /etc/hosts foo bar\n\nOutput from copy-pasting the block above\nverbose=1, output_file='/etc/hosts', Leftovers: foo bar\n\nUsage\ndemo-getopts.sh -vf /etc/hosts foo bar\n\nThe advantages of getopts are:\n\nIt's more portable, and will work in other shells like dash.\nIt can handle multiple single options like -vf filename in the typical Unix way, automatically.\n\nThe disadvantage of getopts is that it can only handle short options (-h, not --help) without additional code.\nThere is a getopts tutorial which explains what all of the syntax and variables mean.  In bash, there is also help getopts, which might be informative.",
    "tag": "bash"
  },
  {
    "question": "How do I set a variable to the output of a command in Bash?",
    "answer": "In addition to backticks `command`, command substitution can be done with $(command) or \"$(command)\", which I find easier to read, and allows for nesting.\nOUTPUT=\"$(ls -1)\"\necho \"${OUTPUT}\"\n\nMULTILINE=\"$(ls \\\n   -1)\"\necho \"${MULTILINE}\"\n\nQuoting (\") does matter to preserve multi-line variable values and it is safer to use with whitespace and special characters such as (*) and therefore advised; it is, however, optional on the right-hand side of an assignment when word splitting is not performed, so OUTPUT=$(ls -1) would work fine.",
    "tag": "bash"
  },
  {
    "question": "How to check if a variable is set in Bash",
    "answer": "(Usually) The right way\nif [ -z ${var+x} ]; then echo \"var is unset\"; else echo \"var is set to '$var'\"; fi\n\nwhere ${var+x} is a parameter expansion which evaluates to nothing if var is unset, and substitutes the string x otherwise.\nQuotes Digression\nQuotes can be omitted (so we can say ${var+x} instead of \"${var+x}\") because this syntax & usage guarantees this will only expand to something that does not require quotes (since it either expands to x (which contains no word breaks so it needs no quotes), or to nothing (which results in [ -z  ], which conveniently evaluates to the same value (true) that [ -z \"\" ] does as well)).\nHowever, while quotes can be safely omitted, and it was not immediately obvious to all (it wasn't even apparent to the first author of this quotes explanation who is also a major Bash coder), it would sometimes be better to write the solution with quotes as [ -z \"${var+x}\" ], at the very small possible cost of an O(1) speed penalty.  The first author also added this as a comment next to the code using this solution giving the URL to this answer, which now also includes the explanation for why the quotes can be safely omitted.\n(Often) The wrong way\nif [ -z \"$var\" ]; then echo \"var is blank\"; else echo \"var is set to '$var'\"; fi\n\nThis is often wrong because it doesn't distinguish between a variable that is unset and a variable that is set to the empty string. That is to say, if var='', then the above solution will output \"var is blank\".\nThe distinction between unset and \"set to the empty string\" is essential in situations where the user has to specify an extension, or additional list of properties, and that not specifying them defaults to a non-empty value, whereas specifying the empty string should make the script use an empty extension or list of additional properties.\nThe distinction may not be essential in every scenario though. In those cases  [ -z \"$var\" ] will be just fine.",
    "tag": "bash"
  },
  {
    "question": "Loop through an array of strings in Bash?",
    "answer": "You can use it like this:\n## declare an array variable\ndeclare -a arr=(\"element1\" \"element2\" \"element3\")\n\n## now loop through the above array\nfor i in \"${arr[@]}\"\ndo\n   echo \"$i\"\n   # or do whatever with individual element of the array\ndone\n\n# You can access them using echo \"${arr[0]}\", \"${arr[1]}\" also\n\nAlso works for multi-line array declaration\ndeclare -a arr=(\"element1\" \n                \"element2\" \"element3\"\n                \"element4\"\n                )",
    "tag": "bash"
  },
  {
    "question": "How to reload .bashrc settings without logging out and back in again?",
    "answer": "You can enter the long form command:\nsource ~/.bashrc\n\nor you can use the shorter version of the command:\n. ~/.bashrc",
    "tag": "bash"
  },
  {
    "question": "How do I iterate over a range of numbers defined by variables in Bash?",
    "answer": "for i in $(seq 1 $END); do echo $i; done\nedit: I prefer seq over the other methods because I can actually remember it ;)",
    "tag": "bash"
  },
  {
    "question": "Looping through the content of a file in Bash",
    "answer": "One way to do it is:\nwhile read p; do\n  echo \"$p\"\ndone <peptides.txt\n\nAs pointed out in the comments, this has the side effects of trimming leading whitespace, interpreting backslash sequences, and skipping the last line if it's missing a terminating linefeed. If these are concerns, you can do:\nwhile IFS=\"\" read -r p || [ -n \"$p\" ]\ndo\n  printf '%s\\n' \"$p\"\ndone < peptides.txt\n\n\nExceptionally, if the loop body may read from standard input, you can open the file using a different file descriptor:\nwhile read -u 10 p; do\n  ...\ndone 10<peptides.txt\n\nHere, 10 is just an arbitrary number (different from 0, 1, 2).",
    "tag": "bash"
  },
  {
    "question": "How can I count all the lines of code in a directory recursively?",
    "answer": "Try:\nfind . -name '*.php' | xargs wc -l\n\nor (when file names include special characters such as spaces)\nfind . -name '*.php' | sed 's/.*/\"&\"/' | xargs  wc -l\n\nThe SLOCCount tool may help as well.\nIt will give an accurate source lines of code count for whatever\nhierarchy you point it at, as well as some additional stats.\nSorted output:\nfind . -name '*.php' | xargs wc -l | sort -nr",
    "tag": "bash"
  },
  {
    "question": "How to redirect and append both standard output and standard error to a file with Bash",
    "answer": "cmd >>file.txt 2>&1\n\nBash executes the redirects from left to right as follows:\n\n>>file.txt: Open file.txt in append mode and redirect stdout there.\n2>&1: Redirect stderr to \"where stdout is currently going\". In this case, that is a file opened in append mode. In other words, the &1 reuses the file descriptor which stdout currently uses.",
    "tag": "bash"
  },
  {
    "question": "Check existence of input argument in a Bash shell script",
    "answer": "It is:\nif [ $# -eq 0 ]\n  then\n    echo \"No arguments supplied\"\nfi\n\nThe $# variable will tell you the number of input arguments the script was passed.\nOr you can check if an argument is an empty string or not like:\nif [ -z \"$1\" ]\n  then\n    echo \"No argument supplied\"\nfi\n\nThe -z switch will test if the expansion of \"$1\" is a null string or not. If it is a null string then the body is executed.",
    "tag": "bash"
  },
  {
    "question": "How do I prompt for Yes/No/Cancel input in a Linux shell script?",
    "answer": "A widely available method to get user input at a shell prompt is the read command. Here is a demonstration:\nwhile true; do\n    read -p \"Do you wish to install this program? \" yn\n    case $yn in\n        [Yy]* ) make install; break;;\n        [Nn]* ) exit;;\n        * ) echo \"Please answer yes or no.\";;\n    esac\ndone\n\n\nAnother method, pointed out by Steven Huwig, is Bash's select command. Here is the same example using select:\necho \"Do you wish to install this program?\"\nselect yn in \"Yes\" \"No\"; do\n    case $yn in\n        Yes ) make install; break;;\n        No ) exit;;\n    esac\ndone\n\nWith select you don't need to sanitize the input – it displays the available choices, and you type a number corresponding to your choice. It also loops automatically, so there's no need for a while true loop to retry if they give invalid input. If you want to allow more flexible input (accepting the words of the options, rather than just their number), you can alter it like this:\necho \"Do you wish to install this program?\"\nselect strictreply in \"Yes\" \"No\"; do\n    relaxedreply=${strictreply:-$REPLY}\n    case $relaxedreply in\n        Yes | yes | y ) make install; break;;\n        No  | no  | n ) exit;;\n    esac\ndone\n\n\nAlso, Léa Gris demonstrated a way to make the request language agnostic in her answer. Adapting my first example to better serve multiple languages might look like this:\nset -- $(locale LC_MESSAGES)\nyesexpr=\"$1\"; noexpr=\"$2\"; yesword=\"$3\"; noword=\"$4\"\n\nwhile true; do\n    read -p \"Install (${yesword} / ${noword})? \" yn\n    if [[ \"$yn\" =~ $yesexpr ]]; then make install; exit; fi\n    if [[ \"$yn\" =~ $noexpr ]]; then exit; fi\n    echo \"Answer ${yesword} / ${noword}.\"\ndone\n\nObviously other communication strings remain untranslated here (Install, Answer) which would need to be addressed in a more fully completed translation, but even a partial translation would be helpful in many cases.\n\nFinally, please check out the excellent answer by F. Hauri.",
    "tag": "bash"
  },
  {
    "question": "Difference between sh and Bash",
    "answer": "What is sh?\nsh (or the Shell Command Language) is a programming language described by the POSIX standard. It has many implementations (ksh88, Dash, ...). Bash can also be considered an implementation of sh (see below).\nBecause sh is a specification, not an implementation, /bin/sh is a symlink (or a hard link) to an actual implementation on most POSIX systems.\nWhat is Bash?\nBash started as an sh-compatible implementation (although it predates the POSIX standard by a few years), but as time passed it has acquired many extensions. Many of these extensions may change the behavior of valid POSIX shell scripts, so by itself Bash is not a valid POSIX shell. Rather, it is a dialect of the POSIX shell language.\nBash supports a --posix switch, which makes it more POSIX-compliant. It also tries to mimic POSIX if invoked as sh.\nsh = bash?\nFor a long time, /bin/sh used to point to /bin/bash on most GNU/Linux systems. As a result, it had almost become safe to ignore the difference between the two. But that started to change recently.\nSome popular examples of systems where /bin/sh does not point to /bin/bash (and on some of which /bin/bash may not even exist) are:\n\nModern Debian and Ubuntu systems, which symlink sh to dash by default;\nBusybox, which is usually run during the Linux system boot time as part of initramfs. It uses the ash shell implementation.\nBSD systems, and in general any non-Linux systems. OpenBSD uses pdksh, a descendant of the KornShell. FreeBSD's sh is a descendant of the original Unix Bourne shell.  Solaris has its own sh which for a long time was not POSIX-compliant; a free implementation is available from the Heirloom project.\n\nHow can you find out what /bin/sh points to on your system?\nThe complication is that /bin/sh could be a symbolic link or a hard link. If it's a symbolic link, a portable way to resolve it is:\n% file -h /bin/sh\n/bin/sh: symbolic link to bash\n\nIf it's a hard link, try\n% find -L /bin -samefile /bin/sh\n/bin/sh\n/bin/bash\n\nIn fact, the -L flag covers both symlinks and hardlinks,\nbut the disadvantage of this method is that it is not portable —\nPOSIX does not require find to support the -samefile option, although both GNU find and FreeBSD find support it.\nShebang line\nUltimately, it's up to you to decide which one to use, by writing the «shebang» line as the very first line of the script.\nE.g.\n#!/bin/sh\n\nwill use sh (and whatever that happens to point to),\n#!/bin/bash\n\nwill use /bin/bash if it's available (and fail with an error message if it's not). Of course, you can also specify another implementation, e.g.\n#!/bin/dash\n\nWhich one to use\nFor my own scripts, I prefer sh for the following reasons:\n\nit is standardized\nit is much simpler and easier to learn\nit is portable across POSIX systems — even if they happen not to have bash, they are required to have sh\n\nThere are advantages to using bash as well. Its features make programming more convenient and similar to programming in other modern programming languages. These include things like scoped local variables and arrays. Plain sh is a very minimalistic programming language.",
    "tag": "bash"
  },
  {
    "question": "How to specify the private SSH-key to use when executing shell command on Git?",
    "answer": "None of these solutions worked for me.\nInstead, I elaborate on @Martin v. Löwis  's mention of setting a config file for SSH.\nSSH will look for the user's ~/.ssh/config file. I have mine setup as:\nHost gitserv\n    Hostname remote.server.com\n    IdentityFile ~/.ssh/id_rsa.github\n    IdentitiesOnly yes # see NOTES below\n    AddKeysToAgent yes\n\nAnd I add a remote git repository:\ngit remote add origin git@gitserv:myrepo.git\n\n(or clone a fresh copy of the repo with git@gitserv:myrepo.git as address)\nAnd then git commands work normally for me.\ngit push -v origin master\n\nIf you have submodules, you can also execute the following in the repo directory, to force the submodules to use the same key:\ngit config url.git@gitserv:.insteadOf https://remote.server.com\n\nNOTES\n\nThe IdentitiesOnly yes is required to prevent the SSH default behavior of sending the identity file matching the default filename for each protocol. If you have a file named ~/.ssh/id_rsa that will get tried BEFORE your ~/.ssh/id_rsa.github without this option.\n\nAddKeysToAgent yes lets you avoid reentering the key passphrase every time.\n\nYou can also add User git to avoid writing git@ every time.\n\n\nReferences\n\nBest way to use multiple SSH private keys on one client\nHow could I stop ssh offering a wrong key",
    "tag": "bash"
  },
  {
    "question": "Make a Bash alias that takes a parameter?",
    "answer": "Bash alias does not directly accept parameters. You will have to create a function.\nalias does not accept parameters but a function can be called just like an alias. For example:\nmyfunction() {\n    #do things with parameters like $1 such as\n    mv \"$1\" \"$1.bak\"\n    cp \"$2\" \"$1\"\n}\n\n\nmyfunction old.conf new.conf #calls `myfunction`\n\nBy the way, Bash functions defined in your .bashrc and other files are available as commands within your shell. So for instance you can call the earlier function like this \n$ myfunction original.conf my.conf",
    "tag": "bash"
  },
  {
    "question": "How to convert a string to lower case in Bash",
    "answer": "There are various ways:\nPOSIX standard\ntr\n$ echo \"$a\" | tr '[:upper:]' '[:lower:]'\nhi all\n\nAWK\n$ echo \"$a\" | awk '{print tolower($0)}'\nhi all\n\nNon-POSIX\nYou may run into portability issues with the following examples:\nBash 4.0\n$ echo \"${a,,}\"\nhi all\n\nsed\n$ echo \"$a\" | sed -e 's/\\(.*\\)/\\L\\1/'\nhi all\n# this also works:\n$ sed -e 's/\\(.*\\)/\\L\\1/' <<< \"$a\"\nhi all\n\nPerl\n$ echo \"$a\" | perl -ne 'print lc'\nhi all\n\nBash\nlc(){\n    case \"$1\" in\n        [A-Z])\n        n=$(printf \"%d\" \"'$1\")\n        n=$((n+32))\n        printf \\\\$(printf \"%o\" \"$n\")\n        ;;\n        *)\n        printf \"%s\" \"$1\"\n        ;;\n    esac\n}\nword=\"I Love Bash\"\nfor((i=0;i<${#word};i++))\ndo\n    ch=\"${word:$i:1}\"\n    lc \"$ch\"\ndone\n\nNote: YMMV on this one. Doesn't work for me (GNU bash version 4.2.46 and 4.0.33 (and same behaviour 2.05b.0 but nocasematch  is not implemented)) even with using shopt -u nocasematch;. Unsetting that nocasematch causes [[ \"fooBaR\" == \"FOObar\" ]] to match OK BUT inside case weirdly [b-z] are incorrectly matched by [A-Z]. Bash is confused by the double-negative (\"unsetting nocasematch\")! :-)",
    "tag": "bash"
  },
  {
    "question": "What is the preferred Bash shebang (\"#!\")?",
    "answer": "You should use #!/usr/bin/env bash for portability: Different *nixes put bash in different places, and using /usr/bin/env is a workaround to run the first bash found on the PATH.\nAnd sh is not bash.",
    "tag": "bash"
  },
  {
    "question": "echo that outputs to stderr",
    "answer": "You could do this, which facilitates reading:\n>&2 echo \"error\"\n\n>&2 copies file descriptor #2 to file descriptor #1. Therefore, after this redirection is performed, both file descriptors will refer to the same file: the one file descriptor #2 was originally referring to. For more information see the Bash Hackers Illustrated Redirection Tutorial.",
    "tag": "bash"
  },
  {
    "question": "How to redirect output to a file and stdout",
    "answer": "The command you want is named tee:\nfoo | tee output.file\n\nFor example, if you only care about stdout:\nls -a | tee output.file\n\nIf you want to include stderr, do:\nprogram [arguments...] 2>&1 | tee outfile\n\n2>&1 redirects channel 2 (stderr/standard error) into channel 1 (stdout/standard output), such that both is written as stdout. It is also directed to the given output file as of the tee command.\nFurthermore, if you want to append to the log file, use tee -a as:\nprogram [arguments...] 2>&1 | tee -a outfile",
    "tag": "bash"
  },
  {
    "question": "YYYY-MM-DD format date in shell script",
    "answer": "In bash (>=4.2) it is preferable to use printf's built-in date formatter (part of bash) rather than the external date (usually GNU date). Note that invoking a subshell has performance problems in Cygwin due to a slow fork() call on Windows.\nAs such:\n# put current date as yyyy-mm-dd in $date\n# -1 -> explicit current date, bash >=4.3 defaults to current time if not provided\n# -2 -> start time for shell\nprintf -v date '%(%Y-%m-%d)T\\n' -1\n\n# put current date as yyyy-mm-dd HH:MM:SS in $date\nprintf -v date '%(%Y-%m-%d %H:%M:%S)T\\n' -1\n\n# to print directly remove -v flag, as such:\nprintf '%(%Y-%m-%d)T\\n' -1\n# -> current date printed to terminal\n\nIn bash (<4.2):\n# put current date as yyyy-mm-dd in $date\ndate=$(date '+%Y-%m-%d')\n\n# put current date as yyyy-mm-dd HH:MM:SS in $date\ndate=$(date '+%Y-%m-%d %H:%M:%S')\n\n# print current date directly\necho $(date '+%Y-%m-%d')\n\nOther available date formats can be viewed from the date man pages (for external non-bash specific command):\nman date",
    "tag": "bash"
  },
  {
    "question": "Passing parameters to a Bash function",
    "answer": "There are two typical ways of declaring a function. I prefer the second approach.\nfunction function_name {\n   command...\n} \n\nor\nfunction_name () {\n   command...\n} \n\nTo call a function with arguments:\nfunction_name \"$arg1\" \"$arg2\"\n\nThe function refers to passed arguments by their position (not by name), that is $1, $2, and so forth. $0 is the name of the script itself.\nExample:\nfunction_name () {\n   echo \"Parameter #1 is $1\"\n}\n\nAlso, you need to call your function after it is declared.\n#!/usr/bin/env sh\n\nfoo 1  # this will fail because foo has not been declared yet.\n\nfoo() {\n    echo \"Parameter #1 is $1\"\n}\n\nfoo 2 # this will work.\n\nOutput:\n./myScript.sh: line 2: foo: command not found\nParameter #1 is 2\n\nReference: Advanced Bash-Scripting Guide.",
    "tag": "bash"
  },
  {
    "question": "How can I declare and use Boolean variables in a shell script?",
    "answer": "Revised Answer (Feb 12, 2014)\nthe_world_is_flat=true\n# ...do something interesting...\nif [ \"$the_world_is_flat\" = true ] ; then\n    echo 'Be careful not to fall off!'\nfi\n\n\nOriginal Answer\nCaveats: https://stackoverflow.com/a/21210966/89391\nthe_world_is_flat=true\n# ...do something interesting...\nif $the_world_is_flat ; then\n    echo 'Be careful not to fall off!'\nfi\n\nFrom: Using boolean variables in Bash\nThe reason the original answer is included here is because the comments before the revision on Feb 12, 2014 pertain only to the original answer, and many of the comments are wrong when associated with the revised answer. For example, Dennis Williamson's comment about Bash's builtin true on Jun 2, 2010 only applies to the original answer, not the revised.",
    "tag": "bash"
  },
  {
    "question": "How to escape single quotes within single quoted strings",
    "answer": "If you really want to use single quotes in the outermost layer, remember that you can glue both kinds of quotation. Example:\n alias rxvt='urxvt -fg '\"'\"'#111111'\"'\"' -bg '\"'\"'#111111'\"'\"\n #                     ^^^^^       ^^^^^     ^^^^^       ^^^^\n #                     12345       12345     12345       1234\n\nExplanation of how '\"'\"' is interpreted as just ':\n\n' End first quotation which uses single quotes.\n\" Start second quotation, using double-quotes.\n' Quoted character.\n\" End second quotation, using double-quotes.\n' Start third quotation, using single quotes.\n\nIf you do not place any whitespaces between (1) and (2), or between (4) and (5), the shell will interpret that string as a one long word:\n$ echo 'abc''123'  \nabc123\n$ echo 'abc'\\''123'\nabc'123\n$ echo 'abc'\"'\"'123'\nabc'123\n\nIt will also keep the internal representation with 'to be joined' strings, and will also prefer the shorter escape syntax when possible:\n$ alias test='echo '\"'\"'hi'\"'\"\n$ alias test\nalias test='echo '\\''hi'\\'''\n$ test\nhi",
    "tag": "bash"
  },
  {
    "question": "Assigning default values to shell variables with a single command in bash",
    "answer": "Very close to what you posted, actually. You can use something called Bash parameter expansion to accomplish this.\nTo get the assigned value, or default if it's missing:\nFOO=\"${VARIABLE:-default}\"  # FOO will be assigned 'default' value if VARIABLE not set or null.\n# The value of VARIABLE remains untouched.\n\nTo do the same, as well as assign default to VARIABLE:\nFOO=\"${VARIABLE:=default}\"  # If VARIABLE not set or null, set its value to 'default'. \n# Then that value will be assigned to FOO",
    "tag": "bash"
  },
  {
    "question": "Count number of lines in a non binary file (Like a CSV or a TXT) file in terminal",
    "answer": "Use wc:\nwc -l <filename>\n\nThis will output the number of lines in <filename>:\n$ wc -l /dir/file.txt\n3272485 /dir/file.txt\n\nOr, to omit the <filename> from the result use wc -l < <filename>:\n$ wc -l < /dir/file.txt\n3272485\n\nYou can also pipe data to wc as well:\n$ cat /dir/file.txt | wc -l\n3272485\n$ curl yahoo.com --silent | wc -l\n63",
    "tag": "bash"
  },
  {
    "question": "Replace one substring for another string in shell script",
    "answer": "To replace the first occurrence of a pattern with a given string, use ${parameter/pattern/string}:\n#!/bin/bash\nfirstString=\"I love Suzi and Marry\"\nsecondString=\"Sara\"\necho \"${firstString/Suzi/\"$secondString\"}\"\n# prints 'I love Sara and Marry'\n\nTo replace all occurrences, use ${parameter//pattern/string}:\nmessage='The secret code is 12345'\necho \"${message//[0-9]/X}\"\n# prints 'The secret code is XXXXX'\n\n(This is documented in the Bash Reference Manual, §3.5.3 \"Shell Parameter Expansion\".)\nNote that this feature is not specified by POSIX — it's a Bash extension — so not all Unix shells implement it. For the relevant POSIX documentation, see The Open Group Technical Standard Base Specifications, Issue 7, the Shell & Utilities volume, §2.6.2 \"Parameter Expansion\".",
    "tag": "bash"
  },
  {
    "question": "How to trim whitespace from a Bash variable?",
    "answer": "A simple answer is:\necho \"   lol  \" | xargs\n\nXargs will do the trimming for you. It's one command/program, no parameters, returns the trimmed string, easy as that!\nNote: this doesn't remove all internal spaces so \"foo bar\" stays the same; it does NOT become \"foobar\". However, multiple spaces will be condensed to single spaces, so \"foo    bar\" will become \"foo bar\". In addition it doesn't remove end of lines characters.",
    "tag": "bash"
  },
  {
    "question": "Listing only directories using ls in Bash?",
    "answer": "*/ is a pattern that matches all of the subdirectories in the current directory (* would match all files and subdirectories; the / restricts it to directories). Similarly, to list all subdirectories under /home/alice/Documents, use ls -d /home/alice/Documents/*/",
    "tag": "bash"
  },
  {
    "question": "How to echo shell commands as they are executed",
    "answer": "set -x or set -o xtrace expands variables and prints a little + sign before the line.\nset -v or set -o verbose does not expand the variables before printing.\nUse set +x and set +v to turn off the above settings.\nOn the first line of the script, one can put #!/bin/sh -x (or -v) to have the same effect as set -x (or -v) later in the script.\nThe above also works with /bin/sh.\nSee the bash-hackers' wiki on set attributes, and on debugging.\n$ cat shl\n#!/bin/bash                                                                     \n\nDIR=/tmp/so\nls $DIR\n\n$ bash -x shl \n+ DIR=/tmp/so\n+ ls /tmp/so\n$",
    "tag": "bash"
  },
  {
    "question": "How to compare strings in Bash",
    "answer": "Using variables in if statements\nif [ \"$x\" = \"valid\" ]; then\n  echo \"x has the value 'valid'\"\nfi\n\nIf you want to do something when they don't match, replace = with !=. You can read more about string operations and arithmetic operations in their respective documentation.\nWhy do we use quotes around $x?\nYou want the quotes around $x, because if it is empty, your Bash script encounters a syntax error as seen below:\nif [ = \"valid\" ]; then\n\n\nNon-standard use of == operator\nNote that Bash allows == to be used for equality with [, but this is not standard.\nUse either the first case wherein the quotes around $x are optional:\nif [[ \"$x\" == \"valid\" ]]; then\n\nor use the second case:\nif [ \"$x\" = \"valid\" ]; then",
    "tag": "bash"
  },
  {
    "question": "Pipe to/from the clipboard in a Bash script",
    "answer": "There are a wealth of clipboards you could be dealing with.  I expect you're probably a Linux user who wants to put stuff in the X Windows primary clipboard.  Usually, the clipboard you want to talk to has a utility that lets you talk to it.\nIn the case of X, there's xclip (and others). xclip -selection c will send data to the clipboard that works with Ctrl + C, Ctrl + V  in most applications.\nIf you're on Mac OS X, there's pbcopy. E.g., cat example.txt | pbcopy\nIf you're in Linux terminal mode (no X) then look into gpm or Screen which has a clipboard.  Try the Screen command readreg.\nUnder Windows 10+ or Cygwin, use /dev/clipboard or clip.",
    "tag": "bash"
  },
  {
    "question": "How can I pipe stderr, and not stdout?",
    "answer": "First redirect stderr to stdout — the pipe; then redirect stdout to /dev/null (without changing where stderr is going):\ncommand 2>&1 >/dev/null | grep 'something'\n\nFor the details of I/O redirection in all its variety, see the chapter on Redirections in the Bash reference manual.\nNote that the sequence of I/O redirections is interpreted left-to-right, but pipes are set up before the I/O redirections are interpreted.  File descriptors such as 1 and 2 are references to open file descriptions.  The operation 2>&1 makes file descriptor 2 aka stderr refer to the same open file description as file descriptor 1 aka stdout is currently referring to (see dup2() and open()).  The operation >/dev/null then changes file descriptor 1 so that it refers to an open file description for /dev/null, but that doesn't change the fact that file descriptor 2 refers to the open file description which file descriptor 1 was originally pointing to — namely, the pipe.",
    "tag": "bash"
  },
  {
    "question": "Parsing JSON with Unix tools",
    "answer": "There are a number of tools specifically designed for the purpose of manipulating JSON from the command line, and will be a lot easier and more reliable than doing it with Awk, such as jq:\ncurl -s 'https://api.github.com/users/lambda' | jq -r '.name'\n\nYou can also do this with tools that are likely already installed on your system, like Python using the json module, and so avoid any extra dependencies, while still having the benefit of a proper JSON parser. The following assume you want to use UTF-8, which the original JSON should be encoded in and is what most modern terminals use as well:\nPython 3:\ncurl -s 'https://api.github.com/users/lambda' | \\\n    python3 -c \"import sys, json; print(json.load(sys.stdin)['name'])\"\n\nPython 2:\nexport PYTHONIOENCODING=utf8\ncurl -s 'https://api.github.com/users/lambda' | \\\n    python2 -c \"import sys, json; print json.load(sys.stdin)['name']\"\n\nFrequently Asked Questions\nWhy not a pure shell solution?\nThe standard POSIX/Single Unix Specification shell is a very limited language which doesn't contain facilities for representing sequences (list or arrays) or associative arrays (also known as hash tables, maps, dicts, or objects in some other languages). This makes representing the result of parsing JSON somewhat tricky in portable shell scripts. There are somewhat hacky ways to do it, but many of them can break if keys or values contain certain special characters.\nBash 4 and later, zsh, and ksh have support for arrays and associative arrays, but these shells are not universally available (macOS stopped updating Bash at Bash 3, due to a change from GPLv2 to GPLv3, while many Linux systems don't have zsh installed out of the box). It's possible that you could write a script that would work in either Bash 4 or zsh, one of which is available on most macOS, Linux, and BSD systems these days, but it would be tough to write a shebang line that worked for such a polyglot script.\nFinally, writing a full fledged JSON parser in shell would be a significant enough dependency that you might as well just use an existing dependency like jq or Python instead. It's not going to be a one-liner, or even small five-line snippet, to do a good implementation.\nWhy not use awk, sed, or grep?\nIt is possible to use these tools to do some quick extraction from JSON with a known shape and formatted in a known way, such as one key per line. There are several examples of suggestions for this in other answers.\nHowever, these tools are designed for line based or record based formats; they are not designed for recursive parsing of matched delimiters with possible escape characters.\nSo these quick and dirty solutions using awk/sed/grep are likely to be fragile, and break if some aspect of the input format changes, such as collapsing whitespace, or adding additional levels of nesting to the JSON objects, or an escaped quote within a string. A solution that is robust enough to handle all JSON input without breaking will also be fairly large and complex, and so not too much different than adding another dependency on jq or Python.\nI have had to deal with large amounts of customer data being deleted due to poor input parsing in a shell script before, so I never recommend quick and dirty methods that may be fragile in this way. If you're doing some one-off processing, see the other answers for suggestions, but I still highly recommend just using an existing tested JSON parser.\nHistorical notes\nThis answer originally recommended jsawk, which should still work, but is a little more cumbersome to use than jq, and depends on a standalone JavaScript interpreter being installed which is less common than a Python interpreter, so the above answers are probably preferable:\ncurl -s 'https://api.github.com/users/lambda' | jsawk -a 'return this.name'\n\nThis answer also originally used the Twitter API from the question, but that API no longer works, making it hard to copy the examples to test out, and the new Twitter API requires API keys, so I've switched to using the GitHub API which can be used easily without API keys.  The first answer for the original question would be:\ncurl 'http://twitter.com/users/username.json' | jq -r '.text'",
    "tag": "bash"
  },
  {
    "question": "Propagate all arguments in a Bash shell script",
    "answer": "Use \"$@\" instead of plain $@ if you actually wish your parameters to be passed the same.\nObserve:\n$ cat no_quotes.sh\n#!/bin/bash\n./echo_args.sh $@\n\n$ cat quotes.sh\n#!/bin/bash\n./echo_args.sh \"$@\"\n\n$ cat echo_args.sh\n#!/bin/bash\necho Received: $1\necho Received: $2\necho Received: $3\necho Received: $4\n\n$ ./no_quotes.sh first second\nReceived: first\nReceived: second\nReceived:\nReceived:\n\n$ ./no_quotes.sh \"one quoted arg\"\nReceived: one\nReceived: quoted\nReceived: arg\nReceived:\n\n$ ./quotes.sh first second\nReceived: first\nReceived: second\nReceived:\nReceived:\n\n$ ./quotes.sh \"one quoted arg\"\nReceived: one quoted arg\nReceived:\nReceived:\nReceived:",
    "tag": "bash"
  },
  {
    "question": "Defining a variable with or without export",
    "answer": "export makes the variable available to sub-processes.\nThat is,\nexport name=value\n\nmeans that the variable name is available to any process you run from that shell process. If you want a process to make use of this variable, use export, and run the process from that shell.\nname=value\n\nmeans the variable scope is restricted to the shell, and is not available to any other process. You would use this for (say) loop variables, temporary variables etc. An important exception to this rule is that if you define the variable while running a command, that variable will be available to child processes. For example\nMY_VAR=yay node my-script.js\n\nIn this case MY_VAR will be available to the node process running my-script.\nIt's important to note that exporting a variable doesn't make it available to parent processes. That is, specifying and exporting a variable in a spawned process doesn't make it available in the process that launched it.",
    "tag": "bash"
  },
  {
    "question": "How to reload .bash_profile from the command line",
    "answer": "Simply type source ~/.bash_profile\nAlternatively, if you like saving keystrokes, you can type . ~/.bash_profile",
    "tag": "bash"
  },
  {
    "question": "How do I clear/delete the current line in terminal?",
    "answer": "You can use Ctrl+U to clear up to the beginning.\nYou can use Ctrl+W to delete just a word.\nYou can also use Ctrl+C to cancel.\nIf you want to keep the history, you can use Alt+Shift+# to make it a comment.\n\nBash Emacs Editing Mode Cheat Sheet",
    "tag": "bash"
  },
  {
    "question": "How do I pause my shell script for a second before continuing?",
    "answer": "Use the sleep command.\nExample:\nsleep .5 # Waits 0.5 second.\nsleep 5  # Waits 5 seconds.\nsleep 5s # Waits 5 seconds.\nsleep 5m # Waits 5 minutes.\nsleep 5h # Waits 5 hours.\nsleep 5d # Waits 5 days.\n\nOne can also employ decimals when specifying a time unit; e.g. sleep 1.5s",
    "tag": "bash"
  },
  {
    "question": "Set environment variables from file of key/value pairs",
    "answer": "This might be helpful:\nexport $(cat .env | xargs) && rails c\n\nReason why I use this is if I want to test .env stuff in my rails console.\ngabrielf came up with a good way to keep the variables local.  This solves the potential problem when going from project to project.\nenv $(cat .env | xargs) rails\n\nI've tested this with bash 3.2.51(1)-release\n\nUpdate:\nTo ignore lines that start with #, use this (thanks to Pete's comment):\nexport $(grep -v '^#' .env | xargs)\n\nAnd if you want to unset all of the variables defined in the file, use this:\nunset $(grep -v '^#' .env | sed -E 's/(.*)=.*/\\1/' | xargs)\n\n\nUpdate:\nTo also handle values with spaces, use:\nexport $(grep -v '^#' .env | xargs -d '\\n')\n\non GNU systems -- or:\nexport $(grep -v '^#' .env | xargs -0)\n\non BSD systems.\n\nFrom this answer you can auto-detect the OS with this:\nexport-env.sh\n#!/bin/sh\n\n## Usage:\n##   . ./export-env.sh ; $COMMAND\n##   . ./export-env.sh ; echo ${MINIENTREGA_FECHALIMITE}\n\nunamestr=$(uname)\nif [ \"$unamestr\" = 'Linux' ]; then\n\n  export $(grep -v '^#' .env | xargs -d '\\n')\n\nelif [ \"$unamestr\" = 'FreeBSD' ] || [ \"$unamestr\" = 'Darwin' ]; then\n\n  export $(grep -v '^#' .env | xargs -0)\n\nfi",
    "tag": "bash"
  },
  {
    "question": "In Bash, how can I check if a string begins with some value?",
    "answer": "This snippet on the Advanced Bash Scripting Guide says:\n# The == comparison operator behaves differently within a double-brackets\n# test than within single brackets.\n\n[[ $a == z* ]]   # True if $a starts with a \"z\" (wildcard matching).\n[[ $a == \"z*\" ]] # True if $a is equal to z* (literal matching).\n\nSo you had it nearly correct; you needed double brackets, not single brackets.\n\nWith regards to your second question, you can write it this way:\nHOST=user1\nif  [[ $HOST == user1 ]] || [[ $HOST == node* ]] ;\nthen\n    echo yes1\nfi\n\nHOST=node001\nif [[ $HOST == user1 ]] || [[ $HOST == node* ]] ;\nthen\n    echo yes2\nfi\n\nWhich will echo\nyes1\nyes2\n\nBash's if syntax is hard to get used to (IMO).",
    "tag": "bash"
  },
  {
    "question": "How to iterate over arguments in a Bash script",
    "answer": "Use \"$@\" to represent all the arguments:\nfor var in \"$@\"\ndo\n    echo \"$var\"\ndone\n\nThis will iterate over each argument and print it out on a separate line.  $@ behaves like $* except that, when quoted, the arguments are broken up properly if there are spaces in them:\nsh test.sh 1 2 '3 4'\n1\n2\n3 4",
    "tag": "bash"
  },
  {
    "question": "Extract substring in Bash",
    "answer": "You can use Parameter Expansion to do this.\nIf a is constant, the following parameter expansion performs substring extraction:\nb=${a:12:5}\n\nwhere 12 is the offset (zero-based) and 5 is the length\nIf the underscores around the digits are the only ones in the input, you can strip off the prefix and suffix (respectively) in two steps:\ntmp=${a#*_}   # remove prefix ending in \"_\"\nb=${tmp%_*}   # remove suffix starting with \"_\"\n\nIf there are other underscores, it's probably feasible anyway, albeit more tricky.  If anyone knows how to perform both expansions in a single expression, I'd like to know too.\nBoth solutions presented are pure bash, with no process spawning involved, hence very fast.",
    "tag": "bash"
  },
  {
    "question": "How to call one shell script from another shell script?",
    "answer": "There are a couple of different ways you can do this:\n\nMake the other script executable with chmod a+x /path/to/file, add the #!/bin/bash line (called shebang) at the top, and the path where the file is to the $PATH environment variable. Then you can call it as a normal command;\n\nOr call it with the source command (which is an alias for .), like this:\nsource /path/to/script\n\n\nOr use the bash command to execute it, like:\n/bin/bash /path/to/script\n\n\n\nThe first and third approaches execute the script as another process, so variables and functions in the other script will not be accessible.\nThe second approach executes the script in the first script's process, and pulls in variables and functions from the other script (so they are usable from the calling script). It will of course run all the commands in the other script, not only set variables.\nIn the second method, if you are using exit in second script, it will exit the first script as well. Which will not happen in first and third methods.",
    "tag": "bash"
  },
  {
    "question": "What does 'set -e' mean in a Bash script?",
    "answer": "From help set and Bash Reference Documentation: The Set Builtin:\n  -e  Exit immediately if a command exits with a non-zero status.\n\nBut it's considered bad practice by some (Bash FAQ and IRC Freenode #bash FAQ authors). It's recommended to use:\ntrap 'do_something' ERR\n\nto run do_something function when errors occur.\nSee Why doesn't set -e (or set -o errexit, or trap ERR) do what I expected?",
    "tag": "bash"
  },
  {
    "question": "Get current directory or folder name (without the full path)",
    "answer": "No need for basename, and especially no need for a subshell running pwd (which adds an extra, and expensive, fork operation); the shell can do this internally using parameter expansion:\nresult=${PWD##*/}          # to assign to a variable\nresult=${result:-/}        # to correct for the case where PWD is / (root)\n\nprintf '%s\\n' \"${PWD##*/}\" # to print to stdout\n                           # ...more robust than echo for unusual names\n                           #    (consider a directory named -e or -n)\n\nprintf '%q\\n' \"${PWD##*/}\" # to print to stdout, quoted for use as shell input\n                           # ...useful to make hidden characters readable.\n\n\nNote that if you're applying this technique in other circumstances (not PWD, but some other variable holding a directory name), you might need to trim any trailing slashes. The below uses bash's extglob support to work even with multiple trailing slashes:\ndirname=/path/to/somewhere//\nshopt -s extglob           # enable +(...) glob syntax\nresult=${dirname%%+(/)}    # trim however many trailing slashes exist\nresult=${result##*/}       # remove everything before the last / that still remains\nresult=${result:-/}        # correct for dirname=/ case\nprintf '%s\\n' \"$result\"\n\nAlternatively, without extglob:\ndirname=\"/path/to/somewhere//\"\nresult=\"${dirname%\"${dirname##*[!/]}\"}\" # extglob-free multi-trailing-/ trim\nresult=\"${result##*/}\"                  # remove everything before the last /\nresult=${result:-/}                     # correct for dirname=/ case",
    "tag": "bash"
  },
  {
    "question": "How do I use sudo to redirect output to a location I don't have permission to write to?",
    "answer": "Your command does not work because the redirection is performed by your shell which does not have the permission to write to /root/test.out. The redirection of the output is not performed by sudo.\nThere are multiple solutions:\n\nRun a shell with sudo and give the command to it by using the -c option:\nsudo sh -c 'ls -hal /root/ > /root/test.out'\n\nCreate a script with your commands and run that script with sudo:\n#!/bin/sh\nls -hal /root/ > /root/test.out\n\nRun sudo ls.sh. See Steve Bennett's answer if you don't want to create a temporary file.\nLaunch a shell with sudo -s then run your commands:\n[nobody@so]$ sudo -s\n[root@so]# ls -hal /root/ > /root/test.out\n[root@so]# ^D\n[nobody@so]$\n\nUse sudo tee (if you have to escape a lot when using the -c option):\nsudo ls -hal /root/ | sudo tee /root/test.out > /dev/null\n\nThe redirect to /dev/null is needed to stop tee from outputting to the screen. To append instead of overwriting the output file \n(>>), use tee -a or tee --append (the last one is specific to GNU coreutils).\n\nThanks go to Jd, Adam J. Forster and Johnathan for the second, third and fourth solutions.",
    "tag": "bash"
  },
  {
    "question": "Recursively counting files in a Linux directory",
    "answer": "This should work:\nfind DIR_NAME -type f | wc -l\n\nExplanation:\n\n-type f to include only files.\n| (and not ¦) redirects find command's standard output to wc command's standard input.\nwc (short for word count) counts newlines, words and bytes on its input (docs).\n-l to count just newlines.\n\nNotes: \n\nReplace DIR_NAME with . to execute the command in the current folder.\nYou can also remove the -type f to include directories (and symlinks) in the count.\nIt's possible this command will overcount if filenames can contain newline characters.\n\nExplanation of why your example does not work:\nIn the command you showed, you do not use the \"Pipe\" (|) to kind-of connect two commands, but the broken bar (¦) which the shell does not recognize as a command or something similar. That's why you get that error message.",
    "tag": "bash"
  },
  {
    "question": "How do I put an already-running process under nohup?",
    "answer": "Using the Job Control of bash to send the process into the background:\n\nCtrl+Z to stop (pause) the program and get back to the shell.\nbg to run it in the background.\ndisown -h [job-spec] where [job-spec] is the job number (like %1 for the first running job; find about your number with the jobs command) so that the job isn't killed when the terminal closes.",
    "tag": "bash"
  },
  {
    "question": "How to add line break to 'git commit -m' from the command line?",
    "answer": "Certainly, how it's done depends on your shell. In Bash, you can use single quotes around the message and can just leave the quote open, which will make Bash prompt for another line, until you close the quote. Like this:\ngit commit -m 'Message\n\ngoes\nhere'\n\nAlternatively, you can use a \"here document\" (also known as heredoc):\ngit commit -F- <<EOF\nMessage\n\ngoes\nhere\nEOF",
    "tag": "bash"
  },
  {
    "question": "How to permanently set $PATH on Linux/Unix",
    "answer": "You need to add it to your ~/.profile or ~/.bashrc file. \nexport PATH=\"$PATH:/path/to/dir\"\n\nDepending on what you're doing, you also may want to symlink to binaries:\ncd /usr/bin\nsudo ln -s /path/to/binary binary-name\n\nNote that this will not automatically update your path for the remainder of the session. To do this, you should run:\nsource ~/.profile \nor\nsource ~/.bashrc",
    "tag": "bash"
  },
  {
    "question": "Count number of lines in a git repository",
    "answer": "xargs will let you cat all the files together before passing them to wc, like you asked:\ngit ls-files | xargs cat | wc -l\n\nBut skipping the intermediate cat gives you more information and is probably better:\ngit ls-files | xargs wc -l",
    "tag": "bash"
  },
  {
    "question": "How does \"cat << EOF\" work in bash?",
    "answer": "The cat <<EOF syntax is very useful when working with multi-line text in Bash, eg. when assigning multi-line string to a shell variable, file or a pipe.\nExamples of cat <<EOF syntax usage in Bash:\n1. Assign multi-line string to a shell variable\n$ sql=$(cat <<EOF\nSELECT foo, bar FROM db\nWHERE foo='baz'\nEOF\n)\n\nThe $sql variable now holds the new-line characters too. You can verify with echo -e \"$sql\".\n2. Pass multi-line string to a file in Bash\n$ cat <<EOF > print.sh\n#!/bin/bash\necho \\$PWD\necho $PWD\nEOF\n\nThe print.sh file now contains:\n#!/bin/bash\necho $PWD\necho /home/user\n\n3. Pass multi-line string to a pipe in Bash\n$ cat <<EOF | grep 'b' | tee b.txt\nfoo\nbar\nbaz\nEOF\n\nThe b.txt file contains bar and baz lines. The same output is printed to stdout.",
    "tag": "bash"
  },
  {
    "question": "How to pass all arguments passed to my Bash script to a function of mine?",
    "answer": "The $@ variable expands to all command-line parameters separated by spaces. Here is an example.\nabc \"$@\"\n\nWhen using $@, you should (almost) always put it in double-quotes to avoid misparsing of arguments containing spaces or wildcards (see below). This works for multiple arguments. It is also portable to all POSIX-compliant shells.\nIt is also worth noting that $0 (generally the script's name or path) is not in $@.\nThe Bash Reference Manual Special Parameters Section says that $@ expands to the positional parameters starting from one. When the expansion occurs within double quotes, each parameter expands to a separate word. That is \"$@\" is equivalent to \"$1\" \"$2\" \"$3\"....\nPassing some arguments:\nIf you want to pass all but the first arguments, you can first use shift to \"consume\" the first argument and then pass \"$@\" to pass the remaining arguments to another command. In Bash (and zsh and ksh, but not in plain POSIX shells like dash), you can do this without messing with the argument list using a variant of array slicing: \"${@:3}\" will get you the arguments starting with \"$3\". \"${@:3:4}\" will get you up to four arguments starting at \"$3\" (i.e. \"$3\" \"$4\" \"$5\" \"$6\"), if that many arguments were passed.\nThings you probably don't want to do:\n\"$*\" gives all of the arguments stuck together into a single string (separated by spaces, or whatever the first character of $IFS is). This loses the distinction between spaces within arguments and the spaces between arguments, so is generally a bad idea. Although it might be ok for printing the arguments, e.g. echo \"$*\", provided you don't care about preserving the space within/between distinction.\nAssigning the arguments to a regular variable (as in args=\"$@\") mashes all the arguments together like \"$*\" does. If you want to store the arguments in a variable, use an array with args=(\"$@\") (the parentheses make it an array), and then reference them as e.g. \"${args[0]}\" etc. Note that in Bash and ksh, array indexes start at 0, so $1 will be in args[0], etc. zsh, on the other hand, starts array indexes at 1, so $1 will be in args[1]. And more basic shells like dash don't have arrays at all.\nLeaving off the double-quotes, with either $@ or $*, will try to split each argument up into separate words (based on whitespace or whatever's in $IFS), and also try to expand anything that looks like a filename wildcard into a list of matching filenames. This can have really weird effects, and should almost always be avoided. (Except in zsh, where this expansion doesn't take place by default.)",
    "tag": "bash"
  },
  {
    "question": "Given two directory trees, how can I find out which files differ by content?",
    "answer": "Try:\ndiff --brief --recursive dir1/ dir2/\n\nOr alternatively, with the short flags -qr:\ndiff -qr dir1/ dir2/\n\nIf you also want to see differences for files that may not exist in either directory:\ndiff --brief --recursive --new-file dir1/ dir2/  # with long options\ndiff -qrN dir1/ dir2/                            # with short flag aliases",
    "tag": "bash"
  },
  {
    "question": "Add a new element to an array without specifying the index in Bash",
    "answer": "Yes there is:\nARRAY=()\nARRAY+=('foo')\nARRAY+=('bar')\n\nBash Reference Manual:\n\nIn the context where an assignment statement is assigning a value to a shell variable or array index (see Arrays), the ‘+=’ operator can be used to append to or add to the variable's previous value.\n\nAlso:\n\nWhen += is applied to an  array  variable  using  compound  assignment  (see  Arrays below), the variable's value is not unset (as it is when using =), and new values are appended to the array beginning at one greater than the array's maximum index (for indexed arrays)",
    "tag": "bash"
  },
  {
    "question": "How can I exclude all \"permission denied\" messages from \"find\"?",
    "answer": "Use:\nfind . 2>/dev/null > files_and_folders\n\nThis hides not just the Permission denied errors, of course, but all error messages.\nIf you really want to keep other possible errors, such as too many hops on a symlink, but not the permission denied ones, then you'd probably have to take a flying guess that you don't have many files called 'permission denied' and try:\nfind . 2>&1 | grep -v 'Permission denied' > files_and_folders\n\n\nIf you strictly want to filter just standard error, you can use the more elaborate construction:\nfind . 2>&1 > files_and_folders | grep -v 'Permission denied' >&2\n\nThe I/O redirection on the find command is: 2>&1 > files_and_folders |.\nThe pipe redirects standard output to the grep command and is applied first.  The 2>&1 sends standard error to the same place as standard output (the pipe). The > files_and_folders sends standard output (but not standard error) to a file.  The net result is that messages written to standard error are sent down the pipe and the regular output of find is written to the file.  The grep filters the standard output (you can decide how selective you want it to be, and may have to change the spelling depending on locale and O/S) and the final >&2 means that the surviving error messages (written to standard output) go to standard error once more. The final redirection could be regarded as optional at the terminal, but would be a very good idea to use it in a script so that error messages appear on standard error.\nThere are endless variations on this theme, depending on what you want to do.  This will work on any variant of Unix with any Bourne shell derivative (Bash, Korn, …) and any POSIX-compliant version of find.\nIf you wish to adapt to the specific version of find you have on your system, there may be alternative options available.  GNU find in particular has a myriad options not available in other versions — see the currently accepted answer for one such set of options.",
    "tag": "bash"
  },
  {
    "question": "When do we need curly braces around shell variables?",
    "answer": "In your particular example, it makes no difference. However, the {} in ${} are useful if you want to expand the variable foo in the string\n\"${foo}bar\"\n\nsince \"$foobar\" would instead expand the variable identified by foobar.\nCurly braces are also unconditionally required when:\n\nexpanding array elements, as in ${array[42]}\nusing parameter expansion operations, as in ${filename%.*} (remove extension; strips smallest match)\nexpanding positional parameters beyond 9: \"$8 $9 ${10} ${11}\"\n\nDoing this everywhere, instead of just in potentially ambiguous cases, can be considered good programming practice. This is both for consistency and to avoid surprises like $foo_$bar.jpg, where it's not visually obvious that the underscore becomes part of the variable name.",
    "tag": "bash"
  },
  {
    "question": "How can I write a heredoc to a file in Bash script?",
    "answer": "Read the Advanced Bash-Scripting Guide — Chapter 19. Here Documents and Bash Reference Manual — Redirections: Here Documents.\nHere's an example which will write the contents to a file at /tmp/yourfilehere\ncat << EOF > /tmp/yourfilehere\nThese contents will be written to the file.\n        This line is indented.\nEOF\n\nNote that the final 'EOF' (The LimitString) should not have any whitespace in front of the word, because it means that the LimitString will not be recognized.\nIn a shell script, you may want to use indentation to make the code readable, however this can have the undesirable effect of indenting the text within your here document. In this case, use <<- (followed by a dash) to disable leading tabs (Note that to test this you will need to replace the leading whitespace with a tab character, since I cannot print actual tab characters here.)\n#!/usr/bin/env bash\n    \nif true ; then\n    cat <<- EOF > /tmp/yourfilehere\n    The leading tab is ignored.\n    EOF\nfi\n\nIf you don't want to interpret variables in the text, then use single quotes:\ncat << 'EOF' > /tmp/yourfilehere\nThe variable $FOO will not be interpreted.\nEOF\n\nTo pipe the heredoc through a command pipeline:\ncat <<'EOF' |  sed 's/a/b/'\nfoo\nbar\nbaz\nEOF\n\nOutput:\nfoo\nbbr\nbbz\n\n... or to write the the heredoc to a file using sudo:\ncat <<'EOF' |  sed 's/a/b/' | sudo tee /etc/config_file.conf\nfoo\nbar\nbaz\nEOF",
    "tag": "bash"
  },
  {
    "question": "Check number of arguments passed to a Bash script",
    "answer": "Just like any other simple command, [ ... ] or test requires spaces between its arguments.\nif [ \"$#\" -ne 1 ]; then\n    echo \"Illegal number of parameters\"\nfi\n\nOr\nif test \"$#\" -ne 1; then\n    echo \"Illegal number of parameters\"\nfi\n\nSuggestions\nWhen in Bash, prefer using [[ ]] instead as it doesn't do word splitting and pathname expansion to its variables that quoting may not be necessary unless it's part of an expression.\n[[ $# -ne 1 ]]\n\nIt also has some other features like unquoted condition grouping, pattern matching (extended pattern matching with extglob) and regex matching.\nThe following example checks if arguments are valid. It allows a single argument or two.\n[[ ($# -eq 1 || ($# -eq 2 && $2 == <glob pattern>)) && $1 =~ <regex pattern> ]]\n\nFor pure arithmetic expressions, using (( )) to some may still be better, but they are still possible in [[ ]] with its arithmetic operators like -eq, -ne, -lt, -le, -gt, or -ge by placing the expression as a single string argument:\nA=1\n[[ 'A + 1' -eq 2 ]] && echo true  ## Prints true.\n\nThat should be helpful if you would need to combine it with other features of [[ ]] as well.\nTake note that [[ ]] and (( )) are keywords which have same level of parsing as if, case, while, and for.\nAlso as Dave suggested, error messages are better sent to stderr so they don't get included when stdout is redirected:\necho \"Illegal number of parameters\" >&2\n\nExiting the script\nIt's also logical to make the script exit when invalid parameters are passed to it.  This has already been suggested in the comments by ekangas but someone edited this answer to have it with -1 as the returned value, so I might as well do it right.\n-1 though accepted by Bash as an argument to exit is not explicitly documented and is not right to be used as a common suggestion.  64 is also the most formal value since it's defined in sysexits.h with #define EX_USAGE 64 /* command line usage error */.  Most tools like ls also return 2 on invalid arguments.  I also used to return 2 in my scripts but lately I no longer really cared, and simply used 1 in all errors.  But let's just place 2 here since it's most common and probably not OS-specific.\nif [[ $# -ne 1 ]]; then\n    echo \"Illegal number of parameters\" >&2\n    exit 2\nfi\n\nReferences\n\nBash Conditional Expressions\nConditional Constructs\nPattern Matching\nWord Splitting\nFilename Expansion (prev. Pathname Expansion)\nSimple Commands",
    "tag": "bash"
  },
  {
    "question": "Read a file line by line assigning the value to a variable",
    "answer": "The following reads a file passed as an argument line by line:\nwhile IFS= read -r line; do\n    echo \"Text read from file: $line\"\ndone < my_filename.txt\n\nThis is the standard form for reading lines from a file in a loop. Explanation:\n\nIFS= (or IFS='') prevents leading/trailing whitespace from being trimmed.\n-r prevents backslash escapes from being interpreted.\n\nOr you can put it in a bash file helper script, example contents:\n#!/bin/bash\nwhile IFS= read -r line; do\n    echo \"Text read from file: $line\"\ndone < \"$1\"\n\nIf the above is saved to a script with filename readfile, it can be run as follows:\nchmod +x readfile\n./readfile filename.txt\n\nIf the file isn’t a standard POSIX text file (= not terminated by a newline character), the loop can be modified to handle trailing partial lines:\nwhile IFS= read -r line || [[ -n \"$line\" ]]; do\n    echo \"Text read from file: $line\"\ndone < \"$1\"\n\nHere, || [[ -n $line ]] prevents the last line from being ignored if it doesn't end with a \\n (since read returns a non-zero exit code when it encounters EOF).\nIf the commands inside the loop also read from standard input, the file descriptor used by read can be chanced to something else (avoid the standard file descriptors), e.g.:\nwhile IFS= read -r -u3 line; do\n    echo \"Text read from file: $line\"\ndone 3< \"$1\"\n\n(Non-Bash shells might not know read -u3; use read <&3 instead.)",
    "tag": "bash"
  },
  {
    "question": "In a Bash script, how can I exit the entire script if a certain condition occurs?",
    "answer": "Try this statement:\nexit 1\n\nReplace 1 with appropriate error codes. See also Exit Codes With Special Meanings.",
    "tag": "bash"
  },
  {
    "question": "How to split a string into an array in Bash?",
    "answer": "IFS=', ' read -r -a array <<< \"$string\"\n\nNote that the characters in $IFS are treated individually as separators so that in this case fields may be separated by either a comma or a space rather than the sequence of the two characters. Interestingly though, empty fields aren't created when comma-space appears in the input because the space is treated specially.\nTo access an individual element:\necho \"${array[0]}\"\n\nTo iterate over the elements:\nfor element in \"${array[@]}\"\ndo\n    echo \"$element\"\ndone\n\nTo get both the index and the value:\nfor index in \"${!array[@]}\"\ndo\n    echo \"$index ${array[index]}\"\ndone\n\nThe last example is useful because Bash arrays are sparse. In other words, you can delete an element or add an element and then the indices are not contiguous.\nunset \"array[1]\"\narray[42]=Earth\n\nTo get the number of elements in an array:\necho \"${#array[@]}\"\n\nAs mentioned above, arrays can be sparse so you shouldn't use the length to get the last element. Here's how you can in Bash 4.2 and later:\necho \"${array[-1]}\"\n\nin any version of Bash (from somewhere after 2.05b):\necho \"${array[@]: -1:1}\"\n\nLarger negative offsets select farther from the end of the array. Note the space before the minus sign in the older form. It is required.",
    "tag": "bash"
  },
  {
    "question": "How can I compare numbers in Bash?",
    "answer": "In Bash, you should do your check in an arithmetic context:\nif (( a > b )); then\n    ...\nfi\n\nFor POSIX shells that don't support (()), you can use -lt and -gt.\nif [ \"$a\" -gt \"$b\" ]; then\n    ...\nfi\n\nYou can get a full list of comparison operators with help test or man test.",
    "tag": "bash"
  },
  {
    "question": "What are the special dollar sign shell variables?",
    "answer": "$1, $2, $3, ... are the positional parameters.\n\"$@\" is an array-like construct of all positional parameters, {$1, $2, $3 ...}.\n\"$*\" is the IFS expansion of all positional parameters, $1 $2 $3 ....\n$# is the number of positional parameters.\n$- current options set for the shell.\n$$ pid of the current shell (not subshell).\n$_ most recent parameter (or the abs path of the command to start the current shell immediately after startup).\n$IFS is the (input) field separator.\n$? is the most recent foreground pipeline exit status.\n$! is the PID of the most recent background command.\n$0 is the name of the shell or shell script.\n\nMost of the above can be found under Special Parameters in the Bash Reference Manual. Here are all the environment variables set by the shell.\nFor a comprehensive index, please see the Reference Manual Variable Index.",
    "tag": "bash"
  },
  {
    "question": "Reliable way for a Bash script to get the full path to itself",
    "answer": "Here's what I've come up with (edit: plus some tweaks provided by sfstewman, levigroker, Kyle Strand, and Rob Kennedy), that seems to mostly fit my \"better\" criteria:\nSCRIPTPATH=\"$( cd -- \"$(dirname \"$0\")\" >/dev/null 2>&1 ; pwd -P )\"\n\nThat SCRIPTPATH line seems particularly roundabout, but we need it rather than SCRIPTPATH=`pwd`  in order to properly handle spaces and symlinks.\nThe inclusion of output redirection (>/dev/null 2>&1) handles the rare(?) case where cd might produce output that would interfere with the surrounding $( ... ) capture. (Such as cd being overridden to also ls a directory after switching to it.)\nNote also that esoteric situations, such as executing a script that isn't coming from a file in an accessible file system at all (which is perfectly possible), is not catered to there (or in any of the other answers I've seen).\nThe -- after cd and before \"$0\" are in case the directory starts with a -.",
    "tag": "bash"
  },
  {
    "question": "How can I do a recursive find/replace of a string with awk or sed?",
    "answer": "find /home/www \\( -type d -name .git -prune \\) -o -type f -print0 | xargs -0 sed -i 's/subdomainA\\.example\\.com/subdomainB.example.com/g'\n\n-print0 tells find to print each of the results separated by a null character, rather than a new line. In the unlikely event that your directory has files with newlines in the names, this still lets xargs work on the correct filenames.\n\\( -type d -name .git -prune \\) is an expression which completely skips over all directories named .git. You could easily expand it, if you use SVN or have other folders you want to preserve -- just match against more names. It's roughly equivalent to -not -path .git, but more efficient, because rather than checking every file in the directory, it skips it entirely. The -o after it is required because of how -prune actually works.\nFor more information, see man find.",
    "tag": "bash"
  },
  {
    "question": "Are double square brackets [[ ]] preferable over single square brackets [ ] in Bash?",
    "answer": "[[ has fewer surprises and is generally safer to use. But it is not portable - POSIX doesn't specify what it does and only some shells support it (beside bash, I heard ksh supports it too). For example, you can do\n[[ -e $b ]]\n\nto test whether a file exists. But with [, you have to wrap $b in double-quotes, because otherwise it splits the argument and expands things like \"a*\" (where [[ takes it literally). That has also to do with how [ can be an external program and receives its argument just normally like every other program (although it can also be a builtin, but then it still has not this special handling).\n[[ also has some other nice features, like regular expression matching with =~ along with operators like they are known in C-like languages. Here is a good page about it: What is the difference between test, [ and [[ ? and Bash Tests",
    "tag": "bash"
  },
  {
    "question": "Difference between single and double quotes in Bash",
    "answer": "Single quotes won't interpolate anything, but double quotes will. For example: variables, backticks, certain \\ escapes, etc. \nExample:\n$ echo \"$(echo \"upg\")\"\nupg\n$ echo '$(echo \"upg\")'\n$(echo \"upg\")\n\nThe Bash manual has this to say:\n\n3.1.2.2 Single Quotes\nEnclosing characters in single quotes (') preserves the literal value of each character within the quotes. A single quote may not occur between single quotes, even when preceded by a backslash. \n3.1.2.3 Double Quotes\nEnclosing characters in double quotes (\") preserves the literal value of all characters within the quotes, with the exception of $, `, \\, and, when history expansion is enabled, !. The characters $ and ` retain their special meaning within double quotes (see Shell Expansions). The backslash retains its special meaning only when followed by one of the following characters: $, `, \", \\, or newline. Within double quotes, backslashes that are followed by one of these characters are removed. Backslashes preceding characters without a special meaning are left unmodified. A double quote may be quoted within double quotes by preceding it with a backslash. If enabled, history expansion will be performed unless an ! appearing in double quotes is escaped using a backslash. The backslash preceding the ! is not removed.\nThe special parameters * and @ have special meaning when in double quotes (see Shell Parameter Expansion).",
    "tag": "bash"
  },
  {
    "question": "Setting environment variables on OS X",
    "answer": "Bruno is right on track. I've done extensive research and if you want to set variables that are available in all GUI applications, your only option is /etc/launchd.conf.\nPlease note that environment.plist does not work for applications launched via Spotlight. This is documented by Steve Sexton here.\n\nOpen a terminal prompt\nType sudo vi /etc/launchd.conf (note: this file might not yet exist)\nPut contents like the following into the file\n# Set environment variables here so they are available globally to all apps\n# (and Terminal), including those launched via Spotlight.\n#\n# After editing this file run the following command from the terminal to update\n# environment variables globally without needing to reboot.\n# NOTE: You will still need to restart the relevant application (including\n# Terminal) to pick up the changes!\n# grep -E \"^setenv\" /etc/launchd.conf | xargs -t -L 1 launchctl\n#\n# See http://www.digitaledgesw.com/node/31\n# and http://stackoverflow.com/questions/135688/setting-environment-variables-in-os-x/\n#\n# Note that you must hardcode the paths below, don't use environment variables.\n# You also need to surround multiple values in quotes, see MAVEN_OPTS example below.\n#\nsetenv JAVA_VERSION 1.6\nsetenv JAVA_HOME /System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home\nsetenv GROOVY_HOME /Applications/Dev/groovy\nsetenv GRAILS_HOME /Applications/Dev/grails\nsetenv NEXUS_HOME /Applications/Dev/nexus/nexus-webapp\nsetenv JRUBY_HOME /Applications/Dev/jruby\n\nsetenv ANT_HOME /Applications/Dev/apache-ant\nsetenv ANT_OPTS -Xmx512M\n\nsetenv MAVEN_OPTS \"-Xmx1024M -XX:MaxPermSize=512m\"\nsetenv M2_HOME /Applications/Dev/apache-maven\n\nsetenv JMETER_HOME /Applications/Dev/jakarta-jmeter\n\nSave your changes in vi and reboot your Mac. Or use the grep/xargs command which is shown in the code comment above.\nProve that your variables are working by opening a Terminal window and typing export and you should see your new variables.  These will also be available in IntelliJ IDEA and other GUI applications you launch via Spotlight.",
    "tag": "bash"
  },
  {
    "question": "Redirect all output to file in Bash",
    "answer": "That part is written to stderr, use 2> to redirect it. For example:\nfoo > stdout.txt 2> stderr.txt\n\nor if you want in same file:\nfoo > allout.txt 2>&1\n\nNote: this works in (ba)sh, check your shell for proper syntax",
    "tag": "bash"
  },
  {
    "question": "Find all files containing a specific text (string) on Linux?",
    "answer": "Do the following:\ngrep -rnw '/path/to/somewhere/' -e 'pattern'\n\n\n-r or -R is recursive,\n-n is line number, and\n-w stands for match the whole word.\n-l (lower-case L) can be added to just give the file name of matching files.\n-e is the pattern used during the search\n\nAlong with these, --exclude, --include, --exclude-dir flags could be used for efficient searching:\n\nThis will only search through those files which have .c or .h extensions:\ngrep --include=\\*.{c,h} -rnw '/path/to/somewhere/' -e \"pattern\"\n\n\nThis will exclude searching all the files ending with .o extension:\ngrep --exclude=\\*.o -rnw '/path/to/somewhere/' -e \"pattern\"\n\n\nFor directories it's possible to exclude one or more directories using the --exclude-dir parameter. For example, this will exclude the dirs dir1/, dir2/ and all of them matching *.dst/:\ngrep --exclude-dir={dir1,dir2,*.dst} -rnw '/path/to/search/' -e \"pattern\"\n\n\n\nThis works very well for me, to achieve almost the same purpose like yours.\nFor more options, see man grep.",
    "tag": "grep"
  },
  {
    "question": "grep: show lines surrounding each match",
    "answer": "For BSD or GNU grep you can use -B num to set how many lines before the match and -A num for the number of lines after the match.\ngrep -B 3 -A 2 foo README.txt\n\nIf you want the same number of lines before and after you can use -C num.\ngrep -C 3 foo README.txt\n\nThis will show 3 lines before and 3 lines after.",
    "tag": "grep"
  },
  {
    "question": "How do I recursively grep all directories and subdirectories?",
    "answer": "grep -r \"texthere\" .\n\nThe first parameter represents the regular expression to search for, while the second one represents the directory that should be searched. In this case, . means the current directory.\nNote: This works for GNU grep, and on some platforms like Solaris you must specifically use GNU grep as opposed to legacy implementation.  For Solaris this is the ggrep command.",
    "tag": "grep"
  },
  {
    "question": "How to grep (search through) committed code in the Git history",
    "answer": "To search for commit content (i.e., actual lines of source, as opposed to commit messages and the like), you need to do:\ngit grep <regexp> $(git rev-list --all)\n\ngit rev-list --all | xargs git grep <expression> will work if you run into an \"Argument list too long\" error.\nIf you want to limit the search to some subtree (for instance, \"lib/util\"), you will need to pass that to the rev-list subcommand and grep as well:\ngit grep <regexp> $(git rev-list --all -- lib/util) -- lib/util\n\nThis will grep through all your commit text for regexp.\nThe reason for passing the path in both commands is because rev-list will return the revisions list where all the changes to lib/util happened, but also you need to pass to grep so that it will only search in lib/util.\nJust imagine the following scenario: grep might find the same <regexp> on other files which are contained in the same revision returned by rev-list (even if there was no change to that file on that revision).\nHere are some other useful ways of searching your source:\nSearch working tree for text matching regular expression regexp:\ngit grep <regexp>\n\nSearch working tree for lines of text matching regular expression regexp1 or regexp2:\ngit grep -e <regexp1> [--or] -e <regexp2>\n\nSearch working tree for lines of text matching regular expression regexp1 and regexp2, reporting file paths only:\ngit grep -l -e <regexp1> --and -e <regexp2>\n\nSearch working tree for files that have lines of text matching regular expression regexp1 and lines of text matching regular expression regexp2:\ngit grep -l --all-match -e <regexp1> -e <regexp2>\n\nSearch working tree for changed lines of text matching pattern:\ngit diff --unified=0 | grep <pattern>\n\nSearch all revisions for text matching regular expression regexp:\ngit grep <regexp> $(git rev-list --all)\n\nSearch all revisions between rev1 and rev2 for text matching regular expression regexp:\ngit grep <regexp> $(git rev-list <rev1>..<rev2>)",
    "tag": "grep"
  },
  {
    "question": "How can I use grep to show just filenames on Linux?",
    "answer": "The standard option grep -l (that is a lowercase L) could do this.\nFrom the Unix standard:\n-l\n    (The letter ell.) Write only the names of files containing selected\n    lines to standard output. Pathnames are written once per file searched.\n    If the standard input is searched, a pathname of (standard input) will\n    be written, in the POSIX locale. In other locales, standard input may be\n    replaced by something more appropriate in those locales.\n\nYou also do not need -H in this case.",
    "tag": "grep"
  },
  {
    "question": "How can I grep recursively, but only in files with certain extensions?",
    "answer": "Just use the --include parameter, like this:\ngrep -inr --include \\*.h --include \\*.cpp CP_Image ~/path[12345] | mailx -s GREP email@domain.example\n\nThat should do what you want.\nTo take the explanation from HoldOffHunger's answer below:\n\ngrep: command\n\n-r: recursively\n\n-i: ignore-case\n\n-n: each output line is preceded by its relative line number in the file\n\n--include \\*.cpp: all *.cpp: C++ files (escape with \\ just in case you have a directory with asterisks in the filenames)\n\n./: Start at current directory.",
    "tag": "grep"
  },
  {
    "question": "Negative matching using grep (match lines that do not contain foo)",
    "answer": "grep -v is your friend:\ngrep --help | grep invert  \n\n\n-v, --invert-match        select non-matching lines\n\nAlso check out the related -L (the complement of -l).\n\n-L, --files-without-match only print FILE names containing no match",
    "tag": "grep"
  },
  {
    "question": "How can I pipe stderr, and not stdout?",
    "answer": "First redirect stderr to stdout — the pipe; then redirect stdout to /dev/null (without changing where stderr is going):\ncommand 2>&1 >/dev/null | grep 'something'\n\nFor the details of I/O redirection in all its variety, see the chapter on Redirections in the Bash reference manual.\nNote that the sequence of I/O redirections is interpreted left-to-right, but pipes are set up before the I/O redirections are interpreted.  File descriptors such as 1 and 2 are references to open file descriptions.  The operation 2>&1 makes file descriptor 2 aka stderr refer to the same open file description as file descriptor 1 aka stdout is currently referring to (see dup2() and open()).  The operation >/dev/null then changes file descriptor 1 so that it refers to an open file description for /dev/null, but that doesn't change the fact that file descriptor 2 refers to the open file description which file descriptor 1 was originally pointing to — namely, the pipe.",
    "tag": "grep"
  },
  {
    "question": "How can I exclude directories from grep -R?",
    "answer": "Recent versions of GNU Grep (>= 2.5.2) provide:\n--exclude-dir=dir\n\nwhich excludes directories matching the pattern dir from recursive directory searches.\nSo you can do:\ngrep -R --exclude-dir=node_modules 'some pattern' /path/to/search\n\nFor a bit more information regarding syntax and usage see\n\nThe GNU man page for File and Directory Selection\nA related StackOverflow answer Use grep --exclude/--include syntax to not grep through certain files\n\nFor older GNU Greps and POSIX Grep, use find as suggested in other answers.\nOr just use ack (Edit: or The Silver Searcher) and be done with it!",
    "tag": "grep"
  },
  {
    "question": "Can grep show only words that match search pattern?",
    "answer": "Try grep -o:\ngrep -oh \"\\w*th\\w*\" *\n\nEdit: matching from Phil's comment.\nFrom the docs:\n-h, --no-filename\n    Suppress the prefixing of file names on output. This is the default\n    when there is only  one  file  (or only standard input) to search.\n-o, --only-matching\n    Print  only  the matched (non-empty) parts of a matching line,\n    with each such part on a separate output line.",
    "tag": "grep"
  },
  {
    "question": "Use grep --exclude/--include syntax to not grep through certain files",
    "answer": "Use the shell globbing syntax:\ngrep pattern -r --include=\\*.cpp --include=\\*.h rootdir\n\nThe syntax for --exclude is identical.\nNote that the star is escaped with a backslash to prevent it from being expanded by the shell (quoting it, such as --include=\"*.cpp\", would work just as well).  Otherwise, if you had any files in the current working directory that matched the pattern, the command line would expand to something like grep pattern -r --include=foo.cpp --include=bar.cpp rootdir, which would only search files named foo.cpp and bar.cpp, which is quite likely not what you wanted.\nUpdate 2021-03-04\nI've edited the original answer to remove the use of brace expansion, which is a feature provided by several shells such as Bash and zsh to simplify patterns like this; but note that brace expansion is not POSIX shell-compliant.\nThe original example was:\ngrep pattern -r --include=\\*.{cpp,h} rootdir\n\nto search through all .cpp and .h files rooted in the directory rootdir.",
    "tag": "grep"
  },
  {
    "question": "How to 'grep' a continuous stream?",
    "answer": "Turn on grep's line buffering mode when using BSD grep (FreeBSD, Mac OS X etc.)\ntail -f file | grep --line-buffered my_pattern\n\nIt looks like a while ago --line-buffered didn't matter for GNU grep (used on pretty much any Linux) as it flushed by default (YMMV for other Unix-likes such as SmartOS, AIX or QNX). However, as of November 2020, --line-buffered is needed (at least with GNU grep 3.5 in openSUSE, but it seems generally needed based on comments below).",
    "tag": "grep"
  },
  {
    "question": "How can I grep Git commits for a certain word?",
    "answer": "If you want to find all commits where the commit message contains a given word, use\ngit log --grep=word\n\nIf you want to find all commits where \"word\" was added or removed in the file contents (to be more exact: where the number of occurrences of \"word\" changed), i.e., search the commit contents, use a so-called 'pickaxe' search with\ngit log -Sword\n\nIn modern Git there is also\ngit log -Gword\n\nto look for differences whose added or removed line matches \"word\" (also commit contents).\nA few things to note:\n\n-G by default accepts a regex, while -S accepts a string, but it can be modified to accept regexes using the --pickaxe-regex.\n-S finds commits where the number of occurrences of \"word\" changed, while -G finds commits where \"word\" appears in the diff.\nThis means that -S<regex> --pickaxe-regex and -G<regex> do not do exactly the same thing.\n\nThe git diff documentation has a nice explanation of the difference:\n\nTo illustrate the difference between -S<regex> --pickaxe-regex and -G<regex>, consider a commit with the following diff in the same file:\n+    return frotz(nitfol, two->ptr, 1, 0);\n...\n-    hit = frotz(nitfol, mf2.ptr, 1, 0);\n\nWhile git log -G\"frotz\\(nitfol\" will show this commit, git log -S\"frotz\\(nitfol\" --pickaxe-regex will not (because the number of occurrences of that string did not change).\n\nThis will show the commits containing the search terms, but if you want to see the actual changes in those commits instead you can use --patch:\ngit log -G\"searchTerm\" --patch\n\nThis can then be piped to grep to isolate the output just to display commit diff lines with that search term.  A common use-case is to display diff lines with that search term in commits since and including a given commit - 3b5ab0f2a1 in this example - like so:\ngit log 3b5ab0f2a1^.. -G\"searchTerm\" --patch | grep searchTerm",
    "tag": "grep"
  },
  {
    "question": "How do I find files that do not contain a given string pattern?",
    "answer": "If your grep has the -L (or --files-without-match) option:\n$ grep -L \"foo\" *",
    "tag": "grep"
  },
  {
    "question": "Delete all local Git branches",
    "answer": "The 'git branch -d' subcommand can delete more than one branch.  So, simplifying @sblom's answer but adding a critical xargs:\ngit branch -D `git branch --merged | grep -v \\* | xargs`\n\nor, further simplified to:\ngit branch --merged | grep -v \\* | xargs git branch -D \n\nImportantly, as noted by @AndrewC, using git branch for scripting is discouraged.  To avoid it use something like:\ngit for-each-ref --format '%(refname:short)' refs/heads | grep -v \"master\\|main\" | xargs git branch -D\n\nCaution warranted on deletes!\n$ mkdir br\n$ cd br; git init\nInitialized empty Git repository in /Users/ebg/test/br/.git/\n$ touch README; git add README; git commit -m 'First commit'\n[master (root-commit) 1d738b5] First commit\n 0 files changed, 0 insertions(+), 0 deletions(-)\n create mode 100644 README\n$ git branch Story-123-a\n$ git branch Story-123-b\n$ git branch Story-123-c\n$ git branch --merged\n  Story-123-a\n  Story-123-b\n  Story-123-c\n* master\n$ git branch --merged | grep -v \\* | xargs\nStory-123-a Story-123-b Story-123-c\n$ git branch --merged | grep -v \\* | xargs git branch -D\nDeleted branch Story-123-a (was 1d738b5).\nDeleted branch Story-123-b (was 1d738b5).\nDeleted branch Story-123-c (was 1d738b5).",
    "tag": "grep"
  },
  {
    "question": "How can I use grep to find a word inside a folder?",
    "answer": "grep -nr 'yourString*' .\n\nThe dot at the end searches the current directory. Meaning for each parameter:\n-n            Show relative line number in the file\n'yourString*' String for search, followed by a wildcard character\n-r            Recursively search subdirectories listed\n.             Directory for search (current directory)\n\ngrep -nr 'MobileAppSer*' .  (Would find MobileAppServlet.java or MobileAppServlet.class or MobileAppServlet.txt; 'MobileAppASer*.*' is another way to do the same thing.)\nTo check more parameters use man grep command.",
    "tag": "grep"
  },
  {
    "question": "Colorized grep -- viewing the entire file with highlighted matches",
    "answer": "Here are some ways to do it:\ngrep --color 'pattern\\|$' file\ngrep --color -E 'pattern|$' file\negrep --color 'pattern|$' file\n\nThe | symbol is the OR operator. Either escape it using \\ or tell grep that the search text has to be interpreted as regular expressions by adding -E or using the egrep command instead of grep.\nThe search text \"pattern|$\" is actually a trick, it will match lines that have pattern OR lines that have an end. Because all lines have an end, all lines are matched, but the end of a line isn't actually any characters, so it won't be colored.\nTo also pass the colored parts through pipes, e.g. towards less, provide the always parameter to --color:\ngrep --color=always 'pattern\\|$' file | less -r\ngrep --color=always -E 'pattern|$' file | less -r\negrep --color=always 'pattern|$' file | less -r",
    "tag": "grep"
  },
  {
    "question": "How can I exclude one word with grep?",
    "answer": "You can do it using -v (for --invert-match) option of grep as:\ngrep -v \"unwanted_word\" file | grep XXXXXXXX\n\ngrep -v \"unwanted_word\" file will filter the lines that have the unwanted_word and grep XXXXXXXX will list only lines with pattern XXXXXXXX.\nEDIT:\nFrom your comment it looks like you want to list all lines without the unwanted_word. In that case all you need is:\ngrep -v 'unwanted_word' file",
    "tag": "grep"
  },
  {
    "question": "Grep only the first match and stop",
    "answer": "-m 1 means return the first match in any given file. But it will still continue to search in other files. Also, if there are two or more matched in the same line, all of them will be displayed.\nYou can use head -1 to solve this problem:\ngrep -o -a -m 1 -h -r \"Pulsanti Operietur\" /path/to/dir | head -1\n\nexplanation of each grep option:\n-o, --only-matching, print only the matched part of the line (instead of the entire line)\n-a, --text, process a binary file as if it were text\n-m 1, --max-count, stop reading a file after 1 matching line\n-h, --no-filename, suppress the prefixing of file names on output\n-r, --recursive, read all files under a directory recursively",
    "tag": "grep"
  },
  {
    "question": "grep a tab in UNIX",
    "answer": "If using GNU grep, you can use the Perl-style regexp:\ngrep -P '\\t' *",
    "tag": "grep"
  },
  {
    "question": "How can I grep for a string that begins with a dash/hyphen?",
    "answer": "Use:\ngrep -- -X\nDocumentation\nRelated: What does a bare double dash mean? (thanks to nutty about natty).",
    "tag": "grep"
  },
  {
    "question": "Capturing Groups From a Grep RegEx",
    "answer": "If you're using Bash, you don't even have to use grep:\nfiles=\"*.jpg\"\nregex=\"[0-9]+_([a-z]+)_[0-9a-z]*\" # put the regex in a variable because some patterns won't work if included literally\nfor f in $files    # unquoted in order to allow the glob to expand\ndo\n    if [[ $f =~ $regex ]]\n    then\n        name=\"${BASH_REMATCH[1]}\"\n        echo \"${name}.jpg\"    # concatenate strings\n        name=\"${name}.jpg\"    # same thing stored in a variable\n    else\n        echo \"$f doesn't match\" >&2 # this could get noisy if there are a lot of non-matching files\n    fi\ndone\n\nIt's better to put the regex in a variable. Some patterns won't work if included literally.\nThis uses  =~ which is Bash's regex match operator. The results of the match are saved to an array called $BASH_REMATCH. The first capture group is stored in index 1, the second (if any) in index 2, etc. Index zero is the full match.\n\n\n\nside note #1 regarding regex anchors:\nYou should be aware that without anchors, this regex (and the one using grep) will match any of the following examples and more, which may not be what you're looking for:\n123_abc_d4e5\nxyz123_abc_d4e5\n123_abc_d4e5.xyz\nxyz123_abc_d4e5.xyz\n\nTo eliminate the second and fourth examples, make your regex like this:\n^[0-9]+_([a-z]+)_[0-9a-z]*\n\nwhich says the string must start with one or more digits. The carat represents the beginning of the string. If you add a dollar sign at the end of the regex, like this:\n^[0-9]+_([a-z]+)_[0-9a-z]*$\n\nthen the third example will also be eliminated since the dot is not among the characters in the regex and the dollar sign represents the end of the string. Note that the fourth example fails this match as well.\nside note #2 regarding grep and the \\K operator:\nIf you have GNU grep (around 2.5 or later, I think, when the \\K operator was added):\nname=$(echo \"$f\" | grep -Po '(?i)[0-9]+_\\K[a-z]+(?=_[0-9a-z]*)').jpg\n\nThe \\K operator (variable-length look-behind) causes the preceding pattern to match, but doesn't include the match in the result. The fixed-length equivalent is (?<=) - the pattern would be included before the closing parenthesis. You must use \\K if quantifiers may match strings of different lengths (e.g. +, *, {2,4}).\nThe (?=) operator matches fixed or variable-length patterns and is called \"look-ahead\". It also does not include the matched string in the result.\nIn order to make the match case-insensitive, the (?i) operator is used. It affects the patterns that follow it so its position is significant.\nThe regex might need to be adjusted depending on whether there are other characters in the filename. You'll note that in this case, I show an example of concatenating a string at the same time that the substring is captured.",
    "tag": "grep"
  },
  {
    "question": "How can I make grep print the lines below and above each matching line?",
    "answer": "grep's -A 1 option will give you one line after; -B 1 will give you one line before; and -C 1 combines both to give you one line both before and after, -1 does the same.",
    "tag": "grep"
  },
  {
    "question": "Get line number while using grep",
    "answer": "grep -n SEARCHTERM file1 file2 ...",
    "tag": "grep"
  },
  {
    "question": "How do I grep for all non-ASCII characters?",
    "answer": "You can use the command:\nLC_ALL=C  grep --color='auto' -P -n \"[\\x80-\\xFF]\" file.xml\n\nThis will give you the line number, and will highlight non-ascii chars in red.\nIn  some systems, depending on your settings, the above will not work, so you can grep by the inverse\nLC_ALL=C  grep --color='auto' -P -n \"[^\\x00-\\x7F]\" file.xml\n\nNote also, that the important bit is the -P flag which equates to --perl-regexp: so it will interpret your pattern as a Perl regular expression. It also says that\n\nthis is highly experimental and grep -P may warn of unimplemented\nfeatures.",
    "tag": "grep"
  },
  {
    "question": "How can I format my grep output to show line numbers at the end of the line, and also the hit count?",
    "answer": "-n returns line number.\n-i is for ignore-case. Only to be used if case matching is not necessary\n$ grep -in null myfile.txt\n\n2:example two null,\n4:example four null,\n\nCombine with awk to print out the line number after the match:\n$ grep -in null myfile.txt | awk -F: '{print $2\" - Line number : \"$1}'\n\nexample two null, - Line number : 2\nexample four null, - Line number : 4\n\nUse command substitution to print out the total null count:\n$ echo \"Total null count :\" $(grep -ic null myfile.txt)\n\nTotal null count : 2",
    "tag": "grep"
  },
  {
    "question": "How to check if a file contains a specific string using Bash",
    "answer": "if grep -q SomeString \"$File\"; then\n  Some Actions # SomeString was found\nfi\n\nYou don't need [[ ]] here. Just run the command directly. Add -q option when you don't need the string displayed when it was found.\nThe grep command returns 0 or 1 in the exit code depending on\nthe result of search. 0 if something was found; 1 otherwise.\n$ echo hello | grep hi ; echo $?\n1\n$ echo hello | grep he ; echo $?\nhello\n0\n$ echo hello | grep -q he ; echo $?\n0\n\nYou can specify commands as an condition of if. If the command returns 0 in its exitcode that means that the condition is true; otherwise false.\n$ if /bin/true; then echo that is true; fi\nthat is true\n$ if /bin/false; then echo that is true; fi\n$\n\nAs you can see you run here the programs directly. No additional [] or [[]].",
    "tag": "grep"
  },
  {
    "question": "How to perform grep operation on all files in a directory?",
    "answer": "In Linux, I normally use this command to recursively grep for a particular text within a directory:\ngrep -rni \"string\" *\n\nwhere\n\nr = recursive i.e, search subdirectories within the current directory\nn = to print the line numbers to stdout\ni = case insensitive search",
    "tag": "grep"
  },
  {
    "question": "Fast way of finding lines in one file that are not in another?",
    "answer": "The comm command (short for \"common\") may be useful comm - compare two sorted files line by line\n#find lines only in file1\ncomm -23 file1 file2 \n\n#find lines only in file2\ncomm -13 file1 file2 \n\n#find lines common to both files\ncomm -12 file1 file2 \n\nThe man file is actually quite readable for this.",
    "tag": "grep"
  },
  {
    "question": "How to suppress binary file matching results in grep",
    "answer": "There are three options, that you can use. -I is to exclude binary files in grep. Other are for line numbers and file names.\ngrep -I -n -H \n\n\n-I -- process a binary file as if it did not contain matching data; \n-n -- prefix each line of output with the 1-based line number within its input file\n-H -- print the file name for each match\n\nSo this might be a way to run grep:\ngrep -InH your-word *",
    "tag": "grep"
  },
  {
    "question": "How to search and replace using grep",
    "answer": "Another option is to use find and then pass it through sed.\nfind /path/to/files -type f -exec sed -i 's/oldstring/new string/g' {} \\;",
    "tag": "grep"
  },
  {
    "question": "Display filename before matching line",
    "answer": "Try this little trick to coax grep into thinking it is dealing with multiple files, so that it displays the filename:\ngrep 'pattern' file /dev/null\n\nTo also get the line number:\ngrep -n 'pattern' file /dev/null",
    "tag": "grep"
  },
  {
    "question": "How can I find all of the distinct file extensions in a folder hierarchy?",
    "answer": "Try this (not sure if it's the best way, but it works):\nfind . -type f | perl -ne 'print $1 if m/\\.([^.\\/]+)$/' | sort -u\n\nIt work as following:\n\nFind all files from current folder\nPrints extension of files if any\nMake a unique sorted list",
    "tag": "grep"
  },
  {
    "question": "Highlight text similar to grep, but don't filter out text",
    "answer": "Use ack. Check out its --passthru option here: ack. It has the added benefit of allowing full Perl regular expressions.\n    $ ack --passthru 'pattern1' file_name\n\n    $ command_here | ack --passthru 'pattern1'\n\nYou can also do it using grep like this:\n    $ grep --color -E '^|pattern1|pattern2' file_name\n\n    $ command_here | grep --color -E '^|pattern1|pattern2'\n\nThis will match all lines and highlight the patterns. The ^ matches every start of the line but won't get printed/highlighted since it's not a character.\n(Note that most of the setups will use --color by default. You may not need that flag).",
    "tag": "grep"
  },
  {
    "question": "Count all occurrences of a string in lots of files with grep",
    "answer": "This works for multiple occurrences per line:\ngrep -o string * | wc -l",
    "tag": "grep"
  },
  {
    "question": "How to invert a grep expression",
    "answer": "Use command-line option -v or --invert-match,\nls -R |grep -v -E .*[\\.exe]$\\|.*[\\.html]$",
    "tag": "grep"
  },
  {
    "question": "Grep regex NOT containing a string",
    "answer": "grep matches, grep -v does the inverse. If you need to \"match A but not B\" you usually use pipes:\ngrep \"${PATT}\" file | grep -v \"${NOTPATT}\"",
    "tag": "grep"
  },
  {
    "question": "How to get the process ID to kill a nohup process?",
    "answer": "When using nohup and you put the task in the background, the background operator (&) will give you the PID at the command prompt. If your plan is to manually manage the process, you can save that PID and use it later to kill the process if needed, via kill PID or kill -9 PID (if you need to force kill). Alternatively, you can find the PID later on by ps -ef | grep \"command name\" and locate the PID from there. Note that nohup keyword/command itself does not appear in the ps output for the command in question.\nIf you use a script, you could do something like this in the script:\nnohup my_command > my.log 2>&1 &\necho $! > save_pid.txt\n\nThis will run my_command saving all output into my.log (in a script, $! represents the PID of the last process executed). The 2 is the file descriptor for standard error (stderr) and 2>&1 tells the shell to route standard error output to the standard output (file descriptor 1). It requires &1 so that the shell knows it's a file descriptor in that context instead of just a file named 1. The 2>&1 is needed to capture any error messages that normally are written to standard error into our my.log file (which is coming from standard output). See I/O Redirection for more details on handling I/O redirection with the shell.\nIf the command sends output on a regular basis, you can check the output occasionally with tail my.log, or if you want to follow it \"live\" you can use tail -f my.log. Finally, if you need to kill the process, you can do it via:\nkill -9 `cat save_pid.txt`\nrm save_pid.txt",
    "tag": "grep"
  },
  {
    "question": "What is makeinfo, and how do I get it?",
    "answer": "In (at least) Ubuntu when using bash, it tells you what package you need to install if you type in a command and its not found in your path. My terminal says you need to install 'texinfo' package.\nsudo apt-get install texinfo",
    "tag": "grep"
  },
  {
    "question": "How to search contents of multiple pdf files?",
    "answer": "There is pdfgrep, which does exactly what its name suggests. \npdfgrep -R 'a pattern to search recursively from path' /some/path\n\nI've used it for simple searches and it worked fine.\n(There are packages in Debian, Ubuntu and Fedora.)\nSince version 1.3.0 pdfgrep supports recursive search. This version is available in Ubuntu since Ubuntu 12.10 (Quantal).",
    "tag": "grep"
  },
  {
    "question": "How can I have grep not print out 'No such file or directory' errors?",
    "answer": "You can use the -s or --no-messages flag to suppress errors.\n\n-s, --no-messages         suppress error messages\n\ngrep pattern * -s -R -n",
    "tag": "grep"
  },
  {
    "question": "Match two strings in one line with grep",
    "answer": "You can use\ngrep 'string1' filename | grep 'string2'\n\nThis searches for string1 followed by string 2 on the same line, or string2 followed by string1 on the same line; it does not answer the question:\ngrep 'string1.*string2\\|string2.*string1' filename",
    "tag": "grep"
  },
  {
    "question": "grep without showing path/file:line",
    "answer": "No need to find. If you are just looking for a pattern within a specific directory, this should suffice:\ngrep -h FOO /your/path/*.bar\n\nWhere -h is the parameter to hide the filename, as from man grep:\n\n-h, --no-filename\nSuppress  the  prefixing of file names on output.  This is the default\nwhen  there is only one file (or only standard input) to search.\n\nNote that you were using\n\n-H, --with-filename\nPrint the file name for each match.  This is the default when there is\nmore than one file to search.",
    "tag": "grep"
  },
  {
    "question": "Regex (grep) for multi-line search needed",
    "answer": "Without the need to install the grep variant pcregrep, you can do a multiline search with grep.\n$ grep -Pzo \"(?s)^(\\s*)\\N*main.*?{.*?^\\1}\" *.c\n\nExplanation:\n-P activate perl-regexp for grep (a powerful extension of regular expressions)\n-z Treat the input as a set of lines, each terminated by a zero byte (the ASCII NUL character) instead of a newline. That is, grep knows where the ends of the lines are, but sees the input as one big line.  Beware this also adds a trailing NUL char if used with -o, see comments.\n-o print only matching. Because we're using -z, the whole file is like a single big line, so if there is a match, the entire file would be printed; this way it won't do that.\nIn regexp:\n(?s) activate PCRE_DOTALL, which means that . finds any character or newline\n\\N find anything except newline, even with PCRE_DOTALL activated\n.*? find . in non-greedy mode, that is, stops as soon as possible.\n^ find start of line\n\\1 backreference to the first group (\\s*). This is a try to find the same indentation of method.\nAs you can imagine, this search prints the main method in a C (*.c) source file.",
    "tag": "grep"
  },
  {
    "question": "How to find patterns across multiple lines using grep?",
    "answer": "Grep is an awkward tool for this operation.\npcregrep which is found in most of the modern Linux systems can be used as\npcregrep -M 'abc.*(\\n|.)*efg' test.txt\n\nwhere -M, --multiline  allow patterns to match more than one line\nThere is a newer pcre2grep also. Both are provided by the PCRE project.\npcre2grep is available for Mac OS X via Mac Ports as part of port pcre2:\n% sudo port install pcre2 \n\nand via Homebrew as:\n% brew install pcre\n\nor for pcre2\n% brew install pcre2\n\npcre2grep is also available on Linux (Ubuntu 18.04+)\n$ sudo apt install pcre2-utils # PCRE2\n$ sudo apt install pcregrep    # Older PCRE",
    "tag": "grep"
  },
  {
    "question": "How to concatenate multiple lines of output to one line?",
    "answer": "Use tr '\\n' ' ' to translate all newline characters to spaces:\n$ grep pattern file | tr '\\n' ' '\n\nNote: grep reads files, cat concatenates files. Don't cat file | grep!\nEdit:\ntr can only handle single character translations. You could use awk to change the output record separator like:\n$ grep pattern file | awk '{print}' ORS='\" '\n\nThis would transform:\none\ntwo \nthree\n\nto:\none\" two\" three\"",
    "tag": "grep"
  },
  {
    "question": "What are good grep tools for Windows?",
    "answer": "FINDSTR is fairly powerful, supports regular expressions and has the advantages of being on all Windows machines already.\nc:\\> FindStr /?\n\nSearches for strings in files.\n\nFINDSTR [/B] [/E] [/L] [/R] [/S] [/I] [/X] [/V] [/N] [/M] [/O] [/P] [/F:file]\n        [/C:string] [/G:file] [/D:dir list] [/A:color attributes] [/OFF[LINE]]\n        strings [[drive:][path]filename[ ...]]\n\n  /B         Matches pattern if at the beginning of a line.\n  /E         Matches pattern if at the end of a line.\n  /L         Uses search strings literally.\n  /R         Uses search strings as regular expressions.\n  /S         Searches for matching files in the current directory and all\n             subdirectories.\n  /I         Specifies that the search is not to be case-sensitive.\n  /X         Prints lines that match exactly.\n  /V         Prints only lines that do not contain a match.\n  /N         Prints the line number before each line that matches.\n  /M         Prints only the filename if a file contains a match.\n  /O         Prints character offset before each matching line.\n  /P         Skip files with non-printable characters.\n  /OFF[LINE] Do not skip files with offline attribute set.\n  /A:attr    Specifies color attribute with two hex digits. See \"color /?\"\n  /F:file    Reads file list from the specified file(/ stands for console).\n  /C:string  Uses specified string as a literal search string.\n  /G:file    Gets search strings from the specified file(/ stands for console).\n  /D:dir     Search a semicolon delimited list of directories\n  strings    Text to be searched for.\n  [drive:][path]filename\n             Specifies a file or files to search.\n\nUse spaces to separate multiple search strings unless the argument is prefixed\nwith /C.  For example, 'FINDSTR \"hello there\" x.y' searches for \"hello\" or\n\"there\" in file x.y.  'FINDSTR /C:\"hello there\" x.y' searches for\n\"hello there\" in file x.y.\n\nRegular expression quick reference:\n  .        Wildcard: any character\n  *        Repeat: zero or more occurances of previous character or class\n  ^        Line position: beginning of line\n  $        Line position: end of line\n  [class]  Character class: any one character in set\n  [^class] Inverse class: any one character not in set\n  [x-y]    Range: any characters within the specified range\n  \\x       Escape: literal use of metacharacter x\n  \\<xyz    Word position: beginning of word\n  xyz\\>    Word position: end of word\n\nExample usage: findstr text_to_find * or to search recursively findstr /s text_to_find *",
    "tag": "grep"
  },
  {
    "question": "Using grep to search for a string that has a dot in it",
    "answer": "grep uses regexes; . means \"any character\" in a regex.  If you want a literal string, use grep -F, fgrep, or escape the . to \\..\nDon't forget to wrap your string in double quotes. Or else you should use \\\\.\nSo, your command would need to be:\ngrep -r \"0\\.49\" *\n\nor \ngrep -r 0\\\\.49 *\n\nor\ngrep -Fr 0.49 *",
    "tag": "grep"
  },
  {
    "question": "How to show only next line after the matched one?",
    "answer": "you can try with awk:\nawk '/blah/{getline; print}' logfile",
    "tag": "grep"
  },
  {
    "question": "Exploitable PHP functions",
    "answer": "To build this list I used 2 sources.  A Study In Scarlet and RATS.   I have also added some of my own to the mix and people on this thread have helped out.\nEdit: After posting this list I contacted the founder of RIPS and as of now this tools searches PHP code for the use of every function in this list.\nMost of these function calls are classified as Sinks. When a tainted variable (like $_REQUEST) is passed to a sink function, then you have a vulnerability.  Programs like RATS and RIPS use grep like functionality to identify all sinks in an application.  This means that programmers should take extra care when using these functions,  but if they where all banned then you wouldn't be able to get much done.  \n\"With great power comes great responsibility.\"\n--Stan Lee \nCommand Execution\nexec           - Returns last line of commands output\npassthru       - Passes commands output directly to the browser\nsystem         - Passes commands output directly to the browser and returns last line\nshell_exec     - Returns commands output\n`` (backticks) - Same as shell_exec()\npopen          - Opens read or write pipe to process of a command\nproc_open      - Similar to popen() but greater degree of control\npcntl_exec     - Executes a program\n\nPHP Code Execution\nApart from eval there are other ways to execute PHP code: include/require can be used for remote code execution in the form of Local File Include and Remote File Include vulnerabilities.\neval()\nassert()  - identical to eval()\npreg_replace('/.*/e',...) - /e does an eval() on the match\ncreate_function()\ninclude()\ninclude_once()\nrequire()\nrequire_once()\n$_GET['func_name']($_GET['argument']);\n$func = new ReflectionFunction($_GET['func_name']); $func->invoke(); or $func->invokeArgs(array());\n\nList of functions which accept callbacks\nThese functions accept a string parameter which could be used to call a function of the attacker's choice.  Depending on the function the attacker may or may not have the ability to pass a parameter.  In that case an Information Disclosure function like phpinfo() could be used.\nFunction                     => Position of callback arguments\n'ob_start'                   =>  0,\n'array_diff_uassoc'          => -1,\n'array_diff_ukey'            => -1,\n'array_filter'               =>  1,\n'array_intersect_uassoc'     => -1,\n'array_intersect_ukey'       => -1,\n'array_map'                  =>  0,\n'array_reduce'               =>  1,\n'array_udiff_assoc'          => -1,\n'array_udiff_uassoc'         => array(-1, -2),\n'array_udiff'                => -1,\n'array_uintersect_assoc'     => -1,\n'array_uintersect_uassoc'    => array(-1, -2),\n'array_uintersect'           => -1,\n'array_walk_recursive'       =>  1,\n'array_walk'                 =>  1,\n'assert_options'             =>  1,\n'uasort'                     =>  1,\n'uksort'                     =>  1,\n'usort'                      =>  1,\n'preg_replace_callback'      =>  1,\n'spl_autoload_register'      =>  0,\n'iterator_apply'             =>  1,\n'call_user_func'             =>  0,\n'call_user_func_array'       =>  0,\n'register_shutdown_function' =>  0,\n'register_tick_function'     =>  0,\n'set_error_handler'          =>  0,\n'set_exception_handler'      =>  0,\n'session_set_save_handler'   => array(0, 1, 2, 3, 4, 5),\n'sqlite_create_aggregate'    => array(2, 3),\n'sqlite_create_function'     =>  2,\n\nInformation Disclosure\nMost of these function calls are not sinks.   But rather it maybe a vulnerability if any of the data returned is viewable to an attacker.  If an attacker can see phpinfo() it is definitely a vulnerability. \nphpinfo\nposix_mkfifo\nposix_getlogin\nposix_ttyname\ngetenv\nget_current_user\nproc_get_status\nget_cfg_var\ndisk_free_space\ndisk_total_space\ndiskfreespace\ngetcwd\ngetlastmo\ngetmygid\ngetmyinode\ngetmypid\ngetmyuid\n\nOther\nextract - Opens the door for register_globals attacks (see study in scarlet).\nparse_str -  works like extract if only one argument is given.  \nputenv\nini_set\nmail - has CRLF injection in the 3rd parameter, opens the door for spam. \nheader - on old systems CRLF injection could be used for xss or other purposes, now it is still a problem if they do a header(\"location: ...\"); and they do not die();. The script keeps executing after a call to header(), and will still print output normally. This is nasty if you are trying to protect an administrative area. \nproc_nice\nproc_terminate\nproc_close\npfsockopen\nfsockopen\napache_child_terminate\nposix_kill\nposix_mkfifo\nposix_setpgid\nposix_setsid\nposix_setuid\n\nFilesystem Functions\nAccording to RATS all filesystem functions in php are nasty. Some of these don't seem very useful to the attacker. Others are more useful than you might think. For instance if allow_url_fopen=On then a url can be used as a file path, so a call to copy($_GET['s'], $_GET['d']); can be used to upload a PHP script anywhere on the system. \nAlso if a site is vulnerable to a request send via GET everyone of those file system functions can be abused to channel and attack to another host through your server.\n// open filesystem handler\nfopen\ntmpfile\nbzopen\ngzopen\nSplFileObject->__construct\n// write to filesystem (partially in combination with reading)\nchgrp\nchmod\nchown\ncopy\nfile_put_contents\nlchgrp\nlchown\nlink\nmkdir\nmove_uploaded_file\nrename\nrmdir\nsymlink\ntempnam\ntouch\nunlink\nimagepng   - 2nd parameter is a path.\nimagewbmp  - 2nd parameter is a path. \nimage2wbmp - 2nd parameter is a path. \nimagejpeg  - 2nd parameter is a path.\nimagexbm   - 2nd parameter is a path.\nimagegif   - 2nd parameter is a path.\nimagegd    - 2nd parameter is a path.\nimagegd2   - 2nd parameter is a path.\niptcembed\nftp_get\nftp_nb_get\n// read from filesystem\nfile_exists\nfile_get_contents\nfile\nfileatime\nfilectime\nfilegroup\nfileinode\nfilemtime\nfileowner\nfileperms\nfilesize\nfiletype\nglob\nis_dir\nis_executable\nis_file\nis_link\nis_readable\nis_uploaded_file\nis_writable\nis_writeable\nlinkinfo\nlstat\nparse_ini_file\npathinfo\nreadfile\nreadlink\nrealpath\nstat\ngzfile\nreadgzfile\ngetimagesize\nimagecreatefromgif\nimagecreatefromjpeg\nimagecreatefrompng\nimagecreatefromwbmp\nimagecreatefromxbm\nimagecreatefromxpm\nftp_put\nftp_nb_put\nexif_read_data\nread_exif_data\nexif_thumbnail\nexif_imagetype\nhash_file\nhash_hmac_file\nhash_update_file\nmd5_file\nsha1_file\nhighlight_file\nshow_source\nphp_strip_whitespace\nget_meta_tags",
    "tag": "grep"
  },
  {
    "question": "How to grep a string in a directory and all its subdirectories?",
    "answer": "If your grep supports -R, do:\ngrep -R 'string' dir/\n\nIf not, then use find:\nfind dir/ -type f -exec grep -H 'string' {} +",
    "tag": "grep"
  },
  {
    "question": "PowerShell equivalent to grep -f",
    "answer": "The -Pattern parameter in Select-String supports an array of patterns. So the one you're looking for is:\nGet-Content .\\doc.txt | Select-String -Pattern (Get-Content .\\regex.txt)\n\nThis searches through the textfile doc.txt by using every regex(one per line) in regex.txt",
    "tag": "grep"
  },
  {
    "question": "Is there a Pattern Matching Utility like GREP in Windows?",
    "answer": "There is a command-line tool called FINDSTR that comes with all Windows NT-class operating systems (type FINDSTR /? into a Command Prompt window for more information) It doesn't support everything grep does but it might be sufficient for your needs.",
    "tag": "grep"
  },
  {
    "question": "grep output to show only matching file",
    "answer": "grep -l \n\n(That's a lowercase L)",
    "tag": "grep"
  },
  {
    "question": "How to grep for case insensitive string in a file?",
    "answer": "You can use the -i flag which makes your pattern case insensitive:\ngrep -iF \"success...\" file1\n\nAlso, there is no need for cat. grep takes a file with the syntax grep <pattern> <file>. I also used the -F flag to search for a fixed string to avoid escaping the ellipsis.",
    "tag": "grep"
  },
  {
    "question": "Remove blank lines with grep",
    "answer": "Try the following:\ngrep -v -e '^$' foo.txt\n\nThe -e option allows regex patterns for matching.\nThe single quotes around ^$ makes it work for Cshell. Other shells will be happy with either single or double quotes.\nUPDATE:  This works for me for a file with blank lines or \"all white space\" (such as windows lines with \\r\\n style line endings), whereas the above only removes files with blank lines and unix style line endings:\ngrep -v -e '^[[:space:]]*$' foo.txt",
    "tag": "grep"
  },
  {
    "question": "Windows recursive grep command-line",
    "answer": "findstr can do recursive searches (/S) and supports some variant of regex syntax (/R). \nC:\\>findstr /?\nSearches for strings in files.\n\nFINDSTR [/B] [/E] [/L] [/R] [/S] [/I] [/X] [/V] [/N] [/M] [/O] [/P] [/F:file]\n        [/C:string] [/G:file] [/D:dir list] [/A:color attributes] [/OFF[LINE]]\n        strings [[drive:][path]filename[ ...]]\n\n  /B         Matches pattern if at the beginning of a line.\n  /E         Matches pattern if at the end of a line.\n  /L         Uses search strings literally.\n  /R         Uses search strings as regular expressions.\n  /S         Searches for matching files in the current directory and all\n             subdirectories.\n  /I         Specifies that the search is not to be case-sensitive.\n  /X         Prints lines that match exactly.\n  /V         Prints only lines that do not contain a match.\n  /N         Prints the line number before each line that matches.\n  /M         Prints only the filename if a file contains a match.\n  /O         Prints character offset before each matching line.\n  /P         Skip files with non-printable characters.\n  /OFF[LINE] Do not skip files with offline attribute set.\n  /A:attr    Specifies color attribute with two hex digits. See \"color /?\"\n  /F:file    Reads file list from the specified file(/ stands for console).\n  /C:string  Uses specified string as a literal search string.\n  /G:file    Gets search strings from the specified file(/ stands for console).\n  /D:dir     Search a semicolon delimited list of directories\n  strings    Text to be searched for.\n  [drive:][path]filename\n             Specifies a file or files to search.\n\nUse spaces to separate multiple search strings unless the argument is prefixed\nwith /C.  For example, 'FINDSTR \"hello there\" x.y' searches for \"hello\" or\n\"there\" in file x.y.  'FINDSTR /C:\"hello there\" x.y' searches for\n\"hello there\" in file x.y.\n\nRegular expression quick reference:\n  .        Wildcard: any character\n  *        Repeat: zero or more occurrences of previous character or class\n  ^        Line position: beginning of line\n  $        Line position: end of line\n  [class]  Character class: any one character in set\n  [^class] Inverse class: any one character not in set\n  [x-y]    Range: any characters within the specified range\n  \\x       Escape: literal use of metacharacter x\n  \\<xyz    Word position: beginning of word\n  xyz\\>    Word position: end of word\n\nFor full information on FINDSTR regular expressions refer to the online Command\nReference.",
    "tag": "grep"
  },
  {
    "question": "How to do a non-greedy match in grep?",
    "answer": "You're looking for a non-greedy (or lazy) match. To get a non-greedy match in regular expressions you need to use the modifier ? after the quantifier. For example you can change .* to .*?.\nBy default grep doesn't support non-greedy modifiers, but you can use grep -P to use the Perl syntax.",
    "tag": "grep"
  },
  {
    "question": "Validating IPv4 addresses with regexp",
    "answer": "Best for Now (43 chars)\n^((25[0-5]|(2[0-4]|1\\d|[1-9]|)\\d)\\.?\\b){4}$\nThis version shortens things by another 6 characters while not making use of the negative lookahead, which is not supported in some regex flavors.\nNewest, Shortest, Least Readable Version (49 chars)\n^((25[0-5]|(2[0-4]|1\\d|[1-9]|)\\d)(\\.(?!$)|$)){4}$\nThe [0-9] blocks can be substituted by \\d in 2 places - makes it a bit less readable, but definitely shorter.\nEven Newer, even Shorter, Second least readable version (55 chars)\n^((25[0-5]|(2[0-4]|1[0-9]|[1-9]|)[0-9])(\\.(?!$)|$)){4}$\nThis version looks for the 250-5 case, after that it cleverly ORs all the possible cases for 200-249 100-199 10-99 cases. Notice that the |) part is not a mistake, but actually ORs the last case for the 0-9 range. I've also omitted the ?: non-capturing group part as we don't really care about the captured items, they would not be captured either way if we didn't have a full-match in the first place.\nOld and shorter version (less readable) (63 chars)\n^(?:(25[0-5]|2[0-4][0-9]|1[0-9]{2}|[1-9]?[0-9])(\\.(?!$)|$)){4}$\nOlder (readable) version (70 chars)\n^(?:(25[0-5]|2[0-4][0-9]|1[0-9][0-9]|[1-9][0-9]|[0-9])(\\.(?!$)|$)){4}$\nIt uses the negative lookahead (?!) to remove the case where the ip might end with a .\nAlternative answer, using some of the newer techniques (71 chars)\n^((25[0-5]|(2[0-4]|1\\d|[1-9]|)\\d)\\.){3}(25[0-5]|(2[0-4]|1\\d|[1-9]|)\\d)$\nUseful in regex implementations where lookaheads are not supported\nOldest answer (115 chars)\n^(?:(?:25[0-5]|2[0-4][0-9]|1[0-9][0-9]|[1-9][0-9]|[0-9])\\.){3}\n    (?:25[0-5]|2[0-4][0-9]|1[0-9][0-9]|[1-9][0-9]|[0-9])$\n\nI think this is the most accurate and strict regex, it doesn't accept things like 000.021.01.0. it seems like most other answers here do and require additional regex to reject cases similar to that one - i.e. 0 starting numbers and an ip that ends with a .",
    "tag": "grep"
  },
  {
    "question": "Grep 'binary file matches'. How to get normal grep output?",
    "answer": "Try:\ngrep --text\n\nor\ngrep -a \n\nfor short. This is equivalent to --binary-files=text and it should show the matches in binary files.",
    "tag": "grep"
  },
  {
    "question": "How can I get `find` to ignore .svn directories?",
    "answer": "why not just \nfind . -not -iwholename '*.svn*'\n\nThe -not predicate negates everything that has .svn anywhere in the path.\nSo in your case it would be\nfind -not -iwholename '*.svn*' -name 'messages.*' -exec grep -Iw uint {} + \\;",
    "tag": "grep"
  },
  {
    "question": "How do I fetch lines before/after the grep result in bash?",
    "answer": "You can use the -B and -A to print lines before and after the match.\ngrep -i -B 10 'error' data\n\nWill print the 10 lines before the match, including the matching line itself.",
    "tag": "grep"
  },
  {
    "question": "Grep characters before and after match?",
    "answer": "3 characters before and 4 characters after\n$> echo \"some123_string_and_another\" | grep -o -P '.{0,3}string.{0,4}'\n23_string_and",
    "tag": "grep"
  },
  {
    "question": "Checking if output of a command contains a certain string in a shell script",
    "answer": "Testing $? is an anti-pattern.\nif ./somecommand | grep -q 'string'; then\n  echo \"matched\"\nfi",
    "tag": "grep"
  },
  {
    "question": "Finding a string in docker logs of container",
    "answer": "this can happen if the container is logging to stderr, piping works only for stdout, so try:\ndocker logs nginx 2>&1 | grep \"127.\"\n\n\n\"2>&1\" will tell the shell to redirect stderr to stdout\n\"|\" to give the \"unified\" output to grep using pipe",
    "tag": "grep"
  },
  {
    "question": "How to use sed/grep to extract text between two words?",
    "answer": "GNU grep can also support positive & negative look-ahead & look-back:\nFor your case, the command would be:\necho \"Here is a string\" | grep -o -P '(?<=Here).*(?=string)'\n\nIf there are multiple occurrences of Here and string, you can choose whether you want to match from the first Here and last string or match them individually. In terms of regex, it is called as greedy match (first case) or non-greedy match (second case)\n$ echo 'Here is a string, and Here is another string.' | grep -oP '(?<=Here).*(?=string)' # Greedy match\n is a string, and Here is another \n$ echo 'Here is a string, and Here is another string.' | grep -oP '(?<=Here).*?(?=string)' # Non-greedy match (Notice the '?' after '*' in .*)\n is a \n is another",
    "tag": "grep"
  },
  {
    "question": "How to merge every two lines into one from the command line?",
    "answer": "paste is good for this job:\npaste -d \" \"  - - < filename",
    "tag": "grep"
  },
  {
    "question": "(grep) Regex to match non-ASCII characters?",
    "answer": "This will match a single non-ASCII character:\n[^\\x00-\\x7F]\n\nThis is a valid PCRE (Perl-Compatible Regular Expression).\nYou can also use the POSIX shorthands:\n\n[[:ascii:]] - matches a single ASCII char\n[^[:ascii:]] - matches a single non-ASCII char\n\n\n[^[:print:]] will probably suffice for you.**",
    "tag": "grep"
  },
  {
    "question": "How can I \"grep\" for a filename instead of the contents of a file?",
    "answer": "You need to use find instead of grep in this case.\nYou can also use find in combination with grep or egrep:\n$ find | grep \"f[[:alnum:]]\\.frm\"",
    "tag": "grep"
  },
  {
    "question": "List files with certain extensions with ls and grep",
    "answer": "Why not:\nls *.{mp3,exe,mp4}\n\nI'm not sure where I learned it - but I've been using this.",
    "tag": "grep"
  },
  {
    "question": "How to remove the lines which appear on file B from another file A?",
    "answer": "If the files are sorted (they are in your example):\ncomm -23 file1 file2\n\n-23 suppresses the lines that are in both files, or only in file 2. If the files are not sorted, pipe them through sort first...\nSee the man page here",
    "tag": "grep"
  },
  {
    "question": "How to get the part of a file after the first line that matches a regular expression",
    "answer": "The following will print the line matching TERMINATE till the end of the file:\nsed -n -e '/TERMINATE/,$p'\n\nExplained: -n disables default behavior of sed of printing each line after executing its script on it, -e indicated a script to sed, /TERMINATE/,$ is an address (line) range selection meaning the first line matching the TERMINATE regular expression (like grep) to the end of the file ($), and p is the print command which prints the current line.\nThis will print from the line that follows the line matching TERMINATE till the end of the file:\n(from AFTER the matching line to EOF, NOT including the matching line)\nsed -e '1,/TERMINATE/d'\n\nExplained: 1,/TERMINATE/ is an address (line) range selection meaning the first line for the input to the 1st line matching the TERMINATE regular expression, and d is the delete command which delete the current line and skip to the next line.  As sed default behavior is to print the lines, it will print the lines after TERMINATE  to the end of input.\nIf you want the lines before TERMINATE:\nsed -e '/TERMINATE/,$d'\n\nAnd if you want both lines before and after TERMINATE in two different files in a single pass:\nsed -e '1,/TERMINATE/w before\n/TERMINATE/,$w after' file\n\nThe before and after files will contain the line with terminate, so to process each you need to use:\nhead -n -1 before\ntail -n +2 after\n\nIF you do not want to hard code the filenames in the sed script, you can:\nbefore=before.txt\nafter=after.txt\nsed -e \"1,/TERMINATE/w $before\n/TERMINATE/,\\$w $after\" file\n\nBut then you have to escape the $ meaning the last line so the shell will not try to expand the $w variable (note that we now use double quotes around the script instead of single quotes).\nI forgot to tell that the new line is important after the filenames in the script so that sed knows that the filenames end.\nHow would you replace the hardcoded TERMINATE by a variable?\nYou would make a variable for the matching text and then do it the same way as the previous example:\nmatchtext=TERMINATE\nbefore=before.txt\nafter=after.txt\nsed -e \"1,/$matchtext/w $before\n/$matchtext/,\\$w $after\" file\n\nto use a variable for the matching text with the previous examples:\n## Print the line containing the matching text, till the end of the file:\n## (from the matching line to EOF, including the matching line)\nmatchtext=TERMINATE\nsed -n -e \"/$matchtext/,\\$p\"\n\n## Print from the line that follows the line containing the\n## matching text, till the end of the file:\n## (from AFTER the matching line to EOF, NOT including the matching line)\nmatchtext=TERMINATE\nsed -e \"1,/$matchtext/d\"\n\n## Print all the lines before the line containing the matching text:\n## (from line-1 to BEFORE the matching line, NOT including the matching line)\nmatchtext=TERMINATE\nsed -e \"/$matchtext/,\\$d\"\n\nThe important points about replacing text with variables in these cases are:\n\nVariables ($variablename) enclosed in single quotes ['] won't \"expand\" but variables inside double quotes [\"] will. So, you have to change all the single quotes to double quotes if they contain text you want to replace with a variable.\nThe sed ranges also contain a $ and are immediately followed by a letter like: $p, $d, $w. They will also look like variables to be expanded, so you have to escape those $ characters with a backslash [\\] like: \\$p, \\$d, \\$w.",
    "tag": "grep"
  },
  {
    "question": "How to exclude certain directories/files from a Git grep search",
    "answer": "In Git 1.9.0, the \"magic word\" exclude was added to pathspecs. So if you want to search for foobar in every file except for those matching *.java you can do:\ngit grep foobar -- ':(exclude)*.java'\n\nOr using the ! \"short form\" for exclude:\ngit grep foobar -- ':!*.java'\n\nNote that in Git versions up to v2.12, when using an exclude pathspec, you must have at least one \"inclusive\" pathspec. In the above examples, you'd want to add ./* (recursively include everything under the current directory) somewhere after the -- as well. In git v2.13 this restriction was lifted and git grep foobar -- ':!*.java' works without the ./*.\nThere's a good reference for all the \"magic words\" allowed in a pathspec at git-scm.com (or just git help glossary).",
    "tag": "grep"
  },
  {
    "question": "How to process each output line in a loop?",
    "answer": "One of the easy ways is not to store the output in a variable, but directly iterate over it with a while/read loop.\nSomething like:\ngrep xyz abc.txt | while read -r line ; do\n    echo \"Processing $line\"\n    # your code goes here\ndone\n\nThere are variations on this scheme depending on exactly what you're after.\nIf you need to change variables inside the loop (and have that change be visible outside of it), you can use process substitution as stated in fedorqui's answer:\nwhile read -r line ; do\n    echo \"Processing $line\"\n    # your code goes here\ndone < <(grep xyz abc.txt)",
    "tag": "grep"
  },
  {
    "question": "Preserve colouring after piping grep to grep",
    "answer": "grep sometimes disables the color output, for example when writing to a pipe. You can override this behavior with grep --color=always \nThe correct command line would be \ngrep --color=always WORD * | grep -v AVOID\n\nThis is pretty verbose, alternatively you can just add the line\nalias cgrep=\"grep --color=always\"\n\nto your .bashrc for example and use cgrep as the colored grep. When redefining grep you might run into trouble with scripts which rely on specific output of grep and don't like ascii escape code.",
    "tag": "grep"
  },
  {
    "question": "How do I use grep to search the current directory for all files having the a string \"hello\" yet display only .h and .cc files?",
    "answer": "grep -r --include=*.{cc,h} \"hello\" .\n\nThis reads: search recursively (in all sub directories also) for all .cc OR .h files that contain \"hello\" at this . (current) directory \nFrom another stackoverflow question",
    "tag": "grep"
  },
  {
    "question": "Is it possible to perform a 'grep search' in all the branches of a Git project?",
    "answer": "The question \"How to grep (search) committed code in the Git history?\" recommends:\n git grep <regexp> $(git rev-list --all)\n\nThat searches through all the commits, which should include all the branches.\nAnother form would be:\ngit rev-list --all | (\n    while read revision; do\n        git grep -F 'yourWord' $revision\n    done\n)\n\nYou can find even more example in this article:\n\nI tried the above on one project large enough that git complained about the argument size, so if you run into this problem, do something like:\ngit rev-list --all | (while read rev; do git grep -e <regexp> $rev; done)\n\n\n(see an alternative in the last section of this answer, below)\nDon't forget those settings, if you want them:\n# Allow Extended Regular Expressions\ngit config --global grep.extendedRegexp true\n# Always Include Line Numbers\ngit config --global grep.lineNumber true\n\nThis alias can help too:\ngit config --global alias.g \"grep --break --heading --line-number\"\n\n\nUpdate August 2016: R.M. recommends in the comments\n\nI got a \"fatal: bad flag '->' used after filename\" when trying the git branch version. The error was associated with a HEAD aliasing notation.\nI solved it by adding a sed '/->/d' in the pipe, between the tr and the xargs commands.\ngit branch -a | tr -d \\* | sed '/->/d' | xargs git grep <regexp>\n\n\nThat is:\nalias grep_all=\"git branch -a | tr -d \\* | sed '/->/d' | xargs git grep\"\ngrep_all <regexp>\n\nThis is an improvement over the solution chernjie had suggested, since git rev-list --all is an overkill.\n\nA more refined command can be:\n# Don't use this, see above\ngit branch -a | tr -d \\* | xargs git grep <regexp>\n\nWhich will allow you to search only branches (including remote branches)\nYou can even create a bash/zsh alias for it:\n# Don't use this, see above  \nalias grep_all=\"git branch -a | tr -d \\* | xargs git grep\"\ngrep_all <regexp>\n\n\n\nJan. 2024: Gabriel Staples adds in the comments:\n\nFor anyone who wants to search just the tip commits on every branch in the repo, rather than every commit in the repo, and if you'd like to do parallel processes to speed up the search, see my very extensive answer here:\nAll about searching (via grep or similar) in your git repositories.\nI build upon some of the content in this answer.",
    "tag": "grep"
  },
  {
    "question": "Exclude .svn directories from grep",
    "answer": "If you have GNU Grep, it should work like this:\ngrep --exclude-dir=\".svn\"\n\nIf happen to be on a Unix System without GNU Grep, try the following:\ngrep -R \"whatever you like\" *|grep -v \"\\.svn/*\"",
    "tag": "grep"
  },
  {
    "question": "More elegant \"ps aux | grep -v grep\"",
    "answer": "The usual technique is this:\nps aux | egrep '[t]erminal'\n\nThis will match lines containing terminal, which egrep '[t]erminal' does not! It also works on many flavours of Unix.",
    "tag": "grep"
  },
  {
    "question": "How to search a specific value in all tables (PostgreSQL)?",
    "answer": "How about dumping the contents of the database, then using grep?\n$ pg_dump --data-only --inserts -U postgres your-db-name > a.tmp\n$ grep United a.tmp\nINSERT INTO countries VALUES ('US', 'United States');\nINSERT INTO countries VALUES ('GB', 'United Kingdom');\n\nThe same utility, pg_dump, can include column names in the output. Just change --inserts to --column-inserts. That way you can search for specific column names, too. But if I were looking for column names, I'd probably dump the schema instead of the data.\n$ pg_dump --data-only --column-inserts -U postgres your-db-name > a.tmp\n$ grep country_code a.tmp\nINSERT INTO countries (iso_country_code, iso_country_name) VALUES ('US', 'United  States');\nINSERT INTO countries (iso_country_code, iso_country_name) VALUES ('GB', 'United Kingdom');",
    "tag": "grep"
  },
  {
    "question": "How can I do a recursive find/replace of a string with awk or sed?",
    "answer": "find /home/www \\( -type d -name .git -prune \\) -o -type f -print0 | xargs -0 sed -i 's/subdomainA\\.example\\.com/subdomainB.example.com/g'\n\n-print0 tells find to print each of the results separated by a null character, rather than a new line. In the unlikely event that your directory has files with newlines in the names, this still lets xargs work on the correct filenames.\n\\( -type d -name .git -prune \\) is an expression which completely skips over all directories named .git. You could easily expand it, if you use SVN or have other folders you want to preserve -- just match against more names. It's roughly equivalent to -not -path .git, but more efficient, because rather than checking every file in the directory, it skips it entirely. The -o after it is required because of how -prune actually works.\nFor more information, see man find.",
    "tag": "awk"
  },
  {
    "question": "Bash tool to get nth line from a file",
    "answer": "head and pipe with tail will be slow for a huge file. I would suggest sed like this:\nsed 'NUMq;d' file\n\nWhere NUM is the number of the line you want to print; so, for example, sed '10q;d' file to print the 10th line of file.\nExplanation:\nNUMq will quit immediately when the line number is NUM.\nd will delete the line instead of printing it; this is inhibited on the last line because the q causes the rest of the script to be skipped when quitting.\nIf you have NUM in a variable, you will want to use double quotes instead of single:\nsed \"${NUM}q;d\" file",
    "tag": "awk"
  },
  {
    "question": "Find and kill a process in one line using bash and regex",
    "answer": "In bash, using only the basic tools listed in your question(1), you should be able to do:\nkill $(ps aux | grep '[p]ython csp_build.py' | awk '{print $2}')\n\nDetails on its workings are as follows:\n\nThe ps gives you the list of all the processes.\nThe grep filters that based on your search string, [p] is a trick to stop you picking up the actual grep process itself.\nThe awk just gives you the second field of each line, which is the PID.\nThe $(x) construct means to execute x then take its output and put it on the command line. The output of that ps pipeline inside that construct above is the list of process IDs so you end up with a command like kill 1234 1122 7654.\n\nHere's a transcript showing it in action:\npax> sleep 3600 &\n[1] 2225\npax> sleep 3600 &\n[2] 2226\npax> sleep 3600 &\n[3] 2227\npax> sleep 3600 &\n[4] 2228\npax> sleep 3600 &\n[5] 2229\npax> kill $(ps aux | grep '[s]leep' | awk '{print $2}')\n[5]+  Terminated              sleep 3600\n[1]   Terminated              sleep 3600\n[2]   Terminated              sleep 3600\n[3]-  Terminated              sleep 3600\n[4]+  Terminated              sleep 3600\n\nand you can see it terminating all the sleepers.\nExplaining the grep '[p]ython csp_build.py' bit in a bit more detail: when you do sleep 3600 & followed by ps -ef | grep sleep, you tend to get two processes with sleep in it, the sleep 3600 and the grep sleep (because they both have sleep in them, that's not rocket science).\nHowever, ps -ef | grep '[s]leep' won't create a grep process with sleep in it, it instead creates one with the command grep '[s]leep' and here's the tricky bit: the grep doesn't find that one, because it's looking for the regular expression \"any character from the character class [s] (which is basically just s) followed by leep.\nIn other words, it's looking for sleep but the grep process is grep '[s]leep' which doesn't have the text sleep in it.\nWhen I was shown this (by someone here on SO), I immediately started using it because\n\nit's one less process than adding | grep -v grep; and\nit's elegant and sneaky, a rare combination :-)\n\n\n(1) If you're not limited to using those basic tools, there's a nifty pgrep command which will find processes based on certain criteria (assuming you have it available on your system, of course).\nFor example, you can use pgrep sleep to output the process IDs for all sleep commands (by default, it matches the process name). If you want to match the entire command line as shown in ps, you can do something like pgrep -f 'sleep 9999'.\nAs an aside, it doesn't list itself if you do pgrep pgrep, so the tricky filter method shown above is not necessary in this case.\nYou can check that the processes are the ones you're interested in by using -a to show the full process names. You can also limit the scope to your own processes (or a specific set of users) with -u or -U. See the man page for pgrep/pkill for more options.\nOnce you're satisfied it will only show the processes you're interested in, you can then use pkill with the same parameters to send a signal to all those processes.",
    "tag": "awk"
  },
  {
    "question": "How to remove double-quotes in jq output for parsing json files in bash?",
    "answer": "Use the -r (or --raw-output) option to emit raw strings as output:\njq -r '.name' <json.txt",
    "tag": "awk"
  },
  {
    "question": "What is the difference between sed and awk?",
    "answer": "sed is a stream editor. It works with streams of characters on a per-line basis. It has a primitive programming language that includes goto-style loops and simple conditionals (in addition to pattern matching and address matching). There are essentially only two \"variables\": pattern space and hold space. Readability of scripts can be difficult. Mathematical operations are extraordinarily awkward at best.\nThere are various versions of sed with different levels of support for command line options and language features.\nawk is oriented toward delimited fields on a per-line basis. It has much more robust programming constructs including if/else, while, do/while and for (C-style and array iteration). There is complete support for variables and single-dimension associative arrays plus (IMO) kludgey multi-dimension arrays. Mathematical operations resemble those in C. It has printf and functions. The \"K\" in \"AWK\" stands for \"Kernighan\" as in \"Kernighan and Ritchie\" of the book \"C Programming Language\" fame (not to forget Aho and Weinberger). One could conceivably write a detector of academic plagiarism using awk.\nGNU awk (gawk) has numerous extensions, including true multidimensional arrays in the latest version. There are other variations of awk including mawk and nawk.\nBoth programs use regular expressions for selecting and processing text.\nI would tend to use sed where there are patterns in the text. For example, you could replace all the negative numbers in some text that are in the form \"minus-sign followed by a sequence of digits\" (e.g. \"-231.45\") with the \"accountant's brackets\" form (e.g. \"(231.45)\") using this (which has room for improvement):\nsed 's/-\\([0-9.]\\+\\)/(\\1)/g' inputfile\n\nI would use awk when the text looks more like rows and columns or, as awk refers to them \"records\" and \"fields\". If I was going to do a similar operation as above, but only on the third field in a simple comma delimited file I might do something like:\nawk -F, 'BEGIN {OFS = \",\"} {gsub(\"-([0-9.]+)\", \"(\" substr($3, 2) \")\", $3); print}' inputfile\n\nOf course those are just very simple examples that don't illustrate the full range of capabilities that each has to offer.",
    "tag": "awk"
  },
  {
    "question": "Using awk to print all columns from the nth to the last",
    "answer": "Print all columns:\nawk '{print $0}' somefile\n\nPrint all but the first column:\nawk '{$1=\"\"; print $0}' somefile\n\nPrint all but the first two columns:\nawk '{$1=$2=\"\"; print $0}' somefile",
    "tag": "awk"
  },
  {
    "question": "How do I use shell variables in an awk script?",
    "answer": "#Getting shell variables into awk\nmay be done in several ways. Some are better than others. This should cover most of them.  If you have a comment, please leave below.                                                                                    v1.5\n\n\nUsing -v  (The best way, most portable)\nUse the -v option: (P.S. use a space after -v or it will be less portable. E.g., awk -v var= not awk -vvar=)\nvariable=\"line one\\nline two\"\nawk -v var=\"$variable\" 'BEGIN {print var}'\nline one\nline two\n\nThis should be compatible with most awk, and the variable is available in the BEGIN block as well:\nIf you have multiple variables:\nawk -v a=\"$var1\" -v b=\"$var2\" 'BEGIN {print a,b}'\n\nWarning.  As Ed Morton writes and as seen in the above example, the shell variable is expanded by the shell before awk then sees its content as awk -v var='line one\\nline two' and so any escape sequences in the content of that shell variable will be interpreted when using -v, just like they are for every other form of assignment of a string to a variable in awk, e.g. awk 'BEGIN{var=\"line one\\nline two\"} {...}' or awk '{...}' var='line one\\nline two', and so \\n becomes a literal LineFeed character and not the 2-character string \\n. For example, given a variable like:\n$ variable='a\\tb\\n$c\\kd'\n\nawk would expand the escape sequences in the assignment:\n$ awk -v var=\"$variable\" 'BEGIN{ printf \"%s\\n\", var }'\nawk: warning: escape sequence `\\k' treated as plain `k'\na       b\n$ckd\n\nIf that's not what you want then, if your shell (e.g. bash) and locale (e.g. LC_ALL=C) support it then you can have backslashes treated literally by using shell parameter substitution to escape any backslashes:\n$ awk -v var=\"${variable//\\\\/\\\\\\\\}\" 'BEGIN{ printf \"%s\\n\", var }'\na\\tb\\n$c\\kd\n\nor by using ENVIRON[] or access it via ARGV[] (see below).\nYou cannot use -v var=\"$(printf '%q' \"$variable\")\" for this as that would also escape $s, nor can you use -v var=\"${variable@Q}\" as that would just add 's around \"$variable\" and the escape sequences would still be interpreted by awk. That's because those 2 approaches both escape chars according to shell syntax for providing command input, not awk syntax for assigning strings to variables.\nPS If you have vertical bar or other regexp meta characters as separator like |?( etc, they must be double escaped. Example 3 vertical bars ||| becomes -F'\\\\|\\\\|\\\\|'. You can also use -F\"[|][|][|]\".\n\nExample on getting data from a program/function in to awk (here date is used)\n\nawk -v time=\"$(date +\"%F %H:%M\" -d '-1 minute')\" 'BEGIN {print time}'\n\n\nExample of testing the contents of a shell variable as a regexp:\n\nawk -v var=\"$variable\" '$0 ~ var{print \"found it\"}'\n\n\nVariable after code block\nHere we get the variable after the awk code. This will work fine as long as you do not need the variable in the BEGIN block:\nvariable=\"line one\\nline two\"\necho \"input data\" | awk '{print var}' var=\"${variable}\"\nor\nawk '{print var}' var=\"${variable}\" file\n\n\nAdding multiple variables:\n\nawk '{print a,b,$0}' a=\"$var1\" b=\"$var2\" file\n\nIn this way we can also set different Field Separator FS for each file.\n\nawk 'some code' FS=',' file1.txt FS=';' file2.ext\n\nVariable after the code block will not work for the BEGIN block:\n\necho \"input data\" | awk 'BEGIN {print var}' var=\"${variable}\"\n\nHere-string\nVariable can also be added to awk using a here-string from shells that support them (including Bash):\nawk '{print $0}' <<< \"$variable\"\ntest\n\nThis is the same as:\necho \"$variable\" | awk '{print $0}'\nprintf '%s' \"$variable\" | awk '{print $0}'\n\nP.S. this treats the variable as a file input.\n\nENVIRON input\nAs TrueY writes, you can use the ENVIRON to print Environment Variables.\nSetting a variable before running AWK, you can print it out like this:\nexport X=MyVar\nawk 'BEGIN{print ENVIRON[\"X\"],ENVIRON[\"SHELL\"]}'\nMyVar /bin/bash\n\nor for a non-exported variable:\nx=MyVar\nx=\"$x\" awk 'BEGIN{print ENVIRON[\"x\"],ENVIRON[\"SHELL\"]}'\nMyVar /bin/bash\n\n\nARGV input\nAs Steven Penny writes, you can use ARGV to get the data into awk:\nv=\"my data\"\nawk 'BEGIN {print ARGV[1]}' \"$v\"\nmy data\n\nTo get the data into the code itself, not just the BEGIN:\nv=\"my data\"\necho \"test\" | awk 'BEGIN{var=ARGV[1];ARGV[1]=\"\"} {print var, $0}' \"$v\"\nmy data test\n\n\nVariable within the code: USE WITH CAUTION\nYou can use a variable within the awk code, but it's messy and hard to read, and as Charles Duffy points out, this version may also be a victim of code injection.  If someone adds bad stuff to the variable, it will be executed as part of the awk code.\nThis works by extracting the variable within the code, so it becomes a part of it.\nIf you want to make an awk that changes dynamically with use of variables, you can do it this way, but DO NOT use it for normal variables.\nvariable=\"line one\\nline two\"\nawk 'BEGIN {print \"'\"$variable\"'\"}'\nline one\nline two\n\nHere is an example of code injection:\nvariable='line one\\nline two\" ; for (i=1;i<=1000;++i) print i\"'\nawk 'BEGIN {print \"'\"$variable\"'\"}'\nline one\nline two\n1\n2\n3\n.\n.\n1000\n\nYou can add lots of commands to awk this way.  Even make it crash with non valid commands.\nOne valid use of this approach, though, is when you want to pass a symbol to awk to be applied to some input, e.g. a simple calculator:\n$ calc() { awk -v x=\"$1\" -v z=\"$3\" 'BEGIN{ print x '\"$2\"' z }'; }\n\n$ calc 2.7 '+' 3.4\n6.1\n\n$ calc 2.7 '*' 3.4\n9.18\n\nThere is no way to do that using an awk variable populated with the value of a shell variable, you NEED the shell variable to expand to become part of the text of the awk script before awk interprets it. (see comment below by Ed M.)\n\nExtra info:\nUse of double quote\nIt's always good to double quote variable \"$variable\"\nIf not, multiple lines will be added as a long single line.\nExample:\nvar=\"Line one\nThis is line two\"\n\necho $var\nLine one This is line two\n\necho \"$var\"\nLine one\nThis is line two\n\nOther errors you can get without double quote:\nvariable=\"line one\\nline two\"\nawk -v var=$variable 'BEGIN {print var}'\nawk: cmd. line:1: one\\nline\nawk: cmd. line:1:    ^ backslash not last character on line\nawk: cmd. line:1: one\\nline\nawk: cmd. line:1:    ^ syntax error\n\nAnd with single quote, it does not expand the value of the variable:\nawk -v var='$variable' 'BEGIN {print var}'\n$variable\n\nMore info about AWK and variables\nRead this faq.",
    "tag": "awk"
  },
  {
    "question": "How can I use \":\" as an AWK field separator?",
    "answer": "-F is a command line argument, not AWK syntax. Try:\necho '1: ' | awk -F  ':' '/1/ {print $1}'",
    "tag": "awk"
  },
  {
    "question": "How can I shuffle the lines of a text file on the Unix command line or in a shell script?",
    "answer": "You can use shuf. On some systems at least (doesn't appear to be in POSIX).\nAs jleedev pointed out: sort -R might also be an option. On some systems at least; well, you get the picture. It has been pointed out that sort -R doesn't really shuffle but instead sort items according to their hash value.\n[Editor's note: sort -R almost shuffles, except that duplicate lines / sort keys always end up next to each other. In other words: only with unique input lines / keys is it a true shuffle. While it's true that the output order is determined by hash values, the randomness comes from choosing a random hash function - see manual.]",
    "tag": "awk"
  },
  {
    "question": "AWK: Access captured group from line pattern",
    "answer": "With gawk, you can use the match function to capture parenthesized groups.\ngawk 'match($0, pattern, ary) {print ary[1]}' \n\nexample:\necho \"abcdef\" | gawk 'match($0, /b(.*)e/, a) {print a[1]}' \n\noutputs cd. \nNote the specific use of gawk which implements the feature in question.\nFor a portable alternative you can achieve similar results with match() and substr.\nexample:\necho \"abcdef\" | awk 'match($0, /b[^e]*/) {print substr($0, RSTART+1, RLENGTH-1)}'\n\noutputs cd.",
    "tag": "awk"
  },
  {
    "question": "Using multiple delimiters in awk",
    "answer": "The delimiter can be a regular expression.\nawk -F'[/=]' '{print $3 \"\\t\" $5 \"\\t\" $8}' file\n\nProduces:\ntc0001   tomcat7.1    demo.example.com  \ntc0001   tomcat7.2    quest.example.com  \ntc0001   tomcat7.5    www.example.com",
    "tag": "awk"
  },
  {
    "question": "What are the differences between Perl, Python, AWK and sed?",
    "answer": "In order of appearance, the languages are sed, awk, perl, python.\nThe sed program is a stream editor and is designed to apply the actions from a script to each line (or, more generally, to specified ranges of lines) of the input file or files. Its language is based on ed, the Unix editor, and although it has conditionals and so on, it is hard to work with for complex tasks. You can work minor miracles with it - but at a cost to the hair on your head. However, it is probably the fastest of the programs when attempting tasks within its remit. (It has the least powerful regular expressions of the programs discussed - adequate for many purposes, but certainly not PCRE - Perl-Compatible Regular Expressions)\nThe awk program (name from the initials of its authors - Aho, Weinberger, and Kernighan) is a tool initially for formatting reports. It can be used as a souped-up sed; in its more recent versions, it is computationally complete. It uses an interesting idea - the program is based on 'patterns matched' and 'actions taken when the pattern matches'. The patterns are fairly powerful (Extended Regular Expressions). The language for the actions is similar to C. One of the key features of awk is that it splits the input automatically into records and each record into fields.\nPerl was written in part as an awk-killer and sed-killer. Two of the programs provided with it are a2p and s2p for converting awk scripts and sed scripts into Perl. Perl is one of the earliest of the next generation of scripting languages (Tcl/Tk can probably claim primacy). It has powerful integrated regular expression handling with a vastly more powerful language. It provides access to almost all system calls and has the extensibility of the CPAN modules. (Neither awk nor sed is extensible.) One of Perl's mottos is \"TMTOWTDI - There's more than one way to do it\" (pronounced \"tim-toady\"). Perl has 'objects', but it is more of an add-on than a fundamental part of the language.\nPython was written last, and probably in part as a reaction to Perl. It has some interesting syntactic ideas (indenting to indicate levels - no braces or equivalents). It is more fundamentally object-oriented than Perl; it is just as extensible as Perl.\nOK - when to use each?\n\nSed - when you need to do simple text transforms on files.\nAwk - when you only need simple formatting and summarisation or transformation of data.\nPerl - for almost any task, but especially when the task needs complex regular expressions.\nPython - for the same tasks that you could use Perl for.\n\nI'm not aware of anything that Perl can do that Python can't, nor vice versa. The choice between the two would depend on other factors. I learned Perl before there was a Python, so I tend to use it. Python has less accreted syntax and is generally somewhat simpler to learn. Perl 6, when it becomes available, will be a fascinating development.\n(Note that the 'overviews' of Perl and Python, in particular, are woefully incomplete; whole books could be written on the topic.)",
    "tag": "awk"
  },
  {
    "question": "How to show only next line after the matched one?",
    "answer": "you can try with awk:\nawk '/blah/{getline; print}' logfile",
    "tag": "awk"
  },
  {
    "question": "How to split a delimited string into an array in awk?",
    "answer": "Have you tried:\necho \"12|23|11\" | awk '{split($0,a,\"|\"); print a[3],a[2],a[1]}'",
    "tag": "awk"
  },
  {
    "question": "How can I quickly sum all numbers in a file?",
    "answer": "You can use awk:\nawk '{ sum += $1 } END { print sum }' file",
    "tag": "awk"
  },
  {
    "question": "How to get the second column from command output?",
    "answer": "Use -F [field separator] to split the lines on \"s:\nawk -F '\"' '{print $2}' your_input_file\n\nor for input from pipe\n<some_command> | awk -F '\"' '{print $2}'\n\noutput:\nA B\nC\nD",
    "tag": "awk"
  },
  {
    "question": "How to grep for case insensitive string in a file?",
    "answer": "You can use the -i flag which makes your pattern case insensitive:\ngrep -iF \"success...\" file1\n\nAlso, there is no need for cat. grep takes a file with the syntax grep <pattern> <file>. I also used the -F flag to search for a fixed string to avoid escaping the ellipsis.",
    "tag": "awk"
  },
  {
    "question": "Insert a line at specific line number with sed or awk",
    "answer": "sed -i '8i This is Line 8' FILE\n\ninserts at line 8\nThis is Line 8\n\ninto file FILE\n-i does the modification directly to file FILE, no output to stdout, as mentioned in the comments by glenn jackman.",
    "tag": "awk"
  },
  {
    "question": "awk without printing newline",
    "answer": "awk '{sum+=$3}; END  {printf \"%f\",sum/NR}' ${file}_${f}_v1.xls >> to-plot-p.xls\nprint will insert a newline by default. You dont want that to happen, hence use printf instead.",
    "tag": "awk"
  },
  {
    "question": "How to merge every two lines into one from the command line?",
    "answer": "paste is good for this job:\npaste -d \" \"  - - < filename",
    "tag": "awk"
  },
  {
    "question": "Print second-to-last column/field in `awk`",
    "answer": "awk '{print $(NF-1)}'\n\nShould work",
    "tag": "awk"
  },
  {
    "question": "Sort a text file by line length including spaces",
    "answer": "Answer\n< testfile awk '{ print length, $0 }' | sort -n -s | cut -d\" \" -f2-\n\nOr, to do your original (perhaps unintentional) sub-sorting of any equal-length lines:\n< testfile awk '{ print length, $0 }' | sort -n | cut -d\" \" -f2-\n\nIn both cases, we have solved your stated problem by moving away from awk for your final cut.\nLines of matching length - what to do in the case of a tie:\nThe question did not specify whether or not further sorting was wanted for lines of matching length.  I've assumed that this is unwanted and suggested the use of -s (--stable) to prevent such lines being sorted against each other, and keep them in the relative order in which they occur in the input.\n(Those who want more control of sorting these ties might look at sort's --key option.)\nWhy the question's attempted solution fails (awk line-rebuilding):\nIt is interesting to note the difference between:\necho \"hello   awk   world\" | awk '{print}'\necho \"hello   awk   world\" | awk '{$1=\"hello\"; print}'\n\nThey yield respectively\nhello   awk   world\nhello awk world\n\nThe relevant section of (gawk's) manual only mentions as an aside that awk is going to rebuild the whole of $0 (based on the separator, etc) when you change one field.  I guess it's not crazy behaviour.  It has this:\n\"Finally, there are times when it is convenient to force awk to rebuild the entire record, using the current value of the fields and OFS. To do this, use the seemingly innocuous assignment:\"\n $1 = $1   # force record to be reconstituted\n print $0  # or whatever else with $0\n\n\"This forces awk to rebuild the record.\"\nTest input including some lines of equal length:\naa A line   with     MORE    spaces\nbb The very longest line in the file\nccb\n9   dd equal len.  Orig pos = 1\n500 dd equal len.  Orig pos = 2\nccz\ncca\nee A line with  some       spaces\n1   dd equal len.  Orig pos = 3\nff\n5   dd equal len.  Orig pos = 4\ng",
    "tag": "awk"
  },
  {
    "question": "How to delete duplicate lines in a file without sorting it in Unix",
    "answer": "awk '!seen[$0]++' file.txt\n\nseen is an associative array that AWK will pass every line of the file to. If a line isn't in the array then seen[$0] will evaluate to false. The ! is the logical NOT operator and will invert the false to true. AWK will print the lines where the expression evaluates to true.\nThe ++ increments seen so that seen[$0] == 1 after the first time a line is found and then seen[$0] == 2, and so on.\nAWK evaluates everything but 0 and \"\" (empty string) to true. If a duplicate line is placed in seen then !seen[$0] will evaluate to false and the line will not be written to the output.",
    "tag": "awk"
  },
  {
    "question": "What is the shortest way to get n-th column of an output?",
    "answer": "You can use cut to access the second field:\ncut -f2\n\nEdit:\nSorry, didn't realise that SVN doesn't use tabs in its output, so that's a bit useless. You can tailor cut to the output but it's a bit fragile - something like cut -c 10-  would work, but the exact value will depend on your setup.\nAnother option is something like: sed 's/.\\s\\+//'",
    "tag": "awk"
  },
  {
    "question": "Printing the last column of a line in a file",
    "answer": "You don't see anything, because of buffering. The output is shown, when there are enough lines or end of file is reached. tail -f means wait for more input, but there are no more lines in file and so the pipe to grep is never closed.\nIf you omit -f from tail the output is shown immediately:\ntail file | grep A1 | awk '{print $NF}'\n\n\n@EdMorton is right of course. Awk can search for A1 as well, which shortens the command line to \ntail file | awk '/A1/ {print $NF}'\n\nor without tail, showing the last column of all lines containing A1\nawk '/A1/ {print $NF}' file\n\n\nThanks to @MitchellTracy's comment, tail might miss the record containing A1 and thus you get no output at all. This may be solved by switching tail and awk, searching first through the file and only then show the last line:\nawk '/A1/ {print $NF}' file | tail -n1",
    "tag": "awk"
  },
  {
    "question": "Is there a Unix utility to prepend timestamps to stdin?",
    "answer": "ts from moreutils will prepend a timestamp to every line of input you give it. You can format it using strftime too.\n$ echo 'foo bar baz' | ts\nMar 21 18:07:28 foo bar baz\n$ echo 'blah blah blah' | ts '%F %T'\n2012-03-21 18:07:30 blah blah blah\n$ \n\nTo install it:\nsudo apt-get install moreutils",
    "tag": "awk"
  },
  {
    "question": "Save modifications in place with awk",
    "answer": "In GNU Awk 4.1.0 (released 2013) and later, it has the option of \"inplace\" file editing:\n\n[...] The \"inplace\" extension, built using the new facility, can be used to simulate the GNU \"sed -i\" feature. [...]\n\nExample usage:\n$ gawk -i inplace '{ gsub(/foo/, \"bar\") }; { print }' file1 file2 file3\n\nTo keep the backup:\n$ gawk -i inplace -v INPLACE_SUFFIX=.bak '{ gsub(/foo/, \"bar\") }\n> { print }' file1 file2 file3",
    "tag": "awk"
  },
  {
    "question": "How can I delete a newline if it is the last character in a file?",
    "answer": "perl -pe 'chomp if eof' filename >filename2\n\nor, to edit the file in place:\nperl -pi -e 'chomp if eof' filename\n\n[Editor's note: -pi -e was originally -pie, but, as noted by several commenters and explained by @hvd, the latter doesn't work.]\nThis was described as a 'perl blasphemy' on the awk website I saw.\nBut, in a test, it worked.",
    "tag": "awk"
  },
  {
    "question": "How to select lines between two marker patterns which may occur multiple times with awk/sed",
    "answer": "Use awk with a flag to trigger the print when necessary:\n$ awk '/abc/{flag=1;next}/mno/{flag=0}flag' file\ndef1\nghi1\njkl1\ndef2\nghi2\njkl2\n\nHow does this work?\n\n/abc/ matches lines having this text, as well as /mno/ does.  \n/abc/{flag=1;next} sets the flag when the text abc is found. Then, it skips the line.  \n/mno/{flag=0} unsets the flag when the text mno is found.\nThe final flag is a pattern with the default action, which is to print $0: if flag is equal 1 the line is printed.\n\nFor a more detailed description and examples, together with cases when the patterns are either shown or not, see How to select lines between two patterns?.",
    "tag": "awk"
  },
  {
    "question": "How to print matched regex pattern using awk?",
    "answer": "This is the very basic\nawk '/pattern/{ print $0 }' file\n\nask awk to search for pattern using //, then print out the line, which by default is called a record, denoted by $0. At least read up the documentation.\nIf you only want to get print out the matched word. \nawk '{for(i=1;i<=NF;i++){ if($i==\"yyy\"){print $i} } }' file",
    "tag": "awk"
  },
  {
    "question": "sed or awk: delete n lines following a pattern",
    "answer": "I'll have a go at this.\nTo delete 5 lines after a pattern (including the line with the pattern):\nsed -e '/pattern/,+5d' file.txt\n\nTo delete 5 lines after a pattern (excluding the line with the pattern):\nsed -e '/pattern/{n;N;N;N;N;d}' file.txt",
    "tag": "awk"
  },
  {
    "question": "Get last field using awk substr",
    "answer": "Use the fact that awk splits the lines in fields based on a field separator, that you can define. Hence, defining the field separator to / you can say:\nawk -F \"/\" '{print $NF}' input\n\nas NF refers to the number of fields of the current record, printing $NF means printing the last one.\nSo given a file like this:\n/home/parent/child1/child2/child3/filename\n/home/parent/child1/child2/filename\n/home/parent/child1/filename\n\nThis would be the output:\n$ awk -F\"/\" '{print $NF}' file\nfilename\nfilename\nfilename",
    "tag": "awk"
  },
  {
    "question": "Printing everything except the first field with awk",
    "answer": "$1=\"\" leaves a space as Ben Jackson mentioned, so use a for loop:\nawk '{for (i=2; i<=NF; i++) print $i}' filename\n\nSo if your string was \"one two three\", the output will be:\ntwo \nthree\nIf you want the result in one row, you could do as follows:\nawk '{for (i=2; i<NF; i++) printf $i \" \"; print $NF}' filename\n\nThis will give you: \"two three\"",
    "tag": "awk"
  },
  {
    "question": "What are the differences among grep, awk & sed?",
    "answer": "Short definition:\ngrep: search for specific terms in a file\n#usage\n$ grep This file.txt\nEvery line containing \"This\"\nEvery line containing \"This\"\nEvery line containing \"This\"\nEvery line containing \"This\"\n\n$ cat file.txt\nEvery line containing \"This\"\nEvery line containing \"This\"\nEvery line containing \"That\"\nEvery line containing \"This\"\nEvery line containing \"This\"\n\nNow awk and sed are completly different than grep.\nawk and sed are text processors. Not only do they have the ability to find what you are looking for in text, they have the ability to remove, add and modify the text as well (and much more). \nawk is mostly used for data extraction and reporting. sed is a stream editor\nEach one of them has its own functionality and specialties. \nExample \nSed\n$ sed -i 's/cat/dog/' file.txt\n# this will replace any occurrence of the characters 'cat' by 'dog'\n\nAwk\n$ awk '{print $2}' file.txt\n# this will print the second column of file.txt\n\nBasic awk usage:\nCompute sum/average/max/min/etc. what ever you may need.\n$ cat file.txt\nA 10\nB 20\nC 60\n$ awk 'BEGIN {sum=0; count=0; OFS=\"\\t\"} {sum+=$2; count++} END {print \"Average:\", sum/count}' file.txt\nAverage:    30\n\nI recommend that you read this book: Sed & Awk: 2nd Ed.\nIt will help you become a proficient sed/awk user on any unix-like environment.",
    "tag": "awk"
  },
  {
    "question": "How to escape a single quote inside awk",
    "answer": "This maybe what you're looking for:\nawk 'BEGIN {FS=\" \";} {printf \"'\\''%s'\\'' \", $1}'\n\nThat is, with '\\'' you close the opening ', then print a literal ' by escaping it and finally open the ' again.",
    "tag": "awk"
  },
  {
    "question": "How to use multiple arguments for awk with a shebang (i.e. #!)?",
    "answer": "The shebang line has never been specified as part of POSIX, SUS, LSB or any other specification. AFAIK, it hasn't even been properly documented.\nThere is a rough consensus about what it does: take everything between the ! and the \\n and exec it. The assumption is that everything between the ! and the \\n is a full absolute path to the interpreter. There is no consensus about what happens if it contains whitespace.\n\nSome operating systems simply treat the entire thing as the path. After all, in most operating systems, whitespace or dashes are legal in a path.\nSome operating systems split at whitespace and treat the first part as the path to the interpreter and the rest as individual arguments.\nSome operating systems split at the first whitespace and treat the front part as the path to the interpeter and the rest as a single argument (which is what you are seeing).\nSome even don't support shebang lines at all.\n\nThankfully, 1. and 4. seem to have died out, but 3. is pretty widespread, so you simply cannot rely on being able to pass more than one argument.\nAnd since the location of commands is also not specified in POSIX or SUS, you generally use up that single argument by passing the executable's name to env so that it can determine the executable's location; e.g.:\n#!/usr/bin/env gawk\n\n[Obviously, this still assumes a particular path for env, but there are only very few systems where it lives in /bin, so this is generally safe. The location of env is a lot more standardized than the location of gawk or even worse something like python or ruby or spidermonkey.]\nWhich means that you cannot actually use any arguments at all.",
    "tag": "awk"
  },
  {
    "question": "How to use awk sort by column 3",
    "answer": "How about just sort. \nsort -t, -nk3 user.csv\n\nwhere \n\n-t, - defines your delimiter as ,.\n-n - gives you numerical sort. Added since you added it in your\nattempt. If your user field is text only then you dont need it. \n-k3 - defines the field (key). user is the third field.",
    "tag": "awk"
  },
  {
    "question": "How to decode URL-encoded string in shell?",
    "answer": "Here is a simple one-line solution.                   \n$ function urldecode() { : \"${*//+/ }\"; echo -e \"${_//%/\\\\x}\"; }\n\nIt may look like perl :) but it is just pure bash. No awks, no seds ... no overheads. Using the : builtin, special parameters, pattern substitution and the echo builtin's -e option to translate hex codes into characters. See bash's manpage for further details. You can use this function as separate command\n$ urldecode https%3A%2F%2Fgoogle.com%2Fsearch%3Fq%3Durldecode%2Bbash\nhttps://google.com/search?q=urldecode+bash\n\nor in variable assignments, like so:\n$ x=\"http%3A%2F%2Fstackoverflow.com%2Fsearch%3Fq%3Durldecode%2Bbash\"\n$ y=$(urldecode \"$x\")\n$ echo \"$y\"\nhttp://stackoverflow.com/search?q=urldecode+bash",
    "tag": "awk"
  },
  {
    "question": "What are NR and FNR and what does \"NR==FNR\" imply?",
    "answer": "In Awk:\n\nFNR refers to the record number (typically the line number) in the current file.\nNR refers to the total record number.\nThe operator == is a comparison operator, which returns true when the two surrounding operands are equal.\n\nThis means that the condition NR==FNR is normally only true for the first file, as FNR resets back to 1 for the first line of each file but NR keeps on increasing.\nThis pattern is typically used to perform actions on only the first file. It works assuming that the first file is not empty, otherwise the two variables would continue to be equal while Awk was processing the second file.\nThe next inside the block means any further commands are skipped, so they are only run on files other than the first.\nThe condition FNR==NR compares the same two operands as NR==FNR, so it behaves in the same way.",
    "tag": "awk"
  },
  {
    "question": "using awk with column value conditions",
    "answer": "If you're looking for a particular string, put quotes around it:\nawk '$1 == \"findtext\" {print $3}'\n\nOtherwise, awk will assume it's a variable name.",
    "tag": "awk"
  },
  {
    "question": "using awk with column value conditions",
    "answer": "If you're looking for a particular string, put quotes around it:\nawk '$1 == \"findtext\" {print $3}'\n\nOtherwise, awk will assume it's a variable name.",
    "tag": "awk"
  },
  {
    "question": "Is there still any reason to learn AWK?",
    "answer": "If you quickly learn the basics of awk, you can indeed do amazing things on the command line.\nBut the real reason to learn awk is to have an excuse to read the superb book The AWK Programming Language by Aho, Kernighan, and Weinberger.\nThe AWK Programming Language at archive.org\nYou would think, from the name, that it simply teaches you awk.  Actually, that is just the beginning.  Launching into the vast array of problems that can be tackled once one is using a concise scripting language that makes string manipulation easy — and awk was one of the first — it proceeds to teach the reader how to implement a database, a parser, an interpreter, and (if memory serves me) a compiler for a small project-specific computer language!  If only they had also programmed an example operating system using awk, the book would have been a fairly complete survey introduction to computer science!\nFamously clear and concise, like the original C Language book, it also is a wonderful example of friendly technical writing done right.  Even the index is a piece of craftsmanship.\nAwk?  If you know it, you'll use it at the command-line occasionally, but for anything larger you'll feel trapped, unable to access the wider features of your system and the Internet that something like Python provides access to.  But the book?  You'll always be glad you read it!",
    "tag": "awk"
  },
  {
    "question": "Show filename and line number in grep output",
    "answer": "I think -l is too restrictive as it suppresses the output of -n. I would suggest -H (--with-filename): Print the filename for each match.\ngrep -Hn \"search\" *\n\nIf that gives too much output, try -o to only print the part that matches.\ngrep -nHo \"search\" *",
    "tag": "awk"
  },
  {
    "question": "How to print last two columns using awk",
    "answer": "You can make use of variable NF which  is set to the total number of fields in the input record:\nawk '{print $(NF-1),\"\\t\",$NF}' file\n\nthis assumes that you have at least 2 fields.",
    "tag": "awk"
  },
  {
    "question": "Print all but the first three columns",
    "answer": "use cut\n$ cut -f4-13 file\n\nor if you insist on awk and $13 is the last field\n$ awk '{$1=$2=$3=\"\";print}' file\n\nelse\n$ awk '{for(i=4;i<=13;i++)printf \"%s \",$i;printf \"\\n\"}' file",
    "tag": "awk"
  },
  {
    "question": "awk partly string match (if column/word partly matches)",
    "answer": "awk '$3 ~ /snow/ { print }' dummy_file",
    "tag": "awk"
  },
  {
    "question": "Append file contents to the bottom of existing file in Bash",
    "answer": "This should work:\n cat \"$API\" >> \"$CONFIG\"\n\nYou need to use the >> operator to append to a file.  Redirecting with > causes the file to be overwritten. (truncated).",
    "tag": "awk"
  },
  {
    "question": "Tab separated values in awk",
    "answer": "You need to set the OFS variable (output field separator) to be a tab:\necho \"$line\" | \nawk -v var=\"$mycol_new\" -F'\\t' 'BEGIN {OFS = FS} {$3 = var; print}'\n\n(make sure you quote the $line variable in the echo statement)",
    "tag": "awk"
  },
  {
    "question": "How to print lines between two patterns, inclusive or exclusive (in sed, AWK or Perl)?",
    "answer": "Print lines between PAT1 and PAT2\n$ awk '/PAT1/,/PAT2/' file\nPAT1\n3    - first block\n4\nPAT2\nPAT1\n7    - second block\nPAT2\nPAT1\n10    - third block\n\nOr, using variables:\nawk '/PAT1/{flag=1} flag; /PAT2/{flag=0}' file\n\nHow does this work?\n\n/PAT1/ matches lines having this text, as well as /PAT2/ does.  \n/PAT1/{flag=1} sets the flag when the text PAT1 is found in a line.\n/PAT2/{flag=0} unsets the flag when the text PAT2 is found in a line.\nflag is a pattern with the default action, which is to print $0: if flag is equal 1 the line is printed. This way, it will print all those lines occurring from the time PAT1 occurs and up to the next PAT2 is seen. This will also print the lines from the last match of PAT1 up to the end of the file.\n\nPrint lines between PAT1 and PAT2 - not including PAT1 and PAT2\n$ awk '/PAT1/{flag=1; next} /PAT2/{flag=0} flag' file\n3    - first block\n4\n7    - second block\n10    - third block\n\nThis uses next to skip the line that contains PAT1 in order to avoid this being printed.\nThis call to next can be dropped by reshuffling the blocks: awk '/PAT2/{flag=0} flag; /PAT1/{flag=1}' file.\nPrint lines between PAT1 and PAT2 - including PAT1\n$ awk '/PAT1/{flag=1} /PAT2/{flag=0} flag' file\nPAT1\n3    - first block\n4\nPAT1\n7    - second block\nPAT1\n10    - third block\n\nBy placing flag at the very end, it triggers the action that was set on either PAT1 or PAT2: to print on PAT1, not to print on PAT2.\nPrint lines between PAT1 and PAT2 - including PAT2\n$ awk 'flag; /PAT1/{flag=1} /PAT2/{flag=0}' file\n3    - first block\n4\nPAT2\n7    - second block\nPAT2\n10    - third block\n\nBy placing flag at the very beginning, it triggers the action that was set previously and hence print the closing pattern but not the starting one.\nPrint lines between PAT1 and PAT2 - excluding lines from the last PAT1 to the end of file if no other PAT2 occurs\nThis is based on a solution by Ed Morton.\nawk 'flag{\n        if (/PAT2/)\n           {printf \"%s\", buf; flag=0; buf=\"\"}\n        else\n            buf = buf $0 ORS\n     }\n     /PAT1/ {flag=1}' file\n\nAs a one-liner:\n$ awk 'flag{ if (/PAT2/){printf \"%s\", buf; flag=0; buf=\"\"} else buf = buf $0 ORS}; /PAT1/{flag=1}' file\n3    - first block\n4\n7    - second block\n\n# note the lack of third block, since no other PAT2 happens after it\n\nThis keeps all the selected lines in a buffer that gets populated from the moment PAT1 is found. Then, it keeps being filled with the following lines until PAT2 is found. In that point, it prints the stored content and empties the buffer.",
    "tag": "awk"
  },
  {
    "question": "Printing column separated by comma using Awk command line",
    "answer": "Try:\nawk -F',' '{print $3}' myfile.txt\n\nHere in -F you are saying to awk that use , as the field separator.",
    "tag": "awk"
  },
  {
    "question": "How to print all the columns after a particular number using awk?",
    "answer": "awk '{ s = \"\"; for (i = 9; i <= NF; i++) s = s $i \" \"; print s }'",
    "tag": "awk"
  },
  {
    "question": "cut or awk command to print first field of first row",
    "answer": "Specify NR if you want to capture output from selected rows:\nawk 'NR==1{print $1}' /etc/*release\n\nAn alternative (ugly) way of achieving the same would be:\nawk '{print $1; exit}'\n\nAn efficient way of getting the first string from a specific line, say line 42, in the output would be:\nawk 'NR==42{print $1; exit}'",
    "tag": "awk"
  },
  {
    "question": "Split one file into multiple files based on delimiter",
    "answer": "A one liner, no programming. (except the regexp etc.)\ncsplit --digits=2  --quiet --prefix=outfile infile \"/-|/+1\" \"{*}\"\n\n\ntested on:\ncsplit (GNU coreutils) 8.30\nNotes about usage on Apple Mac\n\"For OS X users, note that the version of csplit that comes with the OS doesn't work. You'll want the version in coreutils (installable via Homebrew), which is called gcsplit.\" — @Danial\n\"Just to add, you can get the version for OS X to work (at least with High Sierra). You just need to tweak the args a bit csplit -k -f=outfile infile \"/-\\|/+1\" \"{3}\". Features that don't seem to work are the \"{*}\", I had to be specific on the number of separators, and needed to add -k to avoid it deleting all outfiles if it can't find a final separator. Also if you want --digits, you need to use -n instead.\" — @Pebbl",
    "tag": "awk"
  },
  {
    "question": "How to print the number of characters in each line of a text file",
    "answer": "Use Awk.\nawk '{ print length }' abc.txt",
    "tag": "awk"
  },
  {
    "question": "Select row and element in awk",
    "answer": "To print the second line:\nawk 'FNR == 2 {print}'\n\nTo print the second field:\nawk '{print $2}'\n\nTo print the third field of the fifth line:\nawk 'FNR == 5 {print $3}'\n\nHere's an example with a header line and (redundant) field descriptions:\nawk 'BEGIN {print \"Name\\t\\tAge\"}  FNR == 5 {print \"Name: \"$3\"\\tAge: \"$2}'\n\nThere are better ways to align columns than \"\\t\\t\" by the way.\nUse exit to stop as soon as you've printed the desired record if there's no reason to process the whole file:\nawk 'FNR == 2 {print; exit}'",
    "tag": "awk"
  },
  {
    "question": "Using awk to remove the Byte-order mark",
    "answer": "Using GNU sed (on Linux or Cygwin):\n# Removing BOM from all text files in current directory:\nsed -i '1 s/^\\xef\\xbb\\xbf//' *.txt\n\nOn FreeBSD:\nsed -i .bak '1 s/^\\xef\\xbb\\xbf//' *.txt\n\nAdvantage of using GNU or FreeBSD sed: the -i parameter means \"in place\", and will update files without the need for redirections or weird tricks.\nOn Mac:\nThis awk solution in another answer works, but the sed command above does not work. At least on Mac (Sierra) sed documentation does not mention supporting hexadecimal escaping ala \\xef.\nA similar trick can be achieved with any program by piping to the sponge tool from moreutils:\nawk '…' INFILE | sponge INFILE",
    "tag": "awk"
  },
  {
    "question": "Turning multiple lines into one comma separated line",
    "answer": "Using paste command:\npaste -d, -s file",
    "tag": "awk"
  },
  {
    "question": "awk - concatenate two string variable and assign to a third",
    "answer": "Just use var = var1 var2 and it will automatically concatenate the vars var1 and var2:\nawk '{new_var=$1$2; print new_var}' file\n\nYou can put an space in between with:\nawk '{new_var=$1\" \"$2; print new_var}' file\n\nWhich in fact is the same as using FS, because it defaults to the space:\nawk '{new_var=$1 FS $2; print new_var}' file\n\nTest\n$ cat file\nhello how are you\ni am fine\n$ awk '{new_var=$1$2; print new_var}' file\nhellohow\niam\n$ awk '{new_var=$1 FS $2; print new_var}' file\nhello how\ni am\n\nYou can play around with it in ideone: http://ideone.com/4u2Aip",
    "tag": "awk"
  },
  {
    "question": "Why doesn't `\\d` work in regular expressions in sed?",
    "answer": "\\d is a switch not a regular expression macro. If you want to use some predefined \"constant\" instead of [0-9] expression just try run this code:\ns/[[:digit:]]+//g",
    "tag": "awk"
  },
  {
    "question": "how to use sed, awk, or gawk to print only what is matched?",
    "answer": "My sed (Mac OS X) didn't work with +. I tried * instead and I added p tag for printing match:\nsed -n 's/^.*abc\\([0-9]*\\)xyz.*$/\\1/p' example.txt\n\nFor matching at least one numeric character without +, I would use:\nsed -n 's/^.*abc\\([0-9][0-9]*\\)xyz.*$/\\1/p' example.txt",
    "tag": "awk"
  },
  {
    "question": "how to use sed, awk, or gawk to print only what is matched?",
    "answer": "My sed (Mac OS X) didn't work with +. I tried * instead and I added p tag for printing match:\nsed -n 's/^.*abc\\([0-9]*\\)xyz.*$/\\1/p' example.txt\n\nFor matching at least one numeric character without +, I would use:\nsed -n 's/^.*abc\\([0-9][0-9]*\\)xyz.*$/\\1/p' example.txt",
    "tag": "awk"
  },
  {
    "question": "find difference between two text files with one item per line",
    "answer": "grep -Fxvf file1 file2\n\nWhat the flags mean:\n-F, --fixed-strings\n              Interpret PATTERN as a list of fixed strings, separated by newlines, any of which is to be matched.    \n-x, --line-regexp\n              Select only those matches that exactly match the whole line.\n-v, --invert-match\n              Invert the sense of matching, to select non-matching lines.\n-f FILE, --file=FILE\n              Obtain patterns from FILE, one per line.  The empty file contains zero patterns, and therefore matches nothing.",
    "tag": "awk"
  },
  {
    "question": "Insert multiple lines into a file after specified pattern using shell script",
    "answer": "Another sed,\nsed '/cdef/r add.txt' input.txt\n\ninput.txt:\nabcd\naccd\ncdef\nline\nweb\n\nadd.txt:\nline1\nline2\nline3\nline4\n\nTest:\nsat:~# sed '/cdef/r add.txt' input.txt\nabcd\naccd\ncdef\nline1\nline2\nline3\nline4\nline\nweb\n\nIf you want to apply the changes in input.txt file. Then, use -i with sed.\nsed -i '/cdef/r add.txt' input.txt\n\nIf you want to use a regex as an expression you have to use the -E tag with sed.\nsed -E '/RegexPattern/r add.txt' input.txt",
    "tag": "awk"
  },
  {
    "question": "Using sed, Insert a line above or below the pattern?",
    "answer": "To append after the pattern: (-i is for in place replace). line1 and line2 are the lines you want to append(or prepend)\nsed -i '/pattern/a \\\nline1 \\\nline2' inputfile\n\nOutput:\n#cat inputfile\n pattern\n line1 line2 \n\nTo prepend the lines before:\nsed -i '/pattern/i \\\nline1 \\\nline2' inputfile\n\nOutput:\n#cat inputfile\n line1 line2 \n pattern",
    "tag": "awk"
  },
  {
    "question": "grep for multiple strings in file on different lines (ie. whole file, not line based search)?",
    "answer": "You can use:    \ngrep -l Dansk * | xargs grep -l Norsk | xargs grep -l Svenska\n\nIf you want also to find in hidden files:\ngrep -l Dansk .* | xargs grep -l Norsk | xargs grep -l Svenska",
    "tag": "awk"
  },
  {
    "question": "How to add to the end of lines containing a pattern with sed or awk?",
    "answer": "This works for me\nsed '/^all:/ s/$/ anotherthing/' file\n\nThe first part is a pattern to find and the second part is an ordinary sed's substitution using $ for the end of a line.\nIf you want to change the file during the process, use -i option\nsed -i '/^all:/ s/$/ anotherthing/' file\n\nOr you can redirect it to another file\nsed '/^all:/ s/$/ anotherthing/' file > output",
    "tag": "awk"
  },
  {
    "question": "Printing with sed or awk a line following a matching pattern",
    "answer": "Never use the word \"pattern\" on it's own in this context as it is ambiguous. Always use \"string\" or \"regexp\" (or in shell \"globbing pattern\"), whichever it is you really mean. See How do I find the text that matches a pattern? for more about that.\nThe specific answer you want is:\nawk 'f{print;f=0} /regexp/{f=1}' file\n\nor specializing the more general solution of the Nth record after a regexp (idiom \"c\" below):\nawk 'c&&!--c; /regexp/{c=1}' file\n\nThe following idioms describe how to select a range of records given a specific regexp to match:\na) Print all records from some regexp:\nawk '/regexp/{f=1}f' file\n\nb) Print all records after some regexp:\nawk 'f;/regexp/{f=1}' file\n\nc) Print the Nth record after some regexp:\nawk 'c&&!--c;/regexp/{c=N}' file\n\nd) Print every record except the Nth record after some regexp:\nawk 'c&&!--c{next}/regexp/{c=N}1' file\n\ne) Print the N records after some regexp:\nawk 'c&&c--;/regexp/{c=N}' file\n\nf) Print every record except the N records after some regexp:\nawk 'c&&c--{next}/regexp/{c=N}1' file\n\ng) Print the N records from some regexp:\nawk '/regexp/{c=N}c&&c--' file\n\nI changed the variable name from \"f\" for \"found\" to \"c\" for \"count\" where\nappropriate as that's more expressive of what the variable actually IS.\nf is short for found. Its a boolean flag that I'm setting to 1 (true) when I find a string matching the regular expression regexp in the input (/regexp/{f=1}). The other place you see f on its own in each script it's being tested as a condition and when true causes awk to execute its default action of printing the current record. So input records only get output after we see regexp and set f to 1/true.\nc && c-- { foo } means \"if c is non-zero then decrement it and if it's still non-zero then execute foo\" so if c starts at 3 then it'll be decremented to 2 and then foo executed, and on the next input line c is now 2 so it'll be decremented to 1 and then foo executed again, and on the next input line c is now 1 so it'll be decremented to 0 but this time foo will not be executed because 0 is a false condition. We do c && c-- instead of just testing for c-- > 0 so we can't run into a case with a huge input file where c hits zero and  continues getting decremented so often it wraps around and becomes positive again.",
    "tag": "awk"
  },
  {
    "question": "How can I format the output of a bash command in neat columns",
    "answer": "column(1) is your friend.\n$ column -t <<< '\"option-y\"      yank-pop\n> \"option-z\"      execute-last-named-cmd\n> \"option-|\"      vi-goto-column\n> \"option-~\"      _bash_complete-word\n> \"option-control-?\"      backward-kill-word\n> \"control-_\"     undo\n> \"control-?\"     backward-delete-char\n> '\n\"option-y\"          yank-pop\n\"option-z\"          execute-last-named-cmd\n\"option-|\"          vi-goto-column\n\"option-~\"          _bash_complete-word\n\"option-control-?\"  backward-kill-word\n\"control-_\"         undo\n\"control-?\"         backward-delete-char",
    "tag": "awk"
  },
  {
    "question": "How to remove leading whitespace from each line in a file",
    "answer": "sed \"s/^[ \\t]*//\" -i youfile\n\nWarning: this will overwrite the original file.",
    "tag": "awk"
  },
  {
    "question": "Can we use shell variables in awk?",
    "answer": "Yes, you can use the shell variables inside awk. There are a bunch of ways of doing it, but my favorite is to define a variable with the -v flag:\n$ echo | awk -v my_var=4 '{print \"My var is \" my_var}'\nMy var is 4\n\nJust pass the environment variable as a parameter to the -v flag. For example, if you have this variable:\n$ VAR=3\n$ echo $VAR\n3\n\nUse it this way:\n$ echo | awk -v env_var=\"$VAR\" '{print \"The value of VAR is \" env_var}'\nThe value of VAR is 3\n\nOf course, you can give the same name, but the $ will not be necessary:\n$ echo | awk -v VAR=\"$VAR\" '{print \"The value of VAR is \" VAR}'\nThe value of VAR is 3\n\nA note about the $ in awk: unlike bash, Perl, PHP etc., it is not part of the variable's name but instead an operator.",
    "tag": "awk"
  },
  {
    "question": "Use awk to find average of a column",
    "answer": "awk '{ sum += $2; n++ } END { if (n > 0) print sum / n; }'\n\nAdd the numbers in $2 (second column) in sum (variables are auto-initialized to zero by awk) and increment the number of rows (which could also be handled via built-in variable NR).  At the end, if there was at least one value read, print the average.\nawk '{ sum += $2 } END { if (NR > 0) print sum / NR }'\n\nIf you want to use the shebang notation, you could write:\n#!/bin/awk\n\n{ sum += $2 }\nEND { if (NR > 0) print sum / NR }\n\nYou can also control the format of the average with printf() and a suitable format (\"%13.6e\\n\", for example).\nYou can also generalize the code to average the Nth column (with N=2 in this sample) using:\nawk -v N=2 '{ sum += $N } END { if (NR > 0) print sum / NR }'",
    "tag": "awk"
  },
  {
    "question": "Swap two columns - awk, sed, python, perl",
    "answer": "You can do this by swapping values of the first two fields:\nawk ' { t = $1; $1 = $2; $2 = t; print; } ' input_file",
    "tag": "awk"
  },
  {
    "question": "Print line numbers starting at zero using awk",
    "answer": "NR starts at 1, so use\nawk '{print NR-1 \",\" $0}'",
    "tag": "awk"
  },
  {
    "question": "Calling an executable program using awk",
    "answer": "From the AWK man page:\n\nsystem(cmd)\n              executes cmd and returns its exit status\n\nThe GNU AWK manual also has a section that, in part, describes the system function and provides an example:\nsystem(\"date | mail -s 'awk run done' root\")",
    "tag": "awk"
  },
  {
    "question": "Command line: search and replace in all filenames matched by grep",
    "answer": "This appears to be what you want, based on the example you gave:\nsed -i 's/foo/bar/g' *\n\nIt is not recursive (it will not descend into subdirectories). For a nice solution replacing in selected files throughout a tree I would use find:\nfind . -name '*.html' -print -exec sed -i.bak 's/foo/bar/g' {} \\;\n\nThe *.html is the expression that files must match, the .bak after the -i makes a copy of the original file, with a .bak extension (it can be any extension you like) and the g at the end of the sed expression tells sed to replace multiple copies on one line (rather than only the first one). The -print to find is a convenience to show which files were being matched. All this depends on the exact versions of these tools on your system.",
    "tag": "awk"
  },
  {
    "question": "How to get first n characters of each line in unix data file",
    "answer": "With cut:\n$ cut -c-22 file\n0000000000011999980001\n0000000000021999980001\n0000000000031999980001\n0000000000041999980001\n0000000000051999980001\n0000000000061999980001\n\nIf I understand the second requirement you want to split the first 22 characters into two columns of length 10 and 12. sed is the best choice for this:\n$ sed -r 's/(.{10})(.{12}).*/\\1 \\2/' file\n0000000000 011999980001\n0000000000 021999980001\n0000000000 031999980001\n0000000000 041999980001\n0000000000 051999980001\n0000000000 061999980001",
    "tag": "awk"
  },
  {
    "question": "how to remove the first two columns in a file using shell (awk, sed, whatever)",
    "answer": "You can do it with cut:\ncut -d \" \" -f 3- input_filename > output_filename\n\nExplanation:\n\ncut: invoke the cut command\n-d \" \": use a single space as the delimiter (cut uses TAB by default)\n-f: specify fields to keep\n3-: all the fields starting with field 3\ninput_filename: use this file as the input\n> output_filename: write the output to this file.\n\nAlternatively, you can do it with awk:\nawk '{$1=\"\"; $2=\"\"; sub(\"  \", \" \"); print}' input_filename > output_filename\n\nExplanation:\n\nawk: invoke the awk command\n$1=\"\"; $2=\"\";: set field 1 and 2 to the empty string\nsub(...);: clean up the output fields because fields 1 & 2 will still be delimited by \" \"\nprint: print the modified line\ninput_filename > output_filename: same as above.",
    "tag": "awk"
  },
  {
    "question": "How can I get the length of an array in awk?",
    "answer": "When you split an array, the number of elements is returned, so you can say:\necho \"hello world\" | awk '{n=split($0, array, \" \")} END{print n }'\n# ------------------------^^^--------------------------------^^\n\nOutput is:\n2",
    "tag": "awk"
  },
  {
    "question": "How to UNCOMMENT a line that contains a specific string using Sed?",
    "answer": "Yes, to comment line containing specific string with sed, simply do:\nsed -i '/<pattern>/s/^/#/g' file\n\nAnd to uncomment it:\nsed -i '/<pattern>/s/^#//g' file\n\nIn your case:\nsed -i '/2001/s/^/#/g' file    (to comment out)\nsed -i '/2001/s/^#//g' file    (to uncomment)\n\nOption \"g\" at the end means global change. If you want to change only a single instance of pattern, just skip this.",
    "tag": "awk"
  },
  {
    "question": "Convert date formats in bash",
    "answer": "#since this was yesterday\ndate -dyesterday +%Y%m%d\n\n#more precise, and more recommended\ndate -d'27 JUN 2011' +%Y%m%d\n\n#assuming this is similar to yesterdays `date` question from you \n#http://stackoverflow.com/q/6497525/638649\ndate -d'last-monday' +%Y%m%d\n\n#going on @seth's comment you could do this\nDATE=\"27 jun 2011\"; date -d\"$DATE\" +%Y%m%d\n\n#or a method to read it from stdin\nread -p \"  Get date >> \" DATE; printf \"  AS YYYYMMDD format >> %s\"  `date\n-d\"$DATE\" +%Y%m%d`    \n\n#which then outputs the following:\n#Get date >> 27 june 2011   \n#AS YYYYMMDD format >> 20110627\n\n#if you really want to use awk\necho \"27 june 2011\" | awk '{print \"date -d\\\"\"$1FS$2FS$3\"\\\" +%Y%m%d\"}' | bash\n\n#note | bash just redirects awk's output to the shell to be executed\n#FS is field separator, in this case you can use $0 to print the line\n#But this is useful if you have more than one date on a line\n\nMore on Dates\nnote this only works on GNU date\nI have read that:\n\nSolaris version of date, which is unable\n  to support -d can be resolve with\n  replacing sunfreeware.com version of\n  date",
    "tag": "awk"
  },
  {
    "question": "How to delete from a text file, all lines that contain a specific string?",
    "answer": "To remove the line and print the output to standard out:\nsed '/pattern to match/d' ./infile\n\nTo directly modify the file – does not work with BSD sed:\nsed -i '/pattern to match/d' ./infile\n\nSame, but for BSD sed (Mac OS X and FreeBSD) – does not work with GNU sed:\nsed -i '' '/pattern to match/d' ./infile\n\nTo directly modify the file (and create a backup) – works with BSD and GNU sed:\nsed -i.bak '/pattern to match/d' ./infile",
    "tag": "sed"
  },
  {
    "question": "How can I replace each newline (\\n) with a space using sed?",
    "answer": "sed is intended to be used on line-based input. Although it can do what you need.\n\nA better option here is to use the tr command as follows:\ntr '\\n' ' ' < input_filename\n\nor remove the newline characters entirely:\ntr -d '\\n' < input.txt > output.txt\n\nor if you have the GNU version (with its long options)\ntr --delete '\\n' < input.txt > output.txt",
    "tag": "sed"
  },
  {
    "question": "How can I do a recursive find/replace of a string with awk or sed?",
    "answer": "find /home/www \\( -type d -name .git -prune \\) -o -type f -print0 | xargs -0 sed -i 's/subdomainA\\.example\\.com/subdomainB.example.com/g'\n\n-print0 tells find to print each of the results separated by a null character, rather than a new line. In the unlikely event that your directory has files with newlines in the names, this still lets xargs work on the correct filenames.\n\\( -type d -name .git -prune \\) is an expression which completely skips over all directories named .git. You could easily expand it, if you use SVN or have other folders you want to preserve -- just match against more names. It's roughly equivalent to -not -path .git, but more efficient, because rather than checking every file in the directory, it skips it entirely. The -o after it is required because of how -prune actually works.\nFor more information, see man find.",
    "tag": "sed"
  },
  {
    "question": "Bash tool to get nth line from a file",
    "answer": "head and pipe with tail will be slow for a huge file. I would suggest sed like this:\nsed 'NUMq;d' file\n\nWhere NUM is the number of the line you want to print; so, for example, sed '10q;d' file to print the 10th line of file.\nExplanation:\nNUMq will quit immediately when the line number is NUM.\nd will delete the line instead of printing it; this is inhibited on the last line because the q causes the rest of the script to be skipped when quitting.\nIf you have NUM in a variable, you will want to use double quotes instead of single:\nsed \"${NUM}q;d\" file",
    "tag": "sed"
  },
  {
    "question": "How can I remove the first line of a text file using bash/sed script?",
    "answer": "Try tail:\ntail -n +2 \"$FILE\"\n\n-n x: Just print the last x lines. tail -n 5 would give you the last 5 lines of the input. The + sign kind of inverts the argument and make tail print anything but the first x-1 lines. tail -n +1 would print the whole file, tail -n +2 everything but the first line, etc.\nGNU tail is much faster than sed. tail is also available on BSD and the -n +2 flag is consistent across both tools. Check the FreeBSD or OS X man pages for more. \nThe BSD version can be much slower than sed, though. I wonder how they managed that; tail should just read a file line by line while sed does pretty complex operations involving interpreting a script, applying regular expressions and the like.\nNote: You may be tempted to use\n# THIS WILL GIVE YOU AN EMPTY FILE!\ntail -n +2 \"$FILE\" > \"$FILE\"\n\nbut this will give you an empty file. The reason is that the redirection (>) happens before tail is invoked by the shell:\n\nShell truncates file $FILE\nShell creates a new process for tail\nShell redirects stdout of the tail process to $FILE\ntail reads from the now empty $FILE\n\nIf you want to remove the first line inside the file, you should use:\ntail -n +2 \"$FILE\" > \"$FILE.tmp\" && mv \"$FILE.tmp\" \"$FILE\"\n\nThe && will make sure that the file doesn't get overwritten when there is a problem.",
    "tag": "sed"
  },
  {
    "question": "How to remove double-quotes in jq output for parsing json files in bash?",
    "answer": "Use the -r (or --raw-output) option to emit raw strings as output:\njq -r '.name' <json.txt",
    "tag": "sed"
  },
  {
    "question": "Find and replace in file and overwrite file doesn't work, it empties the file",
    "answer": "When the shell sees  > index.html in the command line it opens the file index.html for writing, wiping off all its previous contents.\nTo fix this you need to pass the -i option to sed to make the changes inline and create a backup of the original file before it does the changes in-place:\nsed -i.bak s/STRING_TO_REPLACE/STRING_TO_REPLACE_IT/g index.html\n\nWithout the .bak the command will fail on some platforms, such as Mac OSX.",
    "tag": "sed"
  },
  {
    "question": "What is the difference between sed and awk?",
    "answer": "sed is a stream editor. It works with streams of characters on a per-line basis. It has a primitive programming language that includes goto-style loops and simple conditionals (in addition to pattern matching and address matching). There are essentially only two \"variables\": pattern space and hold space. Readability of scripts can be difficult. Mathematical operations are extraordinarily awkward at best.\nThere are various versions of sed with different levels of support for command line options and language features.\nawk is oriented toward delimited fields on a per-line basis. It has much more robust programming constructs including if/else, while, do/while and for (C-style and array iteration). There is complete support for variables and single-dimension associative arrays plus (IMO) kludgey multi-dimension arrays. Mathematical operations resemble those in C. It has printf and functions. The \"K\" in \"AWK\" stands for \"Kernighan\" as in \"Kernighan and Ritchie\" of the book \"C Programming Language\" fame (not to forget Aho and Weinberger). One could conceivably write a detector of academic plagiarism using awk.\nGNU awk (gawk) has numerous extensions, including true multidimensional arrays in the latest version. There are other variations of awk including mawk and nawk.\nBoth programs use regular expressions for selecting and processing text.\nI would tend to use sed where there are patterns in the text. For example, you could replace all the negative numbers in some text that are in the form \"minus-sign followed by a sequence of digits\" (e.g. \"-231.45\") with the \"accountant's brackets\" form (e.g. \"(231.45)\") using this (which has room for improvement):\nsed 's/-\\([0-9.]\\+\\)/(\\1)/g' inputfile\n\nI would use awk when the text looks more like rows and columns or, as awk refers to them \"records\" and \"fields\". If I was going to do a similar operation as above, but only on the third field in a simple comma delimited file I might do something like:\nawk -F, 'BEGIN {OFS = \",\"} {gsub(\"-([0-9.]+)\", \"(\" substr($3, 2) \")\", $3); print}' inputfile\n\nOf course those are just very simple examples that don't illustrate the full range of capabilities that each has to offer.",
    "tag": "sed"
  },
  {
    "question": "sed edit file in-place",
    "answer": "The -i option streams the edited content into a new file and then renames it behind the scenes, anyway.\nExample:\nsed -i 's/STRING_TO_REPLACE/STRING_TO_REPLACE_IT/g' filename\n\nwhile on macOS you need:\nsed -i '' 's/STRING_TO_REPLACE/STRING_TO_REPLACE_IT/g' filename",
    "tag": "sed"
  },
  {
    "question": "Delete empty lines using sed",
    "answer": "You may have spaces or tabs in your \"empty\" line. Use POSIX classes with sed to remove all lines containing only whitespace:\nsed '/^[[:space:]]*$/d'\n\nA shorter version that uses ERE, for example with gnu sed:\nsed -r '/^\\s*$/d'\n\n(Note that sed does NOT support PCRE.)",
    "tag": "sed"
  },
  {
    "question": "Non greedy (reluctant) regex matching in sed?",
    "answer": "Neither basic nor extended Posix/GNU regex recognizes the non-greedy quantifier; you need a later regex.  Fortunately, Perl regex for this context is pretty easy to get:\nperl -pe 's|(http://.*?/).*|\\1|'",
    "tag": "sed"
  },
  {
    "question": "Remove first and last quote (\") from a variable",
    "answer": "Use tr to delete \":\n echo \"$opt\" | tr -d '\"'\n\nNOTE: This does not fully answer the question, removes all double quotes, not just leading and trailing. See other answers below.",
    "tag": "sed"
  },
  {
    "question": "sed command with -i option failing on Mac, but works on Linux",
    "answer": "Portable solution below\n\nWhy you get the error\nThe -i option (alternatively, --in-place) means that you want files edited in-place, rather than streaming the change to a new place.\nModifying a file in-place suggests a need for a backup file - and so a user-specified extension is expected after -i, but the parsing of the extension argument is handled differently under GNU sed & Mac (BSD) sed:\n\nGNU : \"If no extension is supplied, the original file is overwritten without making a backup.\" - effectively, you can omit specify a file extension altogether. The extension must be supplied immediately after the -i, with no intervening space.\nMac (BSD) : \"If a zero-length extension is given, no backup will be saved.\" - you must supply an extension, but it can be the empty string '' if you want, to disable the backup.\n\nSo GNU & Mac will interpret this differently:\nsed -i 's/hello/bye/g' just_a_file.txt\n\n\nGNU : No extension is supplied immediately after the -i, so create no backup, use s/hello/bye/g as the text-editing command, and act on the file just_a_file.txt in-place.\nMac (BSD) : Use s/hello/bye/g as the backup file extension (!) and just_a_file.txt as the editing command (regular expression).\nResult: the command code used is j (not a valid command code for substitution, e.g. s), hence we get the error invalid command code j.\n\n# This still create a `my_file.txt-e` backup on macOS Sonoma (14.5)\n# and a `my_file.txt''` on Linux\nsed -i'' -e 's/hello/bye/g' my_file.txt\n\nPlacing the extension immediately after the -i (eg -i'' or -i'.bak', without a space) is what GNU sed expects, but macOS expect a space after -i (eg -i '' or -i '.bak').\nand is now accepted by Mac (BSD) sed too, though it wasn't tolerated by earlier versions (eg with Mac OS X v10.6, a space was required after -i, eg -i '.bak').\nThe -e parameter allows us to be explicit about where we're declaring the edit command.\nUntil Mac OS was updated in 2013, there wasn't\nStill there isn't any portable command across GNU and Mac (BSD), as these variants all failed (with an error or unexpected backup files):\n\nsed -i -e ... - works on Linux but does not work on macOS as it creates -e backups\nsed -i '' -e ... - works on macOS but fails on Linux\nsed -i='' -e ... - create = backups files on macOS and Linux\nsed -i'' -e ... - create -e backups files on macOS\n\nPortable solution\nYou have few options to achieve the same result on Linux and macOS, e.g.:\n\nUse Perl: perl -i -pe's/old_link/new_link/g' *.\n\nUse gnu-sed on macOS (Install using Homebrew)\n\n\n# Install 'gnu-sed' on macOS using Homebrew\nbrew install gnu-sed\n# Use 'gsed' instead of 'sed' on macOS.\ngsed -i'' -e 's/hello/bye/g' my_file.txt\n\n\nNote: On macOS, you could add the bin path of gnu-sed containing the sed command to the PATH environment variable in your shell configuration file (.zshrc). \nIt is best not to do this, since there may be scripts that rely on the macOS built-in version.\nYou can add an alias for gsed as sed using alias sed=gsed (replacing macOS sed with GNU sed) in your ~/.zshrc. This should allow you to use sed \"linux-stile\" in your shell and will have no effects on scripts unless they contain shopt -s expand_aliases.\n\nIf you are using sed in a script, you can try to automate switching to gsed:\n#!/usr/bin/env bash\nset -Eeuo pipefail\n\nif [[ \"$OSTYPE\" == \"darwin\"* ]]; then\n  # Require gnu-sed.\n  if ! [ -x \"$(command -v gsed)\" ]; then\n    echo \"Error: 'gsed' is not istalled.\" >&2\n    echo \"If you are using Homebrew, install with 'brew install gnu-sed'.\" >&2\n    exit 1\n  fi\n  SED_CMD=gsed\nelse\n  SED_CMD=sed\nfi\n\n# Use '${SED_CMD}' instead of 'sed'\n${SED_CMD} -i'' -e 's/hello/bye/g' my_file.txt\n\nYou can temporarily set PATH to use \"gnu-sed\" sed for a script:\n# run a linux script containing sed without changing it\nPATH=\"$(brew --prefix)/opt/gnu-sed/libexec/gnubin:$PATH\" ./linux_script_using_sed.sh\n\nIf you are copy/pasting linux scripts, you can alias gsed to sed in the current shell:\nalias sed=gsed\nsed -i 's/hello/bye/g' just_a_file.txt\n\n\nUse -i '' on macOS and BSD or -i (GNU sed) otherwise\n\n#!/usr/bin/env bash\nset -Eeuo pipefail\n\ncase \"$OSTYPE\" in\n  darwin*|bsd*)\n    echo \"Using BSD sed style\"\n    sed_no_backup=( -i '' )\n    ;; \n  *)\n    echo \"Using GNU sed style\"\n    sed_no_backup=( -i )\n    ;;\nesac\n\nsed ${sed_no_backup[@]} -e 's/hello/bye/g' my_file.txt",
    "tag": "sed"
  },
  {
    "question": "Replace whole line containing a string using Sed",
    "answer": "You can use the change command to replace the entire line, and the -i flag to make the changes in-place. For example, using GNU sed:\nsed -i '/TEXT_TO_BE_REPLACED/c\\This line is removed by the admin.' /tmp/foo",
    "tag": "sed"
  },
  {
    "question": "Escape a string for a sed replace pattern",
    "answer": "Warning: This does not consider newlines. For a more in-depth answer, see this SO-question instead. (Thanks, Ed Morton & Niklas Peter)\nNote that escaping everything is a bad idea. Sed needs many characters to be escaped to get their special meaning. For example, if you escape a digit in the replacement string, it will turn in to a backreference.\nAs Ben Blank said, there are only three characters that need to be escaped in the replacement string (escapes themselves, forward slash for end of statement and & for replace all):\nESCAPED_REPLACE=$(printf '%s\\n' \"$REPLACE\" | sed -e 's/[\\/&]/\\\\&/g')\n# Now you can use ESCAPED_REPLACE in the original sed statement\nsed \"s/KEYWORD/$ESCAPED_REPLACE/g\"\n\nIf you ever need to escape the KEYWORD string, the following is the one you need:\nsed -e 's/[]\\/$*.^[]/\\\\&/g'\nAnd can be used by:\nKEYWORD=\"The Keyword You Need\";\nESCAPED_KEYWORD=$(printf '%s\\n' \"$KEYWORD\" | sed -e 's/[]\\/$*.^[]/\\\\&/g');\n\n# Now you can use it inside the original sed statement to replace text\nsed \"s/$ESCAPED_KEYWORD/$ESCAPED_REPLACE/g\"\n\nRemember, if you use a character other than / as delimiter, you need replace the slash in the expressions above wih the character you are using. See PeterJCLaw's comment for explanation.\nEdited: Due to some corner cases previously not accounted for, the commands above have changed several times. Check the edit history for details.",
    "tag": "sed"
  },
  {
    "question": "How can I output only captured groups with sed?",
    "answer": "The key to getting this to work is to tell sed to exclude what you don't want to be output as well as specifying what you do want. This technique depends on knowing how many matches you're looking for. The grep command below works for an unspecified number of matches.\nstring='This is a sample 123 text and some 987 numbers'\necho \"$string\" | sed -rn 's/[^[:digit:]]*([[:digit:]]+)[^[:digit:]]+([[:digit:]]+)[^[:digit:]]*/\\1 \\2/p'\n\nThis says:\n\nuse extended regular expressions (-r)\ndon't default to printing each line (-n)\nexclude zero or more non-digits\ninclude one or more digits\nexclude one or more non-digits\ninclude one or more digits\nexclude zero or more non-digits\nprint the substitution (p) (on one line)\n\nIn general, in sed you capture groups using parentheses and output what you capture using a back reference:\necho \"foobarbaz\" | sed 's/^foo\\(.*\\)baz$/\\1/'\n\nwill output \"bar\". If you use -r (-E for OS X) for extended regex, you don't need to escape the parentheses:\necho \"foobarbaz\" | sed -r 's/^foo(.*)baz$/\\1/'\n\nThere can be up to 9 capture groups and their back references. The back references are numbered in the order the groups appear, but they can be used in any order and can be repeated:\necho \"foobarbaz\" | sed -r 's/^foo(.*)b(.)z$/\\2 \\1 \\2/'\n\noutputs \"a bar a\".\nIf you have GNU grep:\necho \"$string\" | grep -Po '\\d+'\n\nIt may also work in BSD, including OS X:\necho \"$string\" | grep -Eo '\\d+'\n\nThese commands will match any number of digit sequences. The output will be on multiple lines.\nor variations such as:\necho \"$string\" | grep -Po '(?<=\\D )(\\d+)'\n\nThe -P option enables Perl Compatible Regular Expressions. See man 3 pcrepattern or man  3 pcresyntax.",
    "tag": "sed"
  },
  {
    "question": "Delete specific line number(s) from a text file using sed?",
    "answer": "If you want to delete lines from 5 through 10 and line 12th:\nsed -e '5,10d;12d' file\n\nThis will print the results to the screen. If you want to save the results to the same file:\nsed -i.bak -e '5,10d;12d' file\n\nThis will store the unmodified file as file.bak, and delete the given lines.\nNote: Line numbers start at 1. The first line of the file is 1, not 0.",
    "tag": "sed"
  },
  {
    "question": "Error when using 'sed' with 'find' command on OS X: \"invalid command code .\"",
    "answer": "If you are on a OS X, this probably has nothing to do with the sed command. On the OSX version of sed, the -i option expects an extension argument so your command is actually parsed as the extension argument and the file path is interpreted as the command code.\nTry adding the -e argument explicitly and giving '' as argument to -i:\nfind ./ -type f -exec sed -i '' -e \"s/192.168.20.1/new.domain.com/\" {} \\;\n\nSee this.",
    "tag": "sed"
  },
  {
    "question": "Insert line after match using sed",
    "answer": "Try doing this using GNU sed:\nsed '/CLIENTSCRIPT=\"foo\"/a CLIENTSCRIPT2=\"hello\"' file\n\nif you want to substitute in-place, use\nsed -i '/CLIENTSCRIPT=\"foo\"/a CLIENTSCRIPT2=\"hello\"' file\n\nOutput\nCLIENTSCRIPT=\"foo\"\nCLIENTSCRIPT2=\"hello\"\nCLIENTFILE=\"bar\"\n\nDoc\n\nsee sed doc and search \\a (append)",
    "tag": "sed"
  },
  {
    "question": "Replace comma with newline in sed on MacOS?",
    "answer": "Use tr instead:\ntr , '\\n' < file",
    "tag": "sed"
  },
  {
    "question": "Retrieve last 100 lines logs",
    "answer": "You can use tail command as follows:\ntail -100 <log file>   > newLogfile\n\nNow last 100 lines will be present in newLogfile\nEDIT:\nMore recent versions of tail as mentioned by twalberg use command:\ntail -n 100 <log file>   > newLogfile",
    "tag": "sed"
  },
  {
    "question": "sed in-place flag that works both on Mac (BSD) and Linux",
    "answer": "If you really want to just use sed -i the 'easy' way, the following DOES work on both GNU and BSD/Mac sed:\nsed -i.bak 's/foo/bar/' filename\n\nNote the lack of space and the dot.\nProof:\n# GNU sed\n% sed --version | head -1\nGNU sed version 4.2.1\n% echo 'foo' > file\n% sed -i.bak 's/foo/bar/' ./file\n% ls\nfile  file.bak\n% cat ./file\nbar\n\n# BSD sed\n% sed --version 2>&1 | head -1\nsed: illegal option -- -\n% echo 'foo' > file\n% sed -i.bak 's/foo/bar/' ./file\n% ls\nfile  file.bak\n% cat ./file\nbar\n\nObviously you could then just delete the .bak files.",
    "tag": "sed"
  },
  {
    "question": "How to swap text based on patterns at once with sed?",
    "answer": "Maybe something like this:\nsed 's/ab/~~/g; s/bc/ab/g; s/~~/bc/g'\n\nReplace ~ with a character that you know won't be in the string.",
    "tag": "sed"
  },
  {
    "question": "How to insert a text at the beginning of a file?",
    "answer": "sed can operate on an address:\n$ sed -i '1s/^/<added text> /' file\n\nWhat is this magical 1s you see on every answer here? Line addressing!.\nWant to add <added text> on the first 10 lines?\n$ sed -i '1,10s/^/<added text> /' file\n\nOr you can use Command Grouping:\n$ { echo -n '<added text> '; cat file; } >file.new\n$ mv file{.new,}",
    "tag": "sed"
  },
  {
    "question": "How to use sed to replace only the first occurrence in a file?",
    "answer": "A sed script that will only replace the first occurrence of \"Apple\" by \"Banana\"\nExample  \n     Input:      Output:\n\n     Apple       Banana\n     Apple       Apple\n     Orange      Orange\n     Apple       Apple\n\nThis is the simple script: Editor's note: works with GNU sed only.\nsed '0,/Apple/{s/Apple/Banana/}' input_filename\n\nThe first two parameters 0 and /Apple/ are the range specifier.  The s/Apple/Banana/ is what is executed within that range. So in this case \"within the range of the beginning (0) up to the first instance of Apple, replace Apple with Banana.  Only the first Apple will be replaced.\nBackground: In traditional sed the range specifier is also \"begin here\" and \"end here\" (inclusive).  However the lowest \"begin\" is the first line (line 1), and if the \"end here\" is a regex, then it is only attempted to match against on the next line after \"begin\", so the earliest possible end is line 2.  So since range is inclusive, smallest possible range is \"2 lines\" and smallest starting range is both lines 1 and 2 (i.e. if there's an occurrence on line 1, occurrences on line 2 will also be changed, not desired in this case). GNU sed adds its own extension of allowing specifying start as the \"pseudo\" line 0 so that the end of the range can be line 1, allowing it a range of \"only the first line\" if the regex  matches the first line.\nOr a simplified version (an empty RE like // means to re-use the one specified before it, so this is equivalent):\nsed '0,/Apple/{s//Banana/}' input_filename\n\nAnd the curly braces are optional for the s command, so this is also equivalent:\nsed '0,/Apple/s//Banana/' input_filename\n\nAll of these work on GNU sed only.\nYou can also install GNU sed on OS X using homebrew brew install gnu-sed.",
    "tag": "sed"
  },
  {
    "question": "Environment variable substitution in sed",
    "answer": "Your two examples look identical, which makes problems hard to diagnose.  Potential problems:\n\nYou may need double quotes, as in sed 's/xxx/'\"$PWD\"'/'\n$PWD may contain a slash, in which case you need to find a character not contained in $PWD to use as a delimiter.\n\nTo nail both issues at once, perhaps\nsed 's@xxx@'\"$PWD\"'@'",
    "tag": "sed"
  },
  {
    "question": "Find and replace with sed in directory and sub directories",
    "answer": "Your find should look like that to avoid sending directory names to sed:\nfind ./ -type f -exec sed -i -e 's/apple/orange/g' {} \\;",
    "tag": "sed"
  },
  {
    "question": "Change multiple files",
    "answer": "I'm surprised nobody has mentioned the -exec argument to find, which is intended for this type of use-case, although it will start a process for each matching file name:\nfind . -type f -name 'xa*' -exec sed -i 's/asd/dsg/g' {} \\;\n\nAlternatively, one could use xargs, which will invoke fewer processes:\nfind . -type f -name 'xa*' | xargs sed -i 's/asd/dsg/g'\n\nOr more simply use the + exec variant instead of ; in find to allow find to provide more than one file per subprocess call:\nfind . -type f -name 'xa*' -exec sed -i 's/asd/dsg/g' {} +",
    "tag": "sed"
  },
  {
    "question": "What are the differences between Perl, Python, AWK and sed?",
    "answer": "In order of appearance, the languages are sed, awk, perl, python.\nThe sed program is a stream editor and is designed to apply the actions from a script to each line (or, more generally, to specified ranges of lines) of the input file or files. Its language is based on ed, the Unix editor, and although it has conditionals and so on, it is hard to work with for complex tasks. You can work minor miracles with it - but at a cost to the hair on your head. However, it is probably the fastest of the programs when attempting tasks within its remit. (It has the least powerful regular expressions of the programs discussed - adequate for many purposes, but certainly not PCRE - Perl-Compatible Regular Expressions)\nThe awk program (name from the initials of its authors - Aho, Weinberger, and Kernighan) is a tool initially for formatting reports. It can be used as a souped-up sed; in its more recent versions, it is computationally complete. It uses an interesting idea - the program is based on 'patterns matched' and 'actions taken when the pattern matches'. The patterns are fairly powerful (Extended Regular Expressions). The language for the actions is similar to C. One of the key features of awk is that it splits the input automatically into records and each record into fields.\nPerl was written in part as an awk-killer and sed-killer. Two of the programs provided with it are a2p and s2p for converting awk scripts and sed scripts into Perl. Perl is one of the earliest of the next generation of scripting languages (Tcl/Tk can probably claim primacy). It has powerful integrated regular expression handling with a vastly more powerful language. It provides access to almost all system calls and has the extensibility of the CPAN modules. (Neither awk nor sed is extensible.) One of Perl's mottos is \"TMTOWTDI - There's more than one way to do it\" (pronounced \"tim-toady\"). Perl has 'objects', but it is more of an add-on than a fundamental part of the language.\nPython was written last, and probably in part as a reaction to Perl. It has some interesting syntactic ideas (indenting to indicate levels - no braces or equivalents). It is more fundamentally object-oriented than Perl; it is just as extensible as Perl.\nOK - when to use each?\n\nSed - when you need to do simple text transforms on files.\nAwk - when you only need simple formatting and summarisation or transformation of data.\nPerl - for almost any task, but especially when the task needs complex regular expressions.\nPython - for the same tasks that you could use Perl for.\n\nI'm not aware of anything that Perl can do that Python can't, nor vice versa. The choice between the two would depend on other factors. I learned Perl before there was a Python, so I tend to use it. Python has less accreted syntax and is generally somewhat simpler to learn. Perl 6, when it becomes available, will be a fascinating development.\n(Note that the 'overviews' of Perl and Python, in particular, are woefully incomplete; whole books could be written on the topic.)",
    "tag": "sed"
  },
  {
    "question": "How to concatenate multiple lines of output to one line?",
    "answer": "Use tr '\\n' ' ' to translate all newline characters to spaces:\n$ grep pattern file | tr '\\n' ' '\n\nNote: grep reads files, cat concatenates files. Don't cat file | grep!\nEdit:\ntr can only handle single character translations. You could use awk to change the output record separator like:\n$ grep pattern file | awk '{print}' ORS='\" '\n\nThis would transform:\none\ntwo \nthree\n\nto:\none\" two\" three\"",
    "tag": "sed"
  },
  {
    "question": "Delete all lines beginning with a # from a file",
    "answer": "This can be done with a sed one-liner:\nsed '/^#/d'\n\nThis says, \"find all lines that start with # and delete them, leaving everything else.\"",
    "tag": "sed"
  },
  {
    "question": "How to show only next line after the matched one?",
    "answer": "you can try with awk:\nawk '/blah/{getline; print}' logfile",
    "tag": "sed"
  },
  {
    "question": "RE error: illegal byte sequence on Mac OS X",
    "answer": "A sample command that exhibits the symptom: sed 's/./@/' <<<$'\\xfc' fails, because byte 0xfc is not a valid UTF-8 char.\nNote that, by contrast, GNU sed (Linux, but also installable on macOS) simply passes the invalid byte through, without reporting an error.\nUsing the formerly accepted answer is an option if you don't mind losing support for your true locale (if you're on a US system and you never need to deal with foreign characters, that may be fine.)\nHowever, the same effect can be had ad-hoc for a single command only:\nLC_ALL=C sed -i \"\" 's|\"iphoneos-cross\",\"llvm-gcc:-O3|\"iphoneos-cross\",\"clang:-Os|g' Configure\n\nNote: What matters is an effective LC_CTYPE setting of C, so LC_CTYPE=C sed ... would normally also work, but if LC_ALL happens to be set (to something other than C), it will override individual LC_*-category variables such as LC_CTYPE. Thus, the most robust approach is to set LC_ALL.\nHowever, (effectively) setting LC_CTYPE to C treats strings as if each byte were its own character (no interpretation based on encoding rules is performed), with no regard for the - multibyte-on-demand - UTF-8 encoding that OS X employs by default, where foreign characters have multibyte encodings.\nIn a nutshell: setting LC_CTYPE to C causes the shell and utilities to only recognize basic English letters as letters (the ones in the 7-bit ASCII range), so that foreign chars. will not be treated as letters, causing, for instance, upper-/lowercase conversions to fail.\nAgain, this may be fine if you needn't match multibyte-encoded characters such as é, and simply want to pass such characters through.\nIf this is insufficient and/or you want to understand the cause of the original error (including determining what input bytes caused the problem) and perform encoding conversions on demand, read on below.\n\nThe problem is that the input file's encoding does not match the shell's.\nMore specifically, the input file contains characters encoded in a way that is not valid in UTF-8 (as @Klas Lindbäck stated in a comment) - that's what the sed error message is trying to say by invalid byte sequence.\nMost likely, your input file uses a single-byte 8-bit encoding such as ISO-8859-1, frequently used to encode \"Western European\" languages.\nExample: \nThe accented letter à has Unicode codepoint 0xE0 (224) - the same as in ISO-8859-1. However, due to the nature of UTF-8 encoding, this single codepoint is represented as 2 bytes - 0xC3 0xA0, whereas trying to pass the single byte 0xE0 is invalid under UTF-8.\nHere's a demonstration of the problem using the string voilà encoded as ISO-8859-1, with the à represented as one byte (via an ANSI-C-quoted bash string ($'...') that uses \\x{e0} to create the byte):\nNote that the sed command is effectively a no-op that simply passes the input through, but we need it to provoke the error:\n  # -> 'illegal byte sequence': byte 0xE0 is not a valid char.\nsed 's/.*/&/' <<<$'voil\\x{e0}'\n\nTo simply ignore the problem, the above LCTYPE=C approach can be used:\n  # No error, bytes are passed through ('á' will render as '?', though).\nLC_CTYPE=C sed 's/.*/&/' <<<$'voil\\x{e0}'\n\nIf you want to determine which parts of the input cause the problem, try the following:\n  # Convert bytes in the 8-bit range (high bit set) to hex. representation.\n  # -> 'voil\\x{e0}'\niconv -f ASCII --byte-subst='\\x{%02x}' <<<$'voil\\x{e0}'\n\nThe output will show you all bytes that have the high bit set (bytes that exceed the 7-bit ASCII range) in hexadecimal form. (Note, however, that that also includes correctly encoded UTF-8 multibyte sequences - a more sophisticated approach would be needed to specifically identify invalid-in-UTF-8 bytes.)\n\nPerforming encoding conversions on demand:\nStandard utility iconv can be used to convert to (-t) and/or from (-f) encodings; iconv -l lists all supported ones.\nExamples:\nConvert FROM ISO-8859-1 to the encoding in effect in the shell (based on LC_CTYPE, which is UTF-8-based by default), building on the above example:\n  # Converts to UTF-8; output renders correctly as 'voilà'\nsed 's/.*/&/' <<<\"$(iconv -f ISO-8859-1 <<<$'voil\\x{e0}')\"\n\nNote that this conversion allows you to properly match foreign characters:\n  # Correctly matches 'à' and replaces it with 'ü': -> 'voilü'\nsed 's/à/ü/' <<<\"$(iconv -f ISO-8859-1 <<<$'voil\\x{e0}')\"\n\nTo convert the input BACK to ISO-8859-1 after processing, simply pipe the result to another iconv command:\nsed 's/à/ü/' <<<\"$(iconv -f ISO-8859-1 <<<$'voil\\x{e0}')\" | iconv -t ISO-8859-1",
    "tag": "sed"
  },
  {
    "question": "Command to get nth line of STDOUT",
    "answer": "Using sed, just for variety:\nls -l | sed -n 2p\n\nUsing this alternative, which looks more efficient since it stops reading the input when the required line is printed, may generate a SIGPIPE in the feeding process, which may in turn generate an unwanted error message:\nls -l | sed -n -e '2{p;q}'\n\nI've seen that often enough that I usually use the first (which is easier to type, anyway), though ls is not a command that complains when it gets SIGPIPE.\nFor a range of lines:\nls -l | sed -n 2,4p\n\nFor several ranges of lines:\nls -l | sed -n -e 2,4p -e 20,30p\nls -l | sed -n -e '2,4p;20,30p'",
    "tag": "sed"
  },
  {
    "question": "In-place edits with sed on OS X",
    "answer": "You can use the -i flag correctly by providing it with a suffix to add to the backed-up file. Extending your example: \nsed -i.bu 's/oldword/newword/' file1.txt\n\nWill give you two files: one with the name file1.txt that contains the substitution, and one with the name file1.txt.bu that has the original content. \nMildly dangerous\nIf you want to destructively overwrite the original file, use something like: \nsed -i '' 's/oldword/newword/' file1.txt\n      ^ note the space\n\nBecause of the way the line gets parsed, a space is required between the option flag and its argument because the argument is zero-length. \nOther than possibly trashing your original, I’m not aware of any further dangers of tricking sed this way. It should be noted, however, that if this invocation of sed is part of a script, The Unix Way™ would (IMHO) be to use sed non-destructively, test that it exited cleanly, and only then remove the extraneous file.",
    "tag": "sed"
  },
  {
    "question": "Appending a line to a file only if it does not already exist",
    "answer": "Just keep it simple :)\ngrep + echo should suffice:\ngrep -qxF 'include \"/configs/projectname.conf\"' foo.bar || echo 'include \"/configs/projectname.conf\"' >> foo.bar\n\n\n-q be quiet\n-x match the whole line\n-F pattern is a plain string\nhttps://linux.die.net/man/1/grep\n\nEdit:\nincorporated @cerin and @thijs-wouters suggestions.",
    "tag": "sed"
  },
  {
    "question": "How to grep for case insensitive string in a file?",
    "answer": "You can use the -i flag which makes your pattern case insensitive:\ngrep -iF \"success...\" file1\n\nAlso, there is no need for cat. grep takes a file with the syntax grep <pattern> <file>. I also used the -F flag to search for a fixed string to avoid escaping the ellipsis.",
    "tag": "sed"
  },
  {
    "question": "sed fails with \"unknown option to `s'\" error",
    "answer": "The problem is with slashes: your variable contains them and the final command will be something like sed \"s/string/path/to/something/g\", containing way too many slashes.\nSince sed can take any char as delimiter (without having to declare the new delimiter), you can try using another one that doesn't appear in your replacement string:\nreplacement=\"/my/path\"\nsed --expression \"s@pattern@$replacement@\"\n\nNote that this is not bullet proof: if the replacement string later contains @ it will break for the same reason, and any backslash sequences like \\1 will still be interpreted according to sed rules. Using | as a delimiter is also a nice option as it is similar in readability to /.",
    "tag": "sed"
  },
  {
    "question": "How to replace a whole line with sed?",
    "answer": "Try this:\nsed \"s/aaa=.*/aaa=xxx/g\"",
    "tag": "sed"
  },
  {
    "question": "Insert a line at specific line number with sed or awk",
    "answer": "sed -i '8i This is Line 8' FILE\n\ninserts at line 8\nThis is Line 8\n\ninto file FILE\n-i does the modification directly to file FILE, no output to stdout, as mentioned in the comments by glenn jackman.",
    "tag": "sed"
  },
  {
    "question": "How to use sed/grep to extract text between two words?",
    "answer": "GNU grep can also support positive & negative look-ahead & look-back:\nFor your case, the command would be:\necho \"Here is a string\" | grep -o -P '(?<=Here).*(?=string)'\n\nIf there are multiple occurrences of Here and string, you can choose whether you want to match from the first Here and last string or match them individually. In terms of regex, it is called as greedy match (first case) or non-greedy match (second case)\n$ echo 'Here is a string, and Here is another string.' | grep -oP '(?<=Here).*(?=string)' # Greedy match\n is a string, and Here is another \n$ echo 'Here is a string, and Here is another string.' | grep -oP '(?<=Here).*?(?=string)' # Non-greedy match (Notice the '?' after '*' in .*)\n is a \n is another",
    "tag": "sed"
  },
  {
    "question": "How to merge every two lines into one from the command line?",
    "answer": "paste is good for this job:\npaste -d \" \"  - - < filename",
    "tag": "sed"
  },
  {
    "question": "How to replace an entire line in a text file by line number",
    "answer": "Not the greatest, but this should work:\nsed -i 'Ns/.*/replacement-line/' file.txt\n\nwhere N should be replaced by your target line number. This replaces the line in the original file. To save the changed text in a different file, drop the -i option:\nsed 'Ns/.*/replacement-line/' file.txt > new_file.txt",
    "tag": "sed"
  },
  {
    "question": "How to delete duplicate lines in a file without sorting it in Unix",
    "answer": "awk '!seen[$0]++' file.txt\n\nseen is an associative array that AWK will pass every line of the file to. If a line isn't in the array then seen[$0] will evaluate to false. The ! is the logical NOT operator and will invert the false to true. AWK will print the lines where the expression evaluates to true.\nThe ++ increments seen so that seen[$0] == 1 after the first time a line is found and then seen[$0] == 2, and so on.\nAWK evaluates everything but 0 and \"\" (empty string) to true. If a duplicate line is placed in seen then !seen[$0] will evaluate to false and the line will not be written to the output.",
    "tag": "sed"
  },
  {
    "question": "How to remove the lines which appear on file B from another file A?",
    "answer": "If the files are sorted (they are in your example):\ncomm -23 file1 file2\n\n-23 suppresses the lines that are in both files, or only in file 2. If the files are not sorted, pipe them through sort first...\nSee the man page here",
    "tag": "sed"
  },
  {
    "question": "how to parse a JSON String with jq (or other alternatives)?",
    "answer": "jq has the fromjson builtin for this:\njq '.c | fromjson | .id' myFile.json\n\nfromjson was added in version 1.4.",
    "tag": "sed"
  },
  {
    "question": "sed one-liner to convert all uppercase to lowercase?",
    "answer": "Here are two methods for doing the conversion using tr and sed:\nUsing tr\nConvert uppercase to lowercase\ntr '[:upper:]' '[:lower:]' < input.txt > output.txt\n\nConvert lowercase to uppercase\ntr '[:lower:]' '[:upper:]' < input.txt > output.txt\n\nUsing sed on GNU (but not BSD or Mac)\nConvert uppercase to lowercase\nsed -e 's/\\(.*\\)/\\L\\1/' input.txt > output.txt\n\nConvert lowercase to uppercase\nsed -e 's/\\(.*\\)/\\U\\1/' input.txt > output.txt\n \n\nThe reason the sed version doesn't work on BSD or Mac is because those systems don't support the \\L or \\U flags",
    "tag": "sed"
  },
  {
    "question": "How do I remove newlines from a text file?",
    "answer": "tr --delete '\\n' < yourfile.txt\ntr -d '\\n' < yourfile.txt\n\nIf none of the commands posted here are working, then you have something other than a newline separating your fields. Possibly you have DOS/Windows line endings in the file (although I would expect the Perl solutions to work even in that case)?\nTry:\ntr -d \"\\n\\r\" < yourfile.txt\n\nIf that doesn't work then you're going to have to inspect your file more closely (e.g., in a hex editor) to find out what characters are actually in there that you want to remove.",
    "tag": "sed"
  },
  {
    "question": "How to use sed to remove the last n lines of a file",
    "answer": "I don't know about sed, but it can be done with head:\nhead -n -2 myfile.txt",
    "tag": "sed"
  },
  {
    "question": "sed: print only matching group",
    "answer": "Match the whole line, so add a .* at the beginning of your regex. This causes the entire line to be replaced with the contents of the group \necho \"foo bar <foo> bla 1 2 3.4\" |\n sed -n  's/.*\\([0-9][0-9]*[\\ \\t][0-9.]*[ \\t]*$\\)/\\1/p'\n2 3.4",
    "tag": "sed"
  },
  {
    "question": "How to insert strings containing slashes with sed?",
    "answer": "The easiest way would be to use a different delimiter in your search/replace lines, e.g.:\ns:?page=one&:pageone:g\n\nYou can use any character as a delimiter that's not part of either string. Or, you could escape it with a backslash:\ns/\\//foo/\n\nWhich would replace / with foo. You'd want to use the escaped backslash in cases where you don't know what characters might occur in the replacement strings (if they are shell variables, for example).",
    "tag": "sed"
  },
  {
    "question": "Is there any sed like utility for cmd.exe?",
    "answer": "Today powershell saved me.\nFor grep there is:\nget-content somefile.txt | where { $_ -match \"expression\"}\n\nor\nselect-string somefile.txt -pattern \"expression\"\n\nand for sed there is:\nget-content somefile.txt | %{$_ -replace \"expression\",\"replace\"}\n\nFor more detail about replace PowerShell function see this Microsoft article.",
    "tag": "sed"
  },
  {
    "question": "sed command with -i option (in-place editing) works fine on Ubuntu but not Mac",
    "answer": "Ubuntu ships with GNU sed, where the suffix for the -i option is optional. OS X ships with BSD sed, where the suffix is mandatory. Try sed -i ''",
    "tag": "sed"
  },
  {
    "question": "Why does sed not replace all occurrences?",
    "answer": "You should add the g modifier so that sed performs a global substitution of the contents of the pattern buffer:\necho dog dog dos | sed -e 's:dog:log:g'\n\nFor a fantastic documentation on sed, check http://www.grymoire.com/Unix/Sed.html. This global flag is explained here: http://www.grymoire.com/Unix/Sed.html#uh-6\nThe official documentation for GNU sed is available at http://www.gnu.org/software/sed/manual/",
    "tag": "sed"
  },
  {
    "question": "How can I delete a newline if it is the last character in a file?",
    "answer": "perl -pe 'chomp if eof' filename >filename2\n\nor, to edit the file in place:\nperl -pi -e 'chomp if eof' filename\n\n[Editor's note: -pi -e was originally -pie, but, as noted by several commenters and explained by @hvd, the latter doesn't work.]\nThis was described as a 'perl blasphemy' on the awk website I saw.\nBut, in a test, it worked.",
    "tag": "sed"
  },
  {
    "question": "How to pass a variable containing slashes to sed",
    "answer": "Use an alternate regex delimiter as sed allows you to use any delimiter (including control characters):\nsed \"s~$var~replace~g\" $file\n\n\nAs mentioned above we can use control character as delimiter as well like:\nsed \"s^[$var^[replace^[g\" file\n\nWhere ^[ is typed by pressing Ctrl-V-3 together.\nOr else in bash shell you can use this sed with \\03 as delimiter:\nd=$'\\03'\nsed \"s${d}$var${d}replace$d\" file",
    "tag": "sed"
  },
  {
    "question": "How can I strip first X characters from string using sed?",
    "answer": "The following should work:\nvar=\"pid: 1234\"\nvar=${var:5}\n\nAre you sure bash is the shell executing your script?\nEven the POSIX-compliant\nvar=${var#?????}\n\nwould be preferable to using an external process, although this requires you to hard-code the 5 in the form of a fixed-length pattern.",
    "tag": "sed"
  },
  {
    "question": "How to select lines between two marker patterns which may occur multiple times with awk/sed",
    "answer": "Use awk with a flag to trigger the print when necessary:\n$ awk '/abc/{flag=1;next}/mno/{flag=0}flag' file\ndef1\nghi1\njkl1\ndef2\nghi2\njkl2\n\nHow does this work?\n\n/abc/ matches lines having this text, as well as /mno/ does.  \n/abc/{flag=1;next} sets the flag when the text abc is found. Then, it skips the line.  \n/mno/{flag=0} unsets the flag when the text mno is found.\nThe final flag is a pattern with the default action, which is to print $0: if flag is equal 1 the line is printed.\n\nFor a more detailed description and examples, together with cases when the patterns are either shown or not, see How to select lines between two patterns?.",
    "tag": "sed"
  },
  {
    "question": "In Bash, how do I add a string after each line in a file?",
    "answer": "If your sed allows in place editing via the -i parameter:\nsed -e 's/$/string after each line/' -i filename\n\nIf not, you have to make a temporary file:\ntypeset TMP_FILE=$( mktemp )\n\ntouch \"${TMP_FILE}\"\ncp -p filename \"${TMP_FILE}\"\nsed -e 's/$/string after each line/' \"${TMP_FILE}\" > filename",
    "tag": "sed"
  },
  {
    "question": "sed or awk: delete n lines following a pattern",
    "answer": "I'll have a go at this.\nTo delete 5 lines after a pattern (including the line with the pattern):\nsed -e '/pattern/,+5d' file.txt\n\nTo delete 5 lines after a pattern (excluding the line with the pattern):\nsed -e '/pattern/{n;N;N;N;N;d}' file.txt",
    "tag": "sed"
  },
  {
    "question": "Add text at the end of each line",
    "answer": "You could try using something like:\nsed -n 's/$/:80/' ips.txt > new-ips.txt\n\nProvided that your file format is just as you have described in your question.\nThe s/// substitution command matches (finds) the end of each line in your file (using the $ character) and then appends (replaces) the :80 to the end of each line.  The ips.txt file is your input file... and new-ips.txt is your newly-created file (the final result of your changes.)\n\nAlso, if you have a list of IP numbers that happen to have port numbers attached already, (as noted by Vlad and as given by aragaer,) you could try using something like:\nsed '/:[0-9]*$/ ! s/$/:80/' ips.txt > new-ips.txt\n\nSo, for example, if your input file looked something like this (note the :80):\n127.0.0.1\n128.0.0.0:80\n121.121.33.111\n\nThe final result would look something like this:\n127.0.0.1:80\n128.0.0.0:80\n121.121.33.111:80",
    "tag": "sed"
  },
  {
    "question": "Printing everything except the first field with awk",
    "answer": "$1=\"\" leaves a space as Ben Jackson mentioned, so use a for loop:\nawk '{for (i=2; i<=NF; i++) print $i}' filename\n\nSo if your string was \"one two three\", the output will be:\ntwo \nthree\nIf you want the result in one row, you could do as follows:\nawk '{for (i=2; i<NF; i++) printf $i \" \"; print $NF}' filename\n\nThis will give you: \"two three\"",
    "tag": "sed"
  },
  {
    "question": "Changing all occurrences in a folder",
    "answer": "There is no way to do it using only sed. You'll need to use at least the find utility together:\nfind . -type f -exec sed -i.bak \"s/foo/bar/g\" {} \\;\n\nThis command will create a .bak file for each changed file.\nNotes: \n\nThe -i argument for sed command is a GNU extension, so, if you are running this command with the BSD's sed you will need to redirect the output to a new file then rename it.\nThe find utility does not implement the -exec argument in old UNIX boxes, so, you will need to use a | xargs instead.",
    "tag": "sed"
  },
  {
    "question": "sed whole word search and replace",
    "answer": "\\b in regular expressions match word boundaries (i.e. the location between the first word character and non-word character):\n$ echo \"bar embarassment\" | sed \"s/\\bbar\\b/no bar/g\"\nno bar embarassment",
    "tag": "sed"
  },
  {
    "question": "sed error: \"invalid reference \\1 on `s' command's RHS\"",
    "answer": "Don't you need to actually capture for that to work? i.e. for variant #2:\n-r -e \"s/WARNING: (\\([a-zA-Z0-9./\\\\ :-]\\+\\))/${warn}WARNING: \\1${c_end}/g\" \\\n\n(Note: untested)\nWithout the -r argument back-references (like \\1) won't work unless each parenthesis is escaped with a \\ character.\nWith -r, argument back-references (like \\1) won't work unless the parenthesis are NOT escaped.",
    "tag": "sed"
  },
  {
    "question": "What are the differences among grep, awk & sed?",
    "answer": "Short definition:\ngrep: search for specific terms in a file\n#usage\n$ grep This file.txt\nEvery line containing \"This\"\nEvery line containing \"This\"\nEvery line containing \"This\"\nEvery line containing \"This\"\n\n$ cat file.txt\nEvery line containing \"This\"\nEvery line containing \"This\"\nEvery line containing \"That\"\nEvery line containing \"This\"\nEvery line containing \"This\"\n\nNow awk and sed are completly different than grep.\nawk and sed are text processors. Not only do they have the ability to find what you are looking for in text, they have the ability to remove, add and modify the text as well (and much more). \nawk is mostly used for data extraction and reporting. sed is a stream editor\nEach one of them has its own functionality and specialties. \nExample \nSed\n$ sed -i 's/cat/dog/' file.txt\n# this will replace any occurrence of the characters 'cat' by 'dog'\n\nAwk\n$ awk '{print $2}' file.txt\n# this will print the second column of file.txt\n\nBasic awk usage:\nCompute sum/average/max/min/etc. what ever you may need.\n$ cat file.txt\nA 10\nB 20\nC 60\n$ awk 'BEGIN {sum=0; count=0; OFS=\"\\t\"} {sum+=$2; count++} END {print \"Average:\", sum/count}' file.txt\nAverage:    30\n\nI recommend that you read this book: Sed & Awk: 2nd Ed.\nIt will help you become a proficient sed/awk user on any unix-like environment.",
    "tag": "sed"
  },
  {
    "question": "Using sed, how do you print the first 'N' characters of a line?",
    "answer": "Don't use sed, use cut:\ngrep .... | cut -c 1-N\n\nIf you MUST use sed:\ngrep ... | sed -e 's/^\\(.\\{12\\}\\).*/\\1/'",
    "tag": "sed"
  },
  {
    "question": "How to remove trailing whitespaces with sed?",
    "answer": "You can use the in place option -i of sed for Linux and Unix:\nsed -i 's/[ \\t]*$//' \"$1\"\n\nBe aware the expression will delete trailing t's on OSX (you can use gsed to avoid this problem). It may delete them on BSD too.\nIf you don't have gsed, here is the correct (but hard-to-read) sed syntax on OSX:\nsed -i '' -E 's/[ '$'\\t'']+$//' \"$1\"\n\nThree single-quoted strings ultimately become concatenated into a single argument/expression. There is no concatenation operator in bash, you just place strings one after the other with no space in between.\nThe $'\\t' resolves as a literal tab-character in bash (using ANSI-C quoting), so the tab is correctly concatenated into the expression.",
    "tag": "sed"
  },
  {
    "question": "How to escape single quote in sed?",
    "answer": "Quote sed codes with double quotes:\n$ sed \"s/ones/one's/\"<<<\"ones thing\"   \none's thing\n\nI don't like escaping codes with hundreds of backslashes – hurts my eyes. Usually I do in this way:\n$ sed 's/ones/one\\x27s/'<<<\"ones thing\"\none's thing",
    "tag": "sed"
  },
  {
    "question": "How to insert a newline in front of a pattern?",
    "answer": "This works in bash and zsh, tested on Linux and OS X:\nsed 's/regexp/\\'$'\\n/g'\n\nIn general, for $ followed by a string literal in single quotes bash performs C-style backslash substitution, e.g. $'\\t' is translated to a literal tab. Plus, sed wants your newline literal to be escaped with a backslash, hence the \\ before $. And finally, the dollar sign itself shouldn't be quoted so that it's interpreted by the shell, therefore we close the quote before the $ and then open it again.\nEdit: As suggested in the comments by @mklement0, this works as well:\nsed $'s/regexp/\\\\\\n/g'\n\nWhat happens here is: the entire sed command is now a C-style string, which means the backslash that sed requires to be placed before the new line literal should now be escaped with another backslash. Though more readable,  in this case you won't be able to do shell string substitutions (without making it ugly again.)",
    "tag": "sed"
  },
  {
    "question": "Replace a string in shell script using a variable",
    "answer": "If you want to interpret $replace, you should not use single quotes since they prevent variable substitution.\nTry:\necho $LINE | sed -e \"s/12345678/${replace}/g\"\n\nTranscript:\npax> export replace=987654321\npax> echo X123456789X | sed \"s/123456789/${replace}/\"\nX987654321X\npax> _\n\nJust be careful to ensure that ${replace} doesn't have any characters of significance to sed (like / for instance) since it will cause confusion unless escaped. But if, as you say, you're replacing one number with another, that shouldn't be a problem.",
    "tag": "sed"
  },
  {
    "question": "How to decode URL-encoded string in shell?",
    "answer": "Here is a simple one-line solution.                   \n$ function urldecode() { : \"${*//+/ }\"; echo -e \"${_//%/\\\\x}\"; }\n\nIt may look like perl :) but it is just pure bash. No awks, no seds ... no overheads. Using the : builtin, special parameters, pattern substitution and the echo builtin's -e option to translate hex codes into characters. See bash's manpage for further details. You can use this function as separate command\n$ urldecode https%3A%2F%2Fgoogle.com%2Fsearch%3Fq%3Durldecode%2Bbash\nhttps://google.com/search?q=urldecode+bash\n\nor in variable assignments, like so:\n$ x=\"http%3A%2F%2Fstackoverflow.com%2Fsearch%3Fq%3Durldecode%2Bbash\"\n$ y=$(urldecode \"$x\")\n$ echo \"$y\"\nhttp://stackoverflow.com/search?q=urldecode+bash",
    "tag": "sed"
  },
  {
    "question": "Preserve line endings",
    "answer": "You can use the -b option for sed to have it treat the file as binary.  This will fix the problem with cygwin's sed on Windows. \nExample: sed -b 's/foo/bar/'\nIf you wish to match the end of the line, remember to match, capture and copy the optional carriage return.\nExample: sed -b 's/foo\\(\\r\\?\\)$/bar\\1/'\nFrom the sed man page: \n\n-b      --binary\n     This option is available on every platform, but is only effective where the operating system makes a distinction between text files and binary files. When such a distinction is made—as is the case for MS-DOS, Windows, Cygwin—text files are composed of lines separated by a carriage return and a line feed character, and sed does not see the ending CR. When this option is specified, sed will open input files in binary mode, thus not requesting this special processing and considering lines to end at a line feed.`",
    "tag": "sed"
  },
  {
    "question": "Have sed ignore non-matching lines",
    "answer": "If you don't want to print lines that don't match, you can use the combination of\n\n-n option which tells sed not to print\np flag which tells sed to print what is matched\n\nThis gives:\nsed -n 's/.../.../p'\n\nAdditionally, you can use a preceding matching pattern /match only these lines/ to only apply the replacement command to lines matching this pattern.\nThis gives:\nsed -n '/.../ s/.../.../p'\n\ne.g.:\n# replace all occurrences of `bar` with `baz`\nsed -n 's/bar/baz/p'\n\n# replace `bar` with `baz` only on lines matching `foo`\nsed -n '/foo/ s/bar/baz/p'\n\nSee also this other answer addressing Rapsey's comment below on multiple replacements",
    "tag": "sed"
  },
  {
    "question": "sed with literal string--not input file",
    "answer": "You have a single quotes conflict, so use:\necho \"A,B,C\" | sed \"s/,/','/g\"\n\nIf using bash, you can do too (<<< is a here-string):\nsed \"s/,/','/g\" <<< \"A,B,C\"\n\nbut not\nsed \"s/,/','/g\"  \"A,B,C\"\n\nbecause sed expect file(s) as argument(s)\nEDIT:\nif you use ksh or any other ones :\necho string | sed ...",
    "tag": "sed"
  },
  {
    "question": "Why is sed not recognizing \\t as a tab?",
    "answer": "Not all versions of sed understand \\t. Just insert a literal tab instead (press Ctrl-V then Tab).",
    "tag": "sed"
  },
  {
    "question": "How to extract text from a string using sed?",
    "answer": "How about using grep -E?\necho \"This is 02G05 a test string 20-Jul-2012\" | grep -Eo '[0-9]+G[0-9]+'",
    "tag": "sed"
  },
  {
    "question": "Case-insensitive search and replace with sed",
    "answer": "Update: Starting with macOS Big Sur (11.0), sed now does support the I flag for case-insensitive matching, so the command in the question should now work (BSD sed doesn't report its version, but you can go by the date at the bottom of the man page, which should be March 27, 2017 or more recent); a simple example:\n# BSD sed on macOS Big Sur and above (and GNU sed, the default on Linux)\n$ sed 's/ö/@/I' <<<'FÖO'\nF@O   # `I` matched the uppercase Ö correctly against its lowercase counterpart\n\nNote: I (uppercase) is the documented form of the flag, but i works as well.\nSimilarly, starting with macOS Big Sur (11.0) awk now is locale-aware (awk --version should report 20200816 or more recent):\n# BSD awk on macOS Big Sur and above (and GNU awk, the default on Linux)\n$ awk 'tolower($0)' <<<'FÖO'\nföo  # non-ASCII character Ö was properly lowercased\n\n\nThe following applies to macOS up to Catalina (10.15):\nTo be clear: On macOS, sed - which is the  BSD implementation - does NOT support case-insensitive matching - hard to believe, but true. The formerly accepted answer, which itself shows a GNU sed command, gained that status because of the perl-based solution mentioned in the comments.\nTo make that Perl solution work with foreign characters as well, via UTF-8, use something like:\nperl -C -Mutf8 -pe 's/öœ/oo/i' <<< \"FÖŒ\" # -> \"Foo\"\n\n\n-C turns on UTF-8 support for streams and files, assuming the current locale is UTF-8-based.\n-Mutf8 tells Perl to interpret the source code as UTF-8 (in this case, the string passed to -pe) - this is the shorter equivalent of the more verbose -e 'use utf8;'.Thanks, Mark Reed\n\n(Note that using awk is not an option either, as awk on macOS (i.e., BWK awk and BSD awk) appears to be completely unaware of locales altogether - its tolower() and toupper() functions ignore foreign characters (and sub() / gsub() don't have case-insensitivity flags to begin with).)\n\nA note on the relationship of sed and awk to the POSIX standard:\nBSD sed and awk limit their functionality mostly to what the POSIX sed and\nPOSIX awk specs mandate, whereas their GNU counterparts implement many more extensions.",
    "tag": "sed"
  },
  {
    "question": "How do I delete all lines in a file starting from after a matching line?",
    "answer": "If you don't want to print the matched line (or any following lines):\nsed -n '/The second line/q;p' inputfile\n\nThis says \"when you reach the line that matches the pattern quit, otherwise print each line\". The -n option prevents implicit printing and the p command is required to explicitly print lines.\nor\nsed '/The second line/,$d' inputfile\n\nThis says \"delete all lines from the output starting at the matched line and continuing to the end of the file\".\nbut the first one is faster. However it will quit processing completely so if you have multiple files as arguments, the ones after the first matching file won't be processed. In this case, the delete form is better.\nIf you do want to print the matched line, but not any following lines:\nsed '/The second line/q' inputfile\n\nThis says \"print all lines and quit when the matched line is reached\" (the -n option (no implicit print) is not used).\nSee man sed for additional information.",
    "tag": "sed"
  },
  {
    "question": "Append file contents to the bottom of existing file in Bash",
    "answer": "This should work:\n cat \"$API\" >> \"$CONFIG\"\n\nYou need to use the >> operator to append to a file.  Redirecting with > causes the file to be overwritten. (truncated).",
    "tag": "sed"
  },
  {
    "question": "How do I POST JSON data with cURL?",
    "answer": "You need to set your content-type to application/json. But -d (or --data) sends the Content-Type application/x-www-form-urlencoded by default, which is not accepted on Spring's side.\nLooking at the curl man page, I think you can use -H (or --header):\n-H \"Content-Type: application/json\"\n\nFull example:\ncurl --header \"Content-Type: application/json\" \\\n  --request POST \\\n  --data '{\"username\":\"xyz\",\"password\":\"xyz\"}' \\\n  http://localhost:3000/api/login\n\n(-H is short for --header, -d for --data)\nNote that -request POST is optional if you use -d, as the -d flag implies a POST request.\n\nOn Windows, things are slightly different. See the comment thread.",
    "tag": "curl"
  },
  {
    "question": "How do I get a YouTube video thumbnail from the YouTube API?",
    "answer": "Each YouTube video has four generated images. They are predictably formatted as follows:\nhttps://img.youtube.com/vi/<insert-youtube-video-id-here>/0.jpg\nhttps://img.youtube.com/vi/<insert-youtube-video-id-here>/1.jpg\nhttps://img.youtube.com/vi/<insert-youtube-video-id-here>/2.jpg\nhttps://img.youtube.com/vi/<insert-youtube-video-id-here>/3.jpg\n\nThe first one in the list is a full size image and others are thumbnail images. The default thumbnail image (i.e., one of 1.jpg, 2.jpg, 3.jpg) is:\nhttps://img.youtube.com/vi/<insert-youtube-video-id-here>/default.jpg\n\nFor the high quality version of the thumbnail use a URL similar to this:\nhttps://img.youtube.com/vi/<insert-youtube-video-id-here>/hqdefault.jpg\n\nThere is also a medium quality version of the thumbnail, using a URL similar to the HQ:\nhttps://img.youtube.com/vi/<insert-youtube-video-id-here>/mqdefault.jpg\n\nFor the standard definition version of the thumbnail, use a URL similar to this:\nhttps://img.youtube.com/vi/<insert-youtube-video-id-here>/sddefault.jpg\n\nFor the maximum resolution version of the thumbnail use a URL similar to this:\nhttps://img.youtube.com/vi/<insert-youtube-video-id-here>/maxresdefault.jpg\n\nAll of the above URLs are available over HTTP too. Additionally, the slightly shorter hostname i3.ytimg.com works in place of img.youtube.com in the example URLs above.\nAlternatively, you can use the YouTube Data API (v3) to get thumbnail images.",
    "tag": "curl"
  },
  {
    "question": "How to send a header using a HTTP request through a cURL call?",
    "answer": "man curl:\n   -H/--header <header>\n          (HTTP)  Extra header to use when getting a web page. You may specify\n          any number of extra headers. Note that if you should  add  a  custom\n          header that has the same name as one of the internal ones curl would\n          use, your externally set header will be used instead of the internal\n          one.  This  allows  you  to make even trickier stuff than curl would\n          normally do. You should not replace internally set  headers  without\n          knowing  perfectly well what you're doing. Remove an internal header\n          by giving a replacement without content on the  right  side  of  the\n          colon, as in: -H \"Host:\".\n\n          curl  will  make sure that each header you add/replace get sent with\n          the proper end of line marker, you should thus not  add  that  as  a\n          part  of the header content: do not add newlines or carriage returns\n          they will only mess things up for you.\n\n          See also the -A/--user-agent and -e/--referer options.\n\n          This option can be used multiple times to add/replace/remove  multi-\n          ple headers.\n\nExample 1: Single Header\ncurl --header \"X-MyHeader: 123\" www.google.com\n\nExample 2: Multiple Headers\ncurl --header \"Accept: text/javascript\" --header \"X-Test: hello\" -v www.google.com\n\nYou can see the request that curl sent by adding the -v option.",
    "tag": "curl"
  },
  {
    "question": "How do I measure request and response times at once using cURL?",
    "answer": "From this brilliant blog post...  https://blog.josephscott.org/2011/10/14/timing-details-with-curl/\ncURL supports formatted output for the details of the request (see the cURL manpage for details, under -w, –write-out <format>). For our purposes we’ll focus just on the timing details that are provided. Times below are in seconds.\n\nCreate a new file, curl-format.txt, and paste in:\n     time_namelookup:  %{time_namelookup}s\\n\n        time_connect:  %{time_connect}s\\n\n     time_appconnect:  %{time_appconnect}s\\n\n    time_pretransfer:  %{time_pretransfer}s\\n\n       time_redirect:  %{time_redirect}s\\n\n  time_starttransfer:  %{time_starttransfer}s\\n\n                     ----------\\n\n          time_total:  %{time_total}s\\n\n\n\nMake a request:\n curl -w \"@curl-format.txt\" -o /dev/null -s \"http://wordpress.com/\"\n\nOr on Windows, it's...\n curl -w \"@curl-format.txt\" -o NUL -s \"http://wordpress.com/\"\n\n\n\n\nWhat this does:\n-w \"@curl-format.txt\" tells cURL to use our format file\n-o /dev/null redirects the output of the request to /dev/null\n-s\ntells cURL not to show a progress meter\n\"http://wordpress.com/\" is\nthe URL we are requesting. Use quotes particularly if your URL has \"&\" query string parameters\n\nAnd here is what you get back:\n   time_namelookup:  0.001s\n      time_connect:  0.037s\n   time_appconnect:  0.000s\n  time_pretransfer:  0.037s\n     time_redirect:  0.000s\ntime_starttransfer:  0.092s\n                   ----------\n        time_total:  0.164s\n\nI have not yet seen an option to output the results in microseconds, but if you're aware of one, post in the comments below.\n\nMake a Linux/Mac shortcut (alias)\nalias curltime=\"curl -w \\\"@$HOME/.curl-format.txt\\\" -o /dev/null -s \"\n\nThen you can simply call...\ncurltime wordpress.org\n\nThanks to commenter Pete Doyle!\n\nMake a Linux/Mac stand-alone script\nThis script does not require a separate .txt file to contain the formatting.\nCreate a new file, curltime, somewhere in your executable path, and paste in:\n#!/bin/bash\n\ncurl -w @- -o /dev/null -s \"$@\" <<'EOF'\n    time_namelookup:  %{time_namelookup}\\n\n       time_connect:  %{time_connect}\\n\n    time_appconnect:  %{time_appconnect}\\n\n   time_pretransfer:  %{time_pretransfer}\\n\n      time_redirect:  %{time_redirect}\\n\n time_starttransfer:  %{time_starttransfer}\\n\n                    ----------\\n\n         time_total:  %{time_total}\\n\nEOF\n\nThen call it the same way as the alias:\ncurltime wordpress.org\n\n\nMake a Windows shortcut (aka BAT file)\nCreate a new text file called curltime.bat in the same folder as curl.exe and curl-format.txt, and paste in the following line:\ncurl -w \"@%~dp0curl-format.txt\" -o NUL -s %*\n\nThen from the command line you can simply call:\ncurltime wordpress.org\n\n(Make sure the folder is listed in your Windows PATH variable to be able to use the command from any folder.)",
    "tag": "curl"
  },
  {
    "question": "How do I get cURL to not show the progress bar?",
    "answer": "curl -s http://google.com > temp.html\n\nworks for curl version 7.19.5 on Ubuntu 9.10 (no progress bar). But if for some reason that does not work on your platform, you could always redirect stderr to /dev/null:\ncurl  http://google.com 2>/dev/null > temp.html",
    "tag": "curl"
  },
  {
    "question": "How to display request headers with command line curl",
    "answer": "curl's -v or --verbose option shows the HTTP request headers, among other things. Here is some sample output:\n$ curl -v http://google.com/\n* About to connect() to google.com port 80 (#0)\n*   Trying 66.102.7.104... connected\n* Connected to google.com (66.102.7.104) port 80 (#0)\n> GET / HTTP/1.1\n> User-Agent: curl/7.16.4 (i386-apple-darwin9.0) libcurl/7.16.4 OpenSSL/0.9.7l zlib/1.2.3\n> Host: google.com\n> Accept: */*\n> \n< HTTP/1.1 301 Moved Permanently\n< Location: http://www.google.com/\n< Content-Type: text/html; charset=UTF-8\n< Date: Thu, 15 Jul 2010 06:06:52 GMT\n< Expires: Sat, 14 Aug 2010 06:06:52 GMT\n< Cache-Control: public, max-age=2592000\n< Server: gws\n< Content-Length: 219\n< X-XSS-Protection: 1; mode=block\n< \n<HTML><HEAD><meta http-equiv=\"content-type\" content=\"text/html;charset=utf-8\">\n<TITLE>301 Moved</TITLE></HEAD><BODY>\n<H1>301 Moved</H1>\nThe document has moved\n<A HREF=\"http://www.google.com/\">here</A>.\n</BODY></HTML>\n* Connection #0 to host google.com left intact\n* Closing connection #0",
    "tag": "curl"
  },
  {
    "question": "Converting a Postman request to curl",
    "answer": "You can see the button </> Code icon in right side of the Postman app (attached screenshot).\nPress it and you can get your code in many different languages including PHP cURL",
    "tag": "curl"
  },
  {
    "question": "How to capture cURL output to a file?",
    "answer": "curl -K myconfig.txt -o output.txt \n\nWrites the first output received in the file you specify (overwrites if an old one exists).\ncurl -K myconfig.txt >> output.txt\n\nAppends all output you receive to the specified file.\nNote: The -K is optional.\nIf you are posting to a URL like https://example.org/?foo=1&baz=4\nthen you need to put double quotes around the URL:\ncurl \\\n   -X POST \\\n   -H \"Content-Type: application/octet-stream\" \\\n   --data-binary \"@/home/path/file.xyz\" \\\n   \"https://xample.org:8080/v1/?filename=file.xyz&food=1&z=bee\" \\\n  >out.txt 2>err.txt",
    "tag": "curl"
  },
  {
    "question": "Getting only response header from HTTP POST using cURL",
    "answer": "-D, --dump-header <file>\n       Write the protocol headers to the specified file.\n\n       This  option  is handy to use when you want to store the headers\n       that a HTTP site sends to you. Cookies from  the  headers  could\n       then  be  read  in  a  second  curl  invocation by using the -b,\n       --cookie option! The -c, --cookie-jar option is however a better\n       way to store cookies.\n\nand\n-S, --show-error\n       When used with -s, --silent, it makes curl show an error message if it fails.\n\nfrom the man page.  so\ncurl -sS -D - www.acooke.org -o /dev/null\n\nfollows redirects, dumps the headers to stdout and sends the data to /dev/null (that's a GET, not a POST, but you can do the same thing with a POST - just add whatever option you're already using for POSTing data)\nnote the - after the -D which indicates that the output \"file\" is stdout.",
    "tag": "curl"
  },
  {
    "question": "Unable to resolve \"unable to get local issuer certificate\" using git on Windows with self-signed certificate",
    "answer": "The problem is that git by default using the \"Linux\" crypto backend.\nBeginning with Git for Windows 2.14, you can now configure Git to use SChannel, the built-in Windows networking layer as the crypto backend. This means that it will use the Windows certificate storage mechanism and you do not need to explicitly configure the curl CA storage mechanism: https://msdn.microsoft.com/en-us/library/windows/desktop/aa380123(v=vs.85).aspx\nJust execute:\ngit config --global http.sslbackend schannel\n\nThat should help.\nUsing schannel is by now the standard setting when installing git for Windows, also it is recommended to not checkout repositories by SSH anmore if possible, as https is easier to configure and less likely to be blocked by a firewall it means less chance of failure.",
    "tag": "curl"
  },
  {
    "question": "How do I install and use cURL on Windows?",
    "answer": "You might already have curl\nIt is possible that you won't need to download anything:\n\nIf you are on Windows 10, version 1803 or later, your OS ships with a copy of curl, already set up and ready to use.\n\nIf you have Git for Windows installed (if you downloaded Git from git-scm.com, the answer is yes), you have curl.exe under:\n C:\\Program Files\\Git\\mingw64\\bin\\\n\nSimply add the above path to PATH.\n\n\nBut curl may be a PowerShell alias to its own Invoke-WebRequest command. If that is the case, you can invoke actual curl as curl.exe, or remove the alias with rm alias:curl.\nInstalling curl with a package manager\nIf you are already using a package manager, it may be more convenient to install with one:\n\nFor Chocolatey, run choco install curl\nFor MSYS2, run pacman -S curl\nFor Scoop, run scoop install curl\nFor Cygwin, add the curl package in Cygwin Setup. EDIT by a reader: Cygwin installer design has changed, please choose curl packages as follows:\n\n\nInstalling curl manually\nDownloading curl\nIt is too easy to accidentally download the wrong thing. If, on the curl homepage, you click the large and prominent \"Download\" section in the site header, and then the large and prominent curl-7.62.0.tar.gz link in its body, you will have downloaded a curl source package, which contains curl's source code but not curl.exe. Watch out for that.\nInstead, click the large and prominent download links on this page. Those are the official Windows builds, and they are provided by the curl-for-win project.\nIf you have more esoteric needs (e.g. you want cygwin builds, third-party builds, libcurl, header files, sources, etc.), use the curl download wizard. After answering five questions, you will be presented with a list of download links.\nExtracting and setting up curl\nFind curl.exe within your downloaded package; it's probably under bin\\.\nPick a location on your hard drive that will serve as a permanent home for curl:\n\nIf you want to give curl its own folder, C:\\Program Files\\curl\\ or C:\\curl\\ will do.\nIf you have many loose executables, and you do not want to add many individual folders to PATH, use a single folder such as C:\\Program Files\\tools\\ or C:\\tools\\ for the purpose.\n\nPlace curl.exe under the folder. And never move the folder or its contents.\nNext, you'll want to make curl available anywhere from the command line. To do this, add the folder to PATH, like this:\n\nClick the Windows 10 start menu. Start typing \"environment\".\nYou'll see the search result Edit the system environment variables. Choose it.\nA System Properties window will popup. Click the Environment Variables button at the bottom.\nSelect the \"Path\" variable under \"System variables\" (the lower box). Click the Edit button.\nClick the Add button and paste in the folder path where curl.exe lives.\nClick OK as needed. Close open console windows and reopen, so they get the new PATH.\n\nNow enjoy typing curl at any command prompt. Party time!",
    "tag": "curl"
  },
  {
    "question": "JSONDecodeError: Expecting value: line 1 column 1 (char 0)",
    "answer": "Your code produced an empty response body; you'd want to check for that or catch the exception raised. It is possible the server responded with a 204 No Content response, or a non-200-range status code was returned (404 Not Found, etc.). Check for this.\nNote:\n\nThere is no need to decode a response from UTF8 to Unicode, the  json.loads() method can handle UTF8-encoded data natively.\n\npycurl has a very archaic API. Unless you have a specific requirement for using it, there are better choices.\n\n\nEither requests or httpx offer much friendlier APIs, including JSON support.\nIf you can, replace your call with the following httpx code:\nimport httpx\n\nresponse = httpx.get(url)\nresponse.raise_for_status()  # raises exception when not a 2xx response\nif response.status_code != 204:\n    return response.json()\n\nOf course, this won't protect you from a URL that doesn't comply with HTTP standards; when using arbitrary URLs where this is a possibility, check if the server intended to give you JSON by checking the Content-Type header, and for good measure catch the exception:\nif (\n    response.status_code != 204 and\n    response.headers[\"content-type\"].strip().startswith(\"application/json\")\n):\n    try:\n        return response.json()\n    except ValueError:\n        # decide how to handle a server that's misbehaving to this extent",
    "tag": "curl"
  },
  {
    "question": "Using cURL with a username and password?",
    "answer": "Use the -u flag to include a username, and curl will prompt for a password:\ncurl -u username http://example.com\n\nYou can also include the password in the command, but then your password will be visible in bash history:\ncurl -u username:password http://example.com",
    "tag": "curl"
  },
  {
    "question": "Is there a way to follow redirects with command line cURL?",
    "answer": "Use the location header flag:\ncurl -L <URL>\ncurl --location <URL>  # or (same thing)",
    "tag": "curl"
  },
  {
    "question": "PHP, cURL, and HTTP POST example?",
    "answer": "A very simple PHP example that sends an HTTP POST request to a remote site\n$url = \"http://www.example.com/tester.phtml\";\n$post_data = array('postvar1' => 'value1');\n\n\n$ch = curl_init($url);\n// return the response instead of sending it to stdout:\ncurl_setopt($ch, CURLOPT_RETURNTRANSFER, true);\n// set the POST data, corresponding method and headers:\ncurl_setopt($ch, CURLOPT_POSTFIELDS, http_build_query($post_data));\n// send the request and get the response\n$server_output = curl_exec($ch);",
    "tag": "curl"
  },
  {
    "question": "How to set the authorization header using cURL",
    "answer": "http://curl.se/docs/httpscripting.html\nSee part 6. HTTP Authentication\n\nHTTP Authentication\n\n\nHTTP Authentication is the ability to tell the server your username and\npassword so that it can verify that you're allowed to do the request you're\ndoing. The Basic authentication used in HTTP (which is the type curl uses by\ndefault) is plain text based, which means it sends username and password\nonly slightly obfuscated, but still fully readable by anyone that sniffs on\nthe network between you and the remote server.\n\n\nTo tell curl to use a user and password for authentication:\n\n\ncurl --user name:password http://www.example.com\n\n\n\nThe site might require a different authentication method (check the headers\nreturned by the server), and then --ntlm, --digest, --negotiate or even\n--anyauth might be options that suit you.\n\n\nSometimes your HTTP access is only available through the use of a HTTP\nproxy. This seems to be especially common at various companies. A HTTP proxy\nmay require its own user and password to allow the client to get through to\nthe Internet. To specify those with curl, run something like:\n\n\ncurl --proxy-user proxyuser:proxypassword curl.haxx.se\n\n\n\nIf your proxy requires the authentication to be done using the NTLM method,\nuse --proxy-ntlm, if it requires Digest use --proxy-digest.\n\n\nIf you use any one these user+password options but leave out the password\npart, curl will prompt for the password interactively.\n\n\nDo note that when a program is run, its parameters might be possible to see\nwhen listing the running processes of the system. Thus, other users may be\nable to watch your passwords if you pass them as plain command line\noptions. There are ways to circumvent this.\n\n\nIt is worth noting that while this is how HTTP Authentication works, very\nmany web sites will not use this concept when they provide logins etc. See\nthe Web Login chapter further below for more details on that.",
    "tag": "curl"
  },
  {
    "question": "wget/curl large file from google drive",
    "answer": "Update: Mars 2025\nYou can use gdown. Consider also visiting that page for full instructions; this is just a summary and the source repo may have more up-to-date instructions.\n\nInstructions\nInstall it with the following command:\npip install gdown\n\nAfter that, you can download any file from Google Drive by running one of these commands:\ngdown https://drive.google.com/uc?id=<file_id>  # for files\ngdown <file_id>                                 # alternative format\ngdown --folder https://drive.google.com/drive/folders/<file_id>  # for folders\ngdown --folder --id <file_id>                                   # this format works for folders too\n\nExample: to download the readme file from this directory\ngdown https://drive.google.com/uc?id=0B7EVK8r0v71pOXBhSUdJWU1MYUk\n\nThe file_id should look something like 0Bz8a_Dbh9QhbNU3SGlFaDg. You can find this ID by right-clicking on the file of interest, and selecting Get link. As of November 2021, this link will be of the form:\n# Files\nhttps://drive.google.com/file/d/<file_id>/view?usp=sharing\n# Folders\nhttps://drive.google.com/drive/folders/<file_id>\n\nCaveats\n\nOnly works on open access files. (\"Anyone who has a link can View\")\nCannot download more than 50 files into a single folder.\n\nIf you have access to the source file, you can consider using tar/zip to make it a single file to work around this limitation.",
    "tag": "curl"
  },
  {
    "question": "performing HTTP requests with cURL (using PROXY)",
    "answer": "From man curl:\n-x, --proxy <[protocol://][user:password@]proxyhost[:port]>\n\n     Use the specified HTTP proxy. \n     If the port number is not specified, it is assumed at port 1080.",
    "tag": "curl"
  },
  {
    "question": "How can I see the request headers made by curl when sending a request to the server?",
    "answer": "I think curl --verbose/-v is the easiest. It will spit out the request headers (lines prefixed with '>') without having to write to a file:\n$ curl -v -I -H \"Testing: Test header so you see this works\" http://stackoverflow.com/\n* About to connect() to stackoverflow.com port 80 (#0)\n*   Trying 69.59.196.211... connected\n* Connected to stackoverflow.com (69.59.196.211) port 80 (#0)\n> HEAD / HTTP/1.1\n> User-Agent: curl/7.16.3 (i686-pc-cygwin) libcurl/7.16.3 OpenSSL/0.9.8h zlib/1.2.3 libssh2/0.15-CVS\n> Host: stackoverflow.com\n> Accept: */*\n> Testing: Test header so you see this works\n>\n< HTTP/1.0 200 OK\n...",
    "tag": "curl"
  },
  {
    "question": "Using cURL to upload POST data with files",
    "answer": "You need to use the -F option:\n-F/--form <name=content> Specify HTTP multipart POST data (H)\nTry this:\ncurl \\\n  -F \"userid=1\" \\\n  -F \"filecomment=This is an image file\" \\\n  -F \"image=@/home/user1/Desktop/test.jpg\" \\\n  localhost/uploader.php",
    "tag": "curl"
  },
  {
    "question": "How to do a PUT request with cURL?",
    "answer": "Using the uppercase -X flag with whatever HTTP verb you want:\ncurl -X PUT -d argument=value -d argument2=value2 http://localhost:8080\n\nThis example also uses the -d flag to provide arguments with your PUT request.",
    "tag": "curl"
  },
  {
    "question": "How to urlencode data for curl command?",
    "answer": "Use curl --data-urlencode; from man curl:\n\nThis posts data, similar to the other --data options with the exception that this performs URL-encoding. To be CGI-compliant, the <data> part should begin with a name followed by a separator and a content specification.\n\nExample usage:\ncurl \\\n    --data-urlencode \"paramName=value\" \\\n    --data-urlencode \"secondParam=value\" \\\n    http://example.com\n\nSee the man page for more info.\nThis requires curl 7.18.0 or newer (released January 2008). Use  curl -V to check which version you have.\nYou can as well encode the query string:\ncurl --get \\\n    --data-urlencode \"p1=value 1\" \\\n    --data-urlencode \"p2=value 2\" \\\n    http://example.com\n    # http://example.com?p1=value%201&p2=value%202",
    "tag": "curl"
  },
  {
    "question": "How can I connect to a Tor hidden service using cURL in PHP?",
    "answer": "You need to set the CURLOPT_PROXYTYPE option to CURLPROXY_SOCKS5_HOSTNAME\ncurl_setopt($ch, CURLOPT_PROXYTYPE, CURLPROXY_SOCKS5_HOSTNAME);\n\nCURLPROXY_SOCKS5_HOSTNAME is defined starting with PHP 5.6.10. If you're using an older version you can explicitly use its value, 7:\ncurl_setopt($ch, CURLOPT_PROXYTYPE, 7);",
    "tag": "curl"
  },
  {
    "question": "curl: (60) SSL certificate problem: unable to get local issuer certificate",
    "answer": "It is failing as cURL is unable to verify the certificate provided by the server.\nThere are two options to get this to work:\n\nUse cURL with -k option which allows curl to make insecure connections, that is cURL does not verify the certificate.\nAdd the root CA (the CA signing the server certificate) to /etc/ssl/certs/ca-certificates.crt\n\nYou should use option 2 as it's the option that ensures that you are connecting to secure FTP server.",
    "tag": "curl"
  },
  {
    "question": "PHP cURL custom headers",
    "answer": "curl_setopt($ch, CURLOPT_HTTPHEADER, [\n    'X-Apple-Tz: 0',\n    'X-Apple-Store-Front: 143444,12'\n]);\n\nhttps://www.php.net/manual/en/function.curl-setopt.php",
    "tag": "curl"
  },
  {
    "question": "How can you debug a CORS request with cURL?",
    "answer": "Here's how you can debug CORS requests using curl.\nSending a regular CORS request using cUrl:\ncurl -H \"Origin: http://example.com\" --verbose \\\n  https://www.googleapis.com/discovery/v1/apis?fields=\n\nThe -H \"Origin: http://example.com\" flag is the third party domain making the request. Substitute in whatever your domain is.\nThe --verbose flag prints out the entire response so you can see the request and response headers.\nThe URL I'm using above is a sample request to a Google API that supports CORS, but you can substitute in whatever URL you are testing.\nThe response should include the Access-Control-Allow-Origin header.\nSending a preflight request using cUrl:\ncurl -H \"Origin: http://example.com\" \\\n  -H \"Access-Control-Request-Method: POST\" \\\n  -H \"Access-Control-Request-Headers: X-Requested-With\" \\\n  -X OPTIONS --verbose \\\n  https://www.googleapis.com/discovery/v1/apis?fields=\n\nThis looks similar to the regular CORS request with a few additions:\nThe -H flags send additional preflight request headers to the server\nThe -X OPTIONS flag indicates that this is an HTTP OPTIONS request.\nIf the preflight request is successful, the response should include the Access-Control-Allow-Origin, Access-Control-Allow-Methods, and  Access-Control-Allow-Headers response headers.  If the preflight request was not successful, these headers shouldn't appear, or the HTTP response won't be 200.\nYou can also specify additional headers, such as User-Agent, by using the -H flag.",
    "tag": "curl"
  },
  {
    "question": "Run cURL commands from Windows console",
    "answer": "If you are not into Cygwin, you can use native Windows builds. Some are here: curl Download Wizard.",
    "tag": "curl"
  },
  {
    "question": "How to count items in JSON object using command line?",
    "answer": "Just throwing another solution in the mix...\nTry jq, a lightweight and flexible command-line JSON processor:\njq length /tmp/test.json\n\nPrints the length of the array of objects.",
    "tag": "curl"
  },
  {
    "question": "Call to undefined function curl_init()?",
    "answer": "If you're on Windows:\nGo to your php.ini file and remove the ; mark from the beginning of the following line:\n;extension=php_curl.dll\n\nAfter you have saved the file you must restart your HTTP server software (e.g. Apache) before this can take effect.\n\nFor Ubuntu 13.0 and above, simply use the debundled package. In a terminal type the following to install it and do not forgot to restart server.\nsudo apt-get install php-curl\n\nOr if you're using the old PHP5\nsudo apt-get install php5-curl\n\nor\nsudo apt-get install php5.6-curl\n\nThen restart apache to activate the package with\nsudo service apache2 restart",
    "tag": "curl"
  },
  {
    "question": "How to use cURL to send Cookies?",
    "answer": "This worked for me:\ncurl -v --cookie \"USER_TOKEN=Yes\" http://127.0.0.1:5000/\n\nI could see the value in backend using\nprint(request.cookies)",
    "tag": "curl"
  },
  {
    "question": "How to define the basic HTTP authentication using cURL correctly?",
    "answer": "curl -u username:password http://\ncurl -u username http://\n\nFrom the documentation page:\n\n-u, --user <user:password>\nSpecify the user name and password to use for server authentication.\nOverrides -n, --netrc and --netrc-optional.\nIf you simply specify the user name, curl will prompt for a password.\nThe user name and passwords are split up on the first colon, which\nmakes it impossible to use a colon in the user name with this option.\nThe password can, still.\nWhen using Kerberos V5 with a Windows based server you should include\nthe Windows domain name in the user name, in order for the server to\nsuccesfully obtain a Kerberos Ticket. If you don't then the initial\nauthentication handshake may fail.\nWhen using NTLM, the user name can be specified simply as the user\nname, without the domain, if there is a single domain and forest in\nyour setup for example.\nTo specify the domain name use either Down-Level Logon Name or UPN\n(User Principal Name) formats. For example, EXAMPLE\\user and\nuser@example.com respectively.\nIf you use a Windows SSPI-enabled curl binary and perform Kerberos V5,\nNegotiate, NTLM or Digest authentication then you can tell curl to\nselect the user name and password from your environment by specifying\na single colon with this option: \"-u :\".\nIf this option is used several times, the last one will be used.\n\nhttp://curl.haxx.se/docs/manpage.html#-u\nNote that you do not need --basic flag as it is the default.",
    "tag": "curl"
  },
  {
    "question": "Curl to return http status code along with the response",
    "answer": "I was able to get a solution by looking at the curl doc which specifies to use - for the output to get the output to stdout.\ncurl -o - -I http://localhost\n\nTo get the response with just the http return code, I could just do\ncurl -o /dev/null -s -w \"%{http_code}\\n\" http://localhost",
    "tag": "curl"
  },
  {
    "question": "HTTP POST and GET using cURL in Linux",
    "answer": "*nix provides a nice little command which makes our lives a lot easier.\nGET:\nwith JSON:\ncurl -i -H \"Accept: application/json\" -H \"Content-Type: application/json\" -X GET http://hostname/resource\n\nwith XML:\ncurl -H \"Accept: application/xml\" -H \"Content-Type: application/xml\" -X GET http://hostname/resource\n\nPOST:\nFor posting data:\ncurl --data \"param1=value1&param2=value2\" http://hostname/resource\n\nFor file upload:\ncurl --form \"fileupload=@filename.txt\" http://hostname/resource\n\nRESTful HTTP Post:\ncurl -X POST -d @filename http://hostname/resource\n\nFor logging into a site (auth):\ncurl -d \"username=admin&password=admin&submit=Login\" --dump-header headers http://localhost/Login\ncurl -L -b headers http://localhost/\n\nPretty-printing the curl results:\nFor JSON:\nIf you use npm and nodejs, you can install json package by running this command:\nnpm install -g json\n\nUsage:\ncurl -i -H \"Accept: application/json\" -H \"Content-Type: application/json\" -X GET http://hostname/resource | json\n\nIf you use pip and python, you can install pjson package by running this command:\npip install pjson\n\nUsage:\ncurl -i -H \"Accept: application/json\" -H \"Content-Type: application/json\" -X GET http://hostname/resource | pjson\n\nIf you use Python 2.6+, json tool is bundled within.\nUsage:\ncurl -i -H \"Accept: application/json\" -H \"Content-Type: application/json\" -X GET http://hostname/resource | python -m json.tool\n\nIf you use gem and ruby, you can install colorful_json package by running this command:\ngem install colorful_json\n\nUsage:\ncurl -i -H \"Accept: application/json\" -H \"Content-Type: application/json\" -X GET http://hostname/resource | cjson\n\nIf you use apt-get (aptitude package manager of your Linux distro), you can install yajl-tools package by running this command:\nsudo apt-get install yajl-tools\n\nUsage:\ncurl -i -H \"Accept: application/json\" -H \"Content-Type: application/json\" -X GET http://hostname/resource |  json_reformat\n\nFor XML:\nIf you use *nix with Debian/Gnome envrionment, install libxml2-utils:\nsudo apt-get install libxml2-utils\n\nUsage:\ncurl -H \"Accept: application/xml\" -H \"Content-Type: application/xml\" -X GET http://hostname/resource | xmllint --format -\n\nor install tidy:\nsudo apt-get install tidy\n\nUsage:\ncurl -H \"Accept: application/xml\" -H \"Content-Type: application/xml\" -X GET http://hostname/resource | tidy -xml -i -\n\nSaving the curl response to a file\ncurl http://hostname/resource >> /path/to/your/file\n\nor\ncurl http://hostname/resource -o /path/to/your/file\n\nFor detailed description of the curl command, hit:\nman curl\n\nFor details about options/switches of the curl command, hit:\ncurl -h",
    "tag": "curl"
  },
  {
    "question": "Passing a URL with brackets to curl",
    "answer": "Add -g to your command:\n-g, --globoff\n      This option switches off the \"URL globbing parser\". When you set this option, you can\n      specify URLs that contain the letters {}[] without having curl itself interpret them.\n      Note that these letters are not normal legal URL contents but they should be encoded\n      according to the URI standard.\n\n      Example:\n       curl -g \"https://example.com/{[]}}}}\"\n\ncurl.se/docs/manpage.html#-g",
    "tag": "curl"
  },
  {
    "question": "Can PHP cURL retrieve response headers AND body in a single request?",
    "answer": "One solution to this was posted in the PHP documentation comments: http://www.php.net/manual/en/function.curl-exec.php#80442\nCode example:\n$ch = curl_init();\ncurl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);\ncurl_setopt($ch, CURLOPT_HEADER, 1);\n// ...\n\n$response = curl_exec($ch);\n\n// Then, after your curl_exec call:\n$header_size = curl_getinfo($ch, CURLINFO_HEADER_SIZE);\n$header = substr($response, 0, $header_size);\n$body = substr($response, $header_size);\n\nWarning: As noted in the comments below, this may not be reliable when used with proxy servers or when handling certain types of redirects. @Geoffrey's answer may handle these more reliably.",
    "tag": "curl"
  },
  {
    "question": "List all indexes on ElasticSearch server?",
    "answer": "For a concise list of all indices in your cluster, call\ncurl http://localhost:9200/_aliases\n\nthis will give you a list of indices and their aliases.\nIf you want it pretty-printed, add pretty=true:\ncurl http://localhost:9200/_aliases?pretty=true\n\nThe result will look something like this, if your indices are called old_deuteronomy and mungojerrie:\n{\n  \"old_deuteronomy\" : {\n    \"aliases\" : { }\n  },\n  \"mungojerrie\" : {\n    \"aliases\" : {\n      \"rumpleteazer\" : { },\n      \"that_horrible_cat\" : { }\n    }\n  }\n}",
    "tag": "curl"
  },
  {
    "question": "Send request to cURL with post data sourced from a file",
    "answer": "You're looking for the --data-binary argument:\ncurl -i -X POST host:port/post-file \\\n  -H \"Content-Type: text/xml\" \\\n  --data-binary \"@path/to/file\"\n\nIn the example above, -i prints out all the headers so that you can see what's going on, and -X POST makes it explicit that this is a post.  Both of these can be safely omitted without changing the behaviour on the wire.  The path to the file needs to be preceded by an @ symbol, so curl knows to read from a file.",
    "tag": "curl"
  },
  {
    "question": "POST XML file using cURL command line",
    "answer": "If that question is connected to your other Hudson questions use the command they provide.  This way with XML from the command line:\n$ curl -X POST -d '<run>...</run>' \\\nhttp://user:pass@myhost:myport/path/of/url\n\nYou need to change it a little bit to read from a file:\n $ curl -X POST -d @myfilename http://user:pass@myhost:myport/path/of/url\n\nRead the manpage. following an abstract for -d Parameter.\n\n-d/--data \n(HTTP) Sends the specified data in a\n  POST request to the HTTP server, in\n  the same way that a browser does when\n  a user has filled in an HTML form and\n  presses the submit button. This will\n  cause curl to pass the data to the\n  server using the content-type\n  application/x-www-form-urlencoded.\n  Compare to -F/--form.\n-d/--data is the same as --data-ascii. To post data purely binary, you should\n  instead use the --data-binary option.\n  To URL-encode the value of a form\n  field you may use --data-urlencode.\nIf any of these options is used more\n  than once on the same command line,\n  the data pieces specified will be\n  merged together with a separating\n  &-symbol. Thus, using '-d name=daniel\n  -d skill=lousy' would generate a post chunk that looks like\n  'name=daniel&skill=lousy'.\nIf you start the data with the letter\n  @, the rest should be a file name to\n  read the data from, or - if you want\n  curl to read the data from stdin. The\n  contents of the file must already be\n  URL-encoded. Multiple files can also\n  be specified. Posting data from a file\n  named 'foobar' would thus be done with\n  --data @foobar.",
    "tag": "curl"
  },
  {
    "question": "Display curl output in readable JSON format in Unix shell script",
    "answer": "A few solutions to choose from:\n\njson json is a fast CLI tool for working with JSON. It is a single-file node.js script with no external deps (other than node.js itself).\n\n$ echo '{\"type\":\"Bar\",\"id\":\"1\",\"title\":\"Foo\"}' | json\n{\n  \"type\": \"Bar\",\n  \"id\": \"1\",\n  \"title\": \"Foo\"\n}\n\nRequire:\n# npm install -g json\n\n\n\njson_pp: command utility available in Linux systems for JSON decoding/encoding\n\necho '{\"type\":\"Bar\",\"id\":\"1\",\"title\":\"Foo\"}' | json_pp -json_opt pretty,canonical\n{\n   \"id\" : \"1\",\n   \"title\" : \"Foo\",\n   \"type\" : \"Bar\"\n}\n\nYou may want to keep the -json_opt pretty,canonical argument for predictable ordering.\n\n\njq: lightweight and flexible command-line JSON processor. It is written in portable C, and it has zero runtime dependencies.\n\necho '{\"type\":\"Bar\",\"id\":\"1\",\"title\":\"Foo\"}' | jq '.'\n{\n  \"type\": \"Bar\",\n  \"id\": \"1\",\n  \"title\": \"Foo\"\n}\n\nThe simplest jq program is the expression ., which takes the input and produces it unchanged as output.\nFor additional jq options check the manual\n\n\npython yq yq: Command-line YAML/XML/TOML processor - jq wrapper for YAML, XML, TOML documents\n\n$ echo '{\"type\":\"Bar\",\"id\":\"1\",\"title\":\"Foo\"}' | yq\n{\n  \"type\": \"Bar\",\n  \"id\": \"1\",\n  \"title\": \"Foo\"\n}\n\nThe go version go yq doesn't work here\n\n\nWith xidel Command line tool to download and extract data from HTML/XML pages or JSON-APIs, using CSS, XPath 3.0, XQuery 3.0, JSONiq or pattern matching. It can also create new or transformed XML/HTML/JSON documents.\n\n$ echo '{\"type\":\"Bar\",\"id\":\"1\",\"title\":\"Foo\"}' | xidel -e '$json'\n{\n  \"type\": \"Bar\",\n  \"id\": \"1\",\n  \"title\": \"Foo\"\n}\n\n\nwith python:\necho '{\"type\":\"Bar\",\"id\":\"1\",\"title\":\"Foo\"}' | python -m json.tool\n{\n    \"id\": \"1\",\n    \"title\": \"Foo\",\n    \"type\": \"Bar\"\n}\n\n\nwith nodejs and bash:\necho '{\"type\":\"Bar\",\"id\":\"1\",\"title\":\"Foo\"}' | node -p \"JSON.stringify( JSON.parse(require('fs').readFileSync(0) ), 0, 1 )\"\n{\n \"type\": \"Bar\",\n \"id\": \"1\",\n \"title\": \"Foo\"\n}",
    "tag": "curl"
  },
  {
    "question": "Simulate a specific CURL in PostMan",
    "answer": "A simpler approach would be:\n\nOpen POSTMAN\nClick on \"import\" tab on the upper left side.\nSelect the Raw Text option and paste your cURL command.\nHit import and you will have the command in your Postman builder!\nClick Send to post the command",
    "tag": "curl"
  },
  {
    "question": "Automatically add newline at end of curl response body",
    "answer": "From the man file:\n\nTo better allow script programmers to get to know about the progress of\n  curl, the -w/--write-out option was introduced. Using this, you can specify\n  what information from the previous transfer you want to extract.\nTo display the amount of bytes downloaded together with some text and an\n  ending newline:\ncurl -w 'We downloaded %{size_download} bytes\\n' www.download.com\n\n\nSo try adding the following to your ~/.curlrc file:\n-w \"\\n\"",
    "tag": "curl"
  },
  {
    "question": "cURL error 60: SSL certificate: unable to get local issuer certificate",
    "answer": "How to solve this problem:\n\ndownload and extract cacert.pem following the instructions at https://curl.se/docs/caextract.html\n\nsave it on your filesystem somewhere (for example, XAMPP users might use C:\\xampp\\php\\extras\\ssl\\cacert.pem)\n\nin your php.ini, put this file location in the [curl] section (putting it in the [openssl] section is also a good idea):\n\n\n[curl]\ncurl.cainfo = \"C:\\xampp\\php\\extras\\ssl\\cacert.pem\"\n\n[openssl]\nopenssl.cafile = \"C:\\xampp\\php\\extras\\ssl\\cacert.pem\"\n\n\nrestart your webserver (e.g. Apache) and PHP FPM server if applicable\n\n(Reference: https://laracasts.com/discuss/channels/general-discussion/curl-error-60-ssl-certificate-problem-unable-to-get-local-issuer-certificate)",
    "tag": "curl"
  },
  {
    "question": "curl: (35) error:1408F10B:SSL routines:ssl3_get_record:wrong version number",
    "answer": "* Uses proxy env variable http_proxy == 'https://proxy.in.tum.de:8080'   \n                                         ^^^^^\n\n\nThe https:// is wrong, it should be http://. The proxy itself should be accessed by HTTP and not HTTPS even though the target URL is HTTPS. The proxy will nevertheless properly handle HTTPS connection and keep the end-to-end encryption. See HTTP CONNECT method for details how this is done.",
    "tag": "curl"
  },
  {
    "question": "How to send file contents as body entity using cURL",
    "answer": "I believe you're looking for the @filename syntax, e.g.:\nstrip new lines\ncurl --data \"@/path/to/filename\" http://...\n\nkeep new lines\ncurl --data-binary \"@/path/to/filename\" http://...\n\n\ncurl will strip all newlines from the file. If you want to send the file with newlines intact, use --data-binary in place of --data",
    "tag": "curl"
  },
  {
    "question": "Save file to specific folder with curl command",
    "answer": "I don't think you can give a path to curl, but you can CD to the location, download and CD back.\ncd target/path && { curl -O URL ; cd -; }\n\nOr using subshell.\n(cd target/path && curl -O URL)\n\nBoth ways will only download if path exists. -O keeps remote file name. After download it will return to original location.\nIf you need to set filename explicitly, you can use small -o option:\ncurl -o target/path/filename URL",
    "tag": "curl"
  },
  {
    "question": "How can I set the request header for curl?",
    "answer": "Just use the -H option several times:\ncurl -H \"Accept-Charset: utf-8\" -H \"Content-Type: application/x-www-form-urlencoded\" http://www.some-domain.example",
    "tag": "curl"
  },
  {
    "question": "How do I make a request using HTTP basic authentication with PHP curl?",
    "answer": "You want this:\ncurl_setopt($ch, CURLOPT_USERPWD, $username . \":\" . $password);  \n\nZend has a REST client and zend_http_client and I'm sure PEAR has some sort of wrapper.\nBut its easy enough to do on your own.\nSo the entire request might look something like this:\n$ch = curl_init($host);\ncurl_setopt($ch, CURLOPT_HTTPHEADER, array('Content-Type: application/xml', $additionalHeaders));\ncurl_setopt($ch, CURLOPT_HEADER, 1);\ncurl_setopt($ch, CURLOPT_USERPWD, $username . \":\" . $password);\ncurl_setopt($ch, CURLOPT_TIMEOUT, 30);\ncurl_setopt($ch, CURLOPT_POST, 1);\ncurl_setopt($ch, CURLOPT_POSTFIELDS, $payloadName);\ncurl_setopt($ch, CURLOPT_RETURNTRANSFER, TRUE);\n$return = curl_exec($ch);\ncurl_close($ch);",
    "tag": "curl"
  },
  {
    "question": "How to use Python to execute a cURL command?",
    "answer": "For the sake of simplicity, you should consider using the Requests library.\nAn example with JSON response content would be something like:\nimport requests\nr = requests.get('https://github.com/timeline.json')\nr.json()\n\nIf you look for further information, in the Quickstart section, they have lots of working examples.\nFor your specific curl translation:\nimport requests\n\nurl = 'https://www.googleapis.com/qpxExpress/v1/trips/search?key=mykeyhere'\npayload = open(\"request.json\")\nheaders = {'content-type': 'application/json', 'Accept-Charset': 'UTF-8'}\nr = requests.post(url, data=payload, headers=headers)",
    "tag": "curl"
  },
  {
    "question": "CURL Command Line URL Parameters",
    "answer": "The application/x-www-form-urlencoded Content-type header is not required (well, kinda depends).   Unless the request handler expects parameters coming from the form body.   Try it out:\ncurl -X DELETE \"http://localhost:5000/locations?id=3\"\n\nor\ncurl -X GET \"http://localhost:5000/locations?id=3\"",
    "tag": "curl"
  },
  {
    "question": "What is the right way to POST multipart/form-data using curl?",
    "answer": "The following syntax fixes it for you:\ncurl -v -F key1=value1 -F upload=@localfilename URL",
    "tag": "curl"
  },
  {
    "question": "Composer install error - requires ext_curl when it's actually enabled",
    "answer": "This is caused because you don't have a library php5-curl installed in your system,\nOn Ubuntu its just simple run the line code below, in your case on Xamp take a look in Xamp documentation\nsudo apt-get install php5-curl\n\nFor anyone who uses php7.0\nsudo apt-get install php7.0-curl\n\nFor those who uses php7.1\nsudo apt-get install php7.1-curl\n\nFor those who use php7.2\nsudo apt-get install php7.2-curl\n\nFor those who use php7.3\nsudo apt-get install php7.3-curl\n\nFor those who use php7.4\nsudo apt-get install php7.4-curl\n\nFor those who use php8.0\nsudo apt-get install php8.0-curl\n\nFor those who use php8.1\nsudo apt-get install php8.1-curl\n\nOr simply run below command to install by your version:\nsudo apt-get install php-curl",
    "tag": "curl"
  },
  {
    "question": "Setting Curl's Timeout in PHP",
    "answer": "See documentation: http://www.php.net/manual/en/function.curl-setopt.php\nCURLOPT_CONNECTTIMEOUT - The number of seconds to wait while trying to connect. Use 0 to wait indefinitely.\nCURLOPT_TIMEOUT - The maximum number of seconds to allow cURL functions to execute.\ncurl_setopt($ch, CURLOPT_CONNECTTIMEOUT, 0); \ncurl_setopt($ch, CURLOPT_TIMEOUT, 400); //timeout in seconds\n\nalso don't forget to enlarge time execution of php script self:\nset_time_limit(0);// to infinity for example",
    "tag": "curl"
  },
  {
    "question": "How do I deal with certificates using cURL while trying to access an HTTPS url?",
    "answer": "I also had the newest version of ca-certificates installed but was still getting the error:\ncurl: (77) error setting certificate verify locations:\n  CAfile: /etc/pki/tls/certs/ca-bundle.crt\n  CApath: none\n\nThe issue was that curl expected the certificate to be at the path /etc/pki/tls/certs/ca-bundle.crt but could not find it because it was at the path /etc/ssl/certs/ca-certificates.crt.\nCopying my certificate to the expected destination by running\nsudo cp /etc/ssl/certs/ca-certificates.crt /etc/pki/tls/certs/ca-bundle.crt\n\nworked for me. You will need to create folders for the target destination if they do not exist by running\nsudo mkdir -p /etc/pki/tls/certs\n\nIf needed, modify the above command to make the destination file name match the path expected by curl, i.e. replace /etc/pki/tls/certs/ca-bundle.crt with the path following \"CAfile:\" in your error message.",
    "tag": "curl"
  },
  {
    "question": "How to pass payload via JSON file for curl?",
    "answer": "curl sends POST requests with the default content type of application/x-www-form-urlencoded. If you want to send a JSON request, you will have to specify the correct content type header:\n$ curl -vX POST http://server/api/v1/places.json -d @testplace.json \\\n--header \"Content-Type: application/json\"\n\nBut that will only work if the server accepts json input. The .json at the end of the url may only indicate that the output is json, it doesn't necessarily mean that it also will handle json input. The API documentation should give you a hint on whether it does or not.     \nThe reason you get a 401 and not some other error is probably because the server can't extract the auth_token from your request.",
    "tag": "curl"
  },
  {
    "question": "Using curl POST with variables defined in bash script functions",
    "answer": "You don't need to pass the quotes enclosing the custom headers to curl. Also, your variables in the middle of the data argument should be quoted.\nFirst, write a function that generates the post data of your script. This saves you from all sort of headaches concerning shell quoting and makes it easier to read and maintain the script than feeding the post data on curl's invocation line as in your attempt:\ngenerate_post_data()\n{\n  cat <<EOF\n{\n  \"account\": {\n    \"email\": \"$email\",\n    \"screenName\": \"$screenName\",\n    \"type\": \"$theType\",\n    \"passwordSettings\": {\n      \"password\": \"$password\",\n      \"passwordConfirm\": \"$password\"\n    }\n  },\n  \"firstName\": \"$firstName\",\n  \"lastName\": \"$lastName\",\n  \"middleName\": \"$middleName\",\n  \"locale\": \"$locale\",\n  \"registrationSiteId\": \"$registrationSiteId\",\n  \"receiveEmail\": \"$receiveEmail\",\n  \"dateOfBirth\": \"$dob\",\n  \"mobileNumber\": \"$mobileNumber\",\n  \"gender\": \"$gender\",\n  \"fuelActivationDate\": \"$fuelActivationDate\",\n  \"postalCode\": \"$postalCode\",\n  \"country\": \"$country\",\n  \"city\": \"$city\",\n  \"state\": \"$state\",\n  \"bio\": \"$bio\",\n  \"jpFirstNameKana\": \"$jpFirstNameKana\",\n  \"jpLastNameKana\": \"$jpLastNameKana\",\n  \"height\": \"$height\",\n  \"weight\": \"$weight\",\n  \"distanceUnit\": \"MILES\",\n  \"weightUnit\": \"POUNDS\",\n  \"heightUnit\": \"FT/INCHES\"\n}\nEOF\n}\n\nIt is then easy to use that function in the invocation of curl:\ncurl -i \\\n-H \"Accept: application/json\" \\\n-H \"Content-Type:application/json\" \\\n-X POST --data \"$(generate_post_data)\" \"https://xxx:xxxxx@xxxx-www.xxxxx.com/xxxxx/xxxx/xxxx\"\n\nThis said, here are a few clarifications about shell quoting rules:\nThe double quotes in the -H arguments (as in -H \"foo bar\") tell bash to keep what's inside as a single argument (even if it contains spaces).\nThe single quotes in the --data argument (as in --data 'foo bar') do the same, except they pass all text verbatim (including double quote characters and the dollar sign).\nTo insert a variable in the middle of a single quoted text, you have to end the single quote, then concatenate with the double quoted variable, and re-open the single quote to continue the text: 'foo bar'\"$variable\"'more foo'.",
    "tag": "curl"
  },
  {
    "question": "How do I install the ext-curl extension with PHP 7?",
    "answer": "Well I was able to install it by :\nsudo apt-get install php-curl\n\non my system. This will install a dependency package, which depends on the default php version.\nAfter that restart apache\nsudo service apache2 restart",
    "tag": "curl"
  },
  {
    "question": "PHP - Debugging Curl",
    "answer": "You can enable the CURLOPT_VERBOSE option Curl, PHP and log that information to a (temporary) CURLOPT_STDERR:\n// CURLOPT_VERBOSE: TRUE to output verbose information.\n// Writes output to STDERR, \n// -or- the file specified using CURLOPT_STDERR.\ncurl_setopt($curlHandle, CURLOPT_VERBOSE, true);\n\n$streamVerboseHandle = fopen('php://temp', 'w+');\ncurl_setopt($curlHandle, CURLOPT_STDERR, $streamVerboseHandle);\n\nYou can then read it after curl has done the request:\n$result = curl_exec($curlHandle);\nif ($result === FALSE) {\n    printf(\"cUrl error (#%d): %s<br>\\n\",\n           curl_errno($curlHandle),\n           htmlspecialchars(curl_error($curlHandle)))\n           ;\n}\n\nrewind($streamVerboseHandle);\n$verboseLog = stream_get_contents($streamVerboseHandle);\n\necho \"cUrl verbose information:\\n\", \n     \"<pre>\", htmlspecialchars($verboseLog), \"</pre>\\n\";\n\n(I originally answered similar but more extended in a related question.)\nMore information like metrics about the last request is available via curl_getinfo. This information can be useful for debugging curl requests, too. A usage example, I would normally wrap that into a function:\n$version = curl_version();\nextract(curl_getinfo($curlHandle));\n$metrics = <<<EOD\nURL....: $url\nCode...: $http_code ($redirect_count redirect(s) in $redirect_time secs)\nContent: $content_type Size: $download_content_length (Own: $size_download) Filetime: $filetime\nTime...: $total_time Start @ $starttransfer_time (DNS: $namelookup_time Connect: $connect_time Request: $pretransfer_time)\nSpeed..: Down: $speed_download (avg.) Up: $speed_upload (avg.)\nCurl...: v{$version['version']}\nEOD;",
    "tag": "curl"
  },
  {
    "question": "curl : (1) Protocol https not supported or disabled in libcurl",
    "answer": "Got the answer HERE for windows,\nit says there that:\ncurl -XPUT 'http://localhost:9200/api/twittervnext/tweet'\n\nWoops, first try and already an error:\ncurl: (1) Protocol 'http not supported or disabled in libcurl\n\nThe reason for this error is kind of stupid, Windows doesn’t like it when you are using single quotes for commands. So the correct command is:\ncurl –XPUT \"http://localhost:9200/api/twittervnext/tweet\"",
    "tag": "curl"
  },
  {
    "question": "Getting HTTP code in PHP using curl",
    "answer": "First make sure if the URL is actually valid (a string, not empty, good syntax), this is quick to check server side. For example, doing this first could save a lot of time:\nif(!$url || !is_string($url) || ! preg_match('/^http(s)?:\\/\\/[a-z0-9-]+(.[a-z0-9-]+)*(:[0-9]+)?(\\/.*)?$/i', $url)){\n    return false;\n}\n\nMake sure you only fetch the headers, not the body content:\n@curl_setopt($ch, CURLOPT_HEADER  , true);  // we want headers\n@curl_setopt($ch, CURLOPT_NOBODY  , true);  // we don't need body\n\nFor more details on getting the URL status http code I refer to another post I made (it also helps with following redirects):\n\nHow can I check if a URL exists via PHP?\n\n\nAs a whole:\n$url = 'http://www.example.com';\n$ch = curl_init($url);\ncurl_setopt($ch, CURLOPT_HEADER, true);    // we want headers\ncurl_setopt($ch, CURLOPT_NOBODY, true);    // we don't need body\ncurl_setopt($ch, CURLOPT_RETURNTRANSFER,1);\ncurl_setopt($ch, CURLOPT_TIMEOUT,10);\n$output = curl_exec($ch);\n$httpcode = curl_getinfo($ch, CURLINFO_HTTP_CODE);\ncurl_close($ch);\n\necho 'HTTP code: ' . $httpcode;",
    "tag": "curl"
  },
  {
    "question": "PHP - SSL certificate error: unable to get local issuer certificate",
    "answer": "Finally got this to work!\n\nDownload the certificate bundle.\nPut it somewhere. In my case, that was c:\\wamp\\ directory (if you are using Wamp 64 bit then it's c:\\wamp64\\).\nEnable mod_ssl in Apache and php_openssl.dll in php.ini (uncomment them by removing ; at the beginning). But be careful, my problem was that I had two php.ini files and I need to do this in both of them. One is the one you get from your WAMP taskbar icon, and another one is, in my case, in C:\\wamp\\bin\\php\\php5.5.12\\\nAdd these lines to your cert in both php.ini files:\ncurl.cainfo=\"C:/wamp/cacert.pem\"\nopenssl.cafile=\"C:/wamp/cacert.pem\"\n\nRestart Wamp services.",
    "tag": "curl"
  },
  {
    "question": "Making an API call in Python with an API that requires a bearer token",
    "answer": "It just means it expects that as a key in your header data\nimport requests\nendpoint = \".../api/ip\"\ndata = {\"ip\": \"1.1.2.3\"}\nheaders = {\"Authorization\": \"Bearer MYREALLYLONGTOKENIGOT\"}\n\nresponse = requests.post(endpoint, data=data, headers=headers)\nprint(response.json())",
    "tag": "curl"
  },
  {
    "question": "How to POST JSON Data With PHP cURL?",
    "answer": "You are POSTing the json incorrectly -- but even if it were correct, you would not be able to test using print_r($_POST) (read why here).  Instead, on your second page, you can nab the incoming request using file_get_contents(\"php://input\"), which will contain the POSTed json.  To view the received data in a more readable format, try this: \necho '<pre>'.print_r(json_decode(file_get_contents(\"php://input\")),1).'</pre>';\n\nIn your code, you are indicating Content-Type:application/json, but you are not json-encoding all of the POST data -- only the value of the \"customer\" POST field.  Instead, do something like this:\n$ch = curl_init( $url );\n# Setup request to send json via POST.\n$payload = json_encode( array( \"customer\"=> $data ) );\ncurl_setopt( $ch, CURLOPT_POSTFIELDS, $payload );\ncurl_setopt( $ch, CURLOPT_HTTPHEADER, array('Content-Type:application/json'));\n# Return response instead of printing.\ncurl_setopt( $ch, CURLOPT_RETURNTRANSFER, true );\n# Send request.\n$result = curl_exec($ch);\ncurl_close($ch);\n# Print response.\necho \"<pre>$result</pre>\";\n\nSidenote: You might benefit from using a third-party library instead of interfacing with the Shopify API directly yourself.",
    "tag": "curl"
  },
  {
    "question": "Why does cURL return error \"(23) Failed writing body\"?",
    "answer": "This happens when a piped program (e.g. grep) closes the read pipe before the previous program is finished writing the whole page.\nIn curl \"url\" | grep -qs foo, as soon as grep has what it wants it will close the read stream from curl. cURL doesn't expect this and emits the \"Failed writing body\" error.\nA workaround is to pipe the stream through an intermediary program that always reads the whole page before feeding it to the next program.\nE.g.\ncurl \"url\" | tac | tac | grep -qs foo\n\ntac is a simple Unix program that reads the entire input page and reverses the line order (hence we run it twice). Because it has to read the whole input to find the last line, it will not output anything to grep until cURL is finished. Grep will still close the read stream when it has what it's looking for, but it will only affect tac, which doesn't emit an error.",
    "tag": "curl"
  },
  {
    "question": "Execute bash script from URL",
    "answer": "source <(curl -s http://mywebsite.example/myscript.txt)\n\nought to do it. Alternately, leave off the initial redirection on yours, which is redirecting standard input; bash takes a filename to execute just fine without redirection, and <(command) syntax provides a path.\nbash <(curl -s http://mywebsite.example/myscript.txt)\n\nIt may be clearer if you look at the output of echo <(cat /dev/null)",
    "tag": "curl"
  },
  {
    "question": "What is cURL in PHP?",
    "answer": "cURL is a library that lets you make HTTP requests in PHP. Everything you need to know about it (and most other extensions) can be found in the PHP manual.\n\nIn order to use PHP's cURL functions\n  you need to install the » libcurl\n  package. PHP requires that you use\n  libcurl 7.0.2-beta or higher. In PHP\n  4.2.3, you will need libcurl version 7.9.0 or higher. From PHP 4.3.0, you will need a libcurl version that's\n  7.9.8 or higher. PHP 5.0.0 requires a libcurl version 7.10.5 or greater.\n\nYou can make HTTP requests  without cURL, too, though it requires allow_url_fopen to be enabled in your php.ini file.\n// Make a HTTP GET request and print it (requires allow_url_fopen to be enabled)\nprint file_get_contents('http://www.example.com/');",
    "tag": "curl"
  },
  {
    "question": "What's the net::ERR_HTTP2_PROTOCOL_ERROR about?",
    "answer": "In my case it was - no disk space left on the web server.",
    "tag": "curl"
  },
  {
    "question": "Assign output to variable in Bash",
    "answer": "In shell, you don't put a $ in front of a variable you're assigning.  You only use $IP when you're referring to the variable.\n#!/bin/bash\n\nIP=$(curl automation.whatismyip.com/n09230945.asp)\n\necho \"$IP\"\n\nsed \"s/IP/$IP/\" nsupdate.txt | nsupdate",
    "tag": "curl"
  },
  {
    "question": "How to catch cURL errors in PHP",
    "answer": "You can use the curl_error() function to detect if there was some error. For example:\n$ch = curl_init();\ncurl_setopt($ch, CURLOPT_URL, $your_url);\ncurl_setopt($ch, CURLOPT_FAILONERROR, true); // Required for HTTP error codes to be reported via our call to curl_error($ch)\n//...\ncurl_exec($ch);\nif (curl_errno($ch)) {\n    $error_msg = curl_error($ch);\n}\ncurl_close($ch);\n\nif (isset($error_msg)) {\n    // TODO - Handle cURL error accordingly\n}\n\nSee the description of libcurl error codes here\nSee the description of PHP curl_errno() function here\nSee the description of PHP curl_error() function here",
    "tag": "curl"
  },
  {
    "question": "How can I enable cURL for an installed Ubuntu LAMP stack?",
    "answer": "From Install Curl Extension for PHP in Ubuntu:\nsudo apt-get install php5-curl\n\nAfter installing libcurl, you should restart the web server with one of the following commands,\nsudo /etc/init.d/apache2 restart\n\nor\nsudo service apache2 restart",
    "tag": "curl"
  },
  {
    "question": "How to make remote REST call inside Node.js? any CURL?",
    "answer": "Look at http.request\nvar options = {\n  host: url,\n  port: 80,\n  path: '/resource?id=foo&bar=baz',\n  method: 'POST'\n};\n\nhttp.request(options, function(res) {\n  console.log('STATUS: ' + res.statusCode);\n  console.log('HEADERS: ' + JSON.stringify(res.headers));\n  res.setEncoding('utf8');\n  res.on('data', function (chunk) {\n    console.log('BODY: ' + chunk);\n  });\n}).end();",
    "tag": "curl"
  },
  {
    "question": "https connection using CURL from command line",
    "answer": "I had the same problem - I was fetching a page from my own site, which was served over HTTPS, but curl was giving the same \"SSL certificate problem\" message. I worked around it by adding a -k flag to the call to allow insecure connections.\ncurl -k https://whatever.com/script.php\n\nEdit: I discovered the root of the problem. I was using an SSL certificate (from StartSSL, but I don't think that matters much) and hadn't set up the intermediate certificate properly. If you're having the same problem as user1270392 above, it's probably a good idea to test your SSL cert and fix any issues with it before resorting to the curl -k fix.",
    "tag": "curl"
  },
  {
    "question": "HTTPS and SSL3_GET_SERVER_CERTIFICATE:certificate verify failed, CA is OK",
    "answer": "It's a pretty common problem in Windows. You need just to set cacert.pem to curl.cainfo.\nSince PHP 5.3.7 you could do:\n\ndownload https://curl.se/ca/cacert.pem and save it somewhere.\nupdate php.ini -- add curl.cainfo = \"PATH_TO/cacert.pem\"\n\nOtherwise you will need to do the following for every cURL resource:\ncurl_setopt ($ch, CURLOPT_CAINFO, \"PATH_TO/cacert.pem\");",
    "tag": "curl"
  },
  {
    "question": "cURL suppress response body",
    "answer": "You can use the -o switch and null pseudo-file :\nUnix\ncurl -s -o /dev/null -v http://google.com\n\nWindows\ncurl -s -o nul -v http://google.com",
    "tag": "curl"
  },
  {
    "question": "Basic HTTP and Bearer Token Authentication",
    "answer": "Try this one to push basic authentication at url:\ncurl -i http://username:password@dev.myapp.com/api/users -H \"Authorization: Bearer mytoken123\"\n               ^^^^^^^^^^^^^^^^^^\n\nIf above one doesn't work, then you have nothing to do with it. So try the following alternates.\nYou can pass the token under another name. Because you are handling the authorization from your Application. So you can easily use this flexibility for this special purpose.\ncurl -i http://dev.myapp.com/api/users \\\n  -H \"Authorization: Basic Ym9zY236Ym9zY28=\" \\\n  -H \"Application-Authorization: mytoken123\"\n\nNotice I have changed the header into Application-Authorization. So from your application catch the token under that header and process what you need to do.\nAnother thing you can do is, to pass the token through the POST parameters and grab the parameter's value from the Server side. For example passing token with curl post parameter:\n-d \"auth-token=mytoken123\"",
    "tag": "curl"
  },
  {
    "question": "How do I make curl ignore the proxy?",
    "answer": "If your curl is at least version 7.19.4, you could just use the --noproxy flag.\ncurl --noproxy '*' http://www.stackoverflow.com\n\nFrom the manual.",
    "tag": "curl"
  },
  {
    "question": "What is the curl error 52 \"empty reply from server\"?",
    "answer": "This can happen if curl is asked to do plain HTTP on a server that does HTTPS.\nExample:\n$ curl http://google.com:443\ncurl: (52) Empty reply from server",
    "tag": "curl"
  },
  {
    "question": "Get final URL after curl is redirected",
    "answer": "curl's -w option and the sub variable url_effective is what you are\nlooking for.\nSomething like\ncurl -Ls -o /dev/null -w %{url_effective} https://example.com\n\nMore info\n\n-L         Follow redirects\n-s         Silent mode. Don't output anything\n-o FILE    Write output to <file> instead of stdout\n-w FORMAT  What to output after completion\n\nMore\nYou might want to add -I (that is an uppercase i) as well, which will make the command not download any \"body\", but it then also uses the HEAD method, which is not what the question included and risk changing what the server does. Sometimes servers don't respond well to HEAD even when they respond fine to GET.",
    "tag": "curl"
  },
  {
    "question": "How to properly handle a gzipped page when using curl?",
    "answer": "curl will automatically decompress the response if you set the --compressed flag:\ncurl --compressed \"http://example.com\"\n\n\n--compressed\n   (HTTP)  Request  a compressed response using one of the algorithms libcurl supports, and save the uncompressed document.  If this option is used and the server sends an unsupported encoding, curl will report an error.\n\ngzip is most likely supported, but you can check this by running curl -V and looking for libz somewhere in the \"Features\" line:\n$ curl -V\n...\nProtocols: ...\nFeatures: GSS-Negotiate IDN IPv6 Largefile NTLM SSL libz \n\n\nNote that it's really the website in question that is at fault here. If curl did not pass an Accept-Encoding: gzip request header, the server should not have sent a compressed response.",
    "tag": "curl"
  },
  {
    "question": "curl -GET and -X GET",
    "answer": "By default you use curl without explicitly saying which request method to use. If you just pass in a HTTP URL like curl http://example.com it will use GET. If you use -d or -F curl will use POST, -I will cause a HEAD and -T will make it a PUT.\nIf for whatever reason you're not happy with these default choices that curl does for you, you can override those request methods by specifying -X [WHATEVER]. This way you can for example send a DELETE by doing curl -X DELETE [URL].\nIt is thus pointless to do curl -X GET [URL] as GET would be used anyway. In the same vein it is pointless to do curl -X POST -d data [URL]... But you can make a fun and somewhat rare request that sends a request-body in a GET request with something like curl -X GET -d data [URL].\nDigging deeper\ncurl -GET (using a single dash) is just wrong for this purpose. That's the equivalent of specifying the -G, -E and -T options and that will do something completely different.\nThere's also a curl option called --get to not confuse matters with either. It is the long form of -G, which is used to convert data specified with -d into a GET request instead of a POST.\n(I subsequently used my own answer here to populate the curl FAQ to cover this.)\nWarnings\nModern versions of curl will inform users about this unnecessary and potentially harmful use of -X when verbose mode is enabled (-v) - to make users aware. Further explained and motivated in this blog post.\n-G converts a POST + body to a GET + query\nYou can ask curl to convert a set of -d options and instead of sending them in the request body with POST, put them at the end of the URL's query string and issue a GET, with the use of `-G. Like this:\ncurl -d name=daniel -d grumpy=yes -G https://example.com/",
    "tag": "curl"
  },
  {
    "question": "cURL equivalent in Node.js?",
    "answer": "See the documentation for the HTTP module for a full example:\nhttps://nodejs.org/api/http.html#http_http_request_options_callback",
    "tag": "curl"
  },
  {
    "question": "How to specify the download location with wget?",
    "answer": "From the manual page:\n-P prefix\n--directory-prefix=prefix\n           Set directory prefix to prefix.  The directory prefix is the\n           directory where all other files and sub-directories will be\n           saved to, i.e. the top of the retrieval tree.  The default\n           is . (the current directory).\n\nSo you need to add -P /tmp/cron_test/ (short form) or --directory-prefix=/tmp/cron_test/ (long form) to your command. Also note that if the directory does not exist it will get created.",
    "tag": "wget"
  },
  {
    "question": "How to change filename of a file downloaded with wget?",
    "answer": "Use the -O file option. \nE.g.\nwget google.com\n...\n16:07:52 (538.47 MB/s) - `index.html' saved [10728]\n\nvs.\nwget -O foo.html google.com\n...\n16:08:00 (1.57 MB/s) - `foo.html' saved [10728]",
    "tag": "wget"
  },
  {
    "question": "Using wget to recursively fetch a directory with arbitrary files in it",
    "answer": "You have to pass the -np/--no-parent option to wget (in addition to -r/--recursive, of course), otherwise it will follow the link in the directory index on my site to the parent directory. So the command would look like this:\nwget --recursive --no-parent http://example.com/configs/.vim/\n\nTo avoid downloading the auto-generated index.html files, use the -R/--reject option:\nwget -r -np -R \"index.html*\" http://example.com/configs/.vim/",
    "tag": "wget"
  },
  {
    "question": "Downloading Java JDK on Linux via wget is shown license page instead",
    "answer": "Works as of December 23rd, 2021 for JDK 17\nwget --no-check-certificate -c --header \"Cookie: oraclelicense=accept-securebackup-cookie\" https://download.oracle.com/java/17/archive/jdk-17.0.1_linux-x64_bin.rpm\n\nWorks as of July 27th, 2021 for JDK 16\nwget --no-check-certificate -c --header \"Cookie: oraclelicense=accept-securebackup-cookie\" https://download.oracle.com/otn-pub/java/jdk/16.0.2%2B7/d4a915d82b4c4fbb9bde534da945d746/jdk-16.0.2_linux-x64_bin.rpm\n\nWorks as of November 5th, 2020 for JDK 15\nwget --no-check-certificate -c --header \"Cookie: oraclelicense=accept-securebackup-cookie\" https://download.oracle.com/otn-pub/java/jdk/15.0.1+9/51f4f36ad4ef43e39d0dfdbaf6549e32/jdk-15.0.1_linux-x64_bin.rpm\n\nWorks as of 07-11-2020 for JDK 14\nwget --no-check-certificate -c --header \"Cookie: oraclelicense=accept-securebackup-cookie\" https://download.oracle.com/otn-pub/java/jdk/14.0.1+7/664493ef4a6946b186ff29eb326336a2/jdk-14.0.1_linux-x64_bin.rpm -O ~/Downloads/jdk-14.0.1_linux-x64_bin.rpm\n\nPS: Alf added this ( me ) :-) this, I couldn't figured out how to just commented at the end... Enjoy it.\nUPDATED FOR Oracle JDK 11\nwget --no-check-certificate -c --header \"Cookie: oraclelicense=accept-securebackup-cookie\" http://download.oracle.com/otn-pub/java/jdk/11+28/55eed80b163941c8885ad9298e6d786a/jdk-11_linux-x64_bin.tar.gz\n\nUPDATED FOR JDK 10.0.2\nwget --no-check-certificate -c --header \"Cookie: oraclelicense=accept-securebackup-cookie\" http://download.oracle.com/otn-pub/java/jdk/10.0.2+13/19aef61b38124481863b1413dce1855f/jdk-10.0.2_linux-x64_bin.tar.gz\n\nUPDATED FOR JDK 10.0.1\nwget --no-check-certificate -c --header \"Cookie: oraclelicense=accept-securebackup-cookie\" http://download.oracle.com/otn-pub/java/jdk/10.0.1+10/fb4372174a714e6b8c52526dc134031e/jdk-10.0.1_linux-x64_bin.tar.gz\n\nUPDATED FOR JDK 9\nit looks like you can download it now directly from java.net without sending a header\nwget http://download.java.net/java/GA/jdk9/9/binaries/jdk-9+181_linux-x64_bin.tar.gz\n\nUPDATED FOR JDK 8u191\nTAR GZ:\nwget --no-cookies --no-check-certificate --header \"Cookie: gpw_e24=http%3a%2F%2Fwww.oracle.com%2Ftechnetwork%2Fjava%2Fjavase%2Fdownloads%2Fjdk8-downloads-2133151.html; oraclelicense=accept-securebackup-cookie;\" \"https://download.oracle.com/otn-pub/java/jdk/8u191-b12/2787e4a523244c269598db4e85c51e0c/jdk-8u191-linux-x64.tar.gz\"\n\nRPM:\nwget --no-cookies --no-check-certificate --header \"Cookie: gpw_e24=http%3a%2F%2Fwww.oracle.com%2Ftechnetwork%2Fjava%2Fjavase%2Fdownloads%2Fjdk8-downloads-2133151.html; oraclelicense=accept-securebackup-cookie;\" \"https://download.oracle.com/otn-pub/java/jdk/8u191-b12/2787e4a523244c269598db4e85c51e0c/jdk-8u191-linux-x64.rpm\"\n\nUPDATED FOR JDK 8u131\nRPM:\n  wget -c --header \"Cookie: oraclelicense=accept-securebackup-cookie\" http://download.oracle.com/otn-pub/java/jdk/8u131-b11/d54c1d3a095b4ff2b6607d096fa80163/jdk-8u131-linux-x64.rpm\n\nTAR GZ:\n wget -c --header \"Cookie: oraclelicense=accept-securebackup-cookie\" http://download.oracle.com/otn-pub/java/jdk/8u131-b11/d54c1d3a095b4ff2b6607d096fa80163/jdk-8u131-linux-x64.tar.gz\n\nRPM using curl:\n curl -v -j -k -L -H \"Cookie: oraclelicense=accept-securebackup-cookie\" http://download.oracle.com/otn-pub/java/jdk/8u131-b11/d54c1d3a095b4ff2b6607d096fa80163/jdk-8u131-linux-x64.rpm > jdk-8u112-linux-x64.rpm\n\nIn all cases above, subst 'i586' for 'x64' to download the 32-bit build.\n\n-j -> junk cookies\n-k -> ignore certificates\n-L -> follow redirects\n-H [arg] -> headers\n\ncurl can be used in place of wget.\nUPDATE FOR JDK 7u79\nTAR GZ:\nwget --no-check-certificate --no-cookies --header \"Cookie: oraclelicense=accept-securebackup-cookie\" http://download.oracle.com/otn-pub/java/jdk/7u79-b15/jdk-7u79-linux-x64.tar.gz\n\nRPM using curl:\ncurl -v -j -k -L -H \"Cookie: oraclelicense=accept-securebackup-cookie\" http://download.oracle.com/otn-pub/java/jdk/7u79-b15/jdk-7u79-linux-x64.rpm > jdk-7u79-linux-x64.rpm\n\nOnce again, make sure you specify the correct URL for the version you are downloading. You can find the URL here: Oracle JDK download site\nORIGINAL ANSWER FROM 9th June 2012\nIf you are looking to download the Oracle JDK from the command line using wget, there is a workaround. Run the wget command as follows:\nwget --no-cookies --header \"Cookie: gpw_e24=http%3A%2F%2Fwww.oracle.com\" \"http://download.oracle.com/otn-pub/java/jdk/7/jdk-7-linux-x64.tar.gz\"\n\nBe sure to replace the download link with the correct one for the version you are downloading.",
    "tag": "wget"
  },
  {
    "question": "wget/curl large file from google drive",
    "answer": "Update: Mars 2025\nYou can use gdown. Consider also visiting that page for full instructions; this is just a summary and the source repo may have more up-to-date instructions.\n\nInstructions\nInstall it with the following command:\npip install gdown\n\nAfter that, you can download any file from Google Drive by running one of these commands:\ngdown https://drive.google.com/uc?id=<file_id>  # for files\ngdown <file_id>                                 # alternative format\ngdown --folder https://drive.google.com/drive/folders/<file_id>  # for folders\ngdown --folder --id <file_id>                                   # this format works for folders too\n\nExample: to download the readme file from this directory\ngdown https://drive.google.com/uc?id=0B7EVK8r0v71pOXBhSUdJWU1MYUk\n\nThe file_id should look something like 0Bz8a_Dbh9QhbNU3SGlFaDg. You can find this ID by right-clicking on the file of interest, and selecting Get link. As of November 2021, this link will be of the form:\n# Files\nhttps://drive.google.com/file/d/<file_id>/view?usp=sharing\n# Folders\nhttps://drive.google.com/drive/folders/<file_id>\n\nCaveats\n\nOnly works on open access files. (\"Anyone who has a link can View\")\nCannot download more than 50 files into a single folder.\n\nIf you have access to the source file, you can consider using tar/zip to make it a single file to work around this limitation.",
    "tag": "wget"
  },
  {
    "question": "How to download HTTP directory with all files and sub-directories as they appear on the online files/folders list?",
    "answer": "Solution:\nwget -r -np -nH --cut-dirs=3 -R index.html http://hostname/aaa/bbb/ccc/ddd/\n\nExplanation:\n\nIt will download all files and subfolders in ddd directory\n-r : recursively \n-np : not going to upper directories, like ccc/…\n-nH : not saving files to hostname folder \n--cut-dirs=3 : but saving it to ddd by omitting\nfirst 3 folders aaa, bbb, ccc\n-R index.html : excluding index.html\nfiles \n\nReference: http://bmwieczorek.wordpress.com/2008/10/01/wget-recursively-download-all-files-from-certain-directory-listed-by-apache/",
    "tag": "wget"
  },
  {
    "question": "Skip download if files already exist in wget?",
    "answer": "Try the following parameter:\n\n-nc, --no-clobber:              skip downloads that would download to \n                                     existing files.\n\nSample usage:\nwget -nc http://example.com/pic.png",
    "tag": "wget"
  },
  {
    "question": "How to install wget in macOS?",
    "answer": "Using brew\nFirst install brew:\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n\nAnd then install wget with brew:\nbrew install wget\n\nUsing MacPorts\nFirst, download and run MacPorts installer (.pkg)\nAnd then install wget:\nsudo port install wget",
    "tag": "wget"
  },
  {
    "question": "How to get past the login page with Wget?",
    "answer": "Based on the manual page:\n# Log in to the server.  This only needs to be done once.\nwget --save-cookies cookies.txt \\\n     --keep-session-cookies \\\n     --post-data 'user=foo&password=bar' \\\n     --delete-after \\\n     http://server.com/auth.php\n\n# Now grab the page or pages we care about.\nwget --load-cookies cookies.txt \\\n     http://server.com/interesting/article.php\n\nMake sure the --post-data parameter is properly percent-encoded (especially ampersands!) or the request will probably fail. Also make sure that user and password are the correct keys; you can find out the correct keys by sleuthing the HTML of the login page (look into your browser’s “inspect element” feature and find the name attribute on the username and password fields).",
    "tag": "wget"
  },
  {
    "question": "How can I set a proxy for Wget?",
    "answer": "For all users of the system via the /etc/wgetrc or for the user only with the ~/.wgetrc file:\nuse_proxy=yes\nhttp_proxy=127.0.0.1:8080\nhttps_proxy=127.0.0.1:8080\n\nor via -e options placed after the URL:\nwget ... -e use_proxy=yes -e http_proxy=127.0.0.1:8080 ...",
    "tag": "wget"
  },
  {
    "question": "Multiple simultaneous downloads using Wget?",
    "answer": "Use the aria2:\naria2c -x 16 [url]\n#          |\n#          |\n#          |\n#          ----> the number of connections \n\nhttp://aria2.sourceforge.net",
    "tag": "wget"
  },
  {
    "question": "Download a working local copy of a webpage",
    "answer": "wget is capable of doing what you are asking. Just try the following:\nwget -p -k http://www.example.com/\n\nThe -p will get you all the required elements to view the site correctly (css, images, etc).\nThe -k will change all links (to include those for CSS & images) to allow you to view the page offline as it appeared online.\nFrom the Wget docs:\n‘-k’\n‘--convert-links’\nAfter the download is complete, convert the links in the document to make them\nsuitable for local viewing. This affects not only the visible hyperlinks, but\nany part of the document that links to external content, such as embedded images,\nlinks to style sheets, hyperlinks to non-html content, etc.\n\nEach link will be changed in one of the two ways:\n\n    The links to files that have been downloaded by Wget will be changed to refer\n    to the file they point to as a relative link.\n\n    Example: if the downloaded file /foo/doc.html links to /bar/img.gif, also\n    downloaded, then the link in doc.html will be modified to point to\n    ‘../bar/img.gif’. This kind of transformation works reliably for arbitrary\n    combinations of directories.\n\n    The links to files that have not been downloaded by Wget will be changed to\n    include host name and absolute path of the location they point to.\n\n    Example: if the downloaded file /foo/doc.html links to /bar/img.gif (or to\n    ../bar/img.gif), then the link in doc.html will be modified to point to\n    http://hostname/bar/img.gif. \n\nBecause of this, local browsing works reliably: if a linked file was downloaded,\nthe link will refer to its local name; if it was not downloaded, the link will\nrefer to its full Internet address rather than presenting a broken link. The fact\nthat the former links are converted to relative links ensures that you can move\nthe downloaded hierarchy to another directory.\n\nNote that only at the end of the download can Wget know which links have been\ndownloaded. Because of that, the work done by ‘-k’ will be performed at the end\nof all the downloads.",
    "tag": "wget"
  },
  {
    "question": "How do I fix certificate errors when running wget on an HTTPS URL in Cygwin?",
    "answer": "If you don't care about checking the validity of the certificate just add the --no-check-certificate option on the wget command-line.  This worked well for me.\nNOTE: This opens you up to man-in-the-middle (MitM) attacks, and is not recommended for anything where you care about security.",
    "tag": "wget"
  },
  {
    "question": "Get final URL after curl is redirected",
    "answer": "curl's -w option and the sub variable url_effective is what you are\nlooking for.\nSomething like\ncurl -Ls -o /dev/null -w %{url_effective} https://example.com\n\nMore info\n\n-L         Follow redirects\n-s         Silent mode. Don't output anything\n-o FILE    Write output to <file> instead of stdout\n-w FORMAT  What to output after completion\n\nMore\nYou might want to add -I (that is an uppercase i) as well, which will make the command not download any \"body\", but it then also uses the HEAD method, which is not what the question included and risk changing what the server does. Sometimes servers don't respond well to HEAD even when they respond fine to GET.",
    "tag": "wget"
  },
  {
    "question": "How can I use Python's Requests to fake a browser visit a.k.a and generate User Agent?",
    "answer": "Provide a User-Agent header:\nimport requests\n\nurl = 'http://www.ichangtou.com/#company:data_000008.html'\nheaders = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n\nresponse = requests.get(url, headers=headers)\nprint(response.content)\n\nFYI, here is a list of User-Agent strings for different browsers:\n\nList of all Browsers\n\n\nAs a side note, there is a pretty useful third-party package called fake-useragent that provides a nice abstraction layer over user agents:\n\nfake-useragent\nUp to date simple useragent faker with real world database\n\nDemo:\n>>> from fake_useragent import UserAgent\n>>> ua = UserAgent()\n>>> ua.chrome\nu'Mozilla/5.0 (Windows NT 6.2; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1667.0 Safari/537.36'\n>>> ua.random\nu'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.67 Safari/537.36'",
    "tag": "wget"
  },
  {
    "question": "How to download an entire directory and subdirectories using wget?",
    "answer": "You may use this in shell:\nwget -r --no-parent http://abc.tamu.edu/projects/tzivi/repository/revisions/2/raw/tzivi/\n\nThe Parameters are:\n-r     //recursive Download\n\nand  \n--no-parent // Don´t download something from the parent directory\n\nIf you don't want to download the entire content, you may use:  \n-l1 just download the directory (tzivi in your case)\n\n-l2 download the directory and all level 1 subfolders ('tzivi/something' but not 'tivizi/somthing/foo')  \n\nAnd so on. If you insert no -l option, wget will use -l 5 automatically.\nIf you insert a -l 0 you´ll download the whole Internet, because wget will follow every link it finds.",
    "tag": "wget"
  },
  {
    "question": "How to download all files (but not HTML) from a website using wget?",
    "answer": "To filter for specific file extensions:\nwget -A pdf,jpg -m -p -E -k -K -np http://site/path/\n\nOr, if you prefer long option names:\nwget --accept pdf,jpg --mirror --page-requisites --adjust-extension --convert-links --backup-converted --no-parent http://site/path/\n\nThis will mirror the site, but the files without jpg or pdf extension will be automatically removed.",
    "tag": "wget"
  },
  {
    "question": "How to `wget` a list of URLs in a text file?",
    "answer": "Quick man wget gives me the following:\n\n[..]\n-i file\n--input-file=file\nRead URLs from a local or external file. If - is specified as file, URLs are read from the standard input. (Use ./- to read from a file literally named -.)\nIf this function is used, no URLs need be present on the command line. If there are URLs both on the command line and in an input file, those on the command lines will be the first ones to be retrieved. If --force-html is not specified, then file should consist of a series of URLs, one per line.\n[..]\n\nSo: wget -i text_file.txt",
    "tag": "wget"
  },
  {
    "question": "Wget output document and headers to STDOUT",
    "answer": "Try the following\nwget -q -S -O - www.google.com 2>&1\n\nNote the trailing -. This is part of the normal command argument for -O to cat out to a file, but since we don't use > to direct to a file, it goes out to the shell. You can use -qO- or -qO -.",
    "tag": "wget"
  },
  {
    "question": "How do I request a file but not save it with Wget?",
    "answer": "Use q flag for quiet mode, and tell wget to output to stdout with O- (uppercase o) and redirect to /dev/null to discard the output:\nwget -qO- $url &> /dev/null\n> redirects application output (to a file). if > is preceded by ampersand, shell redirects all outputs (error and normal) to the file right of >. If you don't specify ampersand, then only normal output is redirected.\n./app &>  file # redirect error and standard output to file\n./app >   file # redirect standard output to file\n./app 2>  file # redirect error output to file\n\nif file is /dev/null then all is discarded.\nThis works as well, and simpler:\nwget -O/dev/null -q $url",
    "tag": "wget"
  },
  {
    "question": "How do I use Wget to download all images into a single folder, from a URL?",
    "answer": "Try this:\nwget -nd -r -P /save/location -A jpeg,jpg,bmp,gif,png http://www.somedomain.com\n\nHere is some more information:\n-nd prevents the creation of a directory hierarchy (i.e. no directories).\n-r enables recursive retrieval. See Recursive Download for more information.\n-P sets the directory prefix where all files and directories are saved to.\n-A sets a whitelist for retrieving only certain file types. Strings and patterns are accepted, and both can be used in a comma separated list (as seen above). See Types of Files for more information.",
    "tag": "wget"
  },
  {
    "question": "What is better, curl or wget?",
    "answer": "If you are programming, you should use curl. It has a nice api and is available for most languages. Shelling out to the os to run wget is a kludge and shouldn't be done if you have an API interface!",
    "tag": "wget"
  },
  {
    "question": "How to hide wget output in Linux?",
    "answer": "Why don't you use -q?\nFrom man wget:\n-q\n--quiet\n   Turn off Wget's output.\n\nTest\n$ wget www.google.com\n--2015-05-08 14:07:42--  http://www.google.com/\nResolving www.google.com (www.google.com)... \n  (...)\nHTTP request sent, awaiting response... 200 OK\nLength: unspecified [text/html]\nSaving to: ‘index.html’\n\n    [ <=>                                                                                       ] 17,913      --.-K/s   in 0.01s   \n\n2015-05-08 14:07:42 (1.37 MB/s) - ‘index.html’ saved [17913]\n\nAnd:\n$ wget -q www.google.com\n$",
    "tag": "wget"
  },
  {
    "question": "How to get the contents of a webpage in a shell variable?",
    "answer": "You can use wget command to download the page and read it into a variable as:\ncontent=$(wget google.com -q -O -)\necho $content\n\nWe use the -O option of wget which allows us to specify the name of the file into which wget dumps the page contents. We specify - to get the dump onto standard output and collect that into the variable content. You can add the -q quiet option to turn off's wget output. \nYou can use the curl command for this aswell as:\ncontent=$(curl -L google.com)\necho $content\n\nWe need to use the -L option as the page we are requesting might have moved. In which case we need to get the page from the new location. The -L or --location option helps us with this.",
    "tag": "wget"
  },
  {
    "question": "How to run wget inside Ubuntu Docker image?",
    "answer": "You need to install it first. Create a new Dockerfile, and install wget in it:\nFROM ubuntu:14.04\nRUN  apt-get update \\\n  && apt-get install -y wget \\\n  && rm -rf /var/lib/apt/lists/*\n\nThen, build that image:\ndocker build -t my-ubuntu .\n\nFinally, run it:\ndocker run my-ubuntu wget https://downloads-packages.s3.amazonaws.com/ubuntu-14.04/gitlab_7.8.2-omnibus.1-1_amd64.deb",
    "tag": "wget"
  },
  {
    "question": "Python equivalent of a given wget command",
    "answer": "There is also a nice Python module named wget that is pretty easy to use. Keep in mind that the package has not been updated since 2015 and has not implemented a number of important features, so it may be better to use other methods. It depends entirely on your use case. For simple downloading, this module is the ticket. If you need to do more, there are other solutions out there.\n>>> import wget\n>>> url = 'http://www.futurecrew.com/skaven/song_files/mp3/razorback.mp3'\n>>> filename = wget.download(url)\n100% [................................................] 3841532 / 3841532>\n>> filename\n'razorback.mp3'\n\nEnjoy.\nHowever, if wget doesn't work (I've had trouble with certain PDF files), try this solution.\nEdit: You can also use the out parameter to use a custom output directory instead of current working directory.\n>>> output_directory = <directory_name>\n>>> filename = wget.download(url, out=output_directory)\n>>> filename\n'razorback.mp3'",
    "tag": "wget"
  },
  {
    "question": "How to send a HTTP OPTIONS request from the command line?",
    "answer": "The curl installed by default in Debian supports HTTPS since a great while back. (a long time ago there were two separate packages, one with and one without SSL but that's not the case anymore)\nOPTIONS /path\nYou can send an OPTIONS request with curl like this:\ncurl -i -X OPTIONS http://example.org/path\n\nYou may also use -v instead of -i to see more output.\nOPTIONS *\nTo send a plain * (instead of the path, see RFC 7231) with the OPTIONS method, you need curl 7.55.0 or later as then you can run a command line like:\ncurl -i --request-target \"*\" -X OPTIONS http://example.org",
    "tag": "wget"
  },
  {
    "question": "How can I show the wget progress bar only?",
    "answer": "Use:\nwget http://somesite.com/TheFile.jpeg -q --show-progress\n\n\n-q: Turn off wget's output\n--show-progress: Force wget to display the progress bar no matter what its verbosity level is set to",
    "tag": "wget"
  },
  {
    "question": "How do I download a tarball from GitHub using cURL?",
    "answer": "Use the -L option to follow redirects:\ncurl -L https://github.com/pinard/Pymacs/tarball/v0.24-beta2 | tar zx",
    "tag": "wget"
  },
  {
    "question": "Does WGET timeout?",
    "answer": "According to the man page of wget, there are a couple of options related to timeouts -- and there is a default read timeout of 900s -- so I say that, yes, it could timeout.\n\nHere are the options in question :\n-T seconds\n--timeout=seconds\n\n\nSet the network timeout to seconds\n  seconds.  This is equivalent to\n  specifying --dns-timeout,\n  --connect-timeout, and\n  --read-timeout, all at the same\n  time.\n\n\nAnd for those three options :\n--dns-timeout=seconds\n\n\nSet the DNS lookup timeout to seconds\n  seconds.  DNS lookups that don't\n  complete within the specified time\n  will fail. By default, there is no\n  timeout on DNS lookups, other than\n  that implemented by system libraries.\n\n--connect-timeout=seconds\n\n\nSet the connect timeout to seconds\n  seconds. TCP connections that take\n  longer to establish will be aborted.\n  By default, there is no connect\n  timeout, other than that implemented\n  by system libraries.\n\n--read-timeout=seconds\n\n\nSet the read (and write) timeout to\n  seconds seconds. The \"time\" of\n  this timeout refers to idle time: if,\n  at any point in the download, no data\n  is received for more than the\n  specified number of seconds, reading\n  fails and the download is restarted.\n  This option does not directly\n  affect the duration of the entire\n  download.\n\n\nI suppose using something like \nwget -O - -q -t 1 --timeout=600 http://www.example.com/cron/run\n\nshould make sure there is no timeout before longer than the duration of your script.\n(Yeah, that's probably the most brutal solution possible ^^ )",
    "tag": "wget"
  },
  {
    "question": "How to rename the downloaded file with wget?",
    "answer": "A redirect of standard output into arbitrary file name always works. You are doing it correctly as man wget says, using -O\nwget http://www.kernel.org/pub/linux/kernel/README -O foo\n--2013-01-13 18:59:44--  http://www.kernel.org/pub/linux/kernel/README\nResolving www.kernel.org... 149.20.4.69, 149.20.20.133\nConnecting to www.kernel.org|149.20.4.69|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 12056 (12K) [text/plain]\nSaving to: `foo'\n\n100%[======================================================================================================================================>] 12,056      --.-K/s   in 0.003s  \n\n2013-01-13 18:59:45 (4.39 MB/s) - `foo' saved [12056/12056]\n\nIndeed, you must be getting an HTML in your file (usually can be checked with man file).\n[EDIT]\nIn your case client is receiving 302 Found (you can check it with curl -v URL).\nThe following curl does the trick by respecting the 3xx:\n$ curl -L http://sourceforge.net/projects/sofastatistics/files/latest/download?source=files -o foo.deb\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0   463    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n  0     0    0     0    0     0      0      0 --:--:--  0:00:02 --:--:--     0\n100 2035k  100 2035k    0     0   390k      0  0:00:05  0:00:05 --:--:-- 1541k\n$ file foo.deb \nfoo.deb: gzip compressed data, was \"sofastats-1.3.1.tar\", last modified: Thu Jan 10 00:30:44 2013, max compression\n\nThere should be similar option for wget to tolerate HTTP redirects.",
    "tag": "wget"
  },
  {
    "question": "Can I use wget to check , but not download",
    "answer": "There is the command line parameter --spider exactly for this. In this mode, wget does not download the files and its return value is zero if the resource was found and non-zero if it was not found. Try this (in your favorite shell):\nwget -q --spider address\necho $?\n\nOr if you want full output, leave the -q off, so just wget --spider address. -nv shows some output, but not as much as the default.",
    "tag": "wget"
  },
  {
    "question": "How to force wget to overwrite an existing file ignoring timestamp?",
    "answer": "If you specify the output file using the -O option it will overwrite any existing file.\nFor example:\nwget -O index.html bbc.co.uk\n\nRun multiple times. It will keep over-writting on index.html.",
    "tag": "wget"
  },
  {
    "question": "POST request with Wget?",
    "answer": "Wget does not support sending \"multipart/form-data\" data. --post-file is not for transmitting files as form attachments, it expects data in this form: key=value&otherkey=example. It is actually possible to POST other formats (like JSON) if you send the corresponding header.\n--post-data and --post-file work the same way: the only difference is that --post-data allows you to specify the data in the command line, while --post-file allows you to specify the path of the file that contains the data to send.\nHere's the documentation:\n\n--post-data=string\n--post-file=file\n\nUse POST as the method for all HTTP requests and send the specified data in the request body. --post-data sends string as data, whereas --post-file sends the contents of file. Other than that, they work in exactly the same way. In particular, they both expect content of the form key1=value1&key2=value2, with percent-encoding for special characters; the only difference is that one expects its content as a command-line parameter and the other accepts its content from a file. In particular, --post-file is not for transmitting files as form attachments: those must appear as key=value data (with appropriate percent-coding) just like everything else. Wget does not currently support multipart/form-data for transmitting POST data; only application/x-www-form-urlencoded. Only one of --post-data and --post-file should be specified.\n\nRegarding your authentication token, it should either be provided in the header, in the path of the URL, or in the data itself. This must be indicated somewhere in the documentation of the service you use. In a POST request, as in a GET request, you must specify the data using keys and values. This way the server will be able to receive multiple pieces of information with specific names. It's similar with variables.\nHence, you can't just send a magic token to the server, you also need to specify the name of the key. If the key is \"token\", then it should be token=YOUR_TOKEN.\nwget --post-data 'user=foo&password=bar' http://example.com/auth.php\n\nAlso, you should consider using curl if you can because it is easier to send files using it.",
    "tag": "wget"
  },
  {
    "question": "Download all files in a path on Jupyter notebook server",
    "answer": "Try running this as separate cell in one of your notebooks:\n!tar chvfz notebook.tar.gz *\n\nIf you want to cover more folders up the tree, write ../ before the * for every step up the directory. The file notebook.tar.gz will be saved in the same folder as your notebook.",
    "tag": "wget"
  },
  {
    "question": "How to download all links to .zip files on a given web page using wget/curl?",
    "answer": "The command is:\nwget -r -np -l 1 -A zip http://example.com/download/\n\nOptions meaning:\n-r,  --recursive          specify recursive download.\n-np, --no-parent          don't ascend to the parent directory.\n-l,  --level=NUMBER       maximum recursion depth (inf or 0 for infinite).\n-A,  --accept=LIST        comma-separated list of accepted extensions.",
    "tag": "wget"
  },
  {
    "question": "Parallel wget in Bash",
    "answer": "Much preferrable to pushing wget into the background using & or -b, you can use xargs to the same effect, and better.\nThe advantage is that xargs will synchronize properly with no extra work. Which means that you are safe to access the downloaded files (assuming no error occurs). All downloads will have completed (or failed) once xargs exits, and you know by the exit code whether all went well. This is much preferrable to busy waiting with sleep and testing for completion manually.\nAssuming that URL_LIST is a variable containing all the URLs (can be constructed with a loop in the OP's example, but could also be a manually generated list), running this:\necho $URL_LIST | xargs -n 1 -P 8 wget -q\n\nwill pass one argument at a time (-n 1) to wget, and execute at most 8 parallel wget processes at a time (-P 8). xarg returns after the last spawned process has finished, which is just what we wanted to know. No extra trickery needed.\nThe \"magic number\" of 8 parallel downloads that I've chosen is not set in stone, but it is probably a good compromise. There are two factors in \"maximising\" a series of downloads:\nOne is filling \"the cable\", i.e. utilizing the available bandwidth. Assuming \"normal\" conditions (server has more bandwidth than client), this is already the case with one or at most two downloads. Throwing more connections at the problem will only result in packets being dropped and TCP congestion control kicking in, and N downloads with asymptotically 1/N bandwidth each, to the same net effect (minus the dropped packets, minus window size recovery). Packets being dropped is a normal thing to happen in an IP network, this is how congestion control is supposed to work (even with a single connection), and normally the impact is practically zero. However, having an unreasonably large number of connections amplifies this effect, so it can be come noticeable. In any case, it doesn't make anything faster.  \nThe second factor is connection establishment and request processing. Here, having a few extra connections in flight really helps. The problem one faces is the latency of two round-trips (typically 20-40ms within the same geographic area, 200-300ms inter-continental) plus the odd 1-2 milliseconds that the server actually needs to process the request and push a reply to the socket. This is not a lot of time per se, but multiplied by a few hundred/thousand requests, it quickly adds up.\nHaving anything from half a dozen to a dozen requests in-flight hides most or all of this latency (it is still there, but since it overlaps, it does not sum up!). At the same time, having only a few concurrent connections does not have adverse effects, such as causing excessive congestion, or forcing a server into forking new processes.",
    "tag": "wget"
  },
  {
    "question": "Get page output with curl --fail",
    "answer": "This is now possible with curl. Since version 7.76.0 you can do\ncurl --fail-with-body ...\n\nWhich does exactly what OP asked: shows the document body and exits with code 22.\nSee https://curl.se/docs/manpage.html#--fail-with-body",
    "tag": "wget"
  },
  {
    "question": "Unable to establish SSL connection, how do I fix my SSL cert?",
    "answer": "SSL23_GET_SERVER_HELLO:unknown protocol\n\nThis error happens when OpenSSL receives something other than a ServerHello in a protocol version it understands from the server.  It can happen if the server answers with a plain (unencrypted) HTTP.  It can also happen if the server only supports e.g. TLS 1.2 and the client does not understand that protocol version.  Normally, servers are backwards compatible to at least SSL 3.0 / TLS 1.0, but maybe this specific server isn't (by implementation or configuration).\nIt is unclear whether you attempted to pass --no-check-certificate or not.  I would be rather surprised if that would work.\nA simple test is to use wget (or a browser) to request http://example.com:443 (note the http://, not https://); if it works, SSL is not enabled on port 443.  To further debug this, use openssl s_client with the -debug option, which right before the error message dumps the first few bytes of the server response which OpenSSL was unable to parse.  This may help to identify the problem, especially if the server does not answer with a ServerHello message.  To see what exactly OpenSSL is expecting, check the source: look for SSL_R_UNKNOWN_PROTOCOL in ssl/s23_clnt.c.\nIn any case, looking at the apache error log may provide some insight too.",
    "tag": "wget"
  },
  {
    "question": "Download file with url redirection",
    "answer": "Use -L, --location to follow redirects:\n$ curl -L http://httpbin.org/redirect/1",
    "tag": "wget"
  },
  {
    "question": "Spider a Website and Return URLs Only",
    "answer": "The absolute last thing I want to do is download and parse all of the content myself (i.e. create my own spider). Once I learned that Wget writes to stderr by default, I was able to redirect it to stdout and filter the output appropriately.\nwget --spider --force-html -r -l2 $url 2>&1 \\\n  | grep '^--' | awk '{ print $3 }' \\\n  | grep -v '\\.\\(css\\|js\\|png\\|gif\\|jpg\\)$' \\\n  > urls.m3u\n\nThis gives me a list of the content resource (resources that aren't images, CSS or JS source files) URIs that are spidered. From there, I can send the URIs off to a third party tool for processing to meet my needs.\nThe output still needs to be streamlined slightly (it produces duplicates as it's shown above), but it's almost there and I haven't had to do any parsing myself.",
    "tag": "wget"
  },
  {
    "question": "download wetransfer files via terminal",
    "answer": "Obtain the real download link by clicking \"Download Link\" on WeTransfer's download page.\nAfter the download begins, right-click on the file being downloaded and select \"Copy Download Link\".\nFind out your browser's User Agent. You can use whatsmyuseragent to grab it.\nPrepare your wget command and download the file.\n\nExample:\nwget --user-agent Mozilla/4.0 '[your big address here]' -O dest_file_name\n\nDon't forget the quotes.\n[your big address here] must be the direct link to the file, not the forwarded html page. You can get the big address by starting the download on any machine, then copy the link from your download-manager (eg. firefox, chrome)",
    "tag": "wget"
  },
  {
    "question": "get file size of a file to wget before wget-ing it?",
    "answer": "Hmm.. for me --spider does display the size:\n$ wget --spider http://henning.makholm.net/\nSpider mode enabled. Check if remote file exists.\n--2011-08-08 19:39:48--  http://henning.makholm.net/\nResolving henning.makholm.net (henning.makholm.net)... 85.81.19.235\nConnecting to henning.makholm.net (henning.makholm.net)|85.81.19.235|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 9535 (9.3K) [text/html]     <-------------------------\nRemote file exists and could contain further links,\nbut recursion is disabled -- not retrieving.\n\n$ \n\n(But beware that not all web servers will inform clients of the length of the data except by closing the connection when it's all been sent.)\nIf you're concerned about wget changing the format it reports the length in, you might use wget --spider --server-response and look for a Content-Length header in the output.",
    "tag": "wget"
  },
  {
    "question": "What does \"wget -O\" mean?",
    "answer": "Here's the man page of wget -O:\nhttp://www.gnu.org/software/wget/manual/html_node/Download-Options.html#Download-Options\nHere's a few examples:\n\nwget with no flag\nwget www.stackoverflow.com\n\nOutput:\nA file named as index.html\nwget with -O flag\nwget -O filename.html www.stackoverflow.com\n\nOutput:\nA file named as filename.html\nwget with -O- flag\nwget -O- www.stackoverflow.com\n\nOutput:\nOutput to stdout",
    "tag": "wget"
  },
  {
    "question": "wget: don't follow redirects",
    "answer": "--max-redirect 0\nI haven't tried this, it will either allow none or allow infinite..",
    "tag": "wget"
  },
  {
    "question": "How do I mirror a directory with wget without creating parent directories?",
    "answer": "For a path like: ftp.site.com/a/b/c/d\n-nH would download all files to the directory a/b/c/d in the current directory, and -nH --cut-dirs=3 would download all files to the directory d in the current directory.",
    "tag": "wget"
  },
  {
    "question": "Why does wget only download the index.html for some websites?",
    "answer": "Wget is also able to download an entire website. But because this can put a heavy load upon the server, wget will obey the robots.txt file.\nwget -r -p http://www.example.com\n\nThe -p parameter tells wget to include all files, including images. This will mean that all of the HTML files will look how they should do.\nSo what if you don't want wget to obey by the robots.txt file? You can simply add -e robots=off to the command like this:\nwget -r -p -e robots=off http://www.example.com\n\nAs many sites will not let you download the entire site, they will check your browsers identity. To get around this, use -U mozilla as I explained above.\nwget -r -p -e robots=off -U mozilla http://www.example.com\n\nA lot of the website owners will not like the fact that you are downloading their entire site. If the server sees that you are downloading a large amount of files, it may automatically add you to it's black list. The way around this is to wait a few seconds after every download. The way to do this using wget is by including --wait=X (where X is the amount of seconds.)\nyou can also use the parameter: --random-wait to let wget chose a random number of seconds to wait. To include this into the command:\nwget --random-wait -r -p -e robots=off -U mozilla http://www.example.com",
    "tag": "wget"
  },
  {
    "question": "Download a file from google drive using wget",
    "answer": "Insert your file ID into this URL (https://drive.google.com/uc?export=download&id=), then surround the URL with quotes so that Bash doesn't misinterpret the &, like so:\nwget \"https://drive.google.com/uc?export=download&id=0Bz7KyqmuGsilT0J5dmRCM0ROVHc\"\nReference here.\n\nWhen downloading big files, Google Drive adds a security warning that breaks the script above. In that case, you can download the file using:\nwget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=FILEID' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=FILEID\" -O FILENAME && rm -rf /tmp/cookies.txt\n\n(Script taken from here)",
    "tag": "wget"
  },
  {
    "question": "How to download a file into a directory using curl or wget?",
    "answer": "The following line will download all the files to a directory mentioned by you.\nwget -P /home/test www.xyz.com\n\nHere the files will be downloaded to /home/test directory",
    "tag": "wget"
  },
  {
    "question": "Sites not accepting wget user agent header",
    "answer": "It seems Yahoo server does some heuristic based on User-Agent in a case Accept header is set to */*. \n\nAccept: text/html\n\ndid the trick for me. \ne.g.\nwget  --header=\"Accept: text/html\" --user-agent=\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.8; rv:21.0) Gecko/20100101 Firefox/21.0\"  http://yahoo.com\n\nNote: if you don't declare Accept header then wget automatically adds Accept:*/* which means give me anything you have.",
    "tag": "wget"
  },
  {
    "question": "Ignore SSL Certificate Error with Wget",
    "answer": "Please use the wget with --no-check-certificate\ne.g.\nwget --no-check-certificate \"https://myPath/myFile.xlsx\"\n\nshould work.",
    "tag": "wget"
  },
  {
    "question": "wget: unable to resolve host address `http'",
    "answer": "The DNS server seems out of order. You can use another DNS server such as 8.8.8.8. Put nameserver 8.8.8.8 to the first line of /etc/resolv.conf.",
    "tag": "wget"
  },
  {
    "question": "wget ssl alert handshake failure",
    "answer": "It works from here with same OpenSSL version, but a newer version of wget (1.15). Looking at the Changelog there is the following significant change regarding your problem:\n\n1.14: Add support for TLS Server Name Indication.\n\nNote that this site does not require SNI. But www.coursera.org requires it.\nAnd if you would call wget with -v --debug (as I've explicitly recommended in my comment!) you will see:\n$ wget https://class.coursera.org\n...\nHTTP request sent, awaiting response...\n  HTTP/1.1 302 Found\n...\nLocation: https://www.coursera.org/ [following]\n...\nConnecting to www.coursera.org (www.coursera.org)|54.230.46.78|:443... connected.\nOpenSSL: error:14077410:SSL routines:SSL23_GET_SERVER_HELLO:sslv3 alert handshake failure\nUnable to establish SSL connection.\n\nSo the error actually happens with www.coursera.org and the reason is missing support for SNI. You need to upgrade your version of wget.",
    "tag": "wget"
  },
  {
    "question": "How to download a Google Drive url via curl or wget",
    "answer": "How about this method? When the file is such large size, Google returns a code for downloading the file. You can download the file using the code. When such large file is downloaded using curl, you can see the code as follows.\n<a id=\"uc-download-link\" class=\"goog-inline-block jfk-button jfk-button-action\" href=\"/uc?export=download&amp;confirm=ABCD&amp;id=### file ID ###\">download</a>\n\nThe query with confirm=ABCD is important for downloading the file. This code is also included in the cookie. At the cookie, you can see it as follows.\n#HttpOnly_.drive.google.com TRUE    /uc TRUE    #####   download_warning_#####  ABCD\n\nIn this case, \"ABCD\" is the code. In order to retrieve the code from the cookie and download the file, you can use the following script.\nSample script :\n#!/bin/bash\nfileid=\"### file id ###\"\nfilename=\"MyFile.csv\"\ncurl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id=${fileid}\" > /dev/null\ncurl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm=`awk '/download/ {print $NF}' ./cookie`&id=${fileid}\" -o ${filename}\n\nIf this was not useful for you, I'm sorry.\n\nUpdated at February 17, 2022\nRecently, it seems that the specification of this flow has been changed. So I updated this answer. In order to download a publicly shared file of large size from Google Drive, you can use the following script.\n#!/bin/bash\nfileid=\"### file id ###\"\nfilename=\"MyFile.csv\"\nhtml=`curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id=${fileid}\"`\ncurl -Lb ./cookie \"https://drive.google.com/uc?export=download&`echo ${html}|grep -Po '(confirm=[a-zA-Z0-9\\-_]+)'`&id=${fileid}\" -o ${filename}\n\n\nIn this case, the ID for downloading is retrieved from the HTML data as follows.\n<form id=\"downloadForm\" action=\"https://drive.google.com/uc?export=download&amp;id={fileId}&amp;confirm={value for downloading}\" method=\"post\">\n\n\nWhen you want to download a publicly shared file of small size from Google Drive, you can use the following command.\ncurl -L \"https://drive.google.com/uc?export=download&id=### fileId ###\" -o sampleoutput.csv\n\n\n\nUpdated at January 21, 2024\nFrom the following @excellproj 's comment,\n\nJanuary 2024\ncurl -L \"https://drive.usercontent.google.com/download?id=${fileId}&export=download&confirm=t\" -o \"file.zip\"\nworking great for me. Small and large files\n\nI checked this endpoint. By this, the following result is obtained. In the current stage, the following endpoint can be used.\nhttps://drive.usercontent.google.com/download?id={fileId}&confirm=xxx\n\nIt seems that in the current stage, various values can be used to xxx of confirm=xxx. So, even confirm=xxx and confirm=yy and confirm=z can be used.\nThe sample curl command is as follows.\ncurl \"https://drive.usercontent.google.com/download?id={fileId}&confirm=xxx\" -o filename\n\nAnd/or,\ncurl -L \"https://drive.usercontent.google.com/download?id={fileId}&confirm=xxx\" -o filename",
    "tag": "wget"
  },
  {
    "question": "How to unzip a piped zip file (from \"wget -qO-\")?",
    "answer": "The ZIP file format includes a directory (index) at the end of the archive. This directory says where, within the archive each file is located and thus allows for quick, random access, without reading the entire archive.\nThis would appear to pose a problem when attempting to read a ZIP archive through a pipe, in that the index is not accessed until the very end and so individual members cannot be correctly extracted until after the file has been entirely read and is no longer available. As such it appears unsurprising that most ZIP decompressors simply fail when the archive is supplied through a pipe.\nThe directory at the end of the archive is not the only location where file meta information is stored in the archive. In addition, individual entries also include this information in a local file header, for redundancy purposes. \nAlthough not every ZIP decompressor will use local file headers when the index is unavailable, the tar and cpio front ends to libarchive (a.k.a. bsdtar and bsdcpio) can and will do so when reading through a pipe, meaning that the following is possible:\nwget -qO- http://downloads.wordpress.org/plugin/akismet.2.5.3.zip | bsdtar -xvf- -C ~/Desktop",
    "tag": "wget"
  },
  {
    "question": "What headers are automatically sent by wget?",
    "answer": "Using the -d (--debug) option I see it set:\n---request begin---\nGET / HTTP/1.0\nUser-Agent: Wget/1.12 (cygwin)\nAccept: */*\nHost: www.uml.edu\nConnection: Keep-Alive\n\n---request end---",
    "tag": "wget"
  },
  {
    "question": "What is the correct wget command syntax for HTTPS with username and password?",
    "answer": "By specifying the option --user and --ask-password wget will ask for the credentials. Below is an example. Change the username and download link to your needs.\nwget --user=username --ask-password https://xyz.com/changelog-6.40.txt",
    "tag": "wget"
  },
  {
    "question": "Using WGET to run a cronjob PHP",
    "answer": "You could tell wget to not download the contents in a couple of different ways:\nwget --spider http://www.example.com/cronit.php\n\nwhich will just perform a HEAD request but probably do what you want\nwget -O /dev/null http://www.example.com/cronit.php\n\nwhich will save the output to /dev/null (a black hole)\nYou might want to look at wget's -q switch too which prevents it from creating output\nI think that the best option would probably be:\nwget -q --spider http://www.example.com/cronit.php\n\nthat's unless you have some special logic checking the HTTP method used to request the page",
    "tag": "wget"
  },
  {
    "question": "wget - Download a sub directory",
    "answer": "You can do:\nwget -r -l1 --no-parent http://www.domain.com/subdirectory/\nwhere:\n-r: recursive retrieving\n-l1: sets the maximum recursion depth to be 1\n--no-parent: does not ascend to the parent; only downloads from the specified subdirectory and downwards hierarchy",
    "tag": "wget"
  },
  {
    "question": "How to mirror only a section of a website?",
    "answer": "Use the --mirror (-m) and --no-parent (-np) options, plus a few of cool ones, like in this example:\nwget --mirror --page-requisites --adjust-extension --no-parent --convert-links\n     --directory-prefix=sousers http://stackoverflow.com/users",
    "tag": "wget"
  },
  {
    "question": "How can I make a Docker healthcheck with wget instead of curl?",
    "answer": "The following seems to be the equivalent:\nHEALTHCHECK  --interval=5m --timeout=3s \\\n  CMD wget --no-verbose --tries=1 --spider http://localhost/ || exit 1\n\nWhere:\n\n--no-verbose - Turn off verbose without being completely quiet (use -q for that), which means that error messages and basic information still get printed.\n--tries=1 - If not set, some wget implementations will retry indefinitely when HTTP 200 response is not returned.\n--spider - Behave as a Web spider, which means that it will not download the pages, just check that they are there.\nexit 1 - Ensures exit code 1 on failure. Heathcheck only expects the following:\n\n0: success - the container is healthy and ready for use\n1: unhealthy - the container is not working correctly\n2: reserved - do not use this exit code\n\n\n\nDocker compose example:\nhealthcheck:\n   test: wget --no-verbose --tries=1 --spider http://localhost || exit 1\n   interval: 5m\n   timeout: 3s\n   retries: 3\n   start_period: 2m\n\nhttps://docs.docker.com/compose/compose-file/compose-file-v3/#healthcheck",
    "tag": "wget"
  },
  {
    "question": "How can I download a file from an S3 bucket with wget?",
    "answer": "You should be able to access it from a url created as follows:\nhttp://{bucket-name}.s3.amazonaws.com/<path-to-file>\nNow, say your s3 file path is:\ns3://test-bucket/test-folder/test-file.txt\nYou should be able to wget this file with following url:\nhttp://test-bucket.s3.amazonaws.com/test-folder/test-file.txt",
    "tag": "wget"
  },
  {
    "question": "Homebrew will not run wget command (Library not loaded)",
    "answer": "brew uninstall wget\nbrew install wget\n\nsolved me both gettext and git clone issues.",
    "tag": "wget"
  },
  {
    "question": "How do I download and save a file locally on iOS using objective C?",
    "answer": "I'm not sure what wget is, but to get a file from the web and store it locally, you can use NSData:\nNSString *stringURL = @\"http://www.somewhere.com/thefile.png\";\nNSURL  *url = [NSURL URLWithString:stringURL];\nNSData *urlData = [NSData dataWithContentsOfURL:url];\nif ( urlData )\n{\n  NSArray       *paths = NSSearchPathForDirectoriesInDomains(NSDocumentDirectory, NSUserDomainMask, YES);\n  NSString  *documentsDirectory = [paths objectAtIndex:0];  \n\n  NSString  *filePath = [NSString stringWithFormat:@\"%@/%@\", documentsDirectory,@\"filename.png\"];\n  [urlData writeToFile:filePath atomically:YES];\n}",
    "tag": "wget"
  },
  {
    "question": "Check wget's return value",
    "answer": "Others have correctly posted that you can use $? to get the most recent exit code:\nwget_output=$(wget -q \"$URL\")\nif [ $? -ne 0 ]; then\n    ...\n\nThis lets you capture both the stdout and the exit code. If you don't actually care what it prints, you can just test it directly:\nif wget -q \"$URL\"; then\n    ...\n\nAnd if you want to suppress the output:\nif wget -q \"$URL\" > /dev/null; then\n    ...",
    "tag": "wget"
  },
  {
    "question": "BASH script: Downloading consecutive numbered files with wget",
    "answer": "#!/bin/sh\n\nif [ $# -lt 3 ]; then\n        echo \"Usage: $0 url_format seq_start seq_end [wget_args]\"\n        exit\nfi\n\nurl_format=$1\nseq_start=$2\nseq_end=$3\nshift 3\n\nprintf \"$url_format\\\\n\" `seq $seq_start $seq_end` | wget -i- \"$@\"\n\nSave the above as seq_wget, give it execution permission (chmod +x seq_wget), and then run, for example:\n\n$ ./seq_wget http://someaddress.com/logs/dbsclog01s%03d.log 1 50\n\nOr, if you have Bash 4.0, you could just type\n\n$ wget http://someaddress.com/logs/dbsclog01s{001..050}.log\n\nOr, if you have curl instead of wget, you could follow Dennis Williamson's answer.",
    "tag": "wget"
  },
  {
    "question": "Unable to establish SSL connection upon wget on Ubuntu 14.04 LTS",
    "answer": "you must be using old version of wget i had same issue. i was using wget 1.12.so to solve this issue there are 2 way:\nUpdate wget or use curl\ncurl -LO 'https://example.com/filename.tar.gz'",
    "tag": "wget"
  },
  {
    "question": "Why does wget ignore the query string in the URL?",
    "answer": "& is a special character in most shell environments. You can use double quotes to quote the URL to pass the whole thing in as the parameter to wget:\nwget \"http://www.ted.com/talks/quick-list?sort=date&order=desc&page=18\"",
    "tag": "wget"
  },
  {
    "question": "Downloading a file with wget using multiple connections",
    "answer": "use aria2\n aria2c -x 16 [url] #where 16 is the number of connections\n\nOR\nJust repeat the wget -r -np -N [url] for as many threads as you need. This isn’t pretty and there are surely better ways to do this, but if you want something quick and dirty it should do the trick.",
    "tag": "wget"
  },
  {
    "question": "wget -O for non-existing save path?",
    "answer": "Try curl\ncurl http://www.site.org/image.jpg --create-dirs -o /path/to/save/images.jpg",
    "tag": "wget"
  },
  {
    "question": "How to ignore specific type of files to download in wget?",
    "answer": "Use the\n --reject jpg,png  --accept html\n\noptions to exclude/include files with certain extensions, see http://www.gnu.org/software/wget/manual/wget.html#Recursive-Accept_002fReject-Options. \nPut patterns with wildcard characters in quotes, otherwise your shell will expand them, see http://www.gnu.org/software/wget/manual/wget.html#Types-of-Files",
    "tag": "wget"
  },
  {
    "question": "Download source code from Apple's website",
    "answer": "I finally figured it out. Sources can be conveniently downloaded from the tarballs directory.",
    "tag": "wget"
  },
  {
    "question": "Downloading Xcode with wget or curl",
    "answer": "For Chrome, \n\nInstall cookies.txt Chrome extension\nLogin to Apple Developer site and get the url for downloading\nRun cookies.txt extension and download cookies.txt file\nFrom the cookies.txt download directory, load cookies into wget and start resumable download. For example, to download Xcode_7.dmg, you would run:\nwget --load-cookies=cookies.txt -c http://adcdownload.apple.com/Developer_Tools/Xcode_7/Xcode_7.dmg",
    "tag": "wget"
  },
  {
    "question": "How to use wget in php?",
    "answer": "wget\nwget is a linux command, not a PHP command, so to run this you woud need to use exec, which is a PHP command for executing shell commands.\nexec(\"wget --http-user=[user] --http-password=[pass] http://www.example.com/file.xml\");\n\nThis can be useful if you are downloading a large file - and would like to monitor the progress, however when working with pages in which you are just interested in the content, there are simple functions for doing just that.\nThe exec function is enabled by default, but may be disabled in some situations. The configuration options for this reside in your php.ini, to enable, remove exec from the disabled_functions config string.\nalternative\nUsing file_get_contents we can retrieve the contents of the specified URL/URI. When you just need to read the file into a variable, this would be the perfect function to use as a replacement for curl - follow the URI syntax when building your URL.\n// standard url\n$content = file_get_contents(\"http://www.example.com/file.xml\");\n\n// or with basic auth\n$content = file_get_contents(\"http://user:pass@www.example.com/file.xml\");\n\nAs noted by Sean the Bean - you may also need to change allow_url_fopen to true in your php.ini to allow the use of a URL in this method, however, this should be true by default.\nIf you want to then store that file locally, there is a function file_put_contents to write that into a file, combined with the previous, this could emulate a file download:\nfile_put_contents(\"local_file.xml\", $content);",
    "tag": "wget"
  },
  {
    "question": "How to download multiple URLs using wget using a single command?",
    "answer": "From man wget:\n\n2 Invoking\nBy default, Wget is very simple to invoke. The basic syntax is:\nwget [option]... [URL]...\n\nSo, just use multiple URLs:\nwget URL1 URL2\n\nOr using the links from comments:\n$ cat list.txt\nhttp://www.vodafone.de/privat/tarife/red-smartphone-tarife.html\nhttp://www.verizonwireless.com/smartphones-2.shtml\nhttp://www.att.com/shop/wireless/devices/smartphones.html\n\nand your command line:\nwget -E -H -k -K -p -e robots=off -P /Downloads/ -i ./list.txt\n\nworks as expected.",
    "tag": "wget"
  },
  {
    "question": "wget can't download - 404 error",
    "answer": "You need to add the referer field in the headers of the HTTP request. With wget, you just need the --header arg :\nwget http://www.icerts.com/images/logo.jpg --header \"Referer: www.icerts.com\"\n\nAnd the result : \n--2011-10-02 02:00:18--  http://www.icerts.com/images/logo.jpg\nRésolution de www.icerts.com (www.icerts.com)... 97.74.86.3\nConnexion vers www.icerts.com (www.icerts.com)|97.74.86.3|:80...connecté.\nrequête HTTP transmise, en attente de la réponse...200 OK\nLongueur: 6102 (6,0K) [image/jpeg]\nSauvegarde en : «logo.jpg»",
    "tag": "wget"
  },
  {
    "question": "wget + JavaScript?",
    "answer": "You could probably make that happen with something like PhantomJS\nYou can write a phantomjs script that will load the page like a browser would, and then either take screenshots or use JS to inspect the page and pull out data.",
    "tag": "wget"
  },
  {
    "question": "ansible wget then exec scripts => get_url equivalent",
    "answer": "This worked for me:\n- name: Download zsh installer\n  get_url: \n    url: https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh dest=/tmp/zsh-installer.sh\n    \n- name: Execute the zsh-installer.sh\n  shell: /tmp/zsh-installer.sh\n\n- name: Remove the zsh-installer.sh\n  file: \n    path: /tmp/zsh-installer.sh \n    state: absent",
    "tag": "wget"
  },
  {
    "question": "wget not recognized as internal or external command",
    "answer": "wget is a third-party program that doesn't come bundled with Windows, so you need to explicitly install it in order to use it.\nYou can find (one of) the Windows versions here: http://gnuwin32.sourceforge.net/packages/wget.htm\nYou will need to add the path of the wget.exe file to your PATH environment variable in order to call the executable as in the batch file above without explicitly specifying the path.\nFor Windows 10: A good link is available here: https://builtvisible.com/download-your-website-with-wget/",
    "tag": "wget"
  },
  {
    "question": "Delays between requests in wget",
    "answer": "You can add the below code into your command line which adds a ten second wait in between server requests.\n\n-w 10\n\nAnd you can also include \n\n--random-wait \n\nInto  your command line with -w option which will vary the wait by 0.5 and 1.5 times the value you provide here.",
    "tag": "wget"
  },
  {
    "question": "Shell command to tar directory excluding certain files/folders",
    "answer": "You can have multiple exclude options for tar so\n$ tar --exclude='./folder' --exclude='./upload/folder2' -zcvf /backup/filename.tgz .\n\netc will work. Make sure to put --exclude before the source and destination items.",
    "tag": "tar"
  },
  {
    "question": "How do I tar a directory of files and folders without including the directory itself?",
    "answer": "Use the -C switch of tar:\ntar -czvf my_directory.tar.gz -C my_directory .\n\nThe -C my_directory tells tar to change the current directory to my_directory, and then . means \"add the entire current directory\" (including hidden files and sub-directories).\nMake sure you do -C my_directory before you do . or else you'll get the files in the current directory.\nWarning: you'll get entries as ./file-name.ext instead of file-name.ext!\nIf you need entries in the form of file-name.ext, read other answers.",
    "tag": "tar"
  },
  {
    "question": "Tar a directory, but don't store full absolute paths in the archive",
    "answer": "tar -cjf site1.tar.bz2 -C /var/www/site1 .\n\nIn the above example, tar will change to directory /var/www/site1 before doing its thing because the option -C /var/www/site1 was given.\nFrom man tar:\nOTHER OPTIONS\n\n  -C, --directory DIR\n       change to directory DIR",
    "tag": "tar"
  },
  {
    "question": "Utilizing multi core for tar+gzip/bzip compression/decompression",
    "answer": "You can also use the tar flag --use-compress-program= to tell tar what compression program to use.\nFor example use:\ntar -c --use-compress-program=pigz -f tar.file dir_to_zip",
    "tag": "tar"
  },
  {
    "question": "How to uncompress a tar.gz in another directory",
    "answer": "You can use the option -C (or --directory if you prefer long options) to give the target directory of your choice in case you are using the Gnu version of tar.  The directory should exist:\nmkdir foo\ntar -xzf bar.tar.gz -C foo\n\nIf you are not using a tar capable of extracting to a specific directory, you can simply cd into your target directory prior to calling tar; then you will have to give a complete path to your archive, of course.  You can do this in a scoping subshell to avoid influencing the surrounding script:\nmkdir foo\n(cd foo; tar -xzf ../bar.tar.gz)  # instead of ../ you can use an absolute path as well\n\nOr, if neither an absolute path nor a relative path to the archive file is suitable, you also can use this to name the archive outside of the scoping subshell:\nTARGET_PATH=a/very/complex/path/which/might/even/be/absolute\nmkdir -p \"$TARGET_PATH\"\n(cd \"$TARGET_PATH\"; tar -xzf -) < bar.tar.gz",
    "tag": "tar"
  },
  {
    "question": "What is the difference between tar and zip?",
    "answer": "tar in itself just bundles files together (the result is called a tarball), while zip applies compression as well.\nUsually you use gzip along with tar to compress the resulting tarball, thus achieving similar results as with zip.\nFor reasonably large archives there are important differences though.  A zip archive is a collection of compressed files.  A gzipped tar is a compressed collection (of uncompressed files).  Thus a zip archive is a randomly accessible list of concatenated compressed items, and a .tar.gz is an archive that must be fully expanded before the catalog is accessible.\n\nThe caveat of a zip is that you don't get compression across files (because each file is compressed independent of the others in the archive, the compression cannot take advantage of similarities among the contents of different files); the advantage is that you can access any of the files contained within by looking at only a specific (target file dependent) section of the archive (as the \"catalog\" of the collection is separate from the collection itself).\nThe caveat of a .tar.gz is that you must decompress the whole archive to access files contained therein (as the files are within the tarball); the advantage is that the compression can take advantage of similarities among the files (as it compresses the whole tarball).",
    "tag": "tar"
  },
  {
    "question": "Tar archiving that takes input from a list of files",
    "answer": "Yes:\ntar -cvf allfiles.tar -T mylist.txt",
    "tag": "tar"
  },
  {
    "question": "Create a tar.xz in one command",
    "answer": "Use the -J compression option for xz. And remember to man tar :)\ntar cfJ <archive.tar.xz> <files>\n\nEdit 2015-08-10:\nIf you're passing the arguments to tar with dashes (ex: tar -cf as opposed to tar cf), then the -f option must come last, since it specifies the filename (thanks to @A-B-B for pointing that out!). In that case, the command looks like:\ntar -cJf <archive.tar.xz> <files>",
    "tag": "tar"
  },
  {
    "question": "How do I turn off the output from tar commands on Unix?",
    "answer": "Just drop the option v.\n-v is for verbose. If you don't use it then it won't display:\ntar -zxf tmp.tar.gz -C ~/tmp1",
    "tag": "tar"
  },
  {
    "question": "Are tar.gz and tgz the same thing?",
    "answer": "I think in the old package repo days, .tgz was used because files on DOS floppies could only have three letter extensions. When this limitation was removed, .tar.gz was used to be more verbose by showing both the archive type (tar) and zipper (gzip).\nThey are identical.",
    "tag": "tar"
  },
  {
    "question": "Excluding directory when creating a .tar.gz file",
    "answer": "Try removing the last / at the end of the directory path to exclude\ntar -pczf MyBackup.tar.gz /home/user/public_html/ --exclude \"/home/user/public_html/tmp\"",
    "tag": "tar"
  },
  {
    "question": "How to check if a Unix .tar.gz file is a valid file without uncompressing?",
    "answer": "What about just getting a listing of the tarball and throw away the output, rather than decompressing the file?\ntar -tzf my_tar.tar.gz >/dev/null\n\nEdit as per comment. Thanks Frozen Flame! This test in no way implies integrity of the data. Because it was designed as a tape archival utility most implementations of tar will allow multiple copies of the same file!",
    "tag": "tar"
  },
  {
    "question": "tar: add all files and directories in current directory INCLUDING .svn and so on",
    "answer": "Don't create the tar file in the directory you are packing up:\ntar -czf /tmp/workspace.tar.gz .\n\ndoes the trick, except it will extract the files all over the current directory when you unpack.  Better to do:\ncd ..\ntar -czf workspace.tar.gz workspace\n\nor, if you don't know the name of the directory you were in:\nbase=$(basename $PWD)\ncd ..\ntar -czf $base.tar.gz $base\n\n(This assumes that you didn't follow symlinks to get to where you are and that the shell doesn't try to second guess you by jumping backwards through a symlink - bash is not trustworthy in this respect.  If you have to worry about that, use cd -P .. to do a physical change directory.  Stupid that it is not the default behaviour in my view - confusing, at least, for those for whom cd .. never had any alternative meaning.)\n\nOne comment in the discussion says:\n\nI [...] need to exclude the top directory and I [...] need to place the tar in the base directory.\n\nThe first part of the comment does not make much sense - if the tar file contains the current directory, it won't be created when you extract file from that archive because, by definition, the current directory already exists (except in very weird circumstances).\nThe second part of the comment can be dealt with in one of two ways:\n\nEither: create the file somewhere else - /tmp is one possible location - and then move it back to the original location after it is complete.\nOr: if you are using GNU Tar, use the --exclude=workspace.tar.gz option.  The string after the = is a pattern - the example is the simplest pattern - an exact match.  You might need to specify --exclude=./workspace.tar.gz if you are working in the current directory contrary to recommendations; you might need to specify --exclude=workspace/workspace.tar.gz if you are working up one level as suggested.  If you have multiple tar files to exclude, use '*', as in --exclude=./*.gz.",
    "tag": "tar"
  },
  {
    "question": "How to extract filename.tar.gz file",
    "answer": "If file filename.tar.gz gives this message: POSIX tar archive, \nthe archive is a tar, not a GZip archive.\nUnpack a tar without the z, it is for gzipped (compressed), only:\nmv filename.tar.gz filename.tar # optional\ntar xvf filename.tar\n\nOr try a generic Unpacker like unp (https://packages.qa.debian.org/u/unp.html), a script for unpacking a wide variety of archive formats.\ndetermine the file type: \n$ file ~/Downloads/filename.tbz2\n/User/Name/Downloads/filename.tbz2: bzip2 compressed data, block size = 400k",
    "tag": "tar"
  },
  {
    "question": "How do I tar a directory without retaining the directory structure?",
    "answer": "Use the --directory option:\n tar czf ~/backup.tgz --directory=/home/username/drupal/sites/default files",
    "tag": "tar"
  },
  {
    "question": "tar: file changed as we read it",
    "answer": "I also encounter the tar messages \"changed as we read it\". For me these message occurred when I was making tar file of Linux file system in bitbake build environment. This error was sporadic.\nFor me this was not due to creating tar file from the same directory. I am assuming there is actually some file overwritten or changed during tar file creation.\nThe message is a warning and it still creates the tar file. We can still suppress these warning message by setting option \n--warning=no-file-changed\n(http://www.gnu.org/software/tar/manual/html_section/warnings.html\n) \nStill the exit code return by the tar is \"1\" in warning message case:\nhttp://www.gnu.org/software/tar/manual/html_section/Synopsis.html\nSo if we are calling the tar file from some function in scripts, we can handle the exit code something like this:\nset +e \ntar -czf sample.tar.gz dir1 dir2\nexitcode=$?\n\nif [ \"$exitcode\" != \"1\" ] && [ \"$exitcode\" != \"0\" ]; then\n    exit $exitcode\nfi\nset -e",
    "tag": "tar"
  },
  {
    "question": "Find files and tar them (with spaces)",
    "answer": "Use this:\nfind . -type f -print0 | tar -czvf backup.tar.gz --null -T -\n\nIt will:\n\ndeal with files with spaces, newlines, leading dashes, and other funniness\nhandle an unlimited number of files\nwon't repeatedly overwrite your backup.tar.gz like using tar -c with xargs will do when you have a large number of files\n\nAlso see:\n\nGNU tar manual\nHow can I build a tar from stdin?, search for null",
    "tag": "tar"
  },
  {
    "question": "How to tar certain file types in all subdirectories?",
    "answer": "find ./someDir -name \"*.php\" -o -name \"*.html\" | tar -cf my_archive -T -",
    "tag": "tar"
  },
  {
    "question": "reading tar file contents without untarring it, in python script",
    "answer": "you can use getmembers()\n>>> import  tarfile\n>>> tar = tarfile.open(\"test.tar\")\n>>> tar.getmembers()\n\nAfter that, you can use extractfile() to extract the members as file object. Just an example\nimport tarfile,os\nimport sys\nos.chdir(\"/tmp/foo\")\ntar = tarfile.open(\"test.tar\")\nfor member in tar.getmembers():\n    f=tar.extractfile(member)\n    content=f.read()\n    print \"%s has %d newlines\" %(member, content.count(\"\\n\"))\n    print \"%s has %d spaces\" % (member,content.count(\" \"))\n    print \"%s has %d characters\" % (member, len(content))\n    sys.exit()\ntar.close()\n\nWith the file object f in the above example, you can use read(), readlines() etc.",
    "tag": "tar"
  },
  {
    "question": "How do I extract files without folder structure using tar",
    "answer": "You can use the --strip-components option of tar.\n\n --strip-components count\n         (x mode only) Remove the specified number of leading path ele-\n         ments.  Pathnames with fewer elements will be silently skipped.\n         Note that the pathname is edited after checking inclusion/exclu-\n         sion patterns but before security checks.\n\n\nI create a tar file with a similar structure to yours:\n$tar -tf tarfolder.tar\ntarfolder/\ntarfolder/file.a\ntarfolder/file.b\n\n$ls -la file.*\nls: file.*: No such file or directory\n\nThen extracted by doing:\n$tar -xf tarfolder.tar --strip-components 1\n$ls -la file.*\n-rw-r--r--  1 ericgorr  wheel  0 Jan 12 12:33 file.a\n-rw-r--r--  1 ericgorr  wheel  0 Jan 12 12:33 file.b",
    "tag": "tar"
  },
  {
    "question": "gzip: stdin: not in gzip format tar: Child returned status 1 tar: Error is not recoverable: exiting now",
    "answer": "First check the type of compression using the file command:\nfile name_name.tgz\n\nOutput: If the output is \" XZ compressed data\", then use tar xf <archive name> to unzip the file, e.g.\n\ntar xf archive.tar.xz\n\ntar xf archive.tar.gz\n\ntar xf archive.tar\n\ntar xf archive.tgz",
    "tag": "tar"
  },
  {
    "question": "Create a .tar.bz2 file Linux",
    "answer": "You are not indicating what to include in the archive.\nGo one level outside your folder and try:\nsudo tar -cvjSf folder.tar.bz2 folder\n\nOr from the same folder try\nsudo tar -cvjSf folder.tar.bz2 *",
    "tag": "tar"
  },
  {
    "question": "I want to create a script for unzip (.tar.gz) file via (Python)",
    "answer": "Why do you want to \"press\" twice to extract a .tar.gz, when you can easily do it once?  Here is a simple code to extract both .tar and .tar.gz in one go:\nimport tarfile\n\nif fname.endswith(\"tar.gz\"):\n    tar = tarfile.open(fname, \"r:gz\")\n    tar.extractall()\n    tar.close()\nelif fname.endswith(\"tar\"):\n    tar = tarfile.open(fname, \"r:\")\n    tar.extractall()\n    tar.close()",
    "tag": "tar"
  },
  {
    "question": "Listing the content of a tar file or a directory only down to some level",
    "answer": "depth=1\n\ntar --exclude=\"*/*\" -tf file.tar\n\n\ndepth=2\n\ntar --exclude=\"*/*/*\" -tf file.tar",
    "tag": "tar"
  },
  {
    "question": "How can you untar more than one file at a time?",
    "answer": "What's going on here?\nOriginally, the tar command was intended for use with magnetic tape devices.  Since it only made sense to execute tar on one device at a time, the syntax was designed to assume one and only one device.  The first file or directory passed was assumed to be the device that held the archive in question and any other files or directories where the contents of the archive to be included in the operation.  So for tar extraction (the x option), the first file passed would be the archive and all other files would be the files to be extracted.  So if there are two *.tar files (say a.tar and b.tar) your command would expand to:\n$ tar xf a.tar b.tar\n\nUnless a.tar contains a file named b.tar, the tar command has nothing to do and exits quietly.  Annoyingly, the Solaris version of tar does not report any problems either in the return code or with the verbose option (v).  Meanwhile, GNU tar returns 2 and spams STDERR even with the verbose option off:\ntar: b.tar: Not found in archive\ntar: Exiting with failure status due to previous errors\n\nHow do I untar a bunch of files at once?\nIt's too late rewrite tar to accept multiple archive files as input, but it's not too hard to work around the limitation.\nFor most people, running tar multiple times for multiple archives is the most expedient option.  Passing just one filename to tar xf will extract all the archived files as one would expect.  One approach is to use a shell for loop:\n$ for f in *.tar; do tar xf \"$f\"; done\n\nAnother method is to use xargs:\n$ ls *.tar | xargs -i tar xf {}\n\nAlternatively, you can use one of a number of alternative tar file readers.  Finally, the truly dedicated programmer could easily write an tar replacement that works exactly as desired.  The format is straightforward and many programming languages have libraries available to read tar files.  If you are a Perl programmer, for instance, take a look at the Archive::Tar module.\nA warning\nBlindly untarring a bunch of files can cause unexpected problems.  The most obvious is that a particular file name may be included in more than one tar file.  Since tar overwrites files by default, the exact version of the file you end up with will depend on the order the archives are processed.  More troubling, you may end up with a corrupted copy of the file if you try this \"clever\" optimization:\nfor f in *.tar; do\n  tar xf \"$f\" &\ndone\nwait\n\nIf both a.tar and b.tar contain the same file and try to extract it at the same time, the results are unpredictable.\nA related issue, especially when taking archives from an untrusted source, is the possibility of a tarbomb.  \nOne partial solution would be to automatically create a new directory to extract into:\nfor f in *.tar; do \n  d=`basename \"$f\" .tar`\n  mkdir \"$d\"\n  (cd \"$d\" && tar xf \"../$f\")\ndone\n\nThis won't help if a file is specified in the archive with an absolute path (which is normally a sign of malicious intent).  Adding that sort of check is left as an exercise for the reader.",
    "tag": "tar"
  },
  {
    "question": "Uncompress tar.gz file",
    "answer": "Use -C option of tar:\ntar zxvf <yourfile>.tar.gz -C /usr/src/\n\nand then, the content of the tar should be in:\n/usr/src/<yourfile>",
    "tag": "tar"
  },
  {
    "question": "Check the total content size of a tar gz file",
    "answer": "This works for any file size:\nzcat archive.tar.gz | wc -c\n\nFor files smaller than 4Gb you could also use the -l option with gzip:\n$ gzip -l compressed.tar.gz\n     compressed        uncompressed  ratio uncompressed_name\n            132               10240  99.1% compressed.tar",
    "tag": "tar"
  },
  {
    "question": "How can files be added to a tarfile with Python, without adding the directory hierarchy?",
    "answer": "Using the arcname argument of TarFile.add() method is an alternate and convenient way to match your destination.\nExample: you want to archive a dir repo/a.git/ to a tar.gz file, but you rather want the tree root in the archive begins by a.git/ but not repo/a.git/, you can do like followings:  \narchive = tarfile.open(\"a.git.tar.gz\", \"w|gz\")\narchive.add(\"repo/a.git\", arcname=\"a.git\")\narchive.close()",
    "tag": "tar"
  },
  {
    "question": "How can I build a tar from stdin?",
    "answer": "Something like:\ntar cfz foo.tgz --files-from=-\n\nBut keep in mind that this won't work for all possible filenames; you should consider the --null option and feed tar from find -print0.  (The xargs example won't quite work for large file lists because it will spawn multiple tar commands.)",
    "tag": "tar"
  },
  {
    "question": "How to extract a single file from tar to a different directory?",
    "answer": "The problem is that your arguments are in incorrect order. The single file argument must be last.\nE.g. \n$ tar xvf test.tar -C anotherDirectory/ testfile1\n\nshould do the trick.\nPS: You should have asked this question on superuser instead of SO",
    "tag": "tar"
  },
  {
    "question": "Deleting files after adding to tar archive",
    "answer": "With GNU tar, use the option --remove-files.",
    "tag": "tar"
  },
  {
    "question": "How to create tar.gz archive file in Windows?",
    "answer": "tar.gz file is just a tar file that's been gzipped. Both tar and gzip are available for windows.\nIf you like GUIs (Graphical user interface), 7zip can pack with both tar and gzip.",
    "tag": "tar"
  },
  {
    "question": "Native .tar extraction in Powershell",
    "answer": "I believe tar has been added as a native function in Windows 10 since the posting of this.\nFrom the command prompt or PowerShell in Windows 10 I can run\ntar -xvzf .\\whatever.tar.gz\n\nNote that the .\\ was added after auto-completing by the use of tab in PowerShell, but I think it should work without that.\nThere may be some underlying differences between this function and its Unix implementation (since it is on Windows after all), but it worked for me.",
    "tag": "tar"
  },
  {
    "question": "How to extract tar archive from stdin?",
    "answer": "Use - as the input file:\ncat largefile.tgz.aa largefile.tgz.ab | tar zxf -\n\nMake sure you cat them in the same order they were split.\nIf you're using zsh you can use the multios feature and avoid invoking cat:\n< largefile.tgz.aa < largefile.tgz.ab tar zxf -\n\nOr if they are in alphabetical order:\n<largefile.tgz.* | tar zxf -",
    "tag": "tar"
  },
  {
    "question": "How to extract tar file in Mac terminal",
    "answer": "Yes, you can run:\ntar -xvf myfile.tar\n\nFor .tar.gz, you can run:\ntar -xzvf myfile.tar.gz\n\nIf you want to extract to any directory other than your cwd, use -C. e.g:\ntar -xvf myfile.tar -C somedirectory\n\nI suggest you read the man page for tar if you wish to do anything further:\nman tar",
    "tag": "tar"
  },
  {
    "question": "How do I extract a tar file in Java?",
    "answer": "You can do this with the Apache Commons Compress library. You can download the 1.2 version from http://mvnrepository.com/artifact/org.apache.commons/commons-compress/1.2. \nHere are two methods: one that unzips a file and another one that untars it. So, for a file\n<fileName>tar.gz, you need to first unzip it and after that untar it. Please note that the tar archive may contain folders as well, case in which they need to be created on the local filesystem.\nEnjoy.\n/** Untar an input file into an output file.\n\n * The output file is created in the output folder, having the same name\n * as the input file, minus the '.tar' extension. \n * \n * @param inputFile     the input .tar file\n * @param outputDir     the output directory file. \n * @throws IOException \n * @throws FileNotFoundException\n *  \n * @return  The {@link List} of {@link File}s with the untared content.\n * @throws ArchiveException \n */\nprivate static List<File> unTar(final File inputFile, final File outputDir) throws FileNotFoundException, IOException, ArchiveException {\n\n    LOG.info(String.format(\"Untaring %s to dir %s.\", inputFile.getAbsolutePath(), outputDir.getAbsolutePath()));\n\n    final List<File> untaredFiles = new LinkedList<File>();\n    final InputStream is = new FileInputStream(inputFile); \n    final TarArchiveInputStream debInputStream = (TarArchiveInputStream) new ArchiveStreamFactory().createArchiveInputStream(\"tar\", is);\n    TarArchiveEntry entry = null; \n    while ((entry = (TarArchiveEntry)debInputStream.getNextEntry()) != null) {\n        final File outputFile = new File(outputDir, entry.getName());\n        if (entry.isDirectory()) {\n            LOG.info(String.format(\"Attempting to write output directory %s.\", outputFile.getAbsolutePath()));\n            if (!outputFile.exists()) {\n                LOG.info(String.format(\"Attempting to create output directory %s.\", outputFile.getAbsolutePath()));\n                if (!outputFile.mkdirs()) {\n                    throw new IllegalStateException(String.format(\"Couldn't create directory %s.\", outputFile.getAbsolutePath()));\n                }\n            }\n        } else {\n            LOG.info(String.format(\"Creating output file %s.\", outputFile.getAbsolutePath()));\n            final OutputStream outputFileStream = new FileOutputStream(outputFile); \n            IOUtils.copy(debInputStream, outputFileStream);\n            outputFileStream.close();\n        }\n        untaredFiles.add(outputFile);\n    }\n    debInputStream.close(); \n\n    return untaredFiles;\n}\n\n/**\n * Ungzip an input file into an output file.\n * <p>\n * The output file is created in the output folder, having the same name\n * as the input file, minus the '.gz' extension. \n * \n * @param inputFile     the input .gz file\n * @param outputDir     the output directory file. \n * @throws IOException \n * @throws FileNotFoundException\n *  \n * @return  The {@File} with the ungzipped content.\n */\nprivate static File unGzip(final File inputFile, final File outputDir) throws FileNotFoundException, IOException {\n\n    LOG.info(String.format(\"Ungzipping %s to dir %s.\", inputFile.getAbsolutePath(), outputDir.getAbsolutePath()));\n\n    final File outputFile = new File(outputDir, inputFile.getName().substring(0, inputFile.getName().length() - 3));\n\n    final GZIPInputStream in = new GZIPInputStream(new FileInputStream(inputFile));\n    final FileOutputStream out = new FileOutputStream(outputFile);\n\n    IOUtils.copy(in, out);\n\n    in.close();\n    out.close();\n\n    return outputFile;\n}",
    "tag": "tar"
  },
  {
    "question": "Rename Directory Name Before tar Happens",
    "answer": "Which tar?\nGNU Tar accepts a --transform argument, to which you give a sed expression to manipulate filenames.\nFor example, to rename during unpacking:\ntar -zxf my-dir.tar.gz --transform s/my-dir/your-dir/\n\nBSD tar and S tar similarly have an -s argument, taking a simple /old/new/ (not a general sed expression).",
    "tag": "tar"
  },
  {
    "question": "How do I untar a subdirectory into the current directory?",
    "answer": "Why don't you untar normally, then just:\nmv wordpress/.* .\nmv wordpress/* .\nrmdir wordpress\n\nBut alas, there's:\ntar --strip-components=1 -zxvf wordpress.tgz",
    "tag": "tar"
  },
  {
    "question": "How to send a compressed archive that contains executables so that Google's attachment filter won't reject it",
    "answer": "tar -cvzf filename.tar.gz directory_to_compress/\n\nMost tar commands have a z option to create a gziped version. \nThough seems to me the question is how to circumvent Google. I'm not sure if renaming your output file would fool Google, but you could try. I.e.,\ntar -cvzf filename.bla directory_to_compress/\n\nand then send the filename.bla - contents will would be a zipped tar, so at the other end it could be retrieved as usual.",
    "tag": "tar"
  },
  {
    "question": "How to install Go in alpine linux",
    "answer": "I just copied it over using multi stage builds, seems to be ok so far\nFROM XXX\n \nCOPY --from=golang:1.13-alpine /usr/local/go/ /usr/local/go/\n \nENV PATH=\"/usr/local/go/bin:${PATH}\"",
    "tag": "tar"
  },
  {
    "question": "How to use Pigz with Tar",
    "answer": "Mark Adler's top voted answer on the SO link that you included in your question does provide a solution for specifying compression-level as well as number of processors to use:\ntar cf - paths-to-archive | pigz -9 -p 32 > archive.tar.gz\n\nSee : https://stackoverflow.com/a/12320421",
    "tag": "tar"
  },
  {
    "question": "Python packaging: wheels vs tarball (tar.gz)",
    "answer": "This answered it for me (directly from the wheel PEP):\n\nPython needs a package format that is easier to install than sdist.\n  Python's sdist packages are defined by and require the distutils and\n  setuptools build systems, running arbitrary code to build-and-install,\n  and re-compile, code just so it can be installed into a new\n  virtualenv. This system of conflating build-install is slow, hard to\n  maintain, and hinders innovation in both build systems and installers.\nWheel attempts to remedy these problems by providing a simpler\n  interface between the build system and the installer. The wheel binary\n  package format frees installers from having to know about the build\n  system, saves time by amortizing compile time over many installations,\n  and removes the need to install a build system in the target\n  environment.\n\nhttps://www.python.org/dev/peps/pep-0427/#rationale\nNote the tarballs we're speaking of are what are referred to as \"sdists\" above.",
    "tag": "tar"
  },
  {
    "question": "Tarballing without Git metadata",
    "answer": "You will get a nasty surprise when the number of files increase to more than one xargs command: Then you will first make a tar file of the first files and then overwrite the same tar file with the rest of the files.\nGNU tar has the --exclude option which will solve this issue:\ntar cvf ~/app.tar --exclude .git --exclude \"*.log\" .",
    "tag": "tar"
  },
  {
    "question": "Extract tar the tar.bz2 file error",
    "answer": "Ensure that you have the bzip2 and bzip2-libs RPMs installed. \nIt looks like the tar command defers to the bzip2 command which the bzip2 RPM provides (/usr/bin/bzip2). In your case, tar specifically tries to call bzip2 -d to decompress the bzipped archive.\nAlso, a couple of tips:\n\nThe -v option is not necessary. It just gives verbose output, which means that it lists the files that were extracted from the archive. Most of the time this prints useless data to your terminal.\nAs @Skynet said, it is helpful to run the file command on your bzip2 archive to ensure that it is actually in bzip2 format.\nAs @Odin said, it appears that you don't need to specify the -j option when extracting the archive, as the tar command seems to be smart enough to figure this out.",
    "tag": "tar"
  },
  {
    "question": "Docker load and save: \"archive/tar: invalid tar header\"",
    "answer": "I wanted to add that the issue probably occurs because of the difference in behaviour of STDOUT between Windows and Unix. Therefore, using the STDOUT way of saving like:\ndocker save [image] > file.tar followed by docker load < file.tar \nwill not work if the save and load are executed on a different OS. Always use: \ndocker save [image] -o file.tar followed by docker load -i file.tar \nto prevent these issues. Comparing the TAR files produced by the different methods, you will find that they have a completely different size (303MB against 614MB for me).",
    "tag": "tar"
  },
  {
    "question": "Shell 'tar: not found in archive' error when using regular expression",
    "answer": "When you write\n tar -xzf *.gz\n\nyour shell expands it to the string:\n tar -xzf 1.gz 2.gz 3.gz\n\n(assuming 1.gz, 2.gz and 3.gz are in you current directory).\ntar thinks that you want to extract 2.gz and 3.gz from 1.gz; it can't find these files in the archives and that causes the error message.\nYou need to use loop for of command xargs to extract your files.\nls *.gz |xargs -n1 tar -xzf\n\nThat means: run me tar -xzf for every gz-file in the current directory.",
    "tag": "tar"
  },
  {
    "question": "How to Compress/Decompress tar.gz files in java",
    "answer": "I've written a wrapper for commons-compress called jarchivelib that makes it easy to extract or compress from and into File objects.\nExample code would look like this:\nFile archive = new File(\"/home/thrau/archive.tar.gz\");\nFile destination = new File(\"/home/thrau/archive/\");\n\nArchiver archiver = ArchiverFactory.createArchiver(\"tar\", \"gz\");\narchiver.extract(archive, destination);",
    "tag": "tar"
  },
  {
    "question": "How can I read tar.gz file using pandas read_csv with gzip compression option?",
    "answer": "df = pd.read_csv('sample.tar.gz', compression='gzip', header=0, sep=' ', quotechar='\"', error_bad_lines=False)\n\nNote: error_bad_lines=False will ignore the offending rows.",
    "tag": "tar"
  },
  {
    "question": "How to create a tar file that omits timestamps for its contents?",
    "answer": "To have a truly idempotent tar, mtime is a good step but not enough.\nYou also need to set the sort order, the owner and group (together with their mapping) and a proper timezone for mtime (since otherwise you're gonna have issues as well between Mac and Linux).\nI ended up with\ntar --sort=name --owner=root:0 --group=root:0 --mtime='UTC 1980-02-01' ... | gzip -n",
    "tag": "tar"
  },
  {
    "question": "How to compare two tarball's content",
    "answer": "Try also pkgdiff to visualize differences between packages (detects added/removed/renamed files and changed content, exist with zero code if unchanged):\npkgdiff PKG-0.tgz PKG-1.tgz",
    "tag": "tar"
  },
  {
    "question": "what does -zxvf mean in tar -zxvf <filename>?",
    "answer": "z means (un)z̲ip.\nx means ex̲tract files from the archive.\nv means print the filenames v̲erbosely.\nf means the following argument is a f̱ilename.\n\nFor more details, see tar's man page.",
    "tag": "tar"
  },
  {
    "question": "Opening a .tar.gz file with a single command",
    "answer": "tar xzf file.tar.gz\n\nThe letters are:\n\nx - extract\nz - gunzip the input\nf - Read from a file, not stdin",
    "tag": "tar"
  },
  {
    "question": "Extract files contained in archive.tar.gz to new directory named archive",
    "answer": "Update since GNU tar 1.28:\nuse --one-top-level, see https://www.gnu.org/software/tar/manual/tar.html#index-one_002dtop_002dlevel_002c-summary\nOlder versions need to script this. You can specify the directory that the extract is placed in by using the tar -C option.\nThe script below assumes that the directories do not exist and must be created. If the directories do exist the script will still work - the mkdir will simply fail.\ntar -xvzf archive.tar.gx -C archive_dir\n\ne.g.\nfor a in *.tar.gz\ndo\n    a_dir=${a%.tar.gz}\n    mkdir --parents $a_dir\n    tar -xvzf $a -C $a_dir\ndone",
    "tag": "tar"
  },
  {
    "question": "How to fix 'tar: Failed to set default locale' error?",
    "answer": "Step 1 (In R Console)\nsystem('defaults write org.R-project.R force.LANG en_US.UTF-8')\n\nStep 2: Restart R\nSource: http://cran.r-project.org/bin/macosx/RMacOSX-FAQ.html#Internationalization-of-the-R_002eapp",
    "tag": "tar"
  },
  {
    "question": "How to construct a TarFile object in memory from byte buffer in Python 3?",
    "answer": "BytesIO() from IO module does exactly what you need.\nimport tarfile, io\nbyte_array = client.read_bytes()\nfile_like_object = io.BytesIO(byte_array)\ntar = tarfile.open(fileobj=file_like_object)\n# use \"tar\" as a regular TarFile object\nfor member in tar.getmembers():\n    f = tar.extractfile(member)\n    print(f)",
    "tag": "tar"
  },
  {
    "question": "python write string directly to tarfile",
    "answer": "I would say it's possible, by playing with TarInfo and TarFile.addfile passing a StringIO as a fileobject.\nVery rough, but it works\nimport tarfile\nimport StringIO\n\ntar = tarfile.TarFile(\"test.tar\",\"w\")\n\nstring = StringIO.StringIO()\nstring.write(\"hello\")\nstring.seek(0)\ninfo = tarfile.TarInfo(name=\"foo\")\ninfo.size=len(string.buf)\ntar.addfile(tarinfo=info, fileobj=string)\n\ntar.close()",
    "tag": "tar"
  },
  {
    "question": "tar: Error is not recoverable: exiting now",
    "answer": "I would try to unzip and untar separately and see what happens:\nmv Doctrine-1.2.0.tgz Doctrine-1.2.0.tar.gz\ngunzip Doctrine-1.2.0.tar.gz\ntar xf Doctrine-1.2.0.tar",
    "tag": "tar"
  },
  {
    "question": "how to make tar exclude hidden directories",
    "answer": "You can use --exclude=\".*\"\n$ tar -czvf test.tgz test/\ntest/\ntest/seen\ntest/.hidden\n$ tar --exclude=\".*\" -czvf test.tgz test/\ntest/\ntest/seen\n\nBe careful if you are taring the current directory, since it will also be excluded by this pattern matching.\n$ cd test\n$ tar --exclude=\".*\" -czvf test.tgz ./\n$ tar -czvf test.tgz ./\n./\n./seen\n./.hidden\n\nThen you need to use --exclude='.[^/]*' as described elsewhere\n$ tar --exclude='.[^/]*' -czvf test.tgz ./\n./\n./seen",
    "tag": "tar"
  },
  {
    "question": "Read .tar.gz file in Python",
    "answer": "The docs tell us that None is returned by extractfile() if the member is a not a regular file or link.\nOne possible solution is to skip over the None results:\nwith tarfile.open(\"filename.tar.gz\", \"r:gz\") as tar:\n    for member in tar.getmembers():\n         f = tar.extractfile(member)\n         if f is not None:\n             content = f.read()",
    "tag": "tar"
  },
  {
    "question": "How to add progress bar to a somearchive.tar.xz extract",
    "answer": "Using pv to pipe the file to tar.\n\nFirstly, you'll need to install pv, which on macOS can be done with:\nbrew install pv\n\nOn Debian or Ubuntu, it can be done with: apt install pv (Thanks @hyperbola!).\n\nPipe the compressed file with pv to the tar command:\npv mysql.tar.gz | tar -xz   \n\n\n\nHere's the sample output of this command:\n\nFor those curious, this works by pv knowing the total file size of the file you pass it and how much of it has been \"piped\" to the tar command. It uses those two things to determine the current progress, the average speed, and the estimated completion time. Neat!",
    "tag": "tar"
  },
  {
    "question": "tar with --include pattern",
    "answer": "GNU tar has a -T or --files-from option that can take a file containing a list of files to include. The file specified with this option can be \"-\" for stdin. So, you can pass an arbitrary list of files for tar to archive from stdin using -files-from -. Using find patterns to generate a list of files, your example becomes:\nfind . -name '*.php' -print0 | tar -cvjf my.tar.bz2 --null --files-from -",
    "tag": "tar"
  },
  {
    "question": "Tar command in mac os x adding \"hidden\" files, why?",
    "answer": "You can add the following to your bashrc file -\nexport COPYFILE_DISABLE=true\n\nOr, you can add this option to your tar command at the extraction time\ntar -xzpvf x.tar --exclude=\"._*\"",
    "tag": "tar"
  },
  {
    "question": "How to fix NPM package Tar, with high vulnerability about Arbitrary File Overwrite, when package is up to date?",
    "answer": "The issue is being tracked on the gitgub page\nhttps://github.com/sass/node-sass/issues/2625",
    "tag": "tar"
  },
  {
    "question": "Programmatically extract tar.gz in a single step (on Windows with 7-Zip)",
    "answer": "Old question, but I was struggling with it today so here's my 2c.  The 7zip commandline tool \"7z.exe\" (I have v9.22 installed) can write to stdout and read from stdin so you can do without the intermediate tar file by using a pipe:\n7z x \"somename.tar.gz\" -so | 7z x -aoa -si -ttar -o\"somename\"\n\nWhere:\nx     = Extract with full paths command\n-so   = write to stdout switch\n-si   = read from stdin switch\n-aoa  = Overwrite all existing files without prompt.\n-ttar = Treat the stdin byte stream as a TAR file\n-o    = output directory\n\nSee the help file (7-zip.chm) in the install directory for more info on the command line commands and switches.\nAs noted by @zespri powershell will buffer the input to the second 7z process so can consume a lot of memory if your tar file is large. i.e:\n& 7z x \"somename.tar.gz\" -so  | & 7z x -aoa -si -ttar -o\"somename\"\n\nA workaround from this SO answer if you want to do this from powershell is to pass the commands to cmd.exe:\n& cmd.exe '/C 7z x \"somename.tar.gz\" -so | 7z x -aoa -si -ttar -o\"somename\"'",
    "tag": "tar"
  },
  {
    "question": "How do I exclude absolute paths for tar?",
    "answer": "You are incorrectly using the -C switch, which is used for changing directories. So what you need to do is:\ntar -cf tarname.tar -C /www/path path/file1.txt path2/path3/file2.xls\n\nor if you want to package everything under /www/path do:\ntar -cf tarname.tar -C /www/path .\n\nYou can use -C switch multiple times.",
    "tag": "tar"
  },
  {
    "question": "How to compare the content of a tarball with a folder",
    "answer": "--compare (-d) is more handy for that.\ntar --compare --file=archive-file.tar\n\nworks if archive-file.tar is in the directory it was created. To compare archive-file.tar against a remote target (eg if you have moved archive-file.tar to /some/where/) use the -C parameter:\ntar --compare --file=archive-file.tar -C /some/where/\n\nIf you want to see tar working, use -v without -v only errors (missing files/folders) are reported.\nTipp: This works with compressed tar.bz/ tar.gz archives, too.",
    "tag": "tar"
  },
  {
    "question": "untar filename.tr.gz to directory \"filename\"",
    "answer": "tar -xzvf filename.tar.gz -C destination_directory",
    "tag": "tar"
  },
  {
    "question": "How do you Tar an svn directory and filter out all the .svn files?",
    "answer": "tar --exclude=.svn -z -c -v -f mytarball.tar.gz mydir/",
    "tag": "tar"
  },
  {
    "question": "unzip (zip, tar, tag.gz) files with ruby",
    "answer": "To extract files from a .tar.gz file you can use the following methods from packages distributed with Ruby:\nrequire 'rubygems/package'\nrequire 'zlib'\ntar_extract = Gem::Package::TarReader.new(Zlib::GzipReader.open('Path/To/myfile.tar.gz'))\ntar_extract.rewind # The extract has to be rewinded after every iteration\ntar_extract.each do |entry|\n  puts entry.full_name\n  puts entry.directory?\n  puts entry.file?\n  # puts entry.read\nend\ntar_extract.close\n\nEach entry of type Gem::Package::TarReader::Entry points to a file or directory within the .tar.gz file.\nSimilar code can be used (replace Reader with Writer) to write files to a .tar.gz file.",
    "tag": "tar"
  },
  {
    "question": "How can I untar a tar.bz file in unix?",
    "answer": "use the -j option of tar.\ntar -xjf /path/to/archive.tar.bz",
    "tag": "tar"
  },
  {
    "question": "Python - mechanism to identify compressed file type and uncompress",
    "answer": "This page has a list of \"magic\" file signatures. Grab the ones you need and put them in a dict like below. Then we need a function that matches the dict keys with the start of the file. I've written a suggestion, though it can be optimized by preprocessing the magic_dict into e.g. one giant compiled regexp.\nmagic_dict = {\n    \"\\x1f\\x8b\\x08\": \"gz\",\n    \"\\x42\\x5a\\x68\": \"bz2\",\n    \"\\x50\\x4b\\x03\\x04\": \"zip\"\n    }\n\nmax_len = max(len(x) for x in magic_dict)\n\ndef file_type(filename):\n    with open(filename) as f:\n        file_start = f.read(max_len)\n    for magic, filetype in magic_dict.items():\n        if file_start.startswith(magic):\n            return filetype\n    return \"no match\"\n\nThis solution should be cross-plattform and is of course not dependent on file name extension, but it may give false positives for files with random content that just happen to start with some specific magic bytes.",
    "tag": "tar"
  },
  {
    "question": "bash: /bin/tar: Argument list too long when compressing many files with tar",
    "answer": "Use the \"-T\" option to pass a file to tar that contains the filenames to tar up.\ntar -czv -T file_list.txt -f tarball.tar.gz",
    "tag": "tar"
  },
  {
    "question": "how to decompress with pigz",
    "answer": "Use pigz -dc for decompression to stdout.  Then use > as you are in your example to direct the output to the desired path and file.\nYou can type just pigz for command options help.",
    "tag": "tar"
  },
  {
    "question": "archiving hidden directories with tar",
    "answer": "With wildcard it will not work. You have to specify . (current directory) if you mean full directory including hidden files. You can do\ntar -cvpzf test.tgz .",
    "tag": "tar"
  },
  {
    "question": "How to count the number of files inside a tar.gz file (without decompressing)?",
    "answer": "Since it is a tar and gzip archive you should use z option to use gzip. Then simply you can count lines with wc.\ntar -tzf file.tar.gz | wc -l",
    "tag": "tar"
  },
  {
    "question": "tar removing leading '/' from member names",
    "answer": "The \"good\" version is also displaying the same message you've just missed it.\nIf you don't like the behaviour, search for \"leading\", in manual. First hit:\n-P, --absolute-names\n       don't strip leading '/'s from file names",
    "tag": "tar"
  },
  {
    "question": "Safely extract zip or tar using Python",
    "answer": "Note: Starting with python 2.7.4, this is a non-issue for ZIP archives. Details at the bottom of the answer. This answer focuses on tar archives.\nTo figure out where a path really points to, use os.path.abspath() (but note the caveat about symlinks as path components). If you normalize a path from your zipfile with abspath and it does not contain the current directory as a prefix, it's pointing outside it.\nBut you also need to check the value of any symlink extracted from your archive (both tarfiles and unix zipfiles can store symlinks). This is important if you are worried about a proverbial \"malicious user\" that would intentionally bypass your security, rather than an application that simply installs itself in system libraries.\nThat's the aforementioned caveat: abspath will be misled if your sandbox already contains a symlink that points to a directory. Even a symlink that points within the sandbox can be dangerous: The symlink sandbox/subdir/foo -> .. points to sandbox, so the path sandbox/subdir/foo/../.bashrc should be disallowed. The easiest way to do so is to wait until the previous files have been extracted and use  os.path.realpath(). Fortunately extractall() accepts a generator, so this is easy to do.\nSince you ask for code, here's a bit that explicates the algorithm. It prohibits not only the extraction of files to locations outside the sandbox (which is what was requested), but also the creation of links inside the sandbox that point to locations outside the sandbox. I'm curious to hear if anyone can sneak any stray files or links past it.\nimport tarfile\nfrom os.path import abspath, realpath, dirname, join as joinpath\nfrom sys import stderr\n\nresolved = lambda x: realpath(abspath(x))\n\ndef badpath(path, base):\n    # joinpath will ignore base if path is absolute\n    return not resolved(joinpath(base,path)).startswith(base)\n\ndef badlink(info, base):\n    # Links are interpreted relative to the directory containing the link\n    tip = resolved(joinpath(base, dirname(info.name)))\n    return badpath(info.linkname, base=tip)\n\ndef safemembers(members):\n    base = resolved(\".\")\n    \n    for finfo in members:\n        if badpath(finfo.name, base):\n            print >>stderr, finfo.name, \"is blocked (illegal path)\"\n        elif finfo.issym() and badlink(finfo,base):\n            print >>stderr, finfo.name, \"is blocked: Symlink to\", finfo.linkname\n        elif finfo.islnk() and badlink(finfo,base):\n            print >>stderr, finfo.name, \"is blocked: Hard link to\", finfo.linkname\n        else:\n            yield finfo\n\nar = tarfile.open(\"testtar.tar\")\nar.extractall(path=\"./sandbox\", members=safemembers(ar))\nar.close()\n\nEdit: Starting with python 2.7.4, this is a non-issue for ZIP archives: The method zipfile.extract() prohibits the creation of files outside the sandbox:\n\nNote: If a member filename is an absolute path, a drive/UNC sharepoint and leading (back)slashes will be stripped, e.g.: ///foo/bar becomes foo/bar on Unix, and C:\\foo\\bar becomes foo\\bar on Windows. And all \"..\" components in a member filename will be removed, e.g.: ../../foo../../ba..r becomes foo../ba..r. On Windows, illegal characters (:, <, >, |, \", ?, and *) [are] replaced by underscore (_).\n\nThe tarfile class has not been similarly sanitized, so the above answer still apllies.",
    "tag": "tar"
  },
  {
    "question": "Creating zip or tar.gz archive without exec",
    "answer": "If you want to create tar.gz and you are using PHP 5.3+, you can use PharData class:\ntry\n{\n    $a = new PharData('archive.tar');\n\n    // ADD FILES TO archive.tar FILE\n    $a->addFile('data.xls');\n    $a->addFile('index.php');\n\n    // COMPRESS archive.tar FILE. COMPRESSED FILE WILL BE archive.tar.gz\n    $a->compress(Phar::GZ);\n\n    // NOTE THAT BOTH FILES WILL EXISTS. SO IF YOU WANT YOU CAN UNLINK archive.tar\n    unlink('archive.tar');\n} \ncatch (Exception $e) \n{\n    echo \"Exception : \" . $e;\n}",
    "tag": "tar"
  },
  {
    "question": "PHP Untar-gz without exec()?",
    "answer": "Since PHP 5.3.0 you do not need to use Archive_Tar.\nThere is new class to work on tar archive: The PharData class.\nTo extract an archive (using PharData::extractTo() which work like the ZipArchive::extractTo()):\ntry {\n    $phar = new PharData('myphar.tar');\n    $phar->extractTo('/full/path'); // extract all files\n} catch (Exception $e) {\n    // handle errors\n}\n\nAnd if you have a tar.gz archive, just decompress it before extract (using PharData::decompress()):\n// decompress from gz\n$p = new PharData('/path/to/my.tar.gz');\n$p->decompress(); // creates /path/to/my.tar\n\n// unarchive from the tar\n$phar = new PharData('/path/to/my.tar');\n$phar->extractTo('/full/path');",
    "tag": "tar"
  },
  {
    "question": "Make tar file by Java",
    "answer": "I would look at Apache Commons Compress.\nThere is an example part way down this examples page, which shows off a tar example.\nTarArchiveEntry entry = new TarArchiveEntry(name);\nentry.setSize(size);\ntarOutput.putArchiveEntry(entry);\ntarOutput.write(contentOfEntry);\ntarOutput.closeArchiveEntry();",
    "tag": "tar"
  },
  {
    "question": "How are zlib, gzip and zip related? What do they have in common and how are they different?",
    "answer": "Short form:\n.zip is an archive format using, usually, the Deflate compression method.  The .gz gzip format is for single files, also using the Deflate compression method.  Often gzip is used in combination with tar to make a compressed archive format, .tar.gz.  The zlib library provides Deflate compression and decompression code for use by zip, gzip, png (which uses the zlib wrapper on deflate data), and many other applications.\nLong form:\nThe ZIP format was developed by Phil Katz as an open format with an open specification, where his implementation, PKZIP, was shareware.  It is an archive format that stores files and their directory structure, where each file is individually compressed.  The file type is .zip.  The files, as well as the directory structure, can optionally be encrypted.\nThe ZIP format supports several compression methods:\n    0 - The file is stored (no compression)\n    1 - The file is Shrunk\n    2 - The file is Reduced with compression factor 1\n    3 - The file is Reduced with compression factor 2\n    4 - The file is Reduced with compression factor 3\n    5 - The file is Reduced with compression factor 4\n    6 - The file is Imploded\n    7 - Reserved for Tokenizing compression algorithm\n    8 - The file is Deflated\n    9 - Enhanced Deflating using Deflate64(tm)\n   10 - PKWARE Data Compression Library Imploding (old IBM TERSE)\n   11 - Reserved by PKWARE\n   12 - File is compressed using BZIP2 algorithm\n   13 - Reserved by PKWARE\n   14 - LZMA\n   15 - Reserved by PKWARE\n   16 - IBM z/OS CMPSC Compression\n   17 - Reserved by PKWARE\n   18 - File is compressed using IBM TERSE (new)\n   19 - IBM LZ77 z Architecture \n   20 - deprecated (use method 93 for zstd)\n   93 - Zstandard (zstd) Compression \n   94 - MP3 Compression \n   95 - XZ Compression \n   96 - JPEG variant\n   97 - WavPack compressed data\n   98 - PPMd version I, Rev 1\n   99 - AE-x encryption marker (see APPENDIX E)\n\nMethods 1 to 7 are historical and are not in use.  Methods 9 through 98 are relatively recent additions and are in varying, small amounts of use.  The only method in truly widespread use in the ZIP format is method 8, Deflate, and to some smaller extent method 0, which is no compression at all.  Virtually every .zip file that you will come across in the wild will use exclusively methods 8 and 0, likely just method 8.  (Method 8 also has a means to effectively store the data with no compression and relatively little expansion, and Method 0 cannot be streamed whereas Method 8 can be.)\nThe ISO/IEC 21320-1:2015 standard for file containers is a restricted zip format, such as used in Java archive files (.jar), Office Open XML files (Microsoft Office .docx, .xlsx, .pptx), Office Document Format files (.odt, .ods, .odp), and EPUB files (.epub). That standard limits the compression methods to 0 and 8, as well as other constraints such as no encryption or signatures.\nAround 1990, the Info-ZIP group wrote portable, free, open-source implementations of zip and unzip utilities, supporting compression with the Deflate format, and decompression of that and the earlier formats.  This greatly expanded the use of the .zip format.\nIn the early '90s, the gzip format was developed as a replacement for the Unix compress utility, derived from the Deflate code in the Info-ZIP utilities.  Unix compress was designed to compress a single file or stream, appending a .Z to the file name.  compress uses the LZW compression algorithm, which at the time was under patent and its free use was in dispute by the patent holders.  Though some specific implementations of Deflate were patented by Phil Katz, the format was not, and so it was possible to write a Deflate implementation that did not infringe on any patents.  That implementation has not been so challenged in the last 20+ years.  The Unix gzip utility was intended as a drop-in replacement for compress, and in fact is able to decompress compress-compressed data (assuming that you were able to parse that sentence).  gzip appends a .gz to the file name.  gzip uses the Deflate compressed data format, which compresses quite a bit better than Unix compress, has very fast decompression, and adds a CRC-32 as an integrity check for the data.  The header format also permits the storage of more information than the compress format allowed, such as the original file name and the file modification time.\nThough compress only compresses a single file, it was common to use the tar utility to create an archive of files, their attributes, and their directory structure into a single .tar file, and then compress it with compress to make a .tar.Z file.  In fact, the tar utility had and still has the option to do the compression at the same time, instead of having to pipe the output of tar to compress.  This all carried forward to the gzip format, and tar has an option to compress directly to the .tar.gz format.  The tar.gz format compresses better than the .zip approach, since the compression of a .tar can take advantage of redundancy across files, especially many small files.  .tar.gz is the most common archive format in use on Unix due to its very high portability, but there are more effective compression methods in use as well, so you will often see .tar.bz2 and .tar.xz archives.\nUnlike .tar, .zip has a central directory at the end, which provides a list of the contents. That and the separate compression provides random access to the individual entries in a .zip file. A .tar file would have to be decompressed and scanned from start to end in order to build a directory, which is how a .tar file is listed.\nShortly after the introduction of gzip, around the mid-1990s, the same patent dispute called into question the free use of the .gif image format, very widely used on bulletin boards and the World Wide Web (a new thing at the time).  So a small group created the PNG losslessly compressed image format, with file type .png, to replace .gif.  That format also uses the Deflate format for compression, which is applied after filters on the image data expose more of the redundancy.  In order to promote widespread usage of the PNG format, two free code libraries were created.  libpng and zlib.  libpng handled all of the features of the PNG format, and zlib provided the compression and decompression code for use by libpng, as well as for other applications.  zlib was adapted from the gzip code.\nAll of the mentioned patents have since expired.\nThe zlib library supports Deflate compression and decompression, and three kinds of wrapping around the deflate streams.  Those are no wrapping at all (\"raw\" deflate), zlib wrapping, which is used in the PNG format data blocks, and gzip wrapping, to provide gzip routines for the programmer.  The main difference between zlib and gzip wrapping is that the zlib wrapping is more compact, six bytes vs. a minimum of 18 bytes for gzip, and the integrity check, Adler-32, runs faster than the CRC-32 that gzip uses.  Raw deflate is used by programs that read and write the .zip format, which is another format that wraps around deflate compressed data.\nzlib is now in wide use for data transmission and storage.  For example, most HTTP transactions by servers and browsers compress and decompress the data using zlib, specifically HTTP header Content-Encoding: deflate means deflate compression method wrapped inside the zlib data format.\nDifferent implementations of deflate can result in different compressed output for the same input data, as evidenced by the existence of selectable compression levels that allow trading off compression effectiveness for CPU time. zlib and PKZIP are not the only implementations of deflate compression and decompression. Both the 7-Zip archiving utility and Google's zopfli library have the ability to use much more CPU time than zlib in order to squeeze out the last few bits possible when using the deflate format, reducing compressed sizes by a few percent as compared to zlib's highest compression level. The pigz utility, a parallel implementation of gzip, includes the option to use zlib (compression levels 1-9) or zopfli (compression level 11), and somewhat mitigates the time impact of using zopfli by splitting the compression of large files over multiple processors and cores.",
    "tag": "gzip"
  },
  {
    "question": "How do I tar a directory of files and folders without including the directory itself?",
    "answer": "Use the -C switch of tar:\ntar -czvf my_directory.tar.gz -C my_directory .\n\nThe -C my_directory tells tar to change the current directory to my_directory, and then . means \"add the entire current directory\" (including hidden files and sub-directories).\nMake sure you do -C my_directory before you do . or else you'll get the files in the current directory.\nWarning: you'll get entries as ./file-name.ext instead of file-name.ext!\nIf you need entries in the form of file-name.ext, read other answers.",
    "tag": "gzip"
  },
  {
    "question": "Utilizing multi core for tar+gzip/bzip compression/decompression",
    "answer": "You can also use the tar flag --use-compress-program= to tell tar what compression program to use.\nFor example use:\ntar -c --use-compress-program=pigz -f tar.file dir_to_zip",
    "tag": "gzip"
  },
  {
    "question": "How to uncompress a tar.gz in another directory",
    "answer": "You can use the option -C (or --directory if you prefer long options) to give the target directory of your choice in case you are using the Gnu version of tar.  The directory should exist:\nmkdir foo\ntar -xzf bar.tar.gz -C foo\n\nIf you are not using a tar capable of extracting to a specific directory, you can simply cd into your target directory prior to calling tar; then you will have to give a complete path to your archive, of course.  You can do this in a scoping subshell to avoid influencing the surrounding script:\nmkdir foo\n(cd foo; tar -xzf ../bar.tar.gz)  # instead of ../ you can use an absolute path as well\n\nOr, if neither an absolute path nor a relative path to the archive file is suitable, you also can use this to name the archive outside of the scoping subshell:\nTARGET_PATH=a/very/complex/path/which/might/even/be/absolute\nmkdir -p \"$TARGET_PATH\"\n(cd \"$TARGET_PATH\"; tar -xzf -) < bar.tar.gz",
    "tag": "gzip"
  },
  {
    "question": "Command Line Tool - Error - xcrun: error: unable to find utility \"xcodebuild\", not a developer tool or in PATH",
    "answer": "I solved that problem by setting the Command Line Tools in Xcode. Go to:\n\nXcode > Preferences > Locations\n\nAnd select the command line tool from the dropdown. If you have only one version of Xcode installed, there should be only one option. If you have several versions of Xcode, then you must choose the one you need.\nUpdate (added image for reference)",
    "tag": "gzip"
  },
  {
    "question": "TypeError: 'str' does not support the buffer interface",
    "answer": "If you use Python3x then string is not the same type as for Python 2.x, you must cast it to bytes (encode it).\nplaintext = input(\"Please enter the text you want to compress\")\nfilename = input(\"Please enter the desired filename\")\nwith gzip.open(filename + \".gz\", \"wb\") as outfile:\n    outfile.write(bytes(plaintext, 'UTF-8'))\n\nAlso do not use variable names like string or file while those are names of module or function.\nEDIT @Tom\nYes, non-ASCII text is also compressed/decompressed. I use Polish letters with UTF-8 encoding:\nplaintext = 'Polish text: ąćęłńóśźżĄĆĘŁŃÓŚŹŻ'\nfilename = 'foo.gz'\nwith gzip.open(filename, 'wb') as outfile:\n    outfile.write(bytes(plaintext, 'UTF-8'))\nwith gzip.open(filename, 'r') as infile:\n    outfile_content = infile.read().decode('UTF-8')\nprint(outfile_content)",
    "tag": "gzip"
  },
  {
    "question": "How to gzip all files in all sub-directories into one compressed file in bash",
    "answer": "tar -zcvf compressFileName.tar.gz folderToCompress\n\neverything in folderToCompress will go to compressFileName\nEdit: After review and comments I realized that people may get confused with compressFileName without an extension. If you want you can use .tar.gz extension(as suggested) with the compressFileName",
    "tag": "gzip"
  },
  {
    "question": "Enable IIS7 gzip",
    "answer": "Configuration\nYou can enable GZIP compression entirely in your Web.config file.  This is particularly useful if you're on shared hosting and can't configure IIS directly, or you want your config to carry between all environments you target.\n<system.webServer>\n  <httpCompression directory=\"%SystemDrive%\\inetpub\\temp\\IIS Temporary Compressed Files\">\n    <scheme name=\"gzip\" dll=\"%Windir%\\system32\\inetsrv\\gzip.dll\"/>\n    <dynamicTypes>\n      <add mimeType=\"text/*\" enabled=\"true\"/>\n      <add mimeType=\"message/*\" enabled=\"true\"/>\n      <add mimeType=\"application/javascript\" enabled=\"true\"/>\n      <add mimeType=\"*/*\" enabled=\"false\"/>\n    </dynamicTypes>\n    <staticTypes>\n      <add mimeType=\"text/*\" enabled=\"true\"/>\n      <add mimeType=\"message/*\" enabled=\"true\"/>\n      <add mimeType=\"application/javascript\" enabled=\"true\"/>\n      <add mimeType=\"*/*\" enabled=\"false\"/>\n    </staticTypes>\n  </httpCompression>\n  <urlCompression doStaticCompression=\"true\" doDynamicCompression=\"true\"/>\n</system.webServer>\n\nTesting\nTo test whether compression is working or not, use the developer tools in Chrome or Firebug for Firefox and ensure the HTTP response header is set:\nContent-Encoding: gzip\n\nNote that this header won't be present if the response code is 304 (Not Modified).  If that's the case, do a full refresh (hold shift or control while you press the refresh button) and check again.",
    "tag": "gzip"
  },
  {
    "question": "Why use deflate instead of gzip for text files served by Apache?",
    "answer": "Why use deflate instead of gzip for text files served by Apache?\n\nThe simple answer is don't. \n\nRFC 2616 defines deflate as: \n\ndeflate The \"zlib\" format defined in RFC 1950 in combination with the \"deflate\" compression mechanism described in RFC 1951\n\nThe zlib format is defined in RFC 1950 as :\n     0   1\n     +---+---+\n     |CMF|FLG|   (more-->)\n     +---+---+\n\n       0   1   2   3\n     +---+---+---+---+\n     |     DICTID    |   (more-->)\n     +---+---+---+---+\n\n     +=====================+---+---+---+---+\n     |...compressed data...|    ADLER32    |\n     +=====================+---+---+---+---+\n\nSo, a few headers and an ADLER32 checksum\nRFC 2616 defines gzip as: \n\ngzip An encoding format produced by the file compression program\n         \"gzip\" (GNU zip) as described in RFC 1952 [25]. This format is a\n         Lempel-Ziv coding (LZ77) with a 32 bit CRC.\n\nRFC 1952 defines the compressed data as: \n\nThe format presently uses the DEFLATE method of compression but can be easily extended to use other compression methods.\n\nCRC-32 is slower than ADLER32\n\nCompared to a cyclic redundancy check of the same length, it trades reliability for speed (preferring the latter). \n\nSo ... we have 2 compression mechanisms that use the same algorithm for compression, but a different algorithm for headers and checksum.\nNow, the underlying TCP packets are already pretty reliable, so the issue here is not Adler 32 vs CRC-32 that GZIP uses.\n\nTurns out many browsers over the years implemented an incorrect deflate algorithm. Instead of expecting the zlib header in RFC 1950 they simply expected the compressed payload. Similarly various web servers made the same mistake. \nSo, over the years browsers started implementing a fuzzy logic deflate implementation, they try for zlib header and adler checksum, if that fails they try for payload.\nThe result of having complex logic like that is that it is often broken. Verve Studio have a user contributed test section that show how bad the situation is. \nFor example: deflate works in Safari 4.0 but is broken in Safari 5.1, it also always has issues on IE. \n\nSo, best thing to do is avoid deflate altogether, the minor speed boost (due to adler 32) is not worth the risk of broken payloads.",
    "tag": "gzip"
  },
  {
    "question": "How can I tell if my server is serving GZipped content?",
    "answer": "It looks like one possible answer is, unsurprisingly, curl:\n$ curl http://example.com/ --silent --write-out \"%{size_download}\\n\" --output /dev/null\n31032\n$ curl http://example.com/ --silent -H \"Accept-Encoding: gzip,deflate\" --write-out \"%{size_download}\\n\" --output /dev/null\n2553\n\nIn the second case the client tells the server that it supports content encoding and you can see that the response was indeed shorter, compressed.",
    "tag": "gzip"
  },
  {
    "question": "JavaScript implementation of Gzip",
    "answer": "Edit There appears to be a better LZW solution that handles Unicode strings correctly at http://pieroxy.net/blog/pages/lz-string/index.html (Thanks to pieroxy in the comments).\n\nI don't know of any gzip implementations, but the jsolait library (the site seems to have gone away) has functions for LZW compression/decompression. The code is covered under the LGPL.\n// LZW-compress a string\nfunction lzw_encode(s) {\n    var dict = {};\n    var data = (s + \"\").split(\"\");\n    var out = [];\n    var currChar;\n    var phrase = data[0];\n    var code = 256;\n    for (var i=1; i<data.length; i++) {\n        currChar=data[i];\n        if (dict[phrase + currChar] != null) {\n            phrase += currChar;\n        }\n        else {\n            out.push(phrase.length > 1 ? dict[phrase] : phrase.charCodeAt(0));\n            dict[phrase + currChar] = code;\n            code++;\n            phrase=currChar;\n        }\n    }\n    out.push(phrase.length > 1 ? dict[phrase] : phrase.charCodeAt(0));\n    for (var i=0; i<out.length; i++) {\n        out[i] = String.fromCharCode(out[i]);\n    }\n    return out.join(\"\");\n}\n\n// Decompress an LZW-encoded string\nfunction lzw_decode(s) {\n    var dict = {};\n    var data = (s + \"\").split(\"\");\n    var currChar = data[0];\n    var oldPhrase = currChar;\n    var out = [currChar];\n    var code = 256;\n    var phrase;\n    for (var i=1; i<data.length; i++) {\n        var currCode = data[i].charCodeAt(0);\n        if (currCode < 256) {\n            phrase = data[i];\n        }\n        else {\n           phrase = dict[currCode] ? dict[currCode] : (oldPhrase + currChar);\n        }\n        out.push(phrase);\n        currChar = phrase.charAt(0);\n        dict[code] = oldPhrase + currChar;\n        code++;\n        oldPhrase = phrase;\n    }\n    return out.join(\"\");\n}",
    "tag": "gzip"
  },
  {
    "question": "How to properly handle a gzipped page when using curl?",
    "answer": "curl will automatically decompress the response if you set the --compressed flag:\ncurl --compressed \"http://example.com\"\n\n\n--compressed\n   (HTTP)  Request  a compressed response using one of the algorithms libcurl supports, and save the uncompressed document.  If this option is used and the server sends an unsupported encoding, curl will report an error.\n\ngzip is most likely supported, but you can check this by running curl -V and looking for libz somewhere in the \"Features\" line:\n$ curl -V\n...\nProtocols: ...\nFeatures: GSS-Negotiate IDN IPv6 Largefile NTLM SSL libz \n\n\nNote that it's really the website in question that is at fault here. If curl did not pass an Accept-Encoding: gzip request header, the server should not have sent a compressed response.",
    "tag": "gzip"
  },
  {
    "question": "Serving gzipped CSS and JavaScript from Amazon CloudFront via S3",
    "answer": "UPDATE:  Amazon now supports gzip compression, so this is no longer needed.  Amazon Announcement\nOriginal answer:\nThe answer is to gzip the CSS and JavaScript files. Yes, you read that right.\ngzip -9 production.min.css\n\nThis will produce production.min.css.gz. Remove the .gz, upload to S3 (or whatever origin server you're using) and explicitly set the Content-Encoding header for the file to gzip.\nIt's not on-the-fly gzipping, but you could very easily wrap it up into your build/deployment scripts. The advantages are:\n\nIt requires no CPU for Apache to gzip the content when the file is requested.\nThe files are gzipped at the highest compression level (assuming gzip -9).\nYou're serving the file from a CDN.\n\nAssuming that your CSS/JavaScript files are (a) minified and (b) large enough to justify the CPU required to decompress on the user's machine, you can get significant performance gains here.\nJust remember: If you make a change to a file that is cached in CloudFront, make sure you invalidate the cache after making this type of change.",
    "tag": "gzip"
  },
  {
    "question": "How to unzip gz file using Python",
    "answer": "import gzip\nimport shutil\nwith gzip.open('file.txt.gz', 'rb') as f_in:\n    with open('file.txt', 'wb') as f_out:\n        shutil.copyfileobj(f_in, f_out)",
    "tag": "gzip"
  },
  {
    "question": "Excluding directory when creating a .tar.gz file",
    "answer": "Try removing the last / at the end of the directory path to exclude\ntar -pczf MyBackup.tar.gz /home/user/public_html/ --exclude \"/home/user/public_html/tmp\"",
    "tag": "gzip"
  },
  {
    "question": "How to check if a Unix .tar.gz file is a valid file without uncompressing?",
    "answer": "What about just getting a listing of the tarball and throw away the output, rather than decompressing the file?\ntar -tzf my_tar.tar.gz >/dev/null\n\nEdit as per comment. Thanks Frozen Flame! This test in no way implies integrity of the data. Because it was designed as a tape archival utility most implementations of tar will allow multiple copies of the same file!",
    "tag": "gzip"
  },
  {
    "question": "Gzip versus minify",
    "answer": "Very simple to test.  I took your js, put them in different files and ran gzip -9 on them.  Here's the result.  This was done on a WinXP machine running Cygwin and gzip 1.3.12.\n-rwx------  1 xxxxxxxx mkgroup-l-d     88 Apr 30 09:17 expanded.js.gz\n\n-rwx------  1 xxxxxxxx mkgroup-l-d     81 Apr 30 09:18 minified.js.gz\n\nHere's a further test using a real-world JS example.  The source file is \"common.js\"  The original file size is 73134 bytes.  Minified, it came to 26232 bytes.\nOriginal file:\n-rwxrwxrwx 1 xxxxxxxx mkgroup-l-d 73134 Apr 13 11:41 common.js\n\nMinified file:\n-rwxr-xr-x 1 xxxxxxxx mkgroup-l-d 26232 Apr 30 10:39 common-min.js\n\nOriginal file gzipped with -9 option (same version as above):\n-rwxrwxrwx 1 xxxxxxxx mkgroup-l-d 12402 Apr 13 11:41 common.js.gz\n\nMinified file gzipped with -9 option (same version as above):\n-rwxr-xr-x 1 xxxxxxxx mkgroup-l-d  5608 Apr 30 10:39 common-min.js.gz\n\nAs you can see, there is a definite difference between the various methods.  The best bet is to both minify as well as gzip them.",
    "tag": "gzip"
  },
  {
    "question": "How to extract filename.tar.gz file",
    "answer": "If file filename.tar.gz gives this message: POSIX tar archive, \nthe archive is a tar, not a GZip archive.\nUnpack a tar without the z, it is for gzipped (compressed), only:\nmv filename.tar.gz filename.tar # optional\ntar xvf filename.tar\n\nOr try a generic Unpacker like unp (https://packages.qa.debian.org/u/unp.html), a script for unpacking a wide variety of archive formats.\ndetermine the file type: \n$ file ~/Downloads/filename.tbz2\n/User/Name/Downloads/filename.tbz2: bzip2 compressed data, block size = 400k",
    "tag": "gzip"
  },
  {
    "question": "Decompressing GZip Stream from HTTPClient Response",
    "answer": "Just instantiate HttpClient like this:\nHttpClientHandler handler = new HttpClientHandler()\n{\n    AutomaticDecompression = DecompressionMethods.GZip | DecompressionMethods.Deflate\n};\n\nusing (var client = new HttpClient(handler)) //see update below\n{\n    // your code\n}\n\nUpdate June 19, 2020:\nIt's not recommended to use  httpclient in a 'using' block as it might cause port exhaustion.\nprivate static HttpClient client = null;\n    \nContructorMethod()\n{\n   if(client == null)\n   {\n        HttpClientHandler handler = new HttpClientHandler()\n        {\n            AutomaticDecompression = DecompressionMethods.GZip | DecompressionMethods.Deflate\n        };        \n        client = new HttpClient(handler);\n   }\n// your code            \n }\n\nIf using .Net Core 2.1+, consider using IHttpClientFactory and injecting like this in your startup code.\n var timeout = Policy.TimeoutAsync<HttpResponseMessage>(\n            TimeSpan.FromSeconds(60));\n\n services.AddHttpClient<XApiClient>().ConfigurePrimaryHttpMessageHandler(() => new HttpClientHandler\n        {\n            AutomaticDecompression = DecompressionMethods.GZip | DecompressionMethods.Deflate\n        }).AddPolicyHandler(request => timeout);",
    "tag": "gzip"
  },
  {
    "question": "Fast Concatenation of Multiple GZip Files",
    "answer": "With gzip files, you can simply concatenate the files together, like so:\ncat file1.gz file2.gz file3.gz > allfiles.gz\n\nPer the gzip RFC, \n\nA gzip file consists of a series of \"members\" (compressed data sets). [...] The members simply appear one after another in the file, with no additional information before, between, or after them.\n\nNote that this is not exactly the same as building a single gzip file of the concatenated data; among other things, all of the original filenames are preserved. However, gunzip seems to handle it as equivalent to a concatenation.\nSince existing tools generally ignore the filename headers for the additional members, it's not easily possible to extract individual files from the result. If you want this to be possible, build a ZIP file instead. ZIP and GZIP both use the DEFLATE algorithm for the actual compression (ZIP supports some other compression algorithms as well as an option - method 8 is the one that corresponds to GZIP's compression); the difference is in the metadata format. Since the metadata is uncompressed, it's simple enough to strip off the gzip headers and tack on ZIP file headers and a central directory record instead. Refer to the gzip format specification and the ZIP format specification.",
    "tag": "gzip"
  },
  {
    "question": "Writing then reading in-memory bytes (BytesIO) gives a blank result",
    "answer": "You need to seek back to the beginning of the file after writing the initial in memory file...\nmyio.seek(0)",
    "tag": "gzip"
  },
  {
    "question": "Using GZIP compression with Spring Boot/MVC/JavaConfig with RESTful",
    "answer": "The rest of these answers are out of date and/or over the top complicated for something that should be simple IMO (how long has gzip been around for now? longer than Java...) From the docs:\nIn application.properties 1.3+\n# 🗜️🗜️🗜️\nserver.compression.enabled=true\n# opt in to content types\nserver.compression.mime-types=application/json,application/xml,text/html,text/xml,text/plain,application/javascript,text/css\n# not worth the CPU cycles at some point, probably\nserver.compression.min-response-size=10240 \n\nIn application.properties 1.2.2 - <1.3\nserver.tomcat.compression=on\nserver.tomcat.compressableMimeTypes=application/json,application/xml,text/html,text/xml,text/plain,application/javascript,text/css\n\nOlder than 1.2.2:\n@Component\npublic class TomcatCustomizer implements TomcatConnectorCustomizer {\n\n  @Override\n  public void customize(Connector connector) {\n    connector.setProperty(\"compression\", \"on\");\n    // Add json and xml mime types, as they're not in the mimetype list by default\n    connector.setProperty(\"compressableMimeType\", \"text/html,text/xml,text/plain,application/json,application/xml\");\n  }\n}\n\nAlso note this will ONLY work if you are running embedded tomcat:\nIf you plan to deploy to a non embedded tomcat you will have to enable it in server.xml http://tomcat.apache.org/tomcat-9.0-doc/config/http.html#Standard_Implementation\nIRL Production Note:\nAlso to avoid all of this consider using a proxy/load balancer setup in front of Tomcat with nginx and/or haproxy or similar since it will handle static assets and gzip MUCH more efficiently and easily than Java/Tomcat's threading model.\nYou don't want to throw 'cat in the bath because it's busy compressing stuff instead of serving up requests (or more likely spinning up threads/eating CPU/heap sitting around waiting for database IO to occur while running up your AWS bill which is why traditional Java/Tomcat might not be a good idea to begin with depending on what you are doing but I digress...)\nrefs:\nhttps://docs.spring.io/spring-boot/docs/current-SNAPSHOT/reference/html/howto.html#how-to-enable-http-response-compression\nhttps://github.com/spring-projects/spring-boot/issues/2031",
    "tag": "gzip"
  },
  {
    "question": "How can I decompress a gzip stream with zlib?",
    "answer": "To decompress a gzip format file with zlib, call inflateInit2 with the windowBits parameter as 16+MAX_WBITS, like this:\ninflateInit2(&stream, 16+MAX_WBITS);\n\nIf you don't do this, zlib will complain about a bad stream format. By default, zlib creates streams with a zlib header, and on inflate does not recognise the different gzip header unless you tell it so. Although this is documented starting in version 1.2.1 of the zlib.h header file, it is not in the zlib manual. From the header file:\n\nwindowBits can also be greater than 15 for optional gzip decoding. Add\n     32 to windowBits to enable zlib and gzip decoding with automatic header\n     detection, or add 16 to decode only the gzip format (the zlib format will\n     return a Z_DATA_ERROR).  If a gzip stream is being decoded, strm->adler is\n     a crc32 instead of an adler32.",
    "tag": "gzip"
  },
  {
    "question": "uncompress a .txt.gz file in mac?",
    "answer": "gunzip file.txt.gz in Terminal.",
    "tag": "gzip"
  },
  {
    "question": "gzip: stdin: not in gzip format tar: Child returned status 1 tar: Error is not recoverable: exiting now",
    "answer": "First check the type of compression using the file command:\nfile name_name.tgz\n\nOutput: If the output is \" XZ compressed data\", then use tar xf <archive name> to unzip the file, e.g.\n\ntar xf archive.tar.xz\n\ntar xf archive.tar.gz\n\ntar xf archive.tar\n\ntar xf archive.tgz",
    "tag": "gzip"
  },
  {
    "question": "Transfer-Encoding: gzip vs. Content-Encoding: gzip",
    "answer": "Quoting Roy T. Fielding, one of the authors of RFC 2616:\n\nchanging content-encoding on the fly in an inconsistent manner\n  (neither \"never\" nor \"always) makes it impossible for later requests\n  regarding that content (e.g., PUT or conditional GET) to be handled\n  correctly.  This is, of course, why performing on-the-fly\n  content-encoding is a stupid idea, and why I added Transfer-Encoding\n  to HTTP as the proper way to do on-the-fly encoding without changing\n  the resource.\n\nSource: https://issues.apache.org/bugzilla/show_bug.cgi?id=39727#c31\nIn other words: Don't do on-the-fly Content-Encoding, use Transfer-Encoding instead!\nEdit: That is, unless you want to serve gzipped content to clients that only understand Content-Encoding. Which, unfortunately, seems to be most of them. But be aware that you leave the realms of the spec and might run into issues such as the one mentioned by Fielding as well as others, e.g. when caching proxies are involved.",
    "tag": "gzip"
  },
  {
    "question": "Read a gzip file in Python",
    "answer": "Try gzipping some data through the gzip libary like this...\nimport gzip\ncontent = \"Lots of content here\"\nf = gzip.open('Onlyfinnaly.log.gz', 'wb')\nf.write(content)\nf.close()\n\n... then run your code as posted ...\nimport gzip\nf=gzip.open('Onlyfinnaly.log.gz','rb')\nfile_content=f.read()\nprint file_content\n\nThis method worked for me as for some reason the gzip library fails to read some files.",
    "tag": "gzip"
  },
  {
    "question": "I want to create a script for unzip (.tar.gz) file via (Python)",
    "answer": "Why do you want to \"press\" twice to extract a .tar.gz, when you can easily do it once?  Here is a simple code to extract both .tar and .tar.gz in one go:\nimport tarfile\n\nif fname.endswith(\"tar.gz\"):\n    tar = tarfile.open(fname, \"r:gz\")\n    tar.extractall()\n    tar.close()\nelif fname.endswith(\"tar\"):\n    tar = tarfile.open(fname, \"r:\")\n    tar.extractall()\n    tar.close()",
    "tag": "gzip"
  },
  {
    "question": "How to get few lines from a .gz compressed file without uncompressing",
    "answer": "zcat(1) can be supplied by either compress(1) or by gzip(1). On your system, it appears to be compress(1) -- it is looking for a file with a .Z extension.\nSwitch to gzip -cd in place of zcat and your command should work fine:\n gzip -cd CONN.20111109.0057.gz | head\n\nExplanation\n   -c --stdout --to-stdout\n          Write output on standard output; keep original files unchanged.  If there are several input files, the output consists of a sequence of independently compressed members. To obtain better compression, concatenate all input files before compressing\n          them.\n\n   -d --decompress --uncompress\n          Decompress.",
    "tag": "gzip"
  },
  {
    "question": "How to 'minify' Javascript code",
    "answer": "DIY Minification\nNo minifier can compress properly a bad code. \nIn this example i just wanna show how much a minifier does.\nWhat you should do before you minify\nAnd regarding jQuery... i don't use jQuery.jQuery is for old browsers,it was made for compatibility reasons .. check caniuse.com, almost everything works on every browser (also ie10 is standardized now) , i think now it's just here to slow down your web application...if you like the $() you should create your own simple function.And why bother to compress your code if your clients need to download the 100kb jquery script everythime?how big is your uncompressed code? 5-6kb..? Not to talk about the tons of plugins you add to to make it easier. \nOriginal Code\nWhen you write a function you have an idea, start to write stuff and sometimes you end up with something like the following code.The code works.Now most people stop thinking and add this to a minifier and publish it.\nfunction myFunction(myNumber){\n     var myArray = new Array(myNumber);\n     var myObject = new Object();\n     var myArray2 = new Array();\n     for(var myCounter = 0 ; myCounter < myArray.length ; myCounter++){\n         myArray2.push(myCounter);\n         var myString = myCounter.toString()\n         myObject[ myString ] = ( myCounter + 1 ).toString();\n     }\n    var myContainer = new Array();\n    myContainer[0] = myArray2;\n    myContainer[1] = myObject;\n    return myContainer;\n}\n\nHere iss the minified code (i added the new lines)\nMinified using (http://javascript-minifier.com/)\nfunction myFunction(r){\n for(var n=new Array(r),t=new Object,e=new Array,a=0;a<n.length;a++){\n  e.push(a);\n  var o=a.toString();\n  t[o]=(a+1).toString()\n }\n var i=new Array;\n return i[0]=e,i[1]=t,i\n}\n\nBut are all those vars , ifs, loops & definitions necessary?\nMost of the time NO !\n\nRemove unnecessary if,loop,var\nKeep a copy of your original code\nUse the minifier\n\nOPTIONAL (increases the performance & shorter code)\n\nuse shorthand operators\nuse bitwise operators (don't use Math)\nuse a,b,c... for your temp vars\nuse the old syntax (while,for... not forEach)\nuse the function arguments as placeholder (in some cases)\nremove unneccessary \"{}\",\"()\",\";\",spaces,newlines\nUse the minifier\n\nNow if a minifier can compress the code your doing it wrong.\nNo minifier can compress properly a bad code. \nDIY\nfunction myFunction(a,b,c){\n for(b=[],c={};a--;)b[a]=a,c[a]=a+1+'';\n return[b,c]\n}\n\nIt does exactly the same thing as the codes above.\nPerformance\nhttp://jsperf.com/diyminify\nYou always need to think what you need:\nBefore you say \"Noone would write code like the one below\" go and check the first 10 questions in here ... \nHere are some common examples i see every ten minutes.\nWant a reusable condition\nif(condition=='true'){\n var isTrue=true;\n}else{\n var isTrue=false;\n}\n//same as\nvar isTrue=!!condition\n\nAlert yes only if it exists\nif(condition==true){\n var isTrue=true;\n}else{\n var isTrue=false;\n}\nif(isTrue){\n alert('yes');\n}\n//same as\n!condition||alert('yes')\n//if the condition is not true alert yes\n\nAlert yes or no\nif(condition==true){\n var isTrue=true;\n}else{\n var isTrue=false;\n}\nif(isTrue){\n alert('yes');\n}else{\n alert('no');\n}\n//same as\nalert(condition?'yes':'no')\n//if the condition is true alert yes else no\n\nConvert a number to a string or viceversa\nvar a=10;\nvar b=a.toString();\nvar c=parseFloat(b)\n//same as\nvar a=10,b,c;\nb=a+'';\nc=b*1\n\n//shorter\nvar a=10;\na+='';//String\na*=1;//Number\n\nRound a number\nvar a=10.3899845\nvar b=Math.round(a);\n//same as\nvar b=(a+.5)|0;//numbers up to 10 decimal digits (32bit)\n\nFloor a number\nvar a=10.3899845\nvar b=Math.floor(a);\n//same as\nvar b=a|0;//numbers up to 10 decimal digits (32bit)\n\nswitch case\nswitch(n)\n{\ncase 1:\n  alert('1');\n  break;\ncase 2:\n  alert('2');\n  break;\ndefault:\n  alert('3');\n}\n\n//same as\nvar a=[1,2];\nalert(a[n-1]||3);\n\n//same as\nvar a={'1':1,'2':2};\nalert(a[n]||3);\n\n//shorter\nalert([1,2][n-1]||3);\n//or\nalert([1,2][--n]||3);\n\ntry catch\nif(a&&a[b]&&a[b][c]&&a[b][c][d]&&a[b][c][d][e]){\n console.log(a[b][c][d][e]);\n}\n\n//this is probably the onle time you should use try catch\nvar x;\ntry{x=a.b.c.d.e}catch(e){}\n!x||conole.log(x);\n\nmore if\nif(a==1||a==3||a==5||a==8||a==9){\n console.log('yes')\n}else{\n console.log('no');\n}\n\nconsole.log([1,3,5,8,9].indexOf(a)!=-1?'yes':'no');\n\nbut indexOf is slow read this https://stackoverflow.com/a/30335438/2450730\nnumbers \n1000000000000\n//same as\n1e12\n\nvar oneDayInMS=1000*60*60*24;\n//same as\nvar oneDayInMS=864e5;\n\nvar a=10;\na=1+a;\na=a*2;\n//same as\na=++a*2;\n\nSome nice articles/sites i found about bitwise/shorthand:\nhttp://mudcu.be/journal/2011/11/bitwise-gems-and-other-optimizations/\nhttp://www.140byt.es/\nhttp://www.jquery4u.com/javascript/shorthand-javascript-techniques/\nThere are also many jsperf sites showing the performance of shorthand & bitwsie if you search with your favorite searchengine.\nI could go one for hours.. but i think it's enough for now. \nif you have some questions just ask.\nAnd remember\nNo minifier can compress properly a bad code.",
    "tag": "gzip"
  },
  {
    "question": "Android: HTTP communication should use \"Accept-Encoding: gzip\"",
    "answer": "You should use http headers to indicate a connection can accept gzip encoded data, e.g:\nHttpUriRequest request = new HttpGet(url);\nrequest.addHeader(\"Accept-Encoding\", \"gzip\");\n// ...\nhttpClient.execute(request);\n\nCheck response for content encoding:\nInputStream instream = response.getEntity().getContent();\nHeader contentEncoding = response.getFirstHeader(\"Content-Encoding\");\nif (contentEncoding != null && contentEncoding.getValue().equalsIgnoreCase(\"gzip\")) {\n    instream = new GZIPInputStream(instream);\n}",
    "tag": "gzip"
  },
  {
    "question": "Node.js: Gzip compression?",
    "answer": "Node v0.6.x has a stable zlib module in core now - there are some examples on how to use it server-side in the docs too.\nAn example (taken from the docs):\n// server example\n// Running a gzip operation on every request is quite expensive.\n// It would be much more efficient to cache the compressed buffer.\nvar zlib = require('zlib');\nvar http = require('http');\nvar fs = require('fs');\nhttp.createServer(function(request, response) {\n  var raw = fs.createReadStream('index.html');\n  var acceptEncoding = request.headers['accept-encoding'];\n  if (!acceptEncoding) {\n    acceptEncoding = '';\n  }\n\n  // Note: this is not a conformant accept-encoding parser.\n  // See http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.3\n  if (acceptEncoding.match(/\\bdeflate\\b/)) {\n    response.writeHead(200, { 'content-encoding': 'deflate' });\n    raw.pipe(zlib.createDeflate()).pipe(response);\n  } else if (acceptEncoding.match(/\\bgzip\\b/)) {\n    response.writeHead(200, { 'content-encoding': 'gzip' });\n    raw.pipe(zlib.createGzip()).pipe(response);\n  } else {\n    response.writeHead(200, {});\n    raw.pipe(response);\n  }\n}).listen(1337);",
    "tag": "gzip"
  },
  {
    "question": "Read lines from compressed text files",
    "answer": "Using gzip.GzipFile:\nimport gzip\n\nwith gzip.open('input.gz','rt') as f:\n    for line in f:\n        print('got line', line)\n\nNote: gzip.open(filename, mode) is an alias for gzip.GzipFile(filename, mode).\nI prefer the former, as it looks similar to with open(...) as f: used for opening uncompressed files.",
    "tag": "gzip"
  },
  {
    "question": "How do I gzip compress a string in Python?",
    "answer": "If you want to produce a complete gzip-compatible binary string, with the header etc, you could use gzip.GzipFile together with StringIO:\ntry:\n    from StringIO import StringIO  # Python 2.7\nexcept ImportError:\n    from io import StringIO  # Python 3.x\nimport gzip\nout = StringIO()\nwith gzip.GzipFile(fileobj=out, mode=\"w\") as f:\n  f.write(\"This is mike number one, isn't this a lot of fun?\")\nout.getvalue()\n\n# returns '\\x1f\\x8b\\x08\\x00\\xbd\\xbe\\xe8N\\x02\\xff\\x0b\\xc9\\xc8,V\\x00\\xa2\\xdc\\xcc\\xecT\\x85\\xbc\\xd2\\xdc\\xa4\\xd4\"\\x85\\xfc\\xbcT\\x1d\\xa0X\\x9ez\\x89B\\tH:Q!\\'\\xbfD!?M!\\xad4\\xcf\\x1e\\x00w\\xd4\\xea\\xf41\\x00\\x00\\x00'",
    "tag": "gzip"
  },
  {
    "question": "How do I enable gzip compression when using MVC3 on IIS7?",
    "answer": "You can configure compression through your web.config file as follows:\n<system.webServer>\n    <urlCompression doStaticCompression=\"true\" doDynamicCompression=\"true\" />\n</system.webServer>\n\nYou can find documentation of this configuration element at iis.net/ConfigReference. This is the equivalent of:\n\nOpening Internet Information Services (IIS Manager)\nNavigating through the tree-view on the left until you reach the virtual directory you wish to modify\nSelecting the appropriate virtual directory so that the title of the right-hand pane becomes the name of said virtual directory.\nChoosing \"Compression\" under \"IIS\" in the right-hand pane\nTicking both options and choosing \"Apply\" under \"Actions\" on the far right.\n\nNote: (As pointed out in the comments) You need to ensure that Http Dynamic Compression is installed otherwise setting doDynamicCompression=\"true\" will not have any effect. The quickest way to do this is:\n\nStart > Type optionalfeatures (this is the quickest way to get to the \"Turn Windows Features on or off\" window)\nNavigate to Internet Information Services > World Wide Web Services > Performance Features in the \"Windows Features\" treeview\nEnsure \"Dynamic Content Compression\" is ticked\nClick \"Ok\" and wait whilst Windows installs the component",
    "tag": "gzip"
  },
  {
    "question": "Is there any way to get curl to decompress a response without sending the Accept headers in the request?",
    "answer": "Probably the easiest thing to do is just use gunzip to do it:\ncurl -sH 'Accept-encoding: gzip' http://example.com/ | gunzip -\n\nOr there's also --compressed, which curl will decompress (I believe) since it knows the response is compressed. But, not sure if that meets your needs.",
    "tag": "gzip"
  },
  {
    "question": "How can I estimate the size of my gzipped script?",
    "answer": "If you're on unix - gzip -c filename.min.js | wc -c will give you a byte count of the gzipped file",
    "tag": "gzip"
  },
  {
    "question": "Extract and delete all .gz in a directory- Linux",
    "answer": "This should do it:\ngunzip *.gz",
    "tag": "gzip"
  },
  {
    "question": "Deflate compression browser compatibility and advantages over GZIP",
    "answer": "UPDATE: Browsers have been dropping support for raw deflate. zOompf has completed some very thorough research on this very topic here. Unfortunately, it appears that raw deflate is NOT safe to use.\n\nCheck http://www.vervestudios.co/projects/compression-tests/results for more results.\n\nHere are the browsers that have been tested:\n/*  Browser                       DEFLATE      ZLIB     */\n    XP Internet Explorer 6        PASS         FAIL\n    XP Internet Explorer 7        PASS         FAIL\n    XP Internet Explorer 8        PASS         FAIL\n    Vista Internet Explorer 8     PASS         FAIL\n    XP Firefox 3.6.*              PASS         PASS\n    XP Firefox 3.5.3              PASS         PASS\n    XP Firefox 3.0.14             PASS         PASS\n    Win 7 Firefox 3.6.*           PASS         PASS\n    Vista Firefox 3.6.*           PASS         PASS\n    Vista Firefox 3.5.3           PASS         PASS\n    XP Safari 3                   PASS         PASS\n    XP Safari 4                   PASS         PASS     \n    XP Chrome 3.0.195.27          PASS         PASS\n    XP Opera 9                    PASS         PASS\n    XP Opera 10                   PASS         PASS\n    XP Sea Monkey 1.1.8           PASS         PASS\n    Android 1.6 Browser (v4)*     N/A          N/A\n    OS-X Safari 4                 PASS         PASS\n    OS X Chrome 7.0.517.44        PASS         PASS\n    OS X Opera 10.63              PASS         PASS\n    iPhone 3.1 Safari             PASS         PASS\n\n* Android Sends HTTP request header \"Accept-Encoding: gzip\". Deflate is not permitted.\n\nI conclude that we can always send raw DEFLATE (when the HTTP request header \"Accept-Encoding\" contains \"deflate\") and the browser will be able to correctly interpret the encoded data. Can someone prove this wrong?\nnote: .NET's native implementation of DEFLATE (System.IO.Compression.DeflateStream) is raw DEFLATE. It also sucks. Please use zlib.net for all of your .NET deflating needs.",
    "tag": "gzip"
  },
  {
    "question": "Uncompress tar.gz file",
    "answer": "Use -C option of tar:\ntar zxvf <yourfile>.tar.gz -C /usr/src/\n\nand then, the content of the tar should be in:\n/usr/src/<yourfile>",
    "tag": "gzip"
  },
  {
    "question": "Check the total content size of a tar gz file",
    "answer": "This works for any file size:\nzcat archive.tar.gz | wc -c\n\nFor files smaller than 4Gb you could also use the -l option with gzip:\n$ gzip -l compressed.tar.gz\n     compressed        uncompressed  ratio uncompressed_name\n            132               10240  99.1% compressed.tar",
    "tag": "gzip"
  },
  {
    "question": "htaccess - How to force the client's browser to clear the cache?",
    "answer": "You can force browsers to cache something, but\nYou can't force browsers to clear their cache.\nThus the only (AMAIK) way is to use a new URL for your resources. Something like versioning.",
    "tag": "gzip"
  },
  {
    "question": "How to implement GZip compression in ASP.NET?",
    "answer": "Here is the solution for css and javascript files. Add the following code to <system.webServer> inside your web.config file:\n<configuration>\n  ...\n  <system.webserver>\n     ...\n      <httpCompression>\n        <scheme name=\"gzip\" dll=\"%Windir%\\system32\\inetsrv\\gzip.dll\"/>\n        <dynamicTypes>\n          <add mimeType=\"text/*\" enabled=\"true\"/>\n          <add mimeType=\"message/*\" enabled=\"true\"/>\n          <add mimeType=\"application/javascript\" enabled=\"true\"/>\n          <add mimeType=\"*/*\" enabled=\"false\"/>\n        </dynamicTypes>\n        <staticTypes>\n          <add mimeType=\"text/*\" enabled=\"true\"/>\n          <add mimeType=\"message/*\" enabled=\"true\"/>\n          <add mimeType=\"application/javascript\" enabled=\"true\"/>\n          <add mimeType=\"*/*\" enabled=\"false\"/>\n        </staticTypes>\n      </httpCompression>\n      <urlCompression doStaticCompression=\"true\" doDynamicCompression=\"true\"/>\n    ...\n  </system.webserver>\n  ...\n<configuration>\n\nCredit: How to GZip on ASP.NET and GoDaddy",
    "tag": "gzip"
  },
  {
    "question": "Decompress gz file using R",
    "answer": "Here is a worked example that may help illustrate what gzfile() and gzcon() are for\nfoo <- data.frame(a=LETTERS[1:3], b=rnorm(3))\nfoo\n#  a        b\n#1 A 0.586882\n#2 B 0.218608\n#3 C 1.290776\nwrite.table(foo, file=\"/tmp/foo.csv\")\nsystem(\"gzip /tmp/foo.csv\")             # being very explicit\n\nNow that the file is written, instead of implicit use of file(), use gzfile():\nread.table(gzfile(\"/tmp/foo.csv.gz\"))   \n#  a        b\n#1 A 0.586882\n#2 B 0.218608\n#3 C 1.290776\n\nThe file you point is a compressed tar archive, and as far as I know, R itself has no interface to tar archives. These are commonly used to distribute source code--as for example for R packages and R sources.",
    "tag": "gzip"
  },
  {
    "question": "Which compression method to use in PHP?",
    "answer": "All of these can be used.  There are subtle differences between the three:\n\ngzencode() uses the GZIP file format, the same as the gzip command line tool.  This file format has a header containing optional metadata, DEFLATE compressed data, and footer containing a CRC32 checksum and length check.\ngzcompress() uses the ZLIB format.  It has a shorter header serving only to identify the compression format, DEFLATE compressed data, and a footer containing an ADLER32 checksum.\ngzdeflate() uses the raw DEFLATE algorithm on its own, which is the basis for both of the other formats.\n\nAll three use the same algorithm under the hood, so they won't differ in speed or efficiency.  gzencode() and gzcompress() both add a checksum, so the integrity of the archive can be verified, which can be useful over unreliable transmission and storage methods.  If everything is stored locally and you don't need any additional metadata then gzdeflate() would suffice.  For portability I'd recommend gzencode() (GZIP format) which is probably better supported than gzcompress() (ZLIB format) among other tools.\nWhen compressing very short strings the overhead of each method becomes relevant since for very short input the overhead can comprise a significant part of the output. The overhead for each method, measured by compressing an empty string, is:\n\ngzencode('') - 20 bytes\ngzcompress('') - 8 bytes\ngzdeflate('') - 2 bytes",
    "tag": "gzip"
  },
  {
    "question": "Python 3, read/write compressed json objects from/to gzip file",
    "answer": "You have four steps of transformation here.\n\na Python data structure (nested dicts, lists, strings, numbers, booleans)\na Python string containing a serialized representation of that data structure (\"JSON\")\na list of bytes containing a representation of that string (\"UTF-8\")\na list of bytes containing a - shorter - representation of that previous byte list (\"gzip\")\n\nSo let's take these steps one by one.\nimport gzip\nimport json\n\ndata = []\nfor i in range(N):\n    uid = \"whatever%i\" % i\n    dv = [1, 2, 3]\n    data.append({\n        'what': uid,\n        'where': dv\n    })                                           # 1. data\n\njson_str = json.dumps(data) + \"\\n\"               # 2. string (i.e. JSON)\njson_bytes = json_str.encode('utf-8')            # 3. bytes (i.e. UTF-8)\n\nwith gzip.open(jsonfilename, 'w') as fout:       # 4. fewer bytes (i.e. gzip)\n    fout.write(json_bytes)                       \n\nNote that adding \"\\n\" is completely superfluous here. It does not break anything, but beyond that it has no use. I've added that only because you have it in your code sample.\nReading works exactly the other way around:\nwith gzip.open(jsonfilename, 'r') as fin:        # 4. gzip\n    json_bytes = fin.read()                      # 3. bytes (i.e. UTF-8)\n\njson_str = json_bytes.decode('utf-8')            # 2. string (i.e. JSON)\ndata = json.loads(json_str)                      # 1. data\n\nprint(data)\n\nOf course the steps can be combined:\nwith gzip.open(jsonfilename, 'w') as fout:\n    fout.write(json.dumps(data).encode('utf-8'))                       \n\nand\nwith gzip.open(jsonfilename, 'r') as fin:\n    data = json.loads(fin.read().decode('utf-8'))",
    "tag": "gzip"
  },
  {
    "question": "zlib.error: Error -3 while decompressing: incorrect header check",
    "answer": "You have this error:\nzlib.error: Error -3 while decompressing: incorrect header check\n\nWhich is most likely because you are trying to check headers that are not there, e.g. your data follows RFC 1951 (deflate compressed format) rather than RFC 1950 (zlib compressed format) or RFC 1952 (gzip compressed format). \nchoosing windowBits\nBut zlib can decompress all those formats:\n\nto (de-)compress deflate format, use wbits = -zlib.MAX_WBITS\nto (de-)compress zlib format, use wbits = zlib.MAX_WBITS\nto (de-)compress gzip format, use wbits = zlib.MAX_WBITS | 16\n\nSee documentation in http://www.zlib.net/manual.html#Advanced (section inflateInit2)\nexamples\ntest data:\n>>> deflate_compress = zlib.compressobj(9, zlib.DEFLATED, -zlib.MAX_WBITS)\n>>> zlib_compress = zlib.compressobj(9, zlib.DEFLATED, zlib.MAX_WBITS)\n>>> gzip_compress = zlib.compressobj(9, zlib.DEFLATED, zlib.MAX_WBITS | 16)\n>>> \n>>> text = '''test'''\n>>> deflate_data = deflate_compress.compress(text) + deflate_compress.flush()\n>>> zlib_data = zlib_compress.compress(text) + zlib_compress.flush()\n>>> gzip_data = gzip_compress.compress(text) + gzip_compress.flush()\n>>> \n\nobvious test for zlib:\n>>> zlib.decompress(zlib_data)\n'test'\n\ntest for deflate:\n>>> zlib.decompress(deflate_data)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nzlib.error: Error -3 while decompressing data: incorrect header check\n>>> zlib.decompress(deflate_data, -zlib.MAX_WBITS)\n'test'\n\ntest for gzip:\n>>> zlib.decompress(gzip_data)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nzlib.error: Error -3 while decompressing data: incorrect header check\n>>> zlib.decompress(gzip_data, zlib.MAX_WBITS|16)\n'test'\n\nthe data is also compatible with gzip module:\n>>> import gzip\n>>> import StringIO\n>>> fio = StringIO.StringIO(gzip_data)  # io.BytesIO for Python 3\n>>> f = gzip.GzipFile(fileobj=fio)\n>>> f.read()\n'test'\n>>> f.close()\n\nautomatic header detection (zlib or gzip)\nadding 32 to windowBits will trigger header detection\n>>> zlib.decompress(gzip_data, zlib.MAX_WBITS|32)\n'test'\n>>> zlib.decompress(zlib_data, zlib.MAX_WBITS|32)\n'test'\n\nusing gzip instead\nor you can ignore zlib and use gzip module directly; but please remember that under the hood, gzip uses zlib.\nfh = gzip.open('abc.gz', 'rb')\ncdata = fh.read()\nfh.close()",
    "tag": "gzip"
  },
  {
    "question": "How to create tar.gz archive file in Windows?",
    "answer": "tar.gz file is just a tar file that's been gzipped. Both tar and gzip are available for windows.\nIf you like GUIs (Graphical user interface), 7zip can pack with both tar and gzip.",
    "tag": "gzip"
  },
  {
    "question": "Does python urllib2 automatically uncompress gzip data fetched from webpage?",
    "answer": "How can I tell if the data at a URL is gzipped?\n\n\nThis checks if the content is gzipped and decompresses it:\nfrom StringIO import StringIO\nimport gzip\n\nrequest = urllib2.Request('http://example.com/')\nrequest.add_header('Accept-encoding', 'gzip')\nresponse = urllib2.urlopen(request)\nif response.info().get('Content-Encoding') == 'gzip':\n    buf = StringIO(response.read())\n    f = gzip.GzipFile(fileobj=buf)\n    data = f.read()\n\n\n\nDoes urllib2 automatically uncompress the data if it is gzipped? Will the data always be a string?\n\n\nNo. The urllib2 doesn't automatically uncompress the data because the 'Accept-Encoding' header is not set by the urllib2 but by you using: request.add_header('Accept-Encoding','gzip, deflate')",
    "tag": "gzip"
  },
  {
    "question": "What is gZip compression?",
    "answer": "GZip is a form of data compression -- i.e. it takes a chunk of data and makes it smaller. The original data can be restored by un-zipping the compressed file.\nIt is relevant to web apps and websites because the HTTP protocol includes the ability to gzip data that is being sent.\nThis means that, when it is in use, your bandwidth costs for serving the site will be lower because people visiting the site will be downloading smaller files.\nThere are a few caveats to using GZip, but, overall, it's usually better to use gzip than not to -- for example, it does take time and processor power to zip and unzip the files, but, typically, this is not a problem because the time it takes to do that is often less than the time that is saved by downloading a larger file. Therefore, the overall effect is time-saving despite the browser having to unzip the file.\nGZip can compress all files; it doesn't make any difference what the file type is or the encoding. Obviously, some files can be compressed more effectively than others, so the bandwidth saving will vary - text files like HTML give the best results; images are not compressed so much by gzip because they already have some compression built-in. Some files (e.g. those that are already heavily compressed like .zip files) may actually get slightly bigger when gzipped because they can't be compressed any further but gzip still needs to add its metadata to the file. But these are edge cases and don't make much difference.\nGZip across HTTP normally happens completely transparently. The end-user should be completely unaware that it is happening; the browser would do it behind the scenes for them. And from the webserver end, it is simply a matter of turning on a config setting in your web server software. From your perspective, that's really all you need to know; just set the gzip setting on your server (or ask your ISP to do it). It's quite possible it may already be active on your site without you even knowing.",
    "tag": "gzip"
  },
  {
    "question": "Compression formats with good support for random access within archives?",
    "answer": "Take a look at dictzip. It is compatible with gzip and allows coarse random access.\nAn excerpt from its man page:\n\ndictzip compresses files using the gzip(1) algorithm (LZ77) in a manner which\n  is completely compatible with the gzip file format.  An extension to the gzip\n  file format (Extra Field, described in 2.3.1.1 of RFC 1952) allows extra data\n  to be stored in the header of a compressed file.  Programs like gzip and zcat\n  will ignore this extra data. However, [dictzcat --start] will make use\n  of this data to perform  pseudo-random  access  on  the file.\n\nI have the package dictzip in Ubuntu. Or its source code is in a dictd-*.tar.gz. Its license is GPL. You are free to study it.\nUpdate:\nI improved dictzip to have no file size limit.\nMy implementation is under MIT license.",
    "tag": "gzip"
  },
  {
    "question": "What 'Content-Type' header to use when serving gzipped files?",
    "answer": "Compressed content in the response is indicated in the Content-Encoding.  The Content-Type should remain the same, that is, it should reflect the underlying media type that is compressed.\nContent-Type: application/javascript\nContent-Encoding: gzip\n\nSee sections 14.11 Content-Encoding and 3.5 Content Codings of RFC 2616 for more information.",
    "tag": "gzip"
  },
  {
    "question": "Does GZIP Compression Level Have Any Impact On Decompression",
    "answer": "Great question, and an underexposed issue. Your intuition is solid – for some compression algorithms, choosing the max level of compression can require more work from the decompressor when it's unpacked.\nLuckily, that's not true for gzip – there's no extra overhead for the client/browser to decompress more heavily compressed gzip files (e.g. choosing 9 for compression instead of 6, assuming the standard zlib codebase that most servers use). The best measure for this is decompression rate, which for present purposes is in units of MB/sec, while also monitoring overhead like memory and CPU. Simply going by decompression time is no good because the file is smaller at higher compression settings, and we're not controlling for that factor if we're only using a stopwatch.\n\ngzip decompression quickly gets asymptotic in terms of both time-to-decompress and memory usage once you get past level 6 compressed content. The time-to-decompress flatlines for levels 7, 8, and 9 in the test results linked by Marcus Müller, though that's coarse-grained data given in whole seconds.\nYou'll also notice in those results that the memory requirements for decompression are flat for all levels of compression at 0.1 MiB. That's almost unbelievable, just a degree of excellence in software that we rarely see. Mark Adler and colleagues deserve massive props for what they achieved. gzip is a very nice format.\n\nThe memory use gets at your question about overhead. There really is none. You don't gain much with level 9 in terms of browser decompression speed, but you don't lose anything.\nNow, check out these test results for a bit more texture. You'll see how the gzip decompression rate is slightly faster with level 9 compressed content than with lower levels (at level 9, decomp rate is about 0.9% faster than at level 6, for example). That is interesting and surprising. I wouldn't expect the rate to increase. That was just one set of test results – it may not hold for other scenarios (and the difference is quite small in any case).\nParting note: Precompressing static files is a good idea, but I don't recommend gzip at level 9. You'll get smaller files than gzip-9 by instead using zopfli or libdeflate. Zopfli is a well-established gzip compressor from Google. libdeflate is new but quite excellent. In my testing it consistently beats gzip-9, but still trails zopfli. You can also use 7-Zip to create gzip files, and it will consistently beat gzip-9. (In the foregoing, gzip-9 refers to using the canonical gzip or zlib application that Apache and nginx use).",
    "tag": "gzip"
  },
  {
    "question": "Apply GZIP compression to a CSV in Python Pandas",
    "answer": "Using df.to_csv() with the keyword argument compression='gzip' should produce a gzip archive. I tested it using same keyword arguments as you, and it worked.\nYou may need to upgrade pandas, as gzip was not implemented until version 0.17.1, but trying to use it on prior versions will not raise an error, and just produce a regular csv.  You can determine your current version of pandas by looking at the output of pd.__version__.",
    "tag": "gzip"
  },
  {
    "question": "Why can't browser send gzip request?",
    "answer": "The client and server have to agree on how to communicate; part of this is whether the communication can be compressed. HTTP was designed as a request/response model, and the original creation was almost certainly envisioned to always have small requests and potentially large responses. Compression is not required to implement HTTP, there are both servers and clients that don't support it.\nHTTP compression is implemented by the client saying it can support compression, and if the server sees this in the request and it supports compression it can compress the response. To compress the request the client would have to have a \"pre-request\" that actually negotiated that the request would be made compressed OR it would have to require compression as a supported encoding for ALL requests.\n* UPDATE Feb '17 *\nIt's been 8 years, but as @Phil_1984_ notes, a 3rd possible solution would be for the client and server to negotiate compression support and then use that for subsequent requests. In fact, things like HSTS work just this way with the client caching that the server expects to only speak TLS and ignore any unencrypted links. HTTP was explicitly designed to be stateless but we've moved beyond that at this point.",
    "tag": "gzip"
  },
  {
    "question": "How can I get gzip compression in IIS7 working?",
    "answer": "There was a thread on forums.iis.net about this during the iis 7 beta.  Turned out the guy didn't have the modules installed, but it sounds like you've ruled that out from your opening sentence.\nMicrosofts key advice for him was to enable failed request tracing to find out what was going wrong.  This is possibly one of the most under-appreciated features of IIS7, but certainly one of the most powerful. \n\nOpen IIS Manager.\nGo to your site, and on the actions pane (the very far right), click 'Failed Request Tracing...' under the 'Configure' section.\nClick 'enable'.  \nThen, in the features view, click 'Failed request tracing rules'. Click add, next, enter 200 for the status code, next, click finish.\n\nIf you don't see \"Failed Request Tracing\" in the actions pane, you'll need to add the feature to the server - either using the \"Add Role Services\" wizard (Health and Diagnostics\\Tracing) or through the Web Platform Installer (Products\\Server\\IIS: Tracing), and then close and re-open IIS Manager.\nNext, rerun your test. This will generate some log info for us to examine.\nLook in c:\\inetpub\\logs\\FailedReqLogFiles\\w3svcx. You will see a bunch of files named fr000xx.xml.  Open up any one of them in your browser.  (By the way, if you copy these files anywhere, make sure freb.xsl is there. Also, don't delete freb.xsl - if you do, just delete the whole directory or copy it from another location, as IIS only creates it once per folder.)\nClick the 'request details' tab and select 'complete request trace'.  Search the page for 'compress' - you should find it in several areas; once for static content, and once for dynamic content.  \nIf you don't find either of them, IIS isn't configured correctly. If you do find them, you should see them followed by a compression_success and  a compression_do.  Success is self explanatory; the 'do' indicates what it did - in my case, it showed \"OriginalSize 1462784 CompressedSize 179482\"\nSince yours isn't working, hopefully you will see something different that helps you solve the problem.\nMake sure you turn this off when you're done by disabling failed request tracing in the actions pane for your website.",
    "tag": "gzip"
  },
  {
    "question": "Compressing HTTP Post Data sent from browser",
    "answer": "Will the browser automatically gzip-encode your data for you? The short answer is no.\nThe long answer is that some user-agents can do things like this, but you definitely can't rely on it. The apache mod_deflate docs state:\n\nsome special applications actually do support request compression, for instance some WebDAV clients.\n\nSo, no, that's not going to work. You'll need to generate the appropriate HTTP request message yourself. The appropriate header in this case is Content-Encoding: gzip and NOT Content-Type: because the content itself is application/json, you're just looking to encode the entity body of your HTTP request message for transport.\nNote that you need to also add the appropriate Content-Length: header specifying the size in bytes of the message entity body after compression -OR- send your HTTP message using Transfer-Encoding: chunked and forego the content-length specification.\nOn the receiving end, you can instruct mod_deflate to use an input filter to decompress the information:\n<Location /dav-area>\nSetInputFilter DEFLATE\n</Location>\n\nThis is a bit heavy handed if you're only receiving compressed message bodies for a couple of resources. Instead, you should probably just use the client-side script to check for the Content-Encoding: gzip header and decompress the request body manually. How to do this in say, PHP, is another question entirely. If you need details for that you should post another question.",
    "tag": "gzip"
  },
  {
    "question": "Why do real-world servers prefer gzip over deflate encoding?",
    "answer": "There is some confusion about the naming between the specifications and the HTTP:\n\nDEFLATE as defined by RFC 1951 is a compressed data format.\nZLIB as defined by RFC 1950 is a compressed data format that uses the DEFLATE data format.\nGZIP as defined by RFC 1952 is a file format that uses the DEFLATE compressed data format.\n\nBut the HTTP uses a different naming:\n\n\ngzip An encoding format produced by the file compression program \"gzip\" (GNU zip) as described in RFC 1952 [25]. This format is a Lempel-Ziv coding (LZ77) with a 32 bit CRC.\n\ndeflate The \"zlib\" format defined in RFC 1950 [31] in combination with the \"deflate\" compression mechanism described in RFC 1951 [29].\n\n\n\nSo to sum up:\n\ngzip is the GZIP file format.\ndeflate is actually the ZLIB data format. (But some clients do also accept the actual DEFLATE data format for deflate.)\n\nSee also this answer on the question What's the difference between the \"gzip\" and \"deflate\" HTTP 1.1 encodings?:\n\nWhat's the difference between the \"gzip\" and \"deflate\" HTTP 1.1 encodings?\n\"gzip\" is the gzip format, and \"deflate\" is the zlib format. They should probably have called the second one \"zlib\" instead to avoid confusion with the raw deflate compressed data format. While the HTTP 1.1 RFC 2616 correctly points to the zlib specification in RFC 1950 for the \"deflate\" transfer encoding, there have been reports of servers and browsers that incorrectly produce or expect raw deflate data per the deflate specification in RFC 1951, most notably Microsoft. So even though the \"deflate\" transfer encoding using the zlib format would be the more efficient approach (and in fact exactly what the zlib format was designed for), using the \"gzip\" transfer encoding is probably more reliable due to an unfortunate choice of name on the part of the HTTP 1.1 authors.",
    "tag": "gzip"
  },
  {
    "question": "Import and insert sql.gz file into database with putty",
    "answer": "Login into your server using a shell program like putty.\nType in the following command on the command line\nzcat DB_File_Name.sql.gz | mysql -u username -p Target_DB_Name\n\nwhere\nDB_File_Name.sql.gz = full path of the sql.gz file to be imported\nusername = your mysql username\nTarget_DB_Name = database name where you want to import the database\nWhen you hit enter in the command line, it will prompt for password. Enter your MySQL password.\nYou are done!",
    "tag": "gzip"
  },
  {
    "question": "Decode gzipped web page retrieved via cURL in PHP",
    "answer": "The following command enables cURL's \"auto encoding\" mode, where it will announce to the server which encoding methods it supports (via the Accept-Encoding header), and then automatically decompress the response for you:\n// Allow cURL to use gzip compression, or any other supported encoding\n// A blank string activates 'auto' mode\ncurl_setopt($ch, CURLOPT_ENCODING , '');\n\nIf you specifically want to force the header to be Accept-Encoding: gzip you can use this command instead:\n// Allow cURL to use gzip compression, or any other supported encoding\ncurl_setopt($ch, CURLOPT_ENCODING , 'gzip');\n\nRead more in the PHP documentation: curl_setopt.\nThanks to commenters for helping improve this answer.",
    "tag": "gzip"
  },
  {
    "question": "Gzip with all cores",
    "answer": "There is an implementation of gzip that is multithreaded, pigz. Since it is compressing one file on multiple threads, it should be able to read from disk more efficiently, compared to compressing multiple files at once.",
    "tag": "gzip"
  },
  {
    "question": "How do you create a .gz file using PHP?",
    "answer": "This code does the trick\n// Name of the file we're compressing\n$file = \"test.txt\";\n\n// Name of the gz file we're creating\n$gzfile = \"test.gz\";\n\n// Open the gz file (w9 is the highest compression)\n$fp = gzopen ($gzfile, 'w9');\n\n// Compress the file\ngzwrite ($fp, file_get_contents($file));\n\n// Close the gz file and we're done\ngzclose($fp);",
    "tag": "gzip"
  },
  {
    "question": "GZip every file separately",
    "answer": "You can use gzip *\n\nNote:\n\nThis will zip each file individually and DELETE the original.\nUse -k (--keep) option to keep the original files. \nThis may not work if you have a huge number of files due to limits of the shell\nTo run gzip in parallel see @MarkSetchell's answer below.",
    "tag": "gzip"
  },
  {
    "question": "Enable GZIP for CSS and JS files on NGINX server for Magento",
    "answer": "This is an working config that I currently use in production.\nhttp://pastie.org/10870547\ngzip on;\ngzip_disable \"msie6\";\n\ngzip_comp_level 6;\ngzip_min_length 1100;\ngzip_buffers 16 8k;\ngzip_proxied any;\ngzip_types\n    text/plain\n    text/css\n    text/js\n    text/xml\n    text/javascript\n    application/javascript\n    application/json\n    application/xml\n    application/rss+xml\n    image/svg+xml;\n\nThis config was tested via tools.pingdom.com.",
    "tag": "gzip"
  },
  {
    "question": "gzip a file in Python",
    "answer": "There is a module gzip. Usage:\nExample of how to create a compressed GZIP file:\nimport gzip\ncontent = b\"Lots of content here\"\nf = gzip.open('/home/joe/file.txt.gz', 'wb')\nf.write(content)\nf.close()\n\nExample of how to GZIP compress an existing file:\nimport gzip\nf_in = open('/home/joe/file.txt')\nf_out = gzip.open('/home/joe/file.txt.gz', 'wb')\nf_out.writelines(f_in)\nf_out.close()\nf_in.close()\n\nEDIT: \nJace Browning's answer using with in Python >= 2.7 is obviously more terse and readable, so my second snippet would (and should) look like:\nimport gzip\nwith open('/home/joe/file.txt', 'rb') as f_in, gzip.open('/home/joe/file.txt.gz', 'wb') as f_out:\n    f_out.writelines(f_in)",
    "tag": "gzip"
  },
  {
    "question": "How to use curl to compare the size of the page with deflate enabled and without using it",
    "answer": "I think the only reliable way to get the size, is to actually download the file. However, curl offers a very convenient option for only outputting data of interest\n-w/--write-out <format>\n    Defines what to display on stdout after a completed and successful operation.\n\n[...]\n\nsize_download  The total amount of bytes that were downloaded.\n\nwhich means you can do something like this:\ncurl -so /dev/null \"http://www.whatsmyip.org/http-compression-test/\" -w '%{size_download}'\n\nOutput:\n8437\n\nAnd to get the compressed size:\ncurl --compressed -so /dev/null \"http://www.whatsmyip.org/http-compression-test/\" -w '%{size_download}'\n\nOutput:\n3225\n\nAfter that your comparison should be trivial.",
    "tag": "gzip"
  },
  {
    "question": "HttpWebRequest & Native GZip Compression",
    "answer": "What about the webrequest AutomaticDecompression Property available since .net 2?  Simply add:\nwebRequest.AutomaticDecompression = DecompressionMethods.GZip | DecompressionMethods.Deflate;\n\nIt also adds the gzip,deflate to the accept encoding header.\nSee http://msdn.microsoft.com/en-us/library/system.net.httpwebrequest.automaticdecompression.aspx",
    "tag": "gzip"
  },
  {
    "question": "GZip Compression On IIS 7.5 is not working",
    "answer": "After a lot of searching, I finally found what got compression working on my IIS 7.5.\nTo start with, IIS will not compress a file unless it loaded often enough.  That brings up the question \"what does IIS consider often enough?\"  Well, the defaults are 2 times every 10 seconds.  Yikes!\nThis setting can be changed in web.config, but the section needs to be unlocked first in applicationHost.config.  Here are the commands:\nFirst unlock the section:\n\nC:\\Windows\\System32\\inetsrv\\appcmd.exe unlock config\n  /section:system.webServer/serverRuntime\nUnlocked section \"system.webServer/serverRuntime\" at configuration path \"MACHINE/WEBROOT/APPHOST\".\n\nNow that is done, edit the web.config file and add the serverRuntime element:\n\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n    <configuration>\n      <system.webServer>\n        <serverRuntime frequentHitThreshold=\"1\" frequentHitTimePeriod=\"10:00:00\" />\n          ...\n\nIn this case, I set it to hit the file once in a 10 hour period.  You can adjust the values as necessary.  Here is the document that explains the serverRuntime element:\nhttp://www.iis.net/configreference/system.webserver/serverruntime\nI hope this helps get your compression working as well.\nNote: you can also set the serverRuntime element up in the applicationHost.config file, but I chose to change it in the web.config because we have a number of servers and farms with various sites, and it is easier for me to control it from this level of granularity.",
    "tag": "gzip"
  },
  {
    "question": "How to decompress Gzip string in ruby?",
    "answer": "The above method didn't work for me.\nI kept getting incorrect header check (Zlib::DataError) error.  Apparently it assumes you have a header by default, which may not always be the case.\nThe work around that I implemented was:\nrequire 'zlib'\nrequire 'stringio'\ngz = Zlib::GzipReader.new(StringIO.new(resp.body.to_s))    \nuncompressed_string = gz.read",
    "tag": "gzip"
  },
  {
    "question": "How to check if InputStream is Gzipped?",
    "answer": "It's not foolproof but it's probably the easiest and doesn't rely on any external data. Like all decent formats, GZip too begins with a magic number which can be quickly checked without reading the entire stream. \npublic static InputStream decompressStream(InputStream input) {\n     PushbackInputStream pb = new PushbackInputStream( input, 2 ); //we need a pushbackstream to look ahead\n     byte [] signature = new byte[2];\n     int len = pb.read( signature ); //read the signature\n     pb.unread( signature, 0, len ); //push back the signature to the stream\n     if( signature[ 0 ] == (byte) 0x1f && signature[ 1 ] == (byte) 0x8b ) //check if matches standard gzip magic number\n       return new GZIPInputStream( pb );\n     else \n       return pb;\n}\n\n(Source for the magic number: GZip file format specification)\nUpdate: I've just dicovered that there is also a constant called GZIP_MAGIC in GZipInputStream which contains this value, so if you really want to, you can use the lower two bytes of it.",
    "tag": "gzip"
  },
  {
    "question": "compression and decompression of string data in java",
    "answer": "This is because of \nString outStr = obj.toString(\"UTF-8\");\n\nSend the byte[] which you can get from your ByteArrayOutputStream and use it as such in your ByteArrayInputStream to construct your GZIPInputStream. Following are the changes which need to be done in your code.\nbyte[] compressed = compress(string); //In the main method\n\npublic static byte[] compress(String str) throws Exception {\n    ...\n    ...\n    return obj.toByteArray();\n}\n\npublic static String decompress(byte[] bytes) throws Exception {\n    ...\n    GZIPInputStream gis = new GZIPInputStream(new ByteArrayInputStream(bytes));\n    ...\n}",
    "tag": "gzip"
  },
  {
    "question": "How can I Zip and Unzip a string using GZIPOutputStream that is compatible with .Net?",
    "answer": "The GZIP methods:\npublic static byte[] compress(String string) throws IOException {\n    ByteArrayOutputStream os = new ByteArrayOutputStream(string.length());\n    GZIPOutputStream gos = new GZIPOutputStream(os);\n    gos.write(string.getBytes());\n    gos.close();\n    byte[] compressed = os.toByteArray();\n    os.close();\n    return compressed;\n}\n\npublic static String decompress(byte[] compressed) throws IOException {\n    final int BUFFER_SIZE = 32;\n    ByteArrayInputStream is = new ByteArrayInputStream(compressed);\n    GZIPInputStream gis = new GZIPInputStream(is, BUFFER_SIZE);\n    StringBuilder string = new StringBuilder();\n    byte[] data = new byte[BUFFER_SIZE];\n    int bytesRead;\n    while ((bytesRead = gis.read(data)) != -1) {\n        string.append(new String(data, 0, bytesRead));\n    }\n    gis.close();\n    is.close();\n    return string.toString();\n}\n\nAnd a test:\nfinal String text = \"hello\";\ntry {\n    byte[] compressed = compress(text);\n    for (byte character : compressed) {\n        Log.d(\"test\", String.valueOf(character));\n    }\n    String decompressed = decompress(compressed);\n    Log.d(\"test\", decompressed);\n} catch (IOException e) {\n    e.printStackTrace();\n}\n\n=== Update ===\nIf you need .Net compability my code has to be changed a little:\npublic static byte[] compress(String string) throws IOException {\n    byte[] blockcopy = ByteBuffer\n        .allocate(4)\n        .order(java.nio.ByteOrder.LITTLE_ENDIAN)\n        .putInt(string.length())\n        .array();\n    ByteArrayOutputStream os = new ByteArrayOutputStream(string.length());\n    GZIPOutputStream gos = new GZIPOutputStream(os);\n    gos.write(string.getBytes());\n    gos.close();\n    os.close();\n    byte[] compressed = new byte[4 + os.toByteArray().length];\n    System.arraycopy(blockcopy, 0, compressed, 0, 4);\n    System.arraycopy(os.toByteArray(), 0, compressed, 4, os.toByteArray().length);\n    return compressed;\n}\n\npublic static String decompress(byte[] compressed) throws IOException {\n    final int BUFFER_SIZE = 32;\n    ByteArrayInputStream is = new ByteArrayInputStream(compressed, 4, compressed.length - 4);\n    GZIPInputStream gis = new GZIPInputStream(is, BUFFER_SIZE);\n    StringBuilder string = new StringBuilder();\n    byte[] data = new byte[BUFFER_SIZE];\n    int bytesRead;\n    while ((bytesRead = gis.read(data)) != -1) {\n        string.append(new String(data, 0, bytesRead));\n    }\n    gis.close();\n    is.close();\n    return string.toString();\n}\n\nYou can use the same test script.",
    "tag": "gzip"
  },
  {
    "question": "How to enable gzip HTTP compression on Windows Azure dynamic content",
    "answer": "Well it took a very long time ... but I have finally solved this, and I want to post the answer for anyone else who is struggling. The solution is very simple and I've verified that it does definitely work!!\nEdit your ServiceDefinition.csdef file to contain this in the WebRole tag:\n    <Startup>\n      <Task commandLine=\"EnableCompression.cmd\" executionContext=\"elevated\" taskType=\"simple\"></Task>\n    </Startup>\n\nIn your web-role, create a text file and save it as \"EnableCompression.cmd\"\nEnableCompression.cmd should contain this:\n%windir%\\system32\\inetsrv\\appcmd set config /section:urlCompression /doDynamicCompression:True /commit:apphost\n%windir%\\system32\\inetsrv\\appcmd set config  -section:system.webServer/httpCompression /+\"dynamicTypes.[mimeType='application/json; charset=utf-8',enabled='True']\" /commit:apphost\n\n.. and that's it! Done! This enables dynamic compression for the json returned by the web-role, which I think I read somewhere has a rather odd mime type, so make sure you copy the code exactly.",
    "tag": "gzip"
  },
  {
    "question": "How can I get Apache gzip compression to work?",
    "answer": "Try this :\n####################\n# GZIP COMPRESSION #\n####################\nSetOutputFilter DEFLATE\nAddOutputFilterByType DEFLATE text/html text/css text/plain text/xml application/x-javascript application/x-httpd-php\nBrowserMatch ^Mozilla/4 gzip-only-text/html\nBrowserMatch ^Mozilla/4\\.0[678] no-gzip\nBrowserMatch \\bMSIE !no-gzip !gzip-only-text/html\nBrowserMatch \\bMSI[E] !no-gzip !gzip-only-text/html\nSetEnvIfNoCase Request_URI \\.(?:gif|jpe?g|png)$ no-gzip",
    "tag": "gzip"
  },
  {
    "question": "Easy HTTP requests with gzip/deflate compression",
    "answer": "For anyone coming across this in recent times, the request library supports gzip decompression out of the box now. Use as follows:\nrequest(\n    { method: 'GET'\n    , uri: 'http://www.google.com'\n    , gzip: true\n    }\n  , function (error, response, body) {\n      // body is the decompressed response body\n      console.log('server encoded the data as: ' + (response.headers['content-encoding'] || 'identity'))\n      console.log('the decoded data is: ' + body)\n    }\n  )\n\nFrom the github readme https://github.com/request/request\n\ngzip - If true, add an Accept-Encoding header to request compressed\n  content encodings from the server (if not already present) and decode\n  supported content encodings in the response. Note: Automatic decoding\n  of the response content is performed on the body data returned through\n  request (both through the request stream and passed to the callback\n  function) but is not performed on the response stream (available from\n  the response event) which is the unmodified http.IncomingMessage\n  object which may contain compressed data. See example below.",
    "tag": "gzip"
  },
  {
    "question": "How to Compress/Decompress tar.gz files in java",
    "answer": "I've written a wrapper for commons-compress called jarchivelib that makes it easy to extract or compress from and into File objects.\nExample code would look like this:\nFile archive = new File(\"/home/thrau/archive.tar.gz\");\nFile destination = new File(\"/home/thrau/archive/\");\n\nArchiver archiver = ArchiverFactory.createArchiver(\"tar\", \"gz\");\narchiver.extract(archive, destination);",
    "tag": "gzip"
  },
  {
    "question": "How to enable GZIP compression in IIS 7.5",
    "answer": "GZip Compression can be enabled directly through IIS.\nFirst, open up IIS, \ngo to the website you are hoping to tweak and hit the Compression page. If Gzip is not installed, you will see something like the following:\n\n“The dynamic content compression module is not installed.” We should fix this. So we go to the “Turn Windows features on or off” and select “Dynamic Content Compression” and click the OK button.\nNow if we go back to IIS, we should see that the compression page has changed. At this point we need to make sure the dynamic compression checkbox is checked and we’re good to go. Compression is enabled and our dynamic content will be Gzipped.\nTesting - Check if GZIP Compression is Enabled\nTo test whether compression is working or not, use the developer tools in Chrome or Firebug for Firefox and ensure the HTTP response header is set:\nContent-Encoding: gzip",
    "tag": "gzip"
  },
  {
    "question": "How can I check that the nginx gzip_static module is working?",
    "answer": "Use strace. First, you need to detect PID of nginx process:\n# ps ax | grep nginx\n25043 ?        Ss     0:00 nginx: master process /usr/sbin/nginx -c /etc/nginx/nginx.conf\n25044 ?        S      0:02 nginx: worker process\n\nOk, so 25044 is the worker process. Now, we trace it:\n# strace -p 25044 2>&1 | grep gz\nopen(\"/var/www/css/ymax.css.gz\", O_RDONLY|O_NONBLOCK) = 438\nopen(\"/var/www/css/patches/patch_my_layout.css.gz\", O_RDONLY|O_NONBLOCK) = -1 ENOENT (No such file or directory)\nopen(\"/var/www/yaml/core/iehacks.css.gz\", O_RDONLY|O_NONBLOCK) = -1 ENOENT (No such file or directory)\nopen(\"/var/www/js/koznazna5.js.gz\", O_RDONLY|O_NONBLOCK) = -1 ENOENT (No such file or directory)\nopen(\"/var/www/css/ymax.css.gz\", O_RDONLY|O_NONBLOCK) = 216\n\nAs you can see, it is trying to find .gz versions of files.",
    "tag": "gzip"
  },
  {
    "question": "How can I read tar.gz file using pandas read_csv with gzip compression option?",
    "answer": "df = pd.read_csv('sample.tar.gz', compression='gzip', header=0, sep=' ', quotechar='\"', error_bad_lines=False)\n\nNote: error_bad_lines=False will ignore the offending rows.",
    "tag": "gzip"
  },
  {
    "question": "How do I use GZipStream with System.IO.MemoryStream?",
    "answer": "What happens in your code is that you keep opening streams, but you never close them.\n\nIn line 2, you create a GZipStream. This stream will not write anything to the underlying stream until it feels it’s the right time. You can tell it to by closing it.\nHowever, if you close it, it will close the underlying stream (outStream) too. Therefore you can’t use mStream.Position = 0 on it.\n\nYou should always use using to ensure that all your streams get closed. Here is a variation on your code that works.\nvar inputString = \"“ ... ”\";\nbyte[] compressed;\nstring output;\n\nusing (var outStream = new MemoryStream())\n{\n    using (var tinyStream = new GZipStream(outStream, CompressionMode.Compress))\n    using (var mStream = new MemoryStream(Encoding.UTF8.GetBytes(inputString)))\n        mStream.CopyTo(tinyStream);\n\n    compressed = outStream.ToArray();\n}\n\n// “compressed” now contains the compressed string.\n// Also, all the streams are closed and the above is a self-contained operation.\n\nusing (var inStream = new MemoryStream(compressed))\nusing (var bigStream = new GZipStream(inStream, CompressionMode.Decompress))\nusing (var bigStreamOut = new MemoryStream())\n{\n    bigStream.CopyTo(bigStreamOut);\n    output = Encoding.UTF8.GetString(bigStreamOut.ToArray());\n}\n\n// “output” now contains the uncompressed string.\nConsole.WriteLine(output);",
    "tag": "gzip"
  },
  {
    "question": "Find all files containing a specific text (string) on Linux?",
    "answer": "Do the following:\ngrep -rnw '/path/to/somewhere/' -e 'pattern'\n\n\n-r or -R is recursive,\n-n is line number, and\n-w stands for match the whole word.\n-l (lower-case L) can be added to just give the file name of matching files.\n-e is the pattern used during the search\n\nAlong with these, --exclude, --include, --exclude-dir flags could be used for efficient searching:\n\nThis will only search through those files which have .c or .h extensions:\ngrep --include=\\*.{c,h} -rnw '/path/to/somewhere/' -e \"pattern\"\n\n\nThis will exclude searching all the files ending with .o extension:\ngrep --exclude=\\*.o -rnw '/path/to/somewhere/' -e \"pattern\"\n\n\nFor directories it's possible to exclude one or more directories using the --exclude-dir parameter. For example, this will exclude the dirs dir1/, dir2/ and all of them matching *.dst/:\ngrep --exclude-dir={dir1,dir2,*.dst} -rnw '/path/to/search/' -e \"pattern\"\n\n\n\nThis works very well for me, to achieve almost the same purpose like yours.\nFor more options, see man grep.",
    "tag": "find"
  },
  {
    "question": "How do I exclude a directory when using `find`?",
    "answer": "If -prune doesn't work for you, this will:\nfind -name \"*.js\" -not -path \"./directory/*\"\n\nCaveat: requires traversing all of the unwanted directories.",
    "tag": "find"
  },
  {
    "question": "How can I exclude all \"permission denied\" messages from \"find\"?",
    "answer": "Use:\nfind . 2>/dev/null > files_and_folders\n\nThis hides not just the Permission denied errors, of course, but all error messages.\nIf you really want to keep other possible errors, such as too many hops on a symlink, but not the permission denied ones, then you'd probably have to take a flying guess that you don't have many files called 'permission denied' and try:\nfind . 2>&1 | grep -v 'Permission denied' > files_and_folders\n\n\nIf you strictly want to filter just standard error, you can use the more elaborate construction:\nfind . 2>&1 > files_and_folders | grep -v 'Permission denied' >&2\n\nThe I/O redirection on the find command is: 2>&1 > files_and_folders |.\nThe pipe redirects standard output to the grep command and is applied first.  The 2>&1 sends standard error to the same place as standard output (the pipe). The > files_and_folders sends standard output (but not standard error) to a file.  The net result is that messages written to standard error are sent down the pipe and the regular output of find is written to the file.  The grep filters the standard output (you can decide how selective you want it to be, and may have to change the spelling depending on locale and O/S) and the final >&2 means that the surviving error messages (written to standard output) go to standard error once more. The final redirection could be regarded as optional at the terminal, but would be a very good idea to use it in a script so that error messages appear on standard error.\nThere are endless variations on this theme, depending on what you want to do.  This will work on any variant of Unix with any Bourne shell derivative (Bash, Korn, …) and any POSIX-compliant version of find.\nIf you wish to adapt to the specific version of find you have on your system, there may be alternative options available.  GNU find in particular has a myriad options not available in other versions — see the currently accepted answer for one such set of options.",
    "tag": "find"
  },
  {
    "question": "Find a value in a list",
    "answer": "As for your first question: \"if item is in my_list:\" is perfectly fine and should work if item equals one of the elements inside my_list. The item must exactly match an item in the list. For instance, \"abc\" and \"ABC\" do not match. Floating point values in particular may suffer from inaccuracy. For instance, 1 - 1/3 != 2/3.\nAs for your second question: There's actually several possible ways if \"finding\" things in lists.\nChecking if something is inside\nThis is the use case you describe: Checking whether something is inside a list or not. As you know, you can use the in operator for that:\n3 in [1, 2, 3] # => True\n\nFiltering a collection\nThat is, finding all elements in a sequence that meet a certain condition. You can use list comprehension or generator expressions for that:\nmatches = [x for x in lst if fulfills_some_condition(x)]\nmatches = (x for x in lst if x > 6)\n\nThe latter will return a generator which you can imagine as a sort of lazy list that will only be built as soon as you iterate through it. By the way, the first one is exactly equivalent to\nmatches = filter(fulfills_some_condition, lst)\n\nin Python 2. Here you can see higher-order functions at work. In Python 3, filter doesn't return a list, but a generator-like object.\nFinding the first occurrence\nIf you only want the first thing that matches a condition (but you don't know what it is yet), it's fine to use a for loop (possibly using the else clause as well, which is not really well-known). You can also use\nnext(x for x in lst if ...)\n\nwhich will return the first match or raise a StopIteration if none is found. Alternatively, you can use\nnext((x for x in lst if ...), [default value])\n\nFinding the location of an item\nFor lists, there's also the index method that can sometimes be useful if you want to know where a certain element is in the list:\n[1,2,3].index(2) # => 1\n[1,2,3].index(4) # => ValueError\n\nHowever, note that if you have duplicates, .index always returns the lowest index:......\n[1,2,3,2].index(2) # => 1\n\nIf there are duplicates and you want all the indexes then you can use enumerate() instead:\n[i for i,x in enumerate([1,2,3,2]) if x==2] # => [1, 3]",
    "tag": "find"
  },
  {
    "question": "How do I find files that do not contain a given string pattern?",
    "answer": "If your grep has the -L (or --files-without-match) option:\n$ grep -L \"foo\" *",
    "tag": "find"
  },
  {
    "question": "Does a \"Find in project...\" feature exist in Eclipse IDE?",
    "answer": "1. Ctrl + H \n2. Choose File Search for plain text search in workspace/selected projects \nFor specific expression searches, choose the relevant tab (such as Java Search which allows you to search for specific identifiers)\nFor whole project search:\n3. Scope (in the form section) > Enclosing project (Radio button selection).",
    "tag": "find"
  },
  {
    "question": "Find duplicate lines in a file and count how many time each line was duplicated?",
    "answer": "Assuming there is one number per line:\nsort <file> | uniq -c\n\nYou can use the more verbose --count flag too with the GNU version, e.g., on Linux:\nsort <file> | uniq --count",
    "tag": "find"
  },
  {
    "question": "How can I get a recursive full-path listing, one line per file?",
    "answer": "Use find:\nfind .\nfind /home/dreftymac\n\nIf you want files only (omit directories, devices, etc):\nfind . -type f\nfind /home/dreftymac -type f",
    "tag": "find"
  },
  {
    "question": "find -exec with multiple commands",
    "answer": "find accepts multiple -exec portions to the command. For example:\nfind . -name \"*.txt\" -exec echo {} \\; -exec grep banana {} \\;\n\nNote that in this case the second command will only run if the first one returns successfully, as mentioned by @Caleb. If you want both commands to run regardless of their success or failure, you could use this construct:\nfind . -name \"*.txt\" \\( -exec echo {} \\; -o -exec true \\; \\) -exec grep banana {} \\;",
    "tag": "find"
  },
  {
    "question": "Find a file by name in Visual Studio Code",
    "answer": "When you have opened a folder in a workspace you can do Ctrl+P (Cmd+P on Mac) and start typing the filename, or extension to filter the list of filenames\nif you have:\n\nplugin.ts\npage.css\nplugger.ts\n\nYou can type css and press enter and it will open the page.css. If you type .ts the list is filtered and contains two items.",
    "tag": "find"
  },
  {
    "question": "Using find to locate files that match one of multiple patterns",
    "answer": "Use -o, which means \"or\":\nfind Documents \\( -name \"*.py\" -o -name \"*.html\" \\)\n\n\nYou'd need to build that command line programmatically, which isn't that easy.\nAre you using bash (or Cygwin on Windows)?  If you are, you should be able to do this:\nls **/*.py **/*.html\n\nwhich might be easier to build programmatically.",
    "tag": "find"
  },
  {
    "question": "How to use regex with find command?",
    "answer": "find . -regextype sed -regex \".*/[a-f0-9\\-]\\{36\\}\\.jpg\"\n\nNote that you need to specify .*/ in the beginning because find matches the whole path.\nExample:\nsusam@nifty:~/so$ find . -name \"*.jpg\"\n./foo-111.jpg\n./test/81397018-b84a-11e0-9d2a-001b77dc0bed.jpg\n./81397018-b84a-11e0-9d2a-001b77dc0bed.jpg\nsusam@nifty:~/so$ \nsusam@nifty:~/so$ find . -regextype sed -regex \".*/[a-f0-9\\-]\\{36\\}\\.jpg\"\n./test/81397018-b84a-11e0-9d2a-001b77dc0bed.jpg\n./81397018-b84a-11e0-9d2a-001b77dc0bed.jpg\n\nMy version of find:\n$ find --version\nfind (GNU findutils) 4.4.2\nCopyright (C) 2007 Free Software Foundation, Inc.\nLicense GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>\nThis is free software: you are free to change and redistribute it.\nThere is NO WARRANTY, to the extent permitted by law.\n\nWritten by Eric B. Decker, James Youngman, and Kevin Dalley.\nBuilt using GNU gnulib version e5573b1bad88bfabcda181b9e0125fb0c52b7d3b\nFeatures enabled: D_TYPE O_NOFOLLOW(enabled) LEAF_OPTIMISATION FTS() CBO(level=0) \nsusam@nifty:~/so$ \nsusam@nifty:~/so$ find . -regextype foo -regex \".*/[a-f0-9\\-]\\{36\\}\\.jpg\"\nfind: Unknown regular expression type `foo'; valid types are `findutils-default', `awk', `egrep', `ed', `emacs', `gnu-awk', `grep', `posix-awk', `posix-basic', `posix-egrep', `posix-extended', `posix-minimal-basic', `sed'.",
    "tag": "find"
  },
  {
    "question": "How to loop through file names returned by find?",
    "answer": "TL;DR: If you're just here for the most correct answer, you probably want my personal preference (see the bottom of this post):\n# execute `process` once for each file\nfind . -name '*.txt' -exec process {} \\;\n\nIf you have time, read through the rest to see several different ways and the problems with most of them.\n\nThe full answer:\nThe best way depends on what you want to do, but here are a few options. As long as no file or folder in the subtree has whitespace in its name, you can just loop over the files:\nfor i in $x; do # Not recommended, will break on whitespace\n    process \"$i\"\ndone\n\nMarginally better, you can cut out the temporary variable x:\nfor i in $(find . -name \\*.txt); do # Not recommended, will break on whitespace\n    process \"$i\"\ndone\n\nIt is much better to glob when you can. White-space safe, for files in the current directory:\nfor i in *.txt; do # Whitespace-safe but not recursive.\n    process \"$i\"\ndone\n\nBy enabling the globstar option, you can glob all matching files in this directory and all subdirectories:\n# Make sure globstar is enabled\nshopt -s globstar\nfor i in **/*.txt; do # Whitespace-safe and recursive\n    process \"$i\"\ndone\n\nIn some cases, e.g. if the file names are already in a file, you may need to use read:\n# IFS= makes sure it doesn't trim leading and trailing whitespace\n# -r prevents interpretation of \\ escapes.\nwhile IFS= read -r line; do # Whitespace-safe EXCEPT newlines\n    process \"$line\"\ndone < filename\n\nread can be used safely in combination with find by setting the delimiter appropriately:\nfind . -name '*.txt' -print0 | \n    while IFS= read -r -d '' line; do \n        process \"$line\"\n    done\n\nFor more complex searches, you will probably want to use find, either with its -exec option or with -print0 | xargs -0:\n# execute `process` once for each file\nfind . -name \\*.txt -exec process {} \\;\n\n# execute `process` once with all the files as arguments*:\nfind . -name \\*.txt -exec process {} +\n\n# using xargs*\nfind . -name \\*.txt -print0 | xargs -0 process\n\n# using xargs with arguments after each filename (implies one run per filename)\nfind . -name \\*.txt -print0 | xargs -0 -I{} process {} argument\n\nfind can also cd into each file's directory before running a command by using -execdir instead of -exec, and can be made interactive (prompt before running the command for each file) using -ok instead of -exec (or -okdir instead of -execdir).\n*: Technically, both find and xargs (by default) will run the command with as many arguments as they can fit on the command line, as many times as it takes to get through all the files. In practice, unless you have a very large number of files it won't matter, and if you exceed the length but need them all on the same command line, you're SOL find a different way.",
    "tag": "find"
  },
  {
    "question": "Make xargs handle filenames that contain spaces",
    "answer": "The xargs command takes white space characters (tabs, spaces, new lines) as delimiters.\nYou can narrow it down only for the new line characters ('\\n') with -d option like this:\nls *.mp3 | xargs -d '\\n' mplayer\n\nIt works only with GNU xargs.\nFor MacOS:\nls *.mp3 | tr \\\\n \\\\0 | xargs -0 mplayer\n\nThe more simplistic and practically useful approach (when don't need to process the filenames further):\nmplayer *.mp3",
    "tag": "find"
  },
  {
    "question": "Fast way of finding lines in one file that are not in another?",
    "answer": "The comm command (short for \"common\") may be useful comm - compare two sorted files line by line\n#find lines only in file1\ncomm -23 file1 file2 \n\n#find lines only in file2\ncomm -13 file1 file2 \n\n#find lines common to both files\ncomm -12 file1 file2 \n\nThe man file is actually quite readable for this.",
    "tag": "find"
  },
  {
    "question": "How can I find elements by text content with jQuery?",
    "answer": "You can use the :contains selector to get elements based on their content.\nDemo here\n\n\n$('div:contains(\"test\")').css('background-color', 'red');\n<div>This is a test</div>\r\n<div>Another Div</div>\r\n\r\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/1.12.4/jquery.min.js\"></script>",
    "tag": "find"
  },
  {
    "question": "How do I find all of the symlinks in a directory tree?",
    "answer": "This will recursively traverse the /path/to/folder directory and list only the symbolic links:\nls -lR /path/to/folder | grep '^l'\n\nIf your intention is to follow the symbolic links too, you should use your find command but you should include the -L option; in fact the find man page says:\n   -L     Follow symbolic links.  When find examines or prints information\n          about files, the information used shall be taken from the  prop‐\n          erties  of  the file to which the link points, not from the link\n          itself (unless it is a broken symbolic link or find is unable to\n          examine  the file to which the link points).  Use of this option\n          implies -noleaf.  If you later use the -P option,  -noleaf  will\n          still  be  in  effect.   If -L is in effect and find discovers a\n          symbolic link to a subdirectory during its search, the subdirec‐\n          tory pointed to by the symbolic link will be searched.\n\n          When the -L option is in effect, the -type predicate will always\n          match against the type of the file that a symbolic  link  points\n          to rather than the link itself (unless the symbolic link is bro‐\n          ken).  Using -L causes the -lname and -ilname predicates  always\n          to return false.\n\nThen try this:\nfind -L /var/www/ -type l\n\nThis will probably work: I found in the find man page this diamond: if you are using the -type option you have to change it to the -xtype option:\n          l      symbolic link; this is never true if the -L option or the\n                 -follow option is in effect, unless the symbolic link  is\n                 broken.  If you want to search for symbolic links when -L\n                 is in effect, use -xtype.\n\nThen:\nfind -L /var/www/ -xtype l",
    "tag": "find"
  },
  {
    "question": "How to only get file name with Linux 'find'?",
    "answer": "In GNU find you can use -printf parameter for that, e.g.:\nfind /dir1 -type f -printf \"%f\\n\"",
    "tag": "find"
  },
  {
    "question": "Find the files that have been changed in last 24 hours",
    "answer": "To find all files modified in the last 24 hours (last full day) in a particular specific directory and its sub-directories:\nfind /directory_path -mtime -1 -ls\n\nShould be to your liking\nThe - before 1 is important - it means anything changed one day or less ago.\nA + before 1 would instead mean anything changed at least one day ago, while having nothing before the 1 would have meant it was changed exacted one day ago, no more, no less.",
    "tag": "find"
  },
  {
    "question": "Error when using 'sed' with 'find' command on OS X: \"invalid command code .\"",
    "answer": "If you are on a OS X, this probably has nothing to do with the sed command. On the OSX version of sed, the -i option expects an extension argument so your command is actually parsed as the extension argument and the file path is interpreted as the command code.\nTry adding the -e argument explicitly and giving '' as argument to -i:\nfind ./ -type f -exec sed -i '' -e \"s/192.168.20.1/new.domain.com/\" {} \\;\n\nSee this.",
    "tag": "find"
  },
  {
    "question": "How to check if a value exists in a dictionary?",
    "answer": ">>> d = {'1': 'one', '3': 'three', '2': 'two', '5': 'five', '4': 'four'}\n>>> 'one' in d.values()\nTrue\n\nOut of curiosity, some comparative timing:\n>>> T(lambda : 'one' in d.itervalues()).repeat()\n[0.28107285499572754, 0.29107213020324707, 0.27941107749938965]\n>>> T(lambda : 'one' in d.values()).repeat()\n[0.38303399085998535, 0.37257885932922363, 0.37096405029296875]\n>>> T(lambda : 'one' in d.viewvalues()).repeat()\n[0.32004380226135254, 0.31716084480285645, 0.3171098232269287]\n>>> T(lambda : 'four' in d.itervalues()).repeat()\n[0.41178202629089355, 0.3959040641784668, 0.3970959186553955]\n>>> T(lambda : 'four' in d.values()).repeat()\n[0.4631338119506836, 0.43541407585144043, 0.4359898567199707]\n>>> T(lambda : 'four' in d.viewvalues()).repeat()\n[0.43414998054504395, 0.4213531017303467, 0.41684913635253906]\n\nThe reason is that each of the above returns a different type of object, which may or may not be well suited for lookup operations:\n>>> type(d.viewvalues())\n<type 'dict_values'>\n>>> type(d.values())\n<type 'list'>\n>>> type(d.itervalues())\n<type 'dictionary-valueiterator'>",
    "tag": "find"
  },
  {
    "question": "find: missing argument to -exec",
    "answer": "A -exec command must be terminated with a ; (so you usually need to type \\; or ';' to avoid interpretion by the shell) or a +. The difference is that with ;, the command is called once per file, with +, it is called just as few times as possible (usually once, but there is a maximum length for a command line, so it might be split up) with all filenames. See this example:\n$ cat /tmp/echoargs\n#!/bin/sh\necho $1 - $2 - $3\n$ find /tmp/foo -exec /tmp/echoargs {} \\;\n/tmp/foo - -\n/tmp/foo/one - -\n/tmp/foo/two - -\n$ find /tmp/foo -exec /tmp/echoargs {} +\n/tmp/foo - /tmp/foo/one - /tmp/foo/two\n\nYour command has two errors:\nFirst, you use {};, but the ; must be a parameter of its own.\nSecond, the command ends at the &&. You specified “run find, and if that was successful, remove the file named {};.“. If you want to use shell stuff in the -exec command, you need to explicitly run it in a shell, such as -exec sh -c 'ffmpeg ... && rm'.\nHowever you should not add the {} inside the bash command, it will produce problems when there are special characters. Instead, you can pass additional parameters to the shell after -c command_string (see man sh):\n$ ls\n$(echo damn.)\n$ find * -exec sh -c 'echo \"{}\"' \\;\ndamn.\n$ find * -exec sh -c 'echo \"$1\"' - {} \\;\n$(echo damn.)\n\nYou see the $ thing is evaluated by the shell in the first example. Imagine there was a file called $(rm -rf /) :-)\n(Side note: The - is not needed, but the first variable after the command is assigned to the variable $0, which is a special variable normally containing the name of the program being run and setting that to a parameter is a little unclean, though it won't cause any harm here probably, so we set that to just - and start with $1.)\nSo your command could be something like\nfind -exec bash -c 'ffmpeg -i \"$1\" -sameq \"$1\".mp3 && rm \"$1\".mp3' - {} \\;\n\nBut there is a better way. find supports and and or, so you may do stuff like find -name foo -or -name bar. But that also works with -exec, which evaluates to true if the command exits successfully, and to false if not. See this example:\n$ ls\nfalse  true\n$ find * -exec {} \\; -and -print\ntrue\n\nIt only runs the print if the command was successfully, which it did for true but not for false.\nSo you can use two exec statements chained with an -and, and it will only execute the latter if the former was run successfully.",
    "tag": "find"
  },
  {
    "question": "How to use 'find' to search for files created on a specific date?",
    "answer": "As pointed out by Max, you can't, but checking files modified or accessed is not all that hard.  I wrote a tutorial about this, as late as today. The essence of which is to use -newerXY and ! -newerXY:\nExample: To find all files modified on the 7th of June, 2007:\n$ find . -type f -newermt 2007-06-07 ! -newermt 2007-06-08\n\nTo find all files accessed on the 29th of september, 2008:\n$ find . -type f -newerat 2008-09-29 ! -newerat 2008-09-30\n\nOr, files which had their permission changed on the same day:\n$ find . -type f -newerct 2008-09-29 ! -newerct 2008-09-30\n\nIf you don't change permissions on the file, 'c' would normally correspond to the creation date, though.",
    "tag": "find"
  },
  {
    "question": "\"find: paths must precede expression:\" How do I specify a recursive search that also finds files in the current directory?",
    "answer": "Try putting it in quotes -- you're running into the shell's wildcard expansion, so what you're acually passing to find will look like:\nfind . -name bobtest.c cattest.c snowtest.c\n\n...causing the syntax error. So try this instead:\nfind . -name '*test.c'\n\nNote the single quotes around your file expression -- these will stop the shell (bash) expanding your wildcards.",
    "tag": "find"
  },
  {
    "question": "How can I find WPF controls by name or type?",
    "answer": "I combined the template format used by John Myczek and Tri Q's algorithm above to create a findChild Algorithm that can be used on any parent. Keep in mind that recursively searching a tree downwards could be a lengthy process. I've only spot-checked this on a WPF application, please comment on any errors you might find and I'll correct my code.\nWPF Snoop is a useful tool in looking at the visual tree - I'd strongly recommend using it while testing or using this algorithm to check your work.\nThere is a small error in Tri Q's Algorithm. After the child is found, if childrenCount is > 1 and we iterate again we can overwrite the properly found child. Therefore I added a if (foundChild != null) break; into my code to deal with this condition.\n/// <summary>\n/// Finds a Child of a given item in the visual tree. \n/// </summary>\n/// <param name=\"parent\">A direct parent of the queried item.</param>\n/// <typeparam name=\"T\">The type of the queried item.</typeparam>\n/// <param name=\"childName\">x:Name or Name of child. </param>\n/// <returns>The first parent item that matches the submitted type parameter or null if not found</returns> \npublic static T FindChild<T>(DependencyObject parent, string childName)\n   where T : DependencyObject\n{    \n  // Confirm parent and childName are valid. \n  if (parent == null) return null;\n\n  T foundChild = null;\n\n  int childrenCount = VisualTreeHelper.GetChildrenCount(parent);\n  for (int i = 0; i < childrenCount; i++)\n  {\n    var child = VisualTreeHelper.GetChild(parent, i);\n    // If the child is not of the request child type child\n    T childType = child as T;\n    if (childType == null)\n    {\n      // recursively drill down the tree\n      foundChild = FindChild<T>(child, childName);\n      \n      // If the child is found, break so we do not overwrite the found child. \n      if (foundChild != null) break;\n    }\n    else if (!string.IsNullOrEmpty(childName))\n    {\n      var frameworkElement = child as FrameworkElement;\n      // If the child's name is set for search\n      if (frameworkElement != null && frameworkElement.Name == childName)\n      {\n        // if the child's name is of the request name\n        foundChild = (T)child;\n        break;\n      }\n    }\n    else\n    {\n      // child element found.\n      foundChild = (T)child;\n      break;\n    }\n  }\n  \n  return foundChild;\n}\n\nCall it like this:\nTextBox foundTextBox = \n   UIHelper.FindChild<TextBox>(Application.Current.MainWindow, \"myTextBoxName\");\n\nNote Application.Current.MainWindow can be any parent window.",
    "tag": "find"
  },
  {
    "question": "grep without showing path/file:line",
    "answer": "No need to find. If you are just looking for a pattern within a specific directory, this should suffice:\ngrep -h FOO /your/path/*.bar\n\nWhere -h is the parameter to hide the filename, as from man grep:\n\n-h, --no-filename\nSuppress  the  prefixing of file names on output.  This is the default\nwhen  there is only one file (or only standard input) to search.\n\nNote that you were using\n\n-H, --with-filename\nPrint the file name for each match.  This is the default when there is\nmore than one file to search.",
    "tag": "find"
  },
  {
    "question": "find without recursion",
    "answer": "I think you'll get what you want with the -maxdepth 1 option, based on your current command structure. If not, you can try looking at the man page for find.\nRelevant entry (for convenience's sake):\n-maxdepth levels\n          Descend at most levels (a non-negative integer) levels of direc-\n          tories below the command line arguments.   `-maxdepth  0'  means\n          only  apply the tests and actions to the command line arguments.\n\nYour options basically are:\n# Do NOT show hidden files (beginning with \".\", i.e., .*):\nfind DirsRoot/* -maxdepth 0 -type f\n\nOr:\n#  DO show hidden files:\nfind DirsRoot/ -maxdepth 1 -type f",
    "tag": "find"
  },
  {
    "question": "find filenames NOT ending in specific extensions on Unix?",
    "answer": "Or without ( and the need to escape it:\nfind . -not -name \"*.exe\" -not -name \"*.dll\"\n\nand to also exclude the listing of directories\nfind . -not -name \"*.exe\" -not -name \"*.dll\" -not -type d\n\nor in positive logic ;-)\nfind . -not -name \"*.exe\" -not -name \"*.dll\" -type f",
    "tag": "find"
  },
  {
    "question": "How to recursively find the latest modified file in a directory?",
    "answer": "find . -type f -printf '%T@ %p\\n' \\\n| sort -n | tail -1 | cut -f2- -d\" \"\n\nFor a huge tree, it might be hard for sort to keep everything in memory.\n%T@ gives you the modification time like a unix timestamp, sort -n sorts numerically, tail -1 takes the last line (highest timestamp), cut -f2 -d\" \" cuts away the first field (the timestamp) from the output.\nEdit: Just as -printf is probably GNU-only, ajreals usage of stat -c is too. Although it is possible to do the same on BSD, the options for formatting is different (-f \"%m %N\" it would seem)\nAnd I missed the part of plural; if you want more then the latest file, just bump up the tail argument.",
    "tag": "find"
  },
  {
    "question": "How to use '-prune' option of 'find' in sh?",
    "answer": "The thing I'd found confusing about -prune is that it's an action (like -print), not a test (like -name). It alters the \"to-do\" list, but always returns true.\nThe general pattern for using -prune is this:\nfind [path] [conditions to prune] -prune -o \\\n            [your usual conditions] [actions to perform]\n\nYou pretty much always want the -o (logical OR) immediately after -prune, because that first part of the test (up to and including -prune) will return false for the stuff you actually want (ie: the stuff you don't want to prune out).\nHere's an example:\nfind . -name .snapshot -prune -o -name '*.foo' -print\n\nThis will find the \"*.foo\" files that aren't under \".snapshot\" directories. In this example, -name .snapshot makes up the [conditions to prune], and -name '*.foo' -print is [your usual conditions] and [actions to perform].\nImportant notes:\n\nIf all you want to do is print the results you might be used to leaving out the -print action. You generally don't want to do that when using -prune.\nThe default behavior of find is to \"and\" the entire expression with the -print action if there are no actions other than -prune (ironically) at the end. That means that writing this:\n find . -name .snapshot -prune -o -name '*.foo'              # DON'T DO THIS\n\nis equivalent to writing this:\n find . \\( -name .snapshot -prune -o -name '*.foo' \\) -print # DON'T DO THIS\n\nwhich means that it'll also print out the name of the directory you're pruning, which usually isn't what you want. Instead it's better to explicitly specify the -print action if that's what you want:\n find . -name .snapshot -prune -o -name '*.foo' -print       # DO THIS\n\n\nIf your \"usual condition\" happens to match files that also match your prune condition, those files will not be included in the output. The way to fix this is to add a -type d predicate to your prune condition.\nFor example, suppose we wanted to prune out any directory that started with .git (this is admittedly somewhat contrived -- normally you only need to remove the thing named exactly .git), but other than that wanted to see all files, including files like .gitignore. You might try this:\nfind . -name '.git*' -prune -o -type f -print               # DON'T DO THIS\n\nThis would not include .gitignore in the output. Here's the fixed version:\nfind . -name '.git*' -type d -prune -o -type f -print       # DO THIS\n\n\n\nExtra tip: if you're using the GNU version of find, the texinfo page for find has a more detailed explanation than its manpage (as is true for most GNU utilities).",
    "tag": "find"
  },
  {
    "question": "How to exclude this / current / dot folder from find \"type d\"",
    "answer": "Not only the recursion depth of find can be controlled by the -maxdepth parameter, the depth can also be limited from “top” using the corresponding -mindepth parameter. So what one actually needs is:\nfind . -mindepth 1 -type d",
    "tag": "find"
  },
  {
    "question": "'find -exec' a shell function in Linux",
    "answer": "Since only the shell knows how to run shell functions, you have to run a shell to run a function.  You also need to mark your function for export with export -f, otherwise the subshell won't inherit them:\nexport -f dosomething\nfind . -exec bash -c 'dosomething \"$0\"' {} \\;",
    "tag": "find"
  },
  {
    "question": "MongoDB Show all contents from all collections",
    "answer": "Once you are in terminal/command line, access the database/collection you want to use as follows:\nshow dbs\nuse <db name>\nshow collections\n\nchoose your collection and type the following to see all contents of that collection:\ndb.collectionName.find()\n\nMore info here on the MongoDB Shell (mongosh) documentation.",
    "tag": "find"
  },
  {
    "question": "Visual Studio Clicking Find Results Opens Code in Wrong Window",
    "answer": "Click Window → Reset Window Layout",
    "tag": "find"
  },
  {
    "question": "How to pipe list of files returned by find command to cat to view all the files",
    "answer": "Piping to another process (although this won't accomplish what you said you are trying to do):\n command1 | command2\n\nThis will send the output of command1 as the input of command2.\n\n-exec on a find (this will do what you want to do, but it's specific to find):\n find . -name '*.foo' -exec cat {} \\;\n\nEverything between find and -exec are the find predicates you were already using. {} will substitute the particular file you found into the command (cat {} in this case); the \\; is to end the -exec command.\n\nSend output of one process as command line arguments to another process:\n command2 `command1`\n\nFor example:\n cat `find . -name '*.foo' -print`\n\nNote these are backquotes not regular quotes (they are under the tilde ~ on my keyboard).\nThis will send the output of command1 into command2 as command line arguments. It's called command substitution. Note that file names containing spaces (newlines, etc) will be broken into separate arguments, though.",
    "tag": "find"
  },
  {
    "question": "Chmod recursively",
    "answer": "You can use chmod with the X mode letter (the capital X) to set the executable flag only for directories.\nIn the example below, the executable flag is cleared and then set for all directories recursively:\n~$ mkdir foo\n~$ mkdir foo/bar\n~$ mkdir foo/baz\n~$ touch foo/x\n~$ touch foo/y\n\n~$ chmod -R go-X foo \n~$ ls -l foo\ntotal 8\ndrwxrw-r-- 2 wq wq 4096 Nov 14 15:31 bar\ndrwxrw-r-- 2 wq wq 4096 Nov 14 15:31 baz\n-rw-rw-r-- 1 wq wq    0 Nov 14 15:31 x\n-rw-rw-r-- 1 wq wq    0 Nov 14 15:31 y\n\n~$ chmod -R go+X foo \n~$ ls -l foo\ntotal 8\ndrwxrwxr-x 2 wq wq 4096 Nov 14 15:31 bar\ndrwxrwxr-x 2 wq wq 4096 Nov 14 15:31 baz\n-rw-rw-r-- 1 wq wq    0 Nov 14 15:31 x\n-rw-rw-r-- 1 wq wq    0 Nov 14 15:31 y\n\nA bit of explanation:\n\nchmod -x foo - clear the eXecutable flag for foo\nchmod +x foo - set the eXecutable flag for foo\nchmod go+x foo - same as above, but set the flag only for Group and Other users, don't touch the User (owner) permission\nchmod go+X foo - same as above, but apply only to directories, don't touch files\nchmod -R go+X foo - same as above, but do this Recursively for all subdirectories of foo",
    "tag": "find"
  },
  {
    "question": "Find all files with a filename beginning with a specified string?",
    "answer": "Use find with a wildcard:\nfind . -name 'mystring*'",
    "tag": "find"
  },
  {
    "question": "Using semicolon (;) vs plus (+) with exec in find",
    "answer": "This might be best illustrated with an example. Let's say that find turns up these files:\nfile1\nfile2\nfile3\n\n\nUsing -exec with a semicolon (find . -exec ls '{}' \\;), will execute\nls file1\nls file2\nls file3\n\nBut if you use a plus sign instead (find . -exec ls '{}' \\+), as many  filenames as possible are passed as arguments to a single command:\nls file1 file2 file3\n\nThe number of filenames is only limited by the system's maximum command line length. If the command exceeds this length, the command will be called multiple times.",
    "tag": "find"
  },
  {
    "question": "How can I get `find` to ignore .svn directories?",
    "answer": "why not just \nfind . -not -iwholename '*.svn*'\n\nThe -not predicate negates everything that has .svn anywhere in the path.\nSo in your case it would be\nfind -not -iwholename '*.svn*' -name 'messages.*' -exec grep -Iw uint {} + \\;",
    "tag": "find"
  },
  {
    "question": "How to go to each directory and execute a command?",
    "answer": "This answer posted by Todd helped me.\nfind . -maxdepth 1 -type d \\( ! -name . \\) -exec bash -c \"cd '{}' && pwd\" \\;\n\nThe \\( ! -name . \\)  avoids executing the command in current directory.",
    "tag": "find"
  },
  {
    "question": "How can I check if character in a string is a letter? (Python)",
    "answer": "You can use str.isalpha().\nFor example:\ns = 'a123b'\n\nfor char in s:\n    print(char, char.isalpha())\n\nOutput:\na True\n1 False\n2 False\n3 False\nb True",
    "tag": "find"
  },
  {
    "question": "Search for executable files using find command",
    "answer": "On GNU versions of find you can use -executable:\nfind . -type f -executable -print\n\nFor BSD (and MacOS) versions of find, you can use -perm with + and an octal mask:\nfind . -type f -perm +111 -print\n\nIn this context \"+\" means \"any of these bits are set\" and 111 is the execute bits.\nNote that this is not identical to the -executable predicate in GNU find. In particular, -executable tests that the file can be executed by the current user, while -perm +111 just tests if any execute permissions are set.\nOlder versions of GNU find also support the -perm +111 syntax, but as of 4.5.12 this syntax is no longer supported. Instead, you can use -perm /111 to get this behavior.",
    "tag": "find"
  },
  {
    "question": "How to use find command to find all files with extensions from list?",
    "answer": "find /path/to -regex \".*\\.\\(jpg\\|gif\\|png\\|jpeg\\)\" > log",
    "tag": "find"
  },
  {
    "question": "Find and copy files",
    "answer": "If your intent is to copy the found files into /home/shantanu/tosend, you have the order of the arguments to cp reversed:\nfind /home/shantanu/processed/ -name '*2011*.xml' -exec cp \"{}\" /home/shantanu/tosend  \\;\n\nPlease note: the find command uses {} as placeholder for the matched file.",
    "tag": "find"
  },
  {
    "question": "Find files containing a given text",
    "answer": "egrep -ir --include=*.{php,html,js} \"(document.cookie|setcookie)\" .\n\nThe r flag means to search recursively (search subdirectories). The i flag means case insensitive.\nIf you just want file names add the l (lowercase L) flag:\negrep -lir --include=*.{php,html,js} \"(document.cookie|setcookie)\" .",
    "tag": "find"
  },
  {
    "question": "Python ElementTree module: How to ignore the namespace of XML files to locate matching element when using the method \"find\", \"findall\"",
    "answer": "Instead of modifying the XML document itself, it's best to parse it and then modify the tags in the result. This way you can handle multiple namespaces and namespace aliases:\nfrom io import StringIO  # for Python 2 import from StringIO instead\nimport xml.etree.ElementTree as ET\n\n# instead of ET.fromstring(xml)\nit = ET.iterparse(StringIO(xml))\nfor _, el in it:\n    _, _, el.tag = el.tag.rpartition('}') # strip ns\nroot = it.root\n\nThis is based on the discussion here.",
    "tag": "find"
  },
  {
    "question": "Command line: piping find results to rm",
    "answer": "You are actually piping rm's output to the input of find. What you want is to use the output of find as arguments to rm:\nfind -type f -name '*.sql' -mtime +15 | xargs rm\n\nxargs is the command that \"converts\" its standard input into arguments of another program, or, as they more accurately put it on the man page,\n\nbuild and execute command lines from standard input\n\nNote that if file names can contain whitespace characters, you should correct for that:\nfind -type f -name '*.sql' -mtime +15 -print0 | xargs -0 rm\n\nBut actually, find has a shortcut for this: the -delete option:\nfind -type f -name '*.sql' -mtime +15 -delete\n\nPlease be aware of the following warnings in man find:\n\n  Warnings:  Don't  forget that the find command line is evaluated\n  as an expression, so putting -delete first will make find try to\n  delete everything below the starting points you specified.  When\n  testing a find command line that you later intend  to  use  with\n  -delete,  you should explicitly specify -depth in order to avoid\n  later surprises.  Because -delete  implies  -depth,  you  cannot\n  usefully use -prune and -delete together.\n\nP.S. Note that piping directly to rm isn't an option, because rm doesn't expect filenames on standard input. What you are currently doing is piping them backwards.",
    "tag": "find"
  },
  {
    "question": "Check if one list contains element from the other",
    "answer": "If you just need to test basic equality, this can be done with the basic JDK without modifying the input lists in the one line\n!Collections.disjoint(list1, list2);\n\nIf you need to test a specific property, that's harder.  I would recommend, by default,\nlist1.stream()\n   .map(Object1::getProperty)\n   .anyMatch(\n     list2.stream()\n       .map(Object2::getProperty)\n       .collect(toSet())\n       ::contains)\n\n...which collects the distinct values in list2 and tests each value in list1 for presence.",
    "tag": "find"
  },
  {
    "question": "How can I store the \"find\" command results as an array in Bash",
    "answer": "Update 2020 for Linux Users:\nIf you have an up-to-date version of bash (4.4-alpha or better), as you probably do if you are on Linux, then you should be using Benjamin W.'s answer.\nIf you are on Mac OS, which —last I checked— still used bash 3.2, or are otherwise using an older bash, then continue on to the next section.\nAnswer for bash 4.3 or earlier\nHere is one solution for getting the output of find into a bash array:\narray=()\nwhile IFS=  read -r -d $'\\0'; do\n    array+=(\"$REPLY\")\ndone < <(find . -name \"${input}\" -print0)\n\nThis is tricky because, in general, file names can have spaces, new lines, and other script-hostile characters.  The only way to use find and have the file names safely separated from each other is to use -print0 which prints the file names separated with a null character.  This would not be much of an inconvenience if bash's readarray/mapfile functions supported null-separated strings but they don't.  Bash's read does and that leads us to the loop above.\n[This answer was originally written in 2014.  If you have a recent version of bash, please see the update below.]\nHow it works\n\nThe first line creates an empty array: array=()\nEvery time that the read statement is executed, a null-separated file name is read from standard input.  The -r option tells read to leave backslash characters alone.  The -d $'\\0' tells read that the input will be null-separated.  Since we omit the name to read, the shell puts the input into the default name: REPLY.\nThe array+=(\"$REPLY\") statement appends the new file name to the array array.\nThe final line combines redirection and command substitution to provide the output of find to the standard input of the while loop. \n\nWhy use process substitution?\nIf we didn't use process substitution, the loop could be written as:\narray=()\nfind . -name \"${input}\" -print0 >tmpfile\nwhile IFS=  read -r -d $'\\0'; do\n    array+=(\"$REPLY\")\ndone <tmpfile\nrm -f tmpfile\n\nIn the above the output of find is stored in a temporary file and that file is used as standard input to the while loop.  The idea of process substitution is to make such temporary files unnecessary.  So, instead of having the while loop get its stdin from tmpfile, we can have it get its stdin from <(find . -name ${input} -print0).\nProcess substitution is widely useful.  In many places where a command wants to read from a file, you can specify process substitution, <(...), instead of a file name.  There is an analogous form, >(...), that can be used in place of a file name where the command wants to write to the file.\nLike arrays, process substitution is a feature of bash and other advanced shells.  It is not part of the POSIX standard.\nAlternative: lastpipe\nIf desired, lastpipe can be used instead of process substitution (hat tip: Caesar):\nset +m\nshopt -s lastpipe\narray=()\nfind . -name \"${input}\" -print0 | while IFS=  read -r -d $'\\0'; do array+=(\"$REPLY\"); done; declare -p array\n\nshopt -s lastpipe tells bash to run the last command in the pipeline in the current shell (not the background).  This way, the array remains in existence after the pipeline completes.  Because lastpipe only takes effect if job control is turned off, we run set +m.  (In a script, as opposed to the command line, job control is off by default.)\nAdditional notes\nThe following command creates a shell variable, not a shell array:\narray=`find . -name \"${input}\"`\n\nIf you wanted to create an array, you would need to put parens around the output of find.  So, naively, one could:\narray=(`find . -name \"${input}\"`)  # don't do this\n\nThe problem is that the shell performs word splitting on the results of find so that the elements of the array are not guaranteed to be what you want.\nUpdate 2019\nStarting with version 4.4-alpha, bash now supports a -d option so that the above loop is no longer necessary.  Instead, one can use:\nmapfile -d $'\\0' array < <(find . -name \"${input}\" -print0)\n\nFor more information on this, please see (and upvote) Benjamin W.'s answer.",
    "tag": "find"
  },
  {
    "question": "How do I get the find command to print out the file size with the file name?",
    "answer": "find . -name '*.ear' -print0 | xargs -0 ls -lhS\n\njust the h extra from jer.drab.org's reply. saves time converting to MB mentally ;)",
    "tag": "find"
  },
  {
    "question": "How to strip leading \"./\" in unix \"find\"?",
    "answer": "Find only regular files under current directory, and print them without \"./\" prefix:\nfind -type f -printf '%P\\n'\n\nFrom man find, description of -printf format:\n\n%P     File's name with the name of the command line argument under which it was found removed.",
    "tag": "find"
  },
  {
    "question": "Best way to handle list.index(might-not-exist) in python?",
    "answer": "thing_index = thing_list.index(elem) if elem in thing_list else -1\n\nOne line. Simple. No exceptions.",
    "tag": "find"
  },
  {
    "question": "How can I search for a multiline pattern in a file?",
    "answer": "Here is the example using GNU grep:\ngrep -Pzo '_name.*\\n.*_description'\n\n\n-z/--null-data Treat the input as a set of lines, each terminated by a zero byte (the ASCII NUL character) instead of a newline.\n\nWhich has the effect of treating the whole file as one large line.\nSee -z description on grep's manual and also common question no 14 on grep's manual usage page",
    "tag": "find"
  },
  {
    "question": "Ruby Array find_first object?",
    "answer": "Either I don't understand your question, or Enumerable#find is the thing you were looking for.",
    "tag": "find"
  },
  {
    "question": "Delete files older than 10 days using shell script in Unix",
    "answer": "find is the common tool for this kind of task :\nfind ./my_dir -mtime +10 -type f -delete\n\nEXPLANATIONS\n\n./my_dir your directory (replace with your own)\n-mtime +10 older than 10 days\n-type f only files\n-delete no surprise. Remove it to test your find filter before executing the whole command\n\nAnd take care that ./my_dir exists to avoid bad surprises !",
    "tag": "find"
  },
  {
    "question": "How to find the files that are created in the last hour in unix",
    "answer": "If the dir to search is srch_dir then either\n$ find srch_dir -cmin -60 # change time \n\nor\n$ find srch_dir -mmin -60 # modification time \n\nor\n$ find srch_dir -amin -60 # access time \n\nshows files whose metadata has been changed (ctime), the file contents itself have been modified (mtime), or accessed (atime) in the last hour, respectively.\nctime is non-unintuitive and warrants further explanation:\n\nctime:\nUnlike mtime, which is only related to the contents inside a file, changed timestamp indicates the last time some metadata of a file was changed. ctime refers to the last time when a file’s metadata.\nFor example, if permission settings of a file were modified, ctime will indicate it.",
    "tag": "find"
  },
  {
    "question": "Find all elements on a page whose element ID contains a certain text using jQuery",
    "answer": "$('*[id*=mytext]:visible').each(function() {\n    $(this).doStuff();\n});\n\nNote the asterisk '*' at the beginning of the selector matches all elements.\nSee the Attribute Contains Selectors, as well as the :visible and :hidden selectors.",
    "tag": "find"
  },
  {
    "question": "How to select where ID in Array Rails ActiveRecord without exception",
    "answer": "If it is just avoiding the exception you are worried about, the \"find_all_by..\" family of functions works without throwing exceptions.\nComment.find_all_by_id([2, 3, 5])\n\nwill work even if some of the ids don't exist.  This works in the \nuser.comments.find_all_by_id(potentially_nonexistent_ids)\n\ncase as well.\nUpdate: Rails 4\nComment.where(id: [2, 3, 5])",
    "tag": "find"
  },
  {
    "question": "How to find the largest file in a directory and its subdirectories?",
    "answer": "Quote from this link-\n\nIf you want to find and print the top 10 largest files names (not\ndirectories) in a particular directory and its sub directories\n$ find . -type f -printf '%s %p\\n'|sort -nr|head\nTo restrict the search to the present directory use \"-maxdepth 1\" with\nfind.\n$ find . -maxdepth 1 -printf '%s %p\\n'|sort -nr|head\nAnd to print the top 10 largest \"files and directories\":\n$ du -a . | sort -nr | head\n** Use \"head -n X\" instead of the only \"head\" above to print the top X largest files (in all the above examples)",
    "tag": "find"
  },
  {
    "question": "How can I find a specific element in a List<T>?",
    "answer": "Use a lambda expression\nMyClass result = list.Find(x => x.GetId() == \"xy\");\n\n\nNote: C# has a built-in syntax for properties. Instead of writing getter and setter as ordinary methods (as you might be used to from Java), write\nprivate string _id;\npublic string Id\n{\n    get\n    {\n        return _id;\n    }\n    set\n    {\n        _id = value;\n    }\n}\n\nvalue is an implicit parameter to the set accessor (but this may be about to change according to this LDM). It contains the value assigned to the property.\nSince this pattern is often used, C# provides  auto-implemented properties. They are a short version of the code above; however, the backing variable is hidden and not accessible (it is accessible from within the class in VB, however).\npublic string Id { get; set; }\n\nYou can simply use properties as if you were accessing a field:\nvar obj = new MyClass();\nobj.Id = \"xy\";       // Calls the setter with \"xy\" assigned to the value parameter.\nstring id = obj.Id;  // Calls the getter.\n\n\nUsing properties, you would search for items in the list like this\nMyClass result = list.Find(x => x.Id == \"xy\"); \n\n\nYou can also use auto-implemented properties if you need a read-only property:\npublic string Id { get; private set; }\n\nThis enables you to set the Id within the class but not from outside. If you need to set it in derived classes as well you can also protect the setter\npublic string Id { get; protected set; }\n\n\nAnd finally, you can declare properties as virtual and override them in deriving classes, allowing you to provide different implementations for getters and setters; just as for ordinary virtual methods. In the deriving class you can override only the getter or only the setter. The other accessor is inherited without changes.\n\nSince C# 6.0 (Visual Studio 2015, Roslyn) you can write getter-only auto-properties with an inline initializer\npublic string Id { get; } = \"A07\"; // Evaluated once when object is initialized.\n\nYou can also initialize getter-only properties within the constructor instead. Getter-only auto-properties are true read-only properties, unlike auto-implemented properties with a private setter.\nThis works also with read-write auto-properties:\npublic string Id { get; set; } = \"A07\";\n\nBeginning with C# 6.0 you can also write properties as expression-bodied members\npublic DateTime Yesterday => DateTime.Date.AddDays(-1); // Evaluated at each call.\n// Instead of\npublic DateTime Yesterday { get { return DateTime.Date.AddDays(-1); } }\n\nSee: .NET Compiler Platform (\"Roslyn\")\n         The history of C# (version history)\nStarting with C# 7.0, both, getter and setter, can be written with expression bodies:\npublic string Name\n{\n    get => _name;                                // getter\n    set => _name = value;                        // setter\n}\n\nNote that in this case the setter must be an expression. It cannot be a statement. The example above works, because in C# an assignment can be used as an expression or as a statement. The value of an assignment expression is the assigned value where the assignment itself is a side effect. This allows you to assign a value to more than one variable at once: x = y = z = 0 is equivalent to x = (y = (z = 0)) and has the same effect as the statements z = 0; y = 0; x = 0;.\nSince C# 9.0 you can use read-only (or better initialize-once) properties that you can initialize in an object initializer. This is currently not possible with getter-only properties.\npublic string Name { get; init; }\n\nvar c = new C { Name = \"c-sharp\" };\n\nBeginning with C# 11, you can have a required property to force client code to initialize it.\nC# 14 (currently in preview) introduces the field keyword. It allows the access to the automatically created backing field in a semi-auto-implemented property.\n// Removes time part in setter\npublic DateTime HiredDate { get; init => field = value.Date(); }\n\npublic Data LazyData => field ??= new Data();",
    "tag": "find"
  },
  {
    "question": "FileNotFoundError: [Errno 2] No such file or directory",
    "answer": "When you open a file with the name address.csv, you are telling the open() function that your file is in the current working directory. This is called a relative path.\nTo give you an idea of what that means, add this to your code:\nimport os\n\ncwd = os.getcwd()  # Get the current working directory (cwd)\nfiles = os.listdir(cwd)  # Get all the files in that directory\nprint(\"Files in %r: %s\" % (cwd, files))\n\nThat will print the current working directory along with all the files in it.\nAnother way to tell the open() function where your file is located is by using an absolute path, e.g.:\nf = open(\"/Users/foo/address.csv\")",
    "tag": "find"
  },
  {
    "question": "PowerShell Script to Find and Replace for all Files with a Specific Extension",
    "answer": "Here a first attempt at the top of my head.  \n$configFiles = Get-ChildItem . *.config -rec\nforeach ($file in $configFiles)\n{\n    (Get-Content $file.PSPath) |\n    Foreach-Object { $_ -replace \"Dev\", \"Demo\" } |\n    Set-Content $file.PSPath\n}",
    "tag": "find"
  },
  {
    "question": "What is the best way to count \"find\" results?",
    "answer": "Why not\nfind <expr> | wc -l\n\nas a simple portable solution? Your original solution is spawning a new process printf for every individual file found, and that's very expensive (as you've just found).\nNote that this will overcount if you have filenames with newlines embedded, but if you have that then I suspect your problems run a little deeper.",
    "tag": "find"
  },
  {
    "question": "Numpy: find first index of value fast",
    "answer": "Although it is way too late for you, but for future reference:\nUsing numba (1) is the easiest way until numpy implements it. If you use anaconda python distribution it should already be installed.\nThe code will be compiled so it will be fast.\n@jit(nopython=True)\ndef find_first(item, vec):\n    \"\"\"return the index of the first occurence of item in vec\"\"\"\n    for i in xrange(len(vec)):\n        if item == vec[i]:\n            return i\n    return -1\n\nand then:\n>>> a = array([1,7,8,32])\n>>> find_first(8,a)\n2",
    "tag": "find"
  },
  {
    "question": "find files by extension, *.html under a folder in nodejs",
    "answer": "node.js, recursive simple function:\nvar path = require('path'),\nfs = require('fs');\n\nfunction fromDir(startPath, filter) {\n\n    //console.log('Starting from dir '+startPath+'/');\n\n    if (!fs.existsSync(startPath)) {\n        console.log(\"no dir \", startPath);\n        return;\n    }\n\n    var files = fs.readdirSync(startPath);\n    for (var i = 0; i < files.length; i++) {\n        var filename = path.join(startPath, files[i]);\n        var stat = fs.lstatSync(filename);\n        if (stat.isDirectory()) {\n            fromDir(filename, filter); //recurse\n        } else if (filename.endsWith(filter)) {\n            console.log('-- found: ', filename);\n        };\n    };\n};\n\nfromDir('../LiteScript', '.html');\n\nadd RegExp if you want to get fancy, and a callback to make it generic.\nvar path = require('path'),\nfs = require('fs');\n\nfunction fromDir(startPath, filter, callback) {\n\n    //console.log('Starting from dir '+startPath+'/');\n\n    if (!fs.existsSync(startPath)) {\n        console.log(\"no dir \", startPath);\n        return;\n    }\n\n    var files = fs.readdirSync(startPath);\n    for (var i = 0; i < files.length; i++) {\n        var filename = path.join(startPath, files[i]);\n        var stat = fs.lstatSync(filename);\n        if (stat.isDirectory()) {\n            fromDir(filename, filter, callback); //recurse\n        } else if (filter.test(filename)) callback(filename);\n    };\n};\n\nfromDir('../LiteScript', /\\.html$/, function(filename) {\n    console.log('-- found: ', filename);\n});",
    "tag": "find"
  },
  {
    "question": "How can I find a file/directory that could be anywhere on linux command line?",
    "answer": "\"Unfortunately this seems to only check the current directory, not the entire folder\".  Presumably you mean it doesn't look in subdirectories.  To fix this, use find -name \"filename\"\nIf the file in question is not in the current working directory, you can search your entire machine via \nfind / -name \"filename\"\n\nThis also works with stuff like find / -name \"*.pdf\", etc.  Sometimes I like to pipe that into a grep statement as well (since, on my machine at least, it highlights the results), so I end up with something like \nfind / -name \"*star*wars*\" | grep star\n\nDoing this or a similar method just helps me instantly find the filename and recognize if it is in fact the file I am looking for.",
    "tag": "find"
  },
  {
    "question": "Linux find file names with given string recursively",
    "answer": "Use the find command,\nfind . -type f -name \"*John*\"",
    "tag": "find"
  },
  {
    "question": "Notepad++ find in files filter EXCLUDE",
    "answer": "Note, as of December 5th, 2019, Notepad++ 7.8.2 now supports exclude filters. The help documentation describes exclude filters in the Find in Files tab section.\nFor example to exclude exe, zip and jar files, your 'find in files' filter will look like this;\n*.* !*.exe !*.zip !*.jar\nRelevant code change in the GitHub commit.",
    "tag": "find"
  },
  {
    "question": "find -exec cmd {} + vs | xargs",
    "answer": "Speed difference will be insignificant.\nBut you have to make sure that:\n\nYour script will not assume that no\nfile will have space, tab, etc in\nfile name; the first version is\nsafe, the second is not.\nYour script will not treat a file starting with \"-\" as an option.\n\nSo your code should look like this:\nfind . -exec cmd -option1 -option2 -- {} +\n\nor\nfind . -print0 | xargs -0 cmd -option1 -option2 --\n\nThe first version is shorter and easier to write as you can ignore 1, but \nthe second version is more portable and safe, as \"-exec cmd {} +\" is a relatively new option in GNU findutils (since 2005, lots of running systems will not have it yet) and it was buggy recently. Also lots of people do not know this \"-exec cmd {} +\", as you can see from other answers.",
    "tag": "find"
  },
  {
    "question": "Explaining the 'find -mtime' command",
    "answer": "The POSIX specification for find says:\n\n-mtimen\nThe primary shall evaluate as true if the file modification time subtracted from the initialization time, divided by 86400 (with any remainder discarded), is n.\n\nInterestingly, the description of find does not further specify 'initialization time'.  It is probably, though, the time when find is initialized (run).\n\nIn the descriptions, wherever n is used as a primary argument, it shall be interpreted as a decimal integer optionally preceded by a plus ( '+' ) or minus-sign ( '-' ) sign, as follows:\n+n\nMore than n.\n  n\nExactly n.\n-n\nLess than n.\n\nTransferring the content of a comment to this answer.\n\nYou can write -mtime 6 or -mtime -6 or -mtime +6:\n\nUsing 6 without sign means \"equal to 6 days old — so modified between 'now - 6 * 86400' and 'now - 7 * 86400'\" (because fractional days are discarded).\nUsing -6 means \"less than 6 days old — so modified on or after 'now - 6 * 86400'\".\nUsing +6 means \"more than 6 days old — so modified on or before 'now - 7 * 86400'\" (where the 7 is a little unexpected, perhaps).\n\n\nAt the given time (2014-09-01 00:53:44 -4:00, where I'm deducing that AST is Atlantic Standard Time, and therefore the time zone offset from UTC is -4:00 in ISO 8601 but +4:00 in ISO 9945 (POSIX), but it doesn't matter all that much):\n1409547224 = 2014-09-01 00:53:44 -04:00\n1409457540 = 2014-08-30 23:59:00 -04:00\n\nso:\n1409547224 - 1409457540 = 89684\n89684 / 86400 = 1\n\nEven if the 'seconds since the epoch' values are wrong, the relative values are correct (for some time zone somewhere in the world, they are correct).\nThe n value calculated for the 2014-08-30 log file, therefore, is exactly 1 (the calculation is done with integer arithmetic), and the +1 rejects it because it is strictly a > 1 comparison (and not >= 1).",
    "tag": "find"
  },
  {
    "question": "Capitalize first letter of each word in a selection using Vim",
    "answer": "You can use the following substitution:\ns/\\<./\\u&/g\n\n\n\\< matches the start of a word\n. matches the first character of a word\n\\u tells Vim to uppercase the following character in the substitution string (&)\n& means substitute whatever was matched on the left-hand side\ng means substitute all matches, not only the first",
    "tag": "find"
  },
  {
    "question": "How to display modified date time with 'find' command?",
    "answer": "The accepted answer works but it's slow. There's no need to exec stat for each directory, find provides the modification date and you can just print it out directly. Here's an equivalent command that's considerably faster:\n find /var -maxdepth 2 -type d -printf \"%p %TY-%Tm-%Td %TH:%TM:%TS %Tz\\n\"",
    "tag": "find"
  },
  {
    "question": "Find and replace globally in Sublime Text 2 (all files and in all directories)",
    "answer": "Yes, there is Multiple Files search and replace.\nPress Ctrl + Shift + F (Cmd + shift + F on macOS):\n\nIn the Where field, you can also add filters to search only folders and files that you need. If nothing is set, the search is made on all files listed in the sidebar.\nIf you double-click on a line of the search result, Sublime Text will jump to that line.\nNotice these icons in the search bar:\n\nThe first (show context) toggles the context in the result format (if enabled, some lines of text are shown before and after the matching line). The second allows to show the result in a new buffer, or in a console.",
    "tag": "find"
  },
  {
    "question": "Linux command: How to 'find' only text files?",
    "answer": "I know this is an old thread, but I stumbled across it and thought I'd share my method which I have found to be a very fast way to use find to find only non-binary files:\nfind . -type f -exec grep -Iq . {} \\; -print\n\nThe -I option to grep tells it to immediately ignore binary files and the . option along with the -q will make it immediately match text files so it goes very fast. You can change the -print to a -print0 for piping into an xargs -0 or something if you are concerned about spaces (thanks for the tip, @lucas.werkmeister!)\nAlso the first dot is only necessary for certain BSD versions of find such as on OS X, but it doesn't hurt anything just having it there all the time if you want to put this in an alias or something.\nEDIT: As @ruslan correctly pointed out, the -and can be omitted since it is implied.",
    "tag": "find"
  },
  {
    "question": "jQuery find elements with value = x",
    "answer": "If the value is hardcoded in the source of the page using the value attribute then you can\n$('#attached_docs :input[value=\"123\"]').remove();\n\nor\n$('#attached_docs :input').filter(function(){return this.value=='123'}).remove();\n\ndemo http://jsfiddle.net/gaby/RcwXh/2/",
    "tag": "find"
  },
  {
    "question": "Find files and tar them (with spaces)",
    "answer": "Use this:\nfind . -type f -print0 | tar -czvf backup.tar.gz --null -T -\n\nIt will:\n\ndeal with files with spaces, newlines, leading dashes, and other funniness\nhandle an unlimited number of files\nwon't repeatedly overwrite your backup.tar.gz like using tar -c with xargs will do when you have a large number of files\n\nAlso see:\n\nGNU tar manual\nHow can I build a tar from stdin?, search for null",
    "tag": "find"
  },
  {
    "question": "jQuery select by attribute using AND and OR operators",
    "answer": "AND operation\na=$('[myc=\"blue\"][myid=\"1\"][myid=\"3\"]');\n\nOR operation, use commas\na=$('[myc=\"blue\"],[myid=\"1\"],[myid=\"3\"]');\n\nAs @Vega commented:\na=$('[myc=\"blue\"][myid=\"1\"],[myc=\"blue\"][myid=\"3\"]');",
    "tag": "find"
  },
  {
    "question": "Use find command but exclude files in two directories",
    "answer": "Here's how you can specify that with find:\nfind . -type f -name \"*_peaks.bed\" ! -path \"./tmp/*\" ! -path \"./scripts/*\"\n\nExplanation:\n\nfind . - Start find from current working directory (recursively by default)\n-type f - Specify to find that you only want files in the results\n-name \"*_peaks.bed\" - Look for files with the name ending in _peaks.bed\n! -path \"./tmp/*\" - Exclude all results whose path starts with ./tmp/\n! -path \"./scripts/*\" - Also exclude all results whose path starts with ./scripts/\n\nTesting the Solution:\n$ mkdir a b c d e\n$ touch a/1 b/2 c/3 d/4 e/5 e/a e/b\n$ find . -type f ! -path \"./a/*\" ! -path \"./b/*\"\n\n./d/4\n./c/3\n./e/a\n./e/b\n./e/5\n\nYou were pretty close, the -name option only considers the basename, where as -path considers the entire path =)",
    "tag": "find"
  },
  {
    "question": "Find and Replace symbol for whole project intellij?",
    "answer": "EDIT:\nHere is the Visual representation for better understanding.\nReplacing a piece of text in all the files within the specified path do one of the following:\n\nOn the main menu, choose Edit | Find | Replace in Path\nPress Ctrl + Shift + R\n\nYou can try Ctrl + Shift + F.\nAnd if you are using Eclipse keymap for IntelliJ then you can use Ctrl + H.",
    "tag": "find"
  },
  {
    "question": "Linux find and grep command together",
    "answer": "You are looking for -H option in gnu grep.\nfind . -name '*bills*' -exec grep -H \"put\" {} \\;\n\nHere is the explanation\n    -H, --with-filename\n      Print the filename for each match.",
    "tag": "find"
  },
  {
    "question": "How do I make Git ignore file mode (chmod) changes?",
    "answer": "Try:\ngit config core.fileMode false\n\nFrom git-config(1):\n\ncore.fileMode\n    Tells Git if the executable bit of files in the working tree\n    is to be honored.\n\n    Some filesystems lose the executable bit when a file that is\n    marked as executable is checked out, or checks out a\n    non-executable file with executable bit on. git-clone(1)\n    or git-init(1) probe the filesystem to see if it handles the \n    executable bit correctly and this variable is automatically\n    set as necessary.\n\n    A repository, however, may be on a filesystem that handles\n    the filemode correctly, and this variable is set to true when\n    created, but later may be made accessible from another\n    environment that loses the filemode (e.g. exporting ext4\n    via CIFS mount, visiting a Cygwin created repository with Git\n    for Windows or Eclipse). In such a case it may be necessary\n    to set this variable to false. See git-update-index(1).\n\n    The default is true (when core.filemode is not specified\n    in the config file).\n\n\nThe -c flag can be used to set this option for one-off commands:\ngit -c core.fileMode=false diff\n\nTyping the -c core.fileMode=false can be bothersome and so you can set this flag for all git repos or just for one git repo:\n# this will set your the flag for your user for all git repos (modifies `$HOME/.gitconfig`)\n# WARNING: this will be override by local config, fileMode value is automatically selected with latest version of git.\n# This mean that if git detect your current filesystem is compatible it will set local core.fileMode to true when you clone or init a repository.\n# Tool like cygwin emulation will be detected as compatible and so your local setting WILL BE SET to true no matter what you set in global setting.\ngit config --global core.fileMode false\n\n# this will set the flag for one git repo (modifies `$current_git_repo/.git/config`)\ngit config core.fileMode false\n\nAdditionally, git clone and git init explicitly set core.fileMode to true in the repo config as discussed in Git global core.fileMode false overridden locally on clone\nWarning\ncore.fileMode is not the best practice and should be used carefully. This setting only covers the executable bit of mode and never the read/write bits. In many cases you think you need this setting because you did something like chmod -R 777, making all your files executable. But in most projects most files don't need and should not be executable for security reasons.\nThe proper way to solve this kind of situation is to handle folder and file permission separately, with something like:\nfind . -type d -exec chmod a+rwx {} \\; # Make folders traversable and read/write\nfind . -type f -exec chmod a+rw {} \\;  # Make files read/write\n\nIf you do that, you'll never need to use core.fileMode, except in very rare environment.",
    "tag": "chmod"
  },
  {
    "question": "How do I change permissions for a folder and its subfolders/files?",
    "answer": "The other answers are correct, in that chmod -R 755 will set these permissions to all files and subfolders in the tree. But why on earth would you want to? It might make sense for the directories, but why set the execute bit on all the files?\nI suspect what you really want to do is set the directories to 755 and either leave the files alone or set them to 644. For this, you can use the find command. For example:\nTo change all the directories to 755 (drwxr-xr-x):\nfind /opt/lampp/htdocs -type d -exec chmod 755 {} \\;\n\nTo change all the files to 644 (-rw-r--r--):\nfind /opt/lampp/htdocs -type f -exec chmod 644 {} \\;\n\nSome splainin': (thanks @tobbez)\n\nchmod 755 {} specifies the command that will be executed by find for each directory\nchmod 644 {} specifies the command that will be executed by find for each file\n{} is replaced by the path\n; the semicolon tells find that this is the end of the command it's supposed to execute\n\\; the semicolon is escaped, otherwise it would be interpreted by the shell instead of find",
    "tag": "chmod"
  },
  {
    "question": "Chmod 777 to a folder and all contents",
    "answer": "If you are going for a console command it would be: \nchmod -R 777 /www/store. The -R (or --recursive) options make it recursive.\nOr if you want to make all the files in the current directory have all permissions type:\nchmod -R 777 ./\nIf you need more info about chmod command see: File permission",
    "tag": "chmod"
  },
  {
    "question": "How to create file execute mode permissions in Git on Windows?",
    "answer": "There's no need to do this in two commits, you can add the file and mark it executable in a single commit:\nC:\\Temp\\TestRepo>touch foo.sh\n\nC:\\Temp\\TestRepo>git add foo.sh\n\nC:\\Temp\\TestRepo>git ls-files --stage\n100644 e69de29bb2d1d6434b8b29ae775ad8c2e48c5391 0       foo.sh\n\nAs you note, after adding, the mode is 0644 (ie, not executable).  However, we can mark it as executable before committing:\nC:\\Temp\\TestRepo>git update-index --chmod=+x foo.sh\n\nC:\\Temp\\TestRepo>git ls-files --stage\n100755 e69de29bb2d1d6434b8b29ae775ad8c2e48c5391 0       foo.sh\n\nAnd now the file is mode 0755 (executable).\nC:\\Temp\\TestRepo>git commit -m\"Executable!\"\n[master (root-commit) 1f7a57a] Executable!\n 1 file changed, 0 insertions(+), 0 deletions(-)\n create mode 100755 foo.sh\n\nAnd now we have a single commit with a single executable file.",
    "tag": "chmod"
  },
  {
    "question": "Correct file permissions for WordPress",
    "answer": "When you setup WP you (the webserver) may need write access to the files. So the access rights may need to be loose.\nchown www-data:www-data  -R * # Let Apache be owner\nfind . -type d -exec chmod 755 {} \\;  # Change directory permissions rwxr-xr-x\nfind . -type f -exec chmod 644 {} \\;  # Change file permissions rw-r--r--\n\nAfter the setup you should tighten the access rights, according to Hardening WordPress all files except for wp-content should be writable by your user account only. wp-content must be writable by www-data too.\nchown <username>:<username>  -R * # Let your useraccount be owner\nchown www-data:www-data wp-content # Let apache be owner of wp-content\n\nMaybe you want to change the contents in wp-content later on. In this case you could\n\ntemporarily change to the user to www-data with su,\ngive wp-content group write access 775 and join the group www-data or \ngive your user the access rights to the folder using ACLs.\n\nWhatever you do, make sure the files have rw permissions for www-data.",
    "tag": "chmod"
  },
  {
    "question": "WARNING: UNPROTECTED PRIVATE KEY FILE! when trying to SSH into Amazon EC2 Instance",
    "answer": "I've chmoded my keypair to 600 in order to get into my personal instance last night,\n\nAnd this is the way it is supposed to be.  \nFrom the EC2 documentation we have \"If you're using OpenSSH (or any reasonably paranoid SSH client) then you'll probably need to set the permissions of this file so that it's only readable by you.\"  The Panda documentation you link to links to Amazon's documentation but really doesn't convey how important it all is.\nThe idea is that the key pair files are like passwords and need to be protected.  So, the ssh client you are using requires that those files be secured and that only your account can read them.\nSetting the directory to 700 really should be enough, but 777 is not going to hurt as long as the files are 600.\nAny problems you are having are client side, so be sure to include local OS information with any follow up questions!",
    "tag": "chmod"
  },
  {
    "question": "Change all files and folders permissions of a directory to 644/755",
    "answer": "One approach could be using find:\nfor directories\nfind /desired_location -type d -print0 | xargs -0 chmod 0755\n\nfor files\nfind /desired_location -type f -print0 | xargs -0 chmod 0644",
    "tag": "chmod"
  },
  {
    "question": "Chmod recursively",
    "answer": "You can use chmod with the X mode letter (the capital X) to set the executable flag only for directories.\nIn the example below, the executable flag is cleared and then set for all directories recursively:\n~$ mkdir foo\n~$ mkdir foo/bar\n~$ mkdir foo/baz\n~$ touch foo/x\n~$ touch foo/y\n\n~$ chmod -R go-X foo \n~$ ls -l foo\ntotal 8\ndrwxrw-r-- 2 wq wq 4096 Nov 14 15:31 bar\ndrwxrw-r-- 2 wq wq 4096 Nov 14 15:31 baz\n-rw-rw-r-- 1 wq wq    0 Nov 14 15:31 x\n-rw-rw-r-- 1 wq wq    0 Nov 14 15:31 y\n\n~$ chmod -R go+X foo \n~$ ls -l foo\ntotal 8\ndrwxrwxr-x 2 wq wq 4096 Nov 14 15:31 bar\ndrwxrwxr-x 2 wq wq 4096 Nov 14 15:31 baz\n-rw-rw-r-- 1 wq wq    0 Nov 14 15:31 x\n-rw-rw-r-- 1 wq wq    0 Nov 14 15:31 y\n\nA bit of explanation:\n\nchmod -x foo - clear the eXecutable flag for foo\nchmod +x foo - set the eXecutable flag for foo\nchmod go+x foo - same as above, but set the flag only for Group and Other users, don't touch the User (owner) permission\nchmod go+X foo - same as above, but apply only to directories, don't touch files\nchmod -R go+X foo - same as above, but do this Recursively for all subdirectories of foo",
    "tag": "chmod"
  },
  {
    "question": "How do you do a simple \"chmod +x\" from within python?",
    "answer": "Use os.stat() to get the current permissions, use | to OR the bits together, and use os.chmod() to set the updated permissions.\nExample:\nimport os\nimport stat\n\nst = os.stat('somefile')\nos.chmod('somefile', st.st_mode | stat.S_IEXEC)",
    "tag": "chmod"
  },
  {
    "question": "How to create a directory and give permission in single command",
    "answer": "According to mkdir's man page...\nmkdir -m 777 dirname",
    "tag": "chmod"
  },
  {
    "question": "Python module os.chmod(file, 664) does not change the permission to rw-rw-r-- but -w--wx----",
    "answer": "Found this on a different forum\n\nIf you're wondering why that leading zero is important, it's because\npermissions are set as an octal integer, and Python automagically\ntreats any integer with a leading zero as octal. So os.chmod(\"file\",\n484) (in decimal) would give the same result.\n\nWhat you are doing is passing 664 which in octal is 1230\nIn your case you would need\nos.chmod(\"/tmp/test_file\", 0o666)\n\n\nusing the prefix with 0o (zero oh).",
    "tag": "chmod"
  },
  {
    "question": "Correct owner/group/permissions for Apache 2 site files/folders under Mac OS X?",
    "answer": "This is the most restrictive and safest way I've found, as explained here for hypothetical ~/my/web/root/ directory for your web content:\n\nFor each parent directory leading to your web root (e.g. ~/my, ~/my/web, ~/my/web/root):\n\nchmod go-rwx DIR (nobody other than owner can access content)\nchmod go+x DIR (to allow \"users\" including _www to \"enter\" the dir)\n\nsudo chgrp -R _www ~/my/web/root (all web content is now group _www)\nchmod -R go-rwx ~/my/web/root (nobody other than owner can access web content)\nchmod -R g+rx ~/my/web/root (all web content is now readable/executable/enterable by _www)\n\nAll other solutions leave files open to other local users (who are part of the \"staff\" group as well as obviously being in the \"o\"/others group).  These users may then freely browse and access DB configurations, source code, or other sensitive details in your web config files and scripts if such are part of your content.  If this is not an issue for you, then by all means go with one of the simpler solutions.",
    "tag": "chmod"
  },
  {
    "question": "chmod WSL (Bash) doesn't work",
    "answer": "To enable changing file owners & permissions, you need to edit /etc/wsl.conf and insert the below config options:\n[automount]\noptions = \"metadata\"\n\nDo this inside the WSL shell, potentially needing sudo to edit/create the file.\nThis may require restarting WSL (such as with wsl --shutdown which is a Windows command, not one within WSL) or the host machine to take effect. This has been possible since 2018:\n\nYou can now set the owner and group of files using chmod/chown and modify read/write/execute permissions in WSL. You can also create special files like fifos, unix sockets, and device files. We’re introducing new mounting options with DrvFs for projecting permissions onto files alongside providing new Linux metadata on files and folders.\n\n[cite: Microsoft Dev Blog]\n\nYou can also temporarily re-mount a drive with the following commands:\nsudo umount /mnt/c\nsudo mount -t drvfs C: /mnt/c -o metadata\n\n...but please note, the command only takes effect in session scope. If you exit current bash, you'll lose your settings (credit: answerer Amade).\n\nReference:\nAutomatically Configuring WSL",
    "tag": "chmod"
  },
  {
    "question": "Difference between using \"chmod a+x\" and \"chmod 755\"",
    "answer": "chmod a+x modifies the argument's mode while chmod 755 sets it. Try both variants on something that has full or no permissions and you will notice the difference.",
    "tag": "chmod"
  },
  {
    "question": "changing chmod for files but not directories",
    "answer": "A find -exec answer is a good one but it suffers from the usually irrelevant shortcoming that it creates a separate sub-process for every single file. However it's perfectly functional and will only perform badly when the number of files gets really large. Using xargs will batch up the file names into large groups before running a sub-process for that group of files.\nYou just have to be careful that, in using xargs, you properly handle filenames with embedded spaces, newlines or other special characters in them.\nA solution that solves both these problems is (assuming you have a decent enough find and xargs implementation):\nfind . -type f -print0 | xargs -0 chmod 644\n\nThe -print0 causes find to terminate the file names on its output stream with a NUL character (rather than a space) and the -0 to xargs lets it know that it should expect that as the input format.",
    "tag": "chmod"
  },
  {
    "question": "How do I give PHP write access to a directory?",
    "answer": "An easy way is to let PHP create the directory itself in the first place.\n<?php\n $dir = 'myDir';\n\n // create new directory with 744 permissions if it does not exist yet\n // owner will be the user/group the PHP script is run under\n if ( !file_exists($dir) ) {\n     mkdir ($dir, 0744);\n }\n\n file_put_contents ($dir.'/test.txt', 'Hello File');\n\nThis saves you the hassle with permissions.",
    "tag": "chmod"
  },
  {
    "question": "find . -type f -exec chmod 644 {} ;",
    "answer": "Piping to xargs is a dirty way of doing that which can be done inside of find.\nfind . -type d -exec chmod 0755 {} \\;\nfind . -type f -exec chmod 0644 {} \\;\n\nYou can be even more controlling with other options, such as:\nfind . -type d -user harry -exec chown daisy {} \\;\n\nYou can do some very cool things with find and you can do some very dangerous things too.  Have a look at \"man find\", it's long but is worth a quick read.  And, as always remember:\n\nIf you are root it will succeed.\nIf you are in root (/) you are going to have a bad day.\nUsing /path/to/directory can make things a lot safer as you are clearly defining where you want find to run.",
    "tag": "chmod"
  },
  {
    "question": "changing the ownership of a folder in linux",
    "answer": "Use chown to change ownership and chmod to change rights.\nuse the -R option to apply the rights for all files inside of a directory too.\nNote that both these commands just work for directories too. The -R option makes them also change the permissions for all files and directories inside of the directory.\nFor example\nsudo chown -R username:group directory\n\nwill change ownership (both user and group) of all files and directories inside of directory and directory itself.\nsudo chown username:group directory\n\nwill only change the permission of the folder directory but will leave the files and folders inside the directory alone.\nyou need to use sudo to change the ownership from root to yourself.\nEdit:\nNote that if you use chown user: file (Note the left-out group), it will use the default group for that user.\nAlso\nYou can change the group ownership of a file or directory with the command:\nchgrp group_name file/directory_name\n\nYou must be a member of the group to which you are changing ownership to.\nYou can find group of file as follows \n# ls -l file\n-rw-r--r-- 1 root family 0 2012-05-22 20:03 file\n\n# chown sujit:friends file\n\nUser 500 is just a normal user. Typically user 500 was the first user on the system, recent changes (to /etc/login.defs) has altered the minimum user id to 1000 in many distributions, so typically 1000 is now the first (non root) user.\nWhat you may be seeing is a system which has been upgraded from the old state to the new state and still has some processes knocking about on uid 500. You can likely change it by first checking if your distro should indeed now use 1000, and if so alter the login.defs file yourself, the renumber the user account in /etc/passwd and chown/chgrp all their files, usually in /home/, then reboot.\nBut in answer to your question, no, you should not really be worried about this in all likelihood. It'll be showing as \"500\" instead of a username because o user in /etc/passwd has a uid set of 500, that's all.\nAlso you can show your current numbers using id i'm willing to bet it comes back as 1000 for you.",
    "tag": "chmod"
  },
  {
    "question": "Cannot change permissions of folders within vagrant home folder",
    "answer": "The format for shared folders changes across different versions of Vagrant. See Fabio's answer https://serverfault.com/questions/398414/vagrant-set-default-share-permissions\nVagrant version 1.3.1 and earlier\nconfig.vm.share_folder \"v-data\", \"/export\", \"/export\",\n    :owner => 'vagrant',\n    :group => 'httpd',\n    :extra => 'dmode=775,fmode=775'\nVagrant version 1.3.1, 1.3.2\nIn Vagrant 1.3.1 and later, the extra option has been replaced with mount_options that expects an array. \nconfig.vm.share_folder \"v-data\", \"/export\", \"/export\",\n    :owner => 'vagrant',\n    :group => 'httpd',\n    :mount_options => ['dmode=775', 'fmode=775']\nVagrant version >=1.3.3\nIn vagrant 1.3.3 it appears config.vm.share_folder has been replaced with config.vm.synced_folder. \nconfig.vm.synced_folder \"v-data\", \"/export\", \"/export\",\n    :owner => 'vagrant',\n    :group => 'httpd',\n    :mount_options => ['dmode=775', 'fmode=775']",
    "tag": "chmod"
  },
  {
    "question": "How do I use chmod with Node.js",
    "answer": "According to its sourcecode /lib/fs.js on line 508:\nfs.chmodSync = function(path, mode) {\n  return binding.chmod(pathModule._makeLong(path), modeNum(mode));\n};\n\nand line 203:\nfunction modeNum(m, def) {\n  switch (typeof m) {\n    case 'number': return m;\n    case 'string': return parseInt(m, 8);\n    default:\n      if (def) {\n        return modeNum(def);\n      } else {\n        return undefined;\n      }\n  }\n}\n\nit takes either an octal number or a string.\ne.g.\nfs.chmodSync('test', 0755);\nfs.chmodSync('test', '755');\n\nIt doesn't work in your case because the file modes only exist on *nix machines.",
    "tag": "chmod"
  },
  {
    "question": "preserving file permissions for samba shares when file is edited",
    "answer": "Could finally figure out why permission was changing. The confusion arose from the map archive = yes setting being the default value in Samba. After setting map archive = no, the owner execute bit started behaving like I expected it to behave.\nFound the answer by reading the documentation over here: http://www.samba.org/samba/docs/using_samba/ch08.html in the File Permissions and Attributes on MS-DOS and Unix section. It clearly mentions this side effect:\n\nConsequently, there is no use for any of the three Unix executable bits that are present on a file in a Samba disk share. DOS files, however, have their own attributes that need to be preserved when they are stored in a Unix environment: the archive, system, and hidden bits. Samba can preserve these bits by reusing the executable permission bits of the file on the Unix side--if it is instructed to do so. Mapping these bits, however, has an unfortunate side effect: if a Windows user stores a file in a Samba share, and you view it on Unix with the ls -al command, some of the executable bits won't mean what you'd expect them to.\n\nHowever, it also mentions this: \n\nWe should warn you that the default value of the map archive option is yes, while the other two options have a default value of no. This is because many programs do not work properly if the archive bit is not stored correctly for DOS and Windows files. The system and hidden attributes, however, are not critical for a program's operation and are left to the discretion of the administrator.\n\nYou can also read more about the archive bit over here: http://en.wikipedia.org/wiki/Archive_bit",
    "tag": "chmod"
  },
  {
    "question": "How to make all files under a directory world readable on linux?",
    "answer": "man 3 chmod contains the information you are looking for.\nchmod -R +r directory\n\nthe -R option tells chmod to operate recursively.",
    "tag": "chmod"
  },
  {
    "question": "How to change permissions to certain file pattern/extension?",
    "answer": "use find:\nfind . -name \"*.sh\" -exec chmod +x {} \\;",
    "tag": "chmod"
  },
  {
    "question": "In a PHP / Apache / Linux context, why exactly is chmod 777 dangerous?",
    "answer": "Here's one scenario:\n\nYou have an unprotected directory that users can upload to.\nThey upload two files: a shell script, and a php file that has a system() call in it to the shell script.\nthey access the php script they just uploaded by visiting the url in their browser, causing the shell script to execute.\n\nIf this directory is 777, that means that anybody (including the user apache, which is what php script will execute as) can execute it! If the execute bit is not set on that directory and presumably the files inside the directory, then step 3 above would do nothing.\nedit from the comments: it's not the PHP file's permissions that matter, it's the system() call inside the PHP file that will be executed as a linux system call by the linux user apache (or whatever you have apache set to run as), and that is PRECISELY where the execution bit matters.",
    "tag": "chmod"
  },
  {
    "question": "What is the normal chmod?",
    "answer": "Here's a summary that I have gathered:\nUsage: chmod <number> <filename>\n\nchmod all files to 644\nchmod all .htaccess files to 644\nchmod all robots.txt files to 644\nchmod all directories to 711\nchmod all directories with directory listing (.htaccess Options +Indexes) to 755\nchmod all directories that users can upload files to, to 755 (ex: /uploads/).\n\nExplanations:\n\n644 means:\n\n6: the owner of the file/directory can read and write, but not execute. Since files are not executable, you don't need to have \"x\" rights here (6 means r+w. 7 means r+w+x).\n44: The group that the file/directory belongs to (see the group by using ls -l) and everyone else (the public) are able to read the file, but not execute or write to it (permission number 4).\n\n\n711 means:\n\n7: the owner of the file/directory can read, write, and execute. This is needed for directories! Without \"execute\" when you try to list the directory you'll get permission denied.\n11: The group that the file/directory belongs to and the public have execute rights only. This is suitable for directories where you don't want other people browsing through the contents but do want to give them access to selected files further down the directory.\n\n\n755 means:\n\n7: the owner of the file/directory can read, write, and execute.\n55: The group that the file/directory belongs to and the public have read and execute permissions but not write. This allows users to be able to view what files are in a directory, and be able to read those files, but not alter them.\n\n\n\nThere's also an interactive online calculator you can use to figure out what permissions to use: https://chmod-calculator.com/",
    "tag": "chmod"
  },
  {
    "question": "Creating executable files in Linux",
    "answer": "Make file executable:\n\nchmod +x file\n\nFind location of perl:\n\nwhich perl\n\nThis should return something like\n\n/bin/perl sometimes /usr/local/bin\n\nThen in the first line of your script add:\n\n#!\"path\"/perl with path from above e.g.\n#!/bin/perl\n\nThen you can execute the file\n\n./file\n\nThere may be some issues with the PATH, so you may want to change that as well ...",
    "tag": "chmod"
  },
  {
    "question": "Why Docker COPY doesn't change file permissions? (--chmod)",
    "answer": "I figured out:\nthe flag --chmod is a new feature from Docker Buildkit, so it is necessary to run the build enabling it via:\nDOCKER_BUILDKIT=1 docker build ./\n\nHowever, it is really not clear why Docker swallows the --chmod option without any error or warn about the non-existing option 😕.",
    "tag": "chmod"
  },
  {
    "question": "Differences between CHMOD 755 vs 750 permissions set",
    "answer": "0755 = User:rwx Group:r-x World:r-x\n0750 = User:rwx Group:r-x World:--- (i.e. World: no access)\nr = read\nw = write\nx = execute (traverse for directories)",
    "tag": "chmod"
  },
  {
    "question": "Linux: Set permission only to directories",
    "answer": "chmod can actually do this itself; the X symbolic permission means \"execute, if it makes sense\" which generally means on directories but not files. So, you can use:\nchmod -R u=rwX,go=rX /path/to/htdocs\n\nThe only potential problem is that if any of the plain files already have execute set, chmod assumes it's intentional and keeps it. If this is a potential problem and you have the GNU version of chmod (i.e. you're on Linux), you can get it to remove any stray execute permissions like this:\nchmod -R a-x,u=rwX,go=rX /path/to/htdocs\n\nUnfortunately, this trick doesn't work with the bsd (/macOS) version of chmod (I'm not sure about other versions). This is because the bsd version applies the X permission based on \"the original (unmodified) mode\", i.e. whether it had any execute bits before the a-x modification was done (see the man page).",
    "tag": "chmod"
  },
  {
    "question": "Trying to get Postgres setup in my environment but can't seem to get permissions to intidb",
    "answer": "This should work just fine:\n# sudo mkdir /usr/local/var/postgres\n# sudo chmod 775 /usr/local/var/postgres\n# sudo chown $(whoami) /usr/local/var/postgres/\n# initdb /usr/local/var/postgres\n\nNote that using $(whoami) in the third line above ensures this works for the user who's running the command.",
    "tag": "chmod"
  },
  {
    "question": "chmod - protect users' file being accessed so only owner can access?",
    "answer": "chmod 600 filename will do it;\nor\nchmod 700 if it is an executable.\nAnother way that is less cryptic is:\nchmod go-rwx filename\n\nThe \"g\" is for group\nThe \"o\" is for others\nThe \"-\" is for removing permissions\nThe \"r\" is for read-permission\nThe \"w\" is for write-permission\nThe \"x\" is for execute permission.\n\nFor the sake of completeness:\n\n\"u\" is for user / owner\n\"+\" is for adding permissions",
    "tag": "chmod"
  },
  {
    "question": "svn: how to set the executable bit on a file?",
    "answer": "svn uses propset to change file attributes:\nsvn propset svn:executable ON sync.py",
    "tag": "chmod"
  },
  {
    "question": "How do I make git accept mode changes without accepting all text changes?",
    "answer": "You should be able to do:\ngit update-index --chmod=(+|-)x <file>\n\nto adjust the executable bit stored in the index.\nYou can then commit this separately from any changes to the files content.",
    "tag": "chmod"
  },
  {
    "question": "Copy file permissions, but not files",
    "answer": "You should have a look at the --reference option for chmod:\nchmod --reference version2/somefile version1/somefile\n\nApply find and xargs in a fitting manner and you should be fine, i.e. something like\n ~/version2$ find . -type f | xargs -I {} chmod --reference {} ../version1/{}\n\nThis even works recursively, and is robust against missing files in the target directory (bar the No such file ... errors, which can be ignored). Of course it won't do anything to files that only exist in the target directory.\nCheers,",
    "tag": "chmod"
  },
  {
    "question": "rsync deploy and file/directories permissions",
    "answer": "Try it like this : \n--chmod=Du=rwx,Dg=rx,Do=rx,Fu=rw,Fg=r,Fo=r\n\nIt worked for me.",
    "tag": "chmod"
  },
  {
    "question": "chmod: cannot read directory `.': Permission denied",
    "answer": "Directories need the execute permission set in order to see their contents.\nFrom http://content.hccfl.edu/pollock/AUnix1/FilePermissions.htm\n\nYou can think of read and execute on directories this way:  directories are data files that hold two pieces of information for each file within, the file's name and it's inode number.  Read permission is needed to access the names of files in a directory.  Execute (a.k.a. search) permission is needed to access the inodes of files in a directory, if you already know the file's name.\n\nWhen you change a directory permission to 644, you are unable to read the files in that directory although you can read that directory to see it exists.\nYou need to do this:\n$ chmod -R 0755 .\n\nA better way might be to use string permission if you simply want to turn off \nOtherwise, you can see the directory, but not access the information in that directory.\nYou maybe better off using relative permissions instead of absolute permissions:\n$ chmod -R go-w .\n\nWill remove write permission from group and other, but not touch execute permission.\nYou can also use find just to set the directories or just to set files:\n$ find . -type d -exec chmod 755 {} \\;\n\nThis will only touch directories, setting read and execute permission on all directories and setting write permission for the owner. This way, you're not setting execute permission on files themselves.",
    "tag": "chmod"
  },
  {
    "question": "Changing permissions via chmod at runtime errors with \"Operation not permitted\"",
    "answer": "$ sudo chmod ...\n\nYou need to either be the owner of the file or be the superuser, i.e., user root. If you own the directory but not the file, you can copy the file, rm the original, then mv it back, and then you will be able to chown it.\nThe easy way to temporarily be root is to run the command via sudo. ($ man 8 sudo)",
    "tag": "chmod"
  },
  {
    "question": "How will a server become vulnerable with chmod 777?",
    "answer": "It allows filesystem content to be viewed and/or modified by anyone: assuming the attacker already has general system access which is very common on shared hosting platforms .. some are more \"hardened\" than others from the start. Here is a small incomplete list of possible attack vectors:\n\n\"your safe code\" could be overwritten with \"their malicious code\" which runs within the same web-server context .. could steal passwords/trojan, expose DB, delete content, etc. That is, someone else's code can run under your security context.\nContent (e.g. \"script source\") can possibly be viewed outside of the web-server (or owner) context. Have a \"secure\" password to connect to the DB? Well, not anymore...\nIf content was protected by permissions (e.g. web-server couldn't access before), the web-server might be able to access/list sensitive information... not good if you didn't mean to share it. Different web-server configurations will also treat \"listings\" differently, which can also expose more than is desired.\n\nIn the above I also assume \"group\" to include the web-server principal and that there is a web-server (and/or shared hosting) involved which can be used as a primary attack vector and/or security vulnerability. However, and I stress this again: the list above is not complete.\nWhile not \"guaranteed safety\", using the most specific permissions can mitigate some vulnerabilities / exposure.",
    "tag": "chmod"
  },
  {
    "question": "What is the difference between `chmod go-rwx` and `chmod 700`",
    "answer": "go-rwx removes read, write, execute permissions from the group and other users. It will not change permissions for the user that owns the file.\nTherefore e.g. a file with 644 (rw-r--r--) permissions will have 600 (rw------) after the command.\nchmod 700 on the other hand will always change the permissions to 700 (rwx------), no matter the previous permissions.\nSo it depends on what you want to accomplish.\nNotes:\n\nEspecially when using -R to change entire directories, this makes go-rwx more useful, as the executable flag is usually only wanted on folders (so they can be entered) and program files that need to be executed.\nUsing 700 would add the executable flag to all files that don't have it yet, which is usually not what you'd want to do.\nWhat the general effect of chmod 700 would actually look like in the other notation is chmod u+rwx,go-rwx or chmod u=rwx,go= (grants all permissions to user that owns file, removes all permissions of group and other)\nNot all versions of chmod support the ugo±rwx syntax scheme.",
    "tag": "chmod"
  },
  {
    "question": "set permissions for all files and folders recursively",
    "answer": "Why don't use find tool for this?\nexec (\"find /path/to/folder -type d -exec chmod 0750 {} +\");\nexec (\"find /path/to/folder -type f -exec chmod 0644 {} +\");",
    "tag": "chmod"
  },
  {
    "question": "How do you add chmod +x permission to an AWS Elastic Beanstalk platform hook?",
    "answer": "Make sure to make your file executable in git\nchmod +x path/to/file\ngit update-index --chmod=+x path/to/file\n\nreference from\nHow to add chmod permissions to file in GIT?",
    "tag": "chmod"
  },
  {
    "question": "Access GPIO (/sys/class/gpio) as non-root",
    "answer": "More common rule for 4.x kernels will be the following\nSUBSYSTEM==\"gpio*\", PROGRAM=\"/bin/sh -c 'find -L /sys/class/gpio/ -maxdepth 2 -exec chown root:gpio {} \\; -exec chmod 770 {} \\; || true'\"\n\nThe rule in the initial answer will fail to chown the exported gpio if there's a symbolic link in the path\nUPD please beg in mind that when you export some GPIO via sysfs, you should wait for udev rule to fire and complete before you get desired access rights. The thing that worked for me was sleep about 100ms before trying to access GPIO files.",
    "tag": "chmod"
  },
  {
    "question": "Linux change group permission to match owner permissions",
    "answer": "Give this a try (test it first):\nchmod -R g=u apps\n\nThe = copies the permissions when you specify a field (u, g or o) on the right side or sets it absolutely when you specify a permission (r, w or x) on the right.",
    "tag": "chmod"
  },
  {
    "question": "PHP: get_current_user() vs. exec('whoami')",
    "answer": "get_current_user() (should) return the owner of the file, which is firstnamelastname in this case. There have been reported issues that this function is inconsistent between platforms however. As such, I would not trust its output. daemon is the user Apache is running as.\nThe owner of the PHP script is the user who owns the file itself according to the operating system. You can run ls -la in the directory your scripts are in to find the user and group the file belongs to.\nWhichever user you're editing your scripts with needs to be able to write it, so most likely, firstnamelastname (+rw).\nFor the folder itself, you should have +rx (execute and read) for daemon and for the PHP file, +r (read). On my installation of XAMMP, they've done this by setting everything in htdocs as public readable, thus daemon can read it, but not write to it.\nMac has a root account that typically owns the htdocs or www directory. It fills the role of a traditional unix root user.\n\nHere is some information on the file owners/groups and the process owner:\nhost:~$ ls -l /Applications/XAMPP/xamppfiles/htdocs\ndrwxr-xr-x 3 root admin  4096 2015-01-01 00:01 .\ndrwxr-xr-x 3 root admin  4096 2015-01-01 00:01 ..\n-rw-r--r-- 1 firstnamelastname admin   189 2015-01-31 20:45 index.php\n\nhost:~$ ps aux | grep httpd | head -n1    \ndaemon          45204   0.0  0.1  2510176  10328   ??  S    Tue11AM   0:01.38 /Applications/XAMPP/xamppfiles/bin/httpd -k start -E /Applications/XAMPP/xamppfiles/logs/error_log -DSSL -DPHP\n\nIf you wanted to make a file writeable by the daemon user, you can create a new folder and name it as the owner with the group admin (so you can use it too), and give it +rwx for the user and group, with +rx for public:\nhost:~$ cd /Applications/XAMPP/xamppfiles/htdocs\nhost:htdocs$ mkdir some_dir\nhost:htdocs$ chmod 775 some_dir",
    "tag": "chmod"
  },
  {
    "question": "Wordpress on Docker: Could not create directory on mounted volume",
    "answer": "Looking at your error message i come to the conclusion that you are trying to install a plugin or update wordpress itself\nThis issue is slightly tricky to figure out. \nexecuting chown -R www-data:www-data /var/www to set correct user:group permissions should technically solve it, however .. \nOn a new wordpress install the upload & plugins folder does not yet exist and therefore when installer trying to create a plugins/subfolder it will throw the error.    \nHow to Fix Wordpress / Docker Plugin install Permission issue\nFixing this issue is however quite easy once grasping it.\nOption A\nin your .Docker file add following towards the very end but before any [CMD] command.\nRUN mkdir /var/www/html/wp-content/plugins\nRUN mkdir /var/www/html/wp-content/uploads\nRUN chown -R www-data:www-data /var/www\nRUN find /var/www/ -type d -exec chmod 0755 {} \\;\nRUN find /var/www/ -type f -exec chmod 644 {} \\; \nOption B\nssh in to your docker container\ndocker exec -it <container_name> /bin/bash \nIf you don't know the container name find it with\ndocker ps \nSimply run same commands as in example above\n$ mkdir /var/www/html/wp-content/plugins\n$ mkdir /var/www/html/wp-content/uploads\n$ chown -R www-data:www-data /var/www\n$ find /var/www/ -type d -exec chmod 0755 {} \\;\n$ find /var/www/ -type f -exec chmod 644 {} \\;",
    "tag": "chmod"
  },
  {
    "question": "ADB Shell giving bad mode when executing chmod (under su)",
    "answer": "Bad mode means the permissions on the file are not set correctly, and some versions of chmod don't understand the o+rw notation. All chmods understand octal notation, where 6=read/write, so try\n chmod 666 /dev/ttyS1\n\nI hope this helps.",
    "tag": "chmod"
  },
  {
    "question": "PHP chmod( ):Operation not permitted, safe_mode deprecation involved?",
    "answer": "The daemon user is not root, so it is not allowed to change the mode of a file owned by a different user. PHP safe_mode is not the cause here. The warning is telling you that the attempted operation failed because the web server user did not have permission to make the mode change.\nThe operation succeeded after you manually changed the ownership of the file to daemon because users are allowed to change the mode of files they own.",
    "tag": "chmod"
  },
  {
    "question": "File Permissions and CHMOD: How to set 777 in PHP upon file creation?",
    "answer": "PHP has a built in function called bool chmod(string $filename, int $mode )\nhttp://php.net/function.chmod\nprivate function writeFileContent($file, $content){\n    $fp = fopen($file, 'w');\n    fwrite($fp, $content);\n    fclose($fp);\n    chmod($file, 0777);  //changed to add the zero\n    return true;\n}",
    "tag": "chmod"
  },
  {
    "question": "Apache - Permissions are missing on a component of the path",
    "answer": "I finally found it! Thanks a ton Justin lurman to poiting out the .htaccess file. It made me see that Wordpress didn't have the right to edit my .htaccess file anymore. \nThat was even more weird because I was 100% sure that permissions were good (even way too permissive if you ask me). \nSo I looked into SElinux, as I know it can play tricks on me at times and I was right. \nIssuing the following command solved it : \nchcon -R --type=httpd_sys_rw_content_t wp-content/\n\nI hope that helps someone else :)",
    "tag": "chmod"
  },
  {
    "question": "How to fix permission denied for .git/ directory when performing git push?",
    "answer": "On the server I ran sudo chown -R git:git /srv/git/ - this fixed my problem but I am wondering if this was the correct thing to do?\n\nAbsolutely. The problem previously was that the git user, who you're logging in as via SSH, could not write to the repository.\nDepending on your needs, you may consider different combinations of users and SSH keys, or one of the many additional programs (gitolite etc) that can be used to more finely control access.",
    "tag": "chmod"
  },
  {
    "question": "Linux: command to make folders recursively writable without affecting the permission of files inside them",
    "answer": "You can say:\nfind foldername -type d -exec chmod 777 {} \\;\n\nThis would only change the file mode for the directories and not the files within those.",
    "tag": "chmod"
  },
  {
    "question": "How to `chmod -R +w` with Ant, files and folders?",
    "answer": "The following does work:\n<chmod file=\"${basedir}/foo/**\" perm=\"g+w\" type=\"both\"/>\n\nCredits shared with the OP.\nSee also\n\nChmod Task",
    "tag": "chmod"
  },
  {
    "question": "chmod a freshly mounted external drive to set up writing access",
    "answer": "Try this first,\numount /dev/sdb1\n\nchmod -R 0777 /mnt/external   \n\nthen mount with\nmount /dev/sdb1 /mnt/external\n\nor  try\nchmod -R 0777 /mnt/external",
    "tag": "chmod"
  },
  {
    "question": "chmod: How to recursively add execute permissions only to files which already have execute permission",
    "answer": "Use find:\nfind . -perm /u+x -execdir chmod a+x {} \\;",
    "tag": "chmod"
  },
  {
    "question": "Git ignoring gitconfig?",
    "answer": "When creating or reinitializing a new repo, git init always sets a new value\nfor core.filemode based on the result of probing the file system. You'll just\nhave to manually:\ngit config core.filemode false\n\nOr:\ngit config --unset core.filemode\n\nto make it respect the one in your ~/.gitconfig. If you run git init again\nthe per-repo setting will go back to true on your system.",
    "tag": "chmod"
  },
  {
    "question": "How to give file permission to a specific user in a Group?",
    "answer": "Assuming Bob can own the file the following should work for you.\n$ chown Bob:g1 file1\n\nFirst set the ownership of the file to Bob to allow for read+write access and set the group ownership to the g1 group.\n$ chmod 640 file1 \n\nSet the owner to a read and write and set the group to read only. This is a common permission structure on webservers. Note that the \"world\" has no permissions in this structure, but $ man chmod can provide further information on file permissions and get you where you are needing to go. Additionally if you need more control over your permissions across the whole system you may want to look into Posix ACLs or SE Linux as you did indicate you are on RedHat",
    "tag": "chmod"
  },
  {
    "question": "How do i secure a web server's image upload directory?",
    "answer": "There are two possible meanings for \"executable\" in this context, and both are things you need to configure against.\n\nAn executable binary file that may run from the command line by typing the file's name into a shell\nA file that may be read and processed as script by a web server.\n\nHandling binary-executable\nTo configure against case 1 on a un*x system, you need to ensure that your upload directory, and files therein, are owned by the web server user and only readable by that user. The directory must be executable; this means that the files inside that directory can be accessed). If the web user needs to list the files in the directory, it must also be readable. It, of course, must also be writable to allow new files to be created. \nThus the octal set you want would be \nchown <web-server-user> <upload-dir>\nchmod 0700 <upload-dir>\n\nThe files must not be executable and should only be readable and writable by the web server,so these should be \nchmod 0600 <uploaded-file>\n\nPlease note that this means that only the web server user will be able to see these files. This is the best situation for security. However, if you really do need other local users to be able to see these files,then use\nchmod 0755 <upload-dir>\nchmod 0644 <uploaded-file>\n\nHandling web-server excutable\nCoding against case 2 is web server specific. \nOne option is to place the upload directory outside of the webroot, dissalowing direct URL access to the uploaded files completely and only to serve them via server-side code. Your code reads and echoes the file content, thus ensuring it is never processed as script by the web server\nThe other option is to configure your web server to not allow script processing of files in the upload directory. This configuration is web-server specific. However, for example, in Apache you can achieve this this by entering into your server configuration:\n<Directory \"path-to-upload-dir\">\n  AllowOverride None\n  Options -ExecCGI\n</Directory>\n\nAllowOverride None is important, as it stops anyone uploading a .htaccess file to your uploads directory and re-configuring the web server permissions for that directory.",
    "tag": "chmod"
  },
  {
    "question": "MySQL LOAD_FILE returns NULL",
    "answer": "I found out that it has to do with AppArmor. I disabled AppArmor for MySQL and it worked.\nFor people having the same problem, please read here: http://www.cyberciti.biz/faq/ubuntu-linux-howto-disable-apparmor-commands/",
    "tag": "chmod"
  },
  {
    "question": "Laravel 5.2 could not open laravel.log",
    "answer": "TLDR;\nRun the following commands on your terminal\n# Clear Laravel cache and the compiled classes\nphp artisan cache:clear\nphp artisan clear-compiled\n\n# Change the storage and cache directories permission\nsudo chmod -R 777 storage\nsudo chmod -R 777 bootstrap/cache\n\n# Regenerate the composer autoload file\ncomposer dump-autoload\n\nMore thorough explanation\nThis usually happens because of the web server needs a write access to the storage and bootstrap/cache directories.\n1. Check which user is being used by web server process\nFirst, make sure that your webserver process is run by an account with limited privileges. Nginx and Apache usually will automatically create and use the less privileged www-data user and group. You can always use the ps command to check which user is being used by the running service:\nps aux | grep nginx\n\n2. Set the owner of your project directory\nNext, make sure that your Laravel project directory is owned by the same user and group who run the web server process. Suppose your web server is run by www-data and your project directory is located at /var/www/laravel, you can set the ownership like so:\nsudo chown -R www-data:www-data /var/www/laravel\n\n3. Give a write access to storage and cache directories\nThis is the IMPORTANT step, make sure you give the write permission both to storage and bootstrap/cache directories.\nsudo chmod -R 775 /var/www/laravel/storage\nsudo chmod -R 775 /var/www/laravel/bootstrap/cache\n\nStill Not Working?\nIf the above steps are still not working, you can try to run the following commands in the shell:\n# 1. Clear Laravel cache\nphp artisan cache:clear\n\n# 2. Delete the compiled class\nphp artisan clear-compiled\n\n# 3. Regenerate the composer autoload file\ncomposer dump-autoload\n\nYet, it still not working?\nFor the last resource, try to set the permission to 777 which means any users will have the ability to read and write to the given directories.\nsudo chmod -R 777 /var/www/laravel/storage\nsudo chmod -R 777 /var/www/laravel/bootstrap/cache\n\nHope this help.",
    "tag": "chmod"
  },
  {
    "question": "Mac OSX file permissions has '@' - how to remove that '@'",
    "answer": "The following commands helped in clearing the extended attribute at file / folder(recursive) level. \nxattr -c <yourfilename>\n\nor \nxattr -cr <yourfoldername>",
    "tag": "chmod"
  },
  {
    "question": "Granting Access Permission to a file to a specific user",
    "answer": "Unix uses discretionary access control (DAC) for permissions and access control. For better security SELinux provide mandatory access control (MAC). This is consider difficult for administrators to set up and maintain.\nUse commands: \nchown user_name file\nchown user_name folder\nchown -R user_name folder #recursive",
    "tag": "chmod"
  },
  {
    "question": "fopen Creates File, But how to change permissions?",
    "answer": "You should be able to chmod it using only the following line:\nchmod($filename, 0777);\n\nNote the 0 before the 777.\nAlso do not change the ownership before it has been chmod'ed",
    "tag": "chmod"
  },
  {
    "question": "Git/GitKraken – File Mode changes to unknown value (14001) after restoring repo from backup",
    "answer": "I had the exact same problem where GitKraken shows \"File Mode Changes from 217 to 14001.\" My teammate ran some command with sudo which changed the file modes. It may be a GitKraken bug – if you use git show -p <commit_hash> you'll see the actual change was \" changed file mode from 100644 to 100755.\"\nThe permission 644 is -rw-r--r--, and 755 is -rwxr-xr-x. You'd want to keep the permissions as 644 as most files should not be executable.\nYou can either revert the commit, or use chmod stated in this answer.",
    "tag": "chmod"
  },
  {
    "question": "Error: The SUID sandbox helper binary was found, but is not configured correctly",
    "answer": "I think that your issue is related to running Electron in a Windows Subsystem for Linux (WSL) environment. Specifically, the message about the SUID sandbox helper indicates that Electron is having difficulty due to permission issues with the chrome-sandbox file.\nHere are the steps:\nStep 1. Open WSL and navigate to your project directory.\nStep 2. Change owner and permissions manually:\nsudo chown root:$(whoami) node_modules/electron/dist/chrome-sandbox\nsudo chmod 4755 node_modules/electron/dist/chrome-sandbox\n\nStep 3. After running the above commands, check the permissions again:\nls -l node_modules/electron/dist/chrome-sandbox\n\nThe output should show that the owner is root, and the permission should be -rwsr-xr-x.\nStep 4(Optional). You may need to ensure that WSL is set up to handle permissions correctly so edit the /etc/wsl.conf file:\nsudo nano /etc/wsl.conf\n\nAdd or update the following lines:\n[automount]\noptions = \"metadata\"\n\n[network]\ngenerateResolvConf = true\n\nAfter saving changes, restart WSL using wsl -shutdown command.\nStep5. After ensuring that permissions are set correctly, try running your application again:\nyarn start",
    "tag": "chmod"
  },
  {
    "question": "OSError: [Errno 30] Read-only file system: '/User'. macOS Catalina",
    "answer": "That error message is bit misleading. In this case, the problem is that there is no /User directory on macOS. The directory is named /Users.\nIn the following line\nto_dir = os.path.dirname('/User/user/Downloads/New Folder/')\nUser should be Users\nto_dir = os.path.dirname('/Users/user/Downloads/New Folder/')\nWhat's happening is that os.mkdirs() is attempting to create a directory User in /. Which is not writable. Which is causing the error message.",
    "tag": "chmod"
  },
  {
    "question": "SDK directory is not writable when building Android project on Ubuntu agent of Azure Pipelines",
    "answer": "Change the ownership of Android SDK:\nsudo chown -R $(whoami) $ANDROID_HOME",
    "tag": "chmod"
  },
  {
    "question": "Why is the argument of os.umask() inverted? (umask 0o000 makes chmod 0o777)",
    "answer": "There is no real inconsistency, as the relation between umask and chmod can purely be written down with equations. Apparently, umask sets the opposite of chmod, it was created like this back in the old days.\nExample: 022 (the default usual umask) creates 755. It works like this:\n\n7 - 0 = 7 becomes the first byte\n7 - 2 = 5 becomes the second and third bytes\n\nUsing this example, umask 777 creates a file with chmod 000, umask 112 will be equal to chmod 664. As far as I know, this happened because the umask command was originally created to indicate what permission bits the file will NOT have after it's created (hence the invertion).\nWhile it could be annoying, it's really not hard to get used to it. Just think how you would chmod your files, and subtract the byte you want from 7, and you will get the umask value. Or, when you are at the IDE, writing your code, don't use umask, but rather create the file (with the default umask of course) and then use, in Python, os.chmod() instead.",
    "tag": "chmod"
  },
  {
    "question": "chmod 775 on a folder but not all files under that folder",
    "answer": "Remove the -R flag. That means it changes all files and folders in the subdirectory as well.\n# chown root:user1 /var/www/project/\n# chmod 775 /var/www/project/",
    "tag": "chmod"
  },
  {
    "question": "The uploaded file could not be moved to wp-content/uploads",
    "answer": "It is most likely a permissions issue. Find the user processes running on the site by navigating to your sites wp-content folder on the server your site is on. Then type this;\nps aux | egrep '(apache|httpd)'\n\nIgnore root, but look at the other users \nroot      2507  0.0  0.3 423820 14328 ?        Ss   Aug04   0:51 /usr/sbin/httpd\napache    4653  0.5  1.9 483900 77252 ?        S    16:27   0:14 /usr/sbin/httpd\napache    4654  0.5  2.1 500160 84912 ?        S    16:27   0:13 /usr/sbin/httpd\napache    4656  0.8  2.0 484708 78712 ?        S    16:27   0:21 /usr/sbin/httpd\n...\n\nFor me it actually was apache (usually www-data). Finally change the users of the uploads folder to this user;\nsudo chown -R apache:apache uploads\n\n(make sure you are in the directory above the uploads folder when running this command)\nThis will permit the correct user to access this directory using the correct access permissions of 755. \nBy using the dreadful '777' advice of others, you are simply allowing the correct user to access the directory assigned to the incorrect user - as well as anyone else who can access that directory!",
    "tag": "chmod"
  },
  {
    "question": "Sync file permissions *only*",
    "answer": "If you have the \"normal\" content of /etc available on the same system (like mounted in some other directory, let's say /mnt/correct/etc), you could use the --reference parameter to chmod and chown commands, and combine it with find that is started from the \"normal\" directory: \n$ cd /mnt/correct/etc\n$ find . ! -type l -exec chown -v --reference='{}' /etc/'{}' \\;\n$ find . ! -type l -exec chmod -v --reference='{}' /etc/'{}' \\;\n\n(I'm assuming you're on a UNIX system with GNU coreutils versions of chmod and chown.)\nThe \"! -type l\" condition in find excludes symbolic links, because otherwise chmod will use the link's permissions to change the file the link points to (and same applies to chown).",
    "tag": "chmod"
  },
  {
    "question": "How to get \"drwx---r-x+\" folder permission using CHMOD? - Bash script",
    "answer": "The permissions drwx---r-x+ break down as follows:\n\nd is a directory, of course.\nrwx means it's readable, writeable and accessible by the user.  These three bits can be represented by the octal number 7.\n--- means that the three aforementioned bits are NOT set for the group assigned to the directory.  No bits are set, so the octal number is 0.\nr-x means that users who aren't matched by the first two categories -- that is, everybody else, or \"other\" -- can read and access content of the directory, but can't write to it.  The bits here are in the ones column and the fours column, so the octal number that represents this permission is 5.\n+ indicates that there is \"extended security information\" associated with this directory which isn't shown in standard ls \"long format\".  An access control list, for example.\n\nTo set the basic permissions of this directory, you can use either the octal short-hand:\n$ chmod 705 directoryname\n\nor you can use the \"symbolic\" representation:\n$ chmod u+rwx,g-rwx,o+rx-w directoryname\n\nObviously, the shorthand is ... shorter.\nFor the extended security information denoted by the +, you'd need to find out what is set up in order to replicate it.  The ls command has a -e option to have it show extended security settings.\nTo actually set your ACLs from the command line, you'd use chmod'a =a, -a and +a options.  Documentation about this is available in OSX from man chmod.  From that man page:\n         Examples\n          # ls -le\n          -rw-r--r--+ 1 juser  wheel  0 Apr 28 14:06 file1\n            owner: juser\n            1: admin allow delete\n          # chmod =a# 1 \"admin allow write,chown\"\n          # ls -le\n          -rw-r--r--+ 1 juser  wheel  0 Apr 28 14:06 file1\n            owner: juser\n            1: admin allow write,chown",
    "tag": "chmod"
  },
  {
    "question": "Chmod not recognized as internal or external command",
    "answer": "I was running this from a Windows cmd prompt. I ran it from MinGW and it got past this part.",
    "tag": "chmod"
  },
  {
    "question": "Why does the lftp mirror command chmod files",
    "answer": "Use the -p option and it shouldn't try to change permissions.  I've never sent to a windows host, but you are correct in that it shouldn't do anything to the permission levels on the windows box.",
    "tag": "chmod"
  },
  {
    "question": "FTP Rights 755 vs 777",
    "answer": "A 777 permission on the directory means that everyone has access to read/write/execute (execute on a directory means that you can do a ls of the directory).\n755 means read and execute access for everyone and also write access for the owner of the file. When you perform chmod 755 filename command you allow everyone to read and execute the file, owner is allowed to write to the file as well.",
    "tag": "chmod"
  },
  {
    "question": "chmod function for PowerShell",
    "answer": "Here is an example with the native way, using ACL and ACE. You have to build your own functions arround that.\n# Get the Access Control List from the file\n# Be careful $acl is more a security descriptor with more information than ACL\n$acl = Get-Acl \"c:\\temp\\test.txt\"\n\n\n# Show here how to refer to useful enumerate values (see MSDN)\n$Right = [System.Security.AccessControl.FileSystemRights]::FullControl\n$Control = [System.Security.AccessControl.AccessControlType]::Allow\n\n# Build the Access Control Entry ACE \n# Be careful you need to replace \"everybody\" by the user or group you want to add rights to\n$ace = New-Object System.Security.AccessControl.FileSystemAccessRule (\"everybody\", $Right, $Control)\n\n# Add ACE to ACL\n$acl.AddAccessRule($ace)\n\n# Put ACL to the file\nSet-Acl \"c:\\temp\\test.txt\" $acl\n(Get-Acl \"c:\\temp\\test.txt\").access\nRead-Host \"--------- Test Here --------------\"\n\n# Remove ACE from ACL\n$acl.RemoveAccessRule($ace)\nSet-Acl \"c:\\temp\\test.txt\" $acl\n(Get-Acl \"c:\\temp\\test.txt\").access",
    "tag": "chmod"
  },
  {
    "question": "Setting group permissions with python",
    "answer": "From the os module documentation:\n\nNote: Although Windows supports chmod(), you can only set the file’s read-only flag with it (via the stat.S_IWRITE and stat.S_IREAD constants or a corresponding integer value). All other bits are ignored.\n\nFor Windows permissions, you manage the ACLs. Adapting from another answer, you need the pywin32 library:\nimport win32security\nimport ntsecuritycon as con\n\nFILENAME = r\"H:\\path_to_file\\file.txt\"\n\nuser, domain, type = win32security.LookupAccountName (\"\", \"Your Username\")\n\nsd = win32security.GetFileSecurity(FILENAME, win32security.DACL_SECURITY_INFORMATION)\ndacl = sd.GetSecurityDescriptorDacl()   # instead of dacl = win32security.ACL()\n\ndacl.AddAccessAllowedAce(win32security.ACL_REVISION, con.FILE_ALL_ACCESS, user)\n\nsd.SetSecurityDescriptorDacl(1, dacl, 0)   # may not be necessary\nwin32security.SetFileSecurity(FILENAME, win32security.DACL_SECURITY_INFORMATION, sd)\n\nChange the con.FILE_ALL_ACCESS flag to the ones you need.",
    "tag": "chmod"
  },
  {
    "question": "Setting write permissions on Django Log files",
    "answer": "Permissions on files created by a particular user depend on what mask is set for this particular user. \nNow you need to set the appropriate permissions for whoever is running the apache service\nps -aux | grep apache | awk '{ print $1 }'\n\nThen for this particular user running apache (www-data?)\nsudo chown -R your_user:user_running_apache directory\n\nwhere directory is the root directory of your django application.\nTo make sure that all the files that will be added to this directory in the future have\nthe correct permissions run:\nsudo chmod -R g+s directory",
    "tag": "chmod"
  },
  {
    "question": "FATAL:credentials.cc(127) Check failed: . : Permission denied (13) Trace/breakpoint trap (core dumped)",
    "answer": "One other thing to watch out for is apparmor. Check your dmesg log for something like:\n[  283.250675] audit: type=1400 audit(1702938463.134:129): apparmor=\"DENIED\" operation=\"userns_create\" class=\"namespace\" info=\"User namespace creation restricted\" error=-13 profile=\"unconfined\" pid=4483 comm=\"your-app\" requested=\"userns_create\" denied=\"userns_create\"\n\nIf that's the case, try:\nsudo sysctl -w kernel.apparmor_restrict_unprivileged_userns=0\n\nIt took me a while to figure this one out, as most answers keep referring to the nvidia modules problems, while this was not the case in my situation.\nref: https://github.com/flatpak/flatpak/issues/5462\nTo make it persist after reboot, add a line to /etc/sysctl.conf",
    "tag": "chmod"
  },
  {
    "question": "chmod 777 to python script",
    "answer": "os.chmod takes a single filename as argument, so you need to loop over the filenames and apply chmod:\nfiles = ['file1', 'file1/tmp', 'file2', 'file2/tmp', 'file3', 'file3/tmp']\nfor file in files:\n    os.chmod(file, 0o0777)\n\nBTW i'm not sure why are you setting the permission bits to 777 -- this is asking for trouble. You should pick the permission bits as restrictive as possible.",
    "tag": "chmod"
  },
  {
    "question": "How to change file permission for all sub-directories (CHMOD)",
    "answer": "If you have shell access to the server you can execute\nchmod -R 755 {DIR}\n\nThe -R means recursive.\nEdit: If you only have access via WinSCP you should be able to select the files/directories you want and change the permissions recursively",
    "tag": "chmod"
  },
  {
    "question": "why doesn't chown work in Dockerfile?",
    "answer": "It's declared to be a volume.  If you take out the VOLUME instruction, the chown takes effect.\nWhat's more, if you declare the volume after running chown, the chown settings remain in effect.",
    "tag": "chown"
  },
  {
    "question": "Correct owner/group/permissions for Apache 2 site files/folders under Mac OS X?",
    "answer": "This is the most restrictive and safest way I've found, as explained here for hypothetical ~/my/web/root/ directory for your web content:\n\nFor each parent directory leading to your web root (e.g. ~/my, ~/my/web, ~/my/web/root):\n\nchmod go-rwx DIR (nobody other than owner can access content)\nchmod go+x DIR (to allow \"users\" including _www to \"enter\" the dir)\n\nsudo chgrp -R _www ~/my/web/root (all web content is now group _www)\nchmod -R go-rwx ~/my/web/root (nobody other than owner can access web content)\nchmod -R g+rx ~/my/web/root (all web content is now readable/executable/enterable by _www)\n\nAll other solutions leave files open to other local users (who are part of the \"staff\" group as well as obviously being in the \"o\"/others group).  These users may then freely browse and access DB configurations, source code, or other sensitive details in your web config files and scripts if such are part of your content.  If this is not an issue for you, then by all means go with one of the simpler solutions.",
    "tag": "chown"
  },
  {
    "question": "\"Command /usr/sbin/chown failed with exit code 1\" when Archiving",
    "answer": "As dumb as it sounds, in XCode 5.1.1, all I had to do was quit out and restart XCode just now. I know it's not much of an \"answer\" but it just worked for me.\nEdit: this is still working as of July 2018 for folks.\nEdit: this is still working as of January 2019 xcode 9.4.1\nEdit: this is still working as of February 2023 xcode 14.2",
    "tag": "chown"
  },
  {
    "question": "chown illegal group name (mac os x)",
    "answer": "Try using just the owner if no group.\nsudo chown -R user: /usr/local/var/log/couchdb",
    "tag": "chown"
  },
  {
    "question": "How to change the user and group permissions for a directory, by name?",
    "answer": "import pwd\nimport grp\nimport os\n\nuid = pwd.getpwnam(\"nobody\").pw_uid\ngid = grp.getgrnam(\"nogroup\").gr_gid\npath = '/tmp/f.txt'\nos.chown(path, uid, gid)",
    "tag": "chown"
  },
  {
    "question": "changing the ownership of a folder in linux",
    "answer": "Use chown to change ownership and chmod to change rights.\nuse the -R option to apply the rights for all files inside of a directory too.\nNote that both these commands just work for directories too. The -R option makes them also change the permissions for all files and directories inside of the directory.\nFor example\nsudo chown -R username:group directory\n\nwill change ownership (both user and group) of all files and directories inside of directory and directory itself.\nsudo chown username:group directory\n\nwill only change the permission of the folder directory but will leave the files and folders inside the directory alone.\nyou need to use sudo to change the ownership from root to yourself.\nEdit:\nNote that if you use chown user: file (Note the left-out group), it will use the default group for that user.\nAlso\nYou can change the group ownership of a file or directory with the command:\nchgrp group_name file/directory_name\n\nYou must be a member of the group to which you are changing ownership to.\nYou can find group of file as follows \n# ls -l file\n-rw-r--r-- 1 root family 0 2012-05-22 20:03 file\n\n# chown sujit:friends file\n\nUser 500 is just a normal user. Typically user 500 was the first user on the system, recent changes (to /etc/login.defs) has altered the minimum user id to 1000 in many distributions, so typically 1000 is now the first (non root) user.\nWhat you may be seeing is a system which has been upgraded from the old state to the new state and still has some processes knocking about on uid 500. You can likely change it by first checking if your distro should indeed now use 1000, and if so alter the login.defs file yourself, the renumber the user account in /etc/passwd and chown/chgrp all their files, usually in /home/, then reboot.\nBut in answer to your question, no, you should not really be worried about this in all likelihood. It'll be showing as \"500\" instead of a username because o user in /etc/passwd has a uid set of 500, that's all.\nAlso you can show your current numbers using id i'm willing to bet it comes back as 1000 for you.",
    "tag": "chown"
  },
  {
    "question": "What is the Python way for recursively setting file permissions?",
    "answer": "The dirs and files lists are all always relative to root - i.e., they are the basename() of the files/folders, i.e. they don't have a / in them (or \\ on windows). You need to join the dirs/files to root to get their whole path if you want your code to work to infinite levels of recursion:\n\nimport os  \npath = \"/tmp/foo\"\n    \n# Change permissions for the top-level folder\nos.chmod(path, 502, 20)\n\nfor root, dirs, files in os.walk(path):\n  # set perms on sub-directories  \n  for momo in dirs:\n    os.chown(os.path.join(root, momo), 502, 20)\n\n  # set perms on files\n  for momo in files:\n    os.chown(os.path.join(root, momo), 502, 20)\n\nSurprisingly, the shutil module doesn't have a function for this.",
    "tag": "chown"
  },
  {
    "question": "Trying to get Postgres setup in my environment but can't seem to get permissions to intidb",
    "answer": "This should work just fine:\n# sudo mkdir /usr/local/var/postgres\n# sudo chmod 775 /usr/local/var/postgres\n# sudo chown $(whoami) /usr/local/var/postgres/\n# initdb /usr/local/var/postgres\n\nNote that using $(whoami) in the third line above ensures this works for the user who's running the command.",
    "tag": "chown"
  },
  {
    "question": "MongoDB only works when run as root on Ubuntu - data directory issue",
    "answer": "You created /data/db as root so it has those permissions. You can change the permissions to your user account, or whatever you have mongo running as.\nchown -R username /data/db\n\nor /data\nYou can also set a group\nchown -R username.groupname\n\nThe -R does it recursively, so it will affect all the files you've created running mongoDB as root already.",
    "tag": "chown"
  },
  {
    "question": ".ssh/id_rsa failed: permission denied",
    "answer": "You should own the permissions to the .ssh dir in your own directory, but in your case, it's owned by root. Try\ncd ~\nsudo chown drewverlee .ssh\n\nand then retry creating keys and connecting.",
    "tag": "chown"
  },
  {
    "question": "What does Linux' chown '-R' parameter mean?",
    "answer": "\"Recursive\" implies that the operation will be performed for all files and directories (and all files and directories within any directory).  So\nchown -R foo /some/path\n\nwould change file owner to foo for all files and directories in /some/path\np.s.  You might have even seen the dictionary entry for recursive:\n\nrecursive, n: See recursive",
    "tag": "chown"
  },
  {
    "question": "Setting read/write permissions on Mongodb folder",
    "answer": "Ensure that user account running mongod has the proper directory permissions. You can check which permissions are set like so:\nls -ld /data/db/\n\nIf they are set properly they should look something like this..\n\ndrwxr-xr-x  X user  wheel  XXX Date Time /data/db/\n\nIf the permissions are set incorrectly you will likely see an error similar to this when attempting to run mongod\n\nexception in initAndListen: XXXXX Unable to create/open lock file: /data/db/mongod.lock errno:13 Permission denied Is a mongod instance already running?, terminating\n\nTo get the permissions set as they should be you can run the following command\nsudo chmod 0755 /data/db && sudo chown $USER /data/db",
    "tag": "chown"
  },
  {
    "question": "Dockerfile \"RUN chmod\" not taking effect",
    "answer": "You should set the owner directly when you copy the files:\nFROM joomla:3.9-php7.2-apache\n\nRUN apt-get update \\\n&& apt-get install -y apt-utils vim curl\n\nCOPY --chown=www-data:www-data ./joomla_html /var/www/html\n\nRUN chmod -R 765 /var/www/html/\n\nCOPY ./docker/php.ini /usr/local/etc/php/conf.d/php-extras.ini\n\nEXPOSE 80",
    "tag": "chown"
  },
  {
    "question": "Why does chown increase size of docker image?",
    "answer": "Every step in a Dockerfile generates a new intermediate image, or \"layer\", consisting of anything that changed on the filesystem from the previous layer.  A docker image consists of a collection of layers that are applied one on top of another to create the final filesystem.\nIf you have:\nRUN adduser example -D -h /example -s /bin/sh\n\nThen you are probably changing nothing other than a few files in /etc (/etc/passwd, /etc/group, and their shadow equivalents).\nIf you have:\nRUN adduser example -D -h /example -s /bin/sh && \\\n    chown -R example.example /lib\n\nThen the list of things that have changed includes, recursively, everything in /lib, which is potentially larger.  In fact, in my alpine:edge container, it looks like the contents of /lib is 3.4MB:\n/ # du -sh /lib\n3.4M    /lib\n\nWhich exactly accounts for the change in image size in your example.\nUPDATE\nUsing your actual Dockerfile, with the npm install ... line commented out, I don't see any difference in the final image size whether or not the adduser and chown commands are run.  Given:\nRUN echo \"http://nl.alpinelinux.org/alpine/edge/main\" > /etc/apk/repositories && \\\n    echo \"http://nl.alpinelinux.org/alpine/edge/testing\" >> /etc/apk/repositories && \\\n    apk add -U wget iojs && \\\n    apk upgrade && \\\n    wget -q --no-check-certificate https://ghost.org/zip/ghost-0.6.0.zip -O /tmp/ghost.zip && \\\n    unzip -q /tmp/ghost.zip -d /ghost && \\\n    cd /ghost && \\\n#    npm install --production && \\\n    sed 's/127.0.0.1/0.0.0.0/' /ghost/config.example.js > /ghost/config.js && \\\n    sed -i 's/\"iojs\": \"~1.2.0\"/\"iojs\": \"~1.6.4\"/' package.json && \\\n#   adduser ghost -D -h /ghost -s /bin/sh && \\\n#   chown -R ghost.ghost * && \\\n    npm cache clean && \\\n    rm -rf /var/cache/apk/* /tmp/*\n\nI get:\n$ docker build -t sotest .\n[...]\nSuccessfully built 058d9f41988a\n$ docker inspect -f '{{.VirtualSize}}' 058d9f41988a\n31783340\n\nWhereas given:\nRUN echo \"http://nl.alpinelinux.org/alpine/edge/main\" > /etc/apk/repositories && \\\n    echo \"http://nl.alpinelinux.org/alpine/edge/testing\" >> /etc/apk/repositories && \\\n    apk add -U wget iojs && \\\n    apk upgrade && \\\n    wget -q --no-check-certificate https://ghost.org/zip/ghost-0.6.0.zip -O /tmp/ghost.zip && \\\n    unzip -q /tmp/ghost.zip -d /ghost && \\\n    cd /ghost && \\\n#    npm install --production && \\\n    sed 's/127.0.0.1/0.0.0.0/' /ghost/config.example.js > /ghost/config.js && \\\n    sed -i 's/\"iojs\": \"~1.2.0\"/\"iojs\": \"~1.6.4\"/' package.json && \\\n    adduser ghost -D -h /ghost -s /bin/sh && \\\n    chown -R ghost.ghost * && \\\n    npm cache clean && \\\n    rm -rf /var/cache/apk/* /tmp/*\n\nI get:\n$ docker build -t sotest .\n[...]\nSuccessfully built 696b481c5790\n$ docker inspect -f '{{.VirtualSize}}' 696b481c5790\n31789262\n\nThat is, the two images are approximately the same size (the\ndifference is around 5 Kb).\nI would of course expect the resulting image to be larger if the npm install command could run successfully (because that would install additional files).",
    "tag": "chown"
  },
  {
    "question": "Access GPIO (/sys/class/gpio) as non-root",
    "answer": "More common rule for 4.x kernels will be the following\nSUBSYSTEM==\"gpio*\", PROGRAM=\"/bin/sh -c 'find -L /sys/class/gpio/ -maxdepth 2 -exec chown root:gpio {} \\; -exec chmod 770 {} \\; || true'\"\n\nThe rule in the initial answer will fail to chown the exported gpio if there's a symbolic link in the path\nUPD please beg in mind that when you export some GPIO via sysfs, you should wait for udev rule to fire and complete before you get desired access rights. The thing that worked for me was sleep about 100ms before trying to access GPIO files.",
    "tag": "chown"
  },
  {
    "question": "Creating a tablespace in postgresql",
    "answer": "I would hazard a guess that the problem lies in the permissions of the parent directory \"/home/john\". Your home directory is probably setup so that only your user has access (i.e chmod 700) to it (it's a good thing for your home directory to be chmod 700, don't change it).\nDoing something like:\n\nmkdir /BSTablespace\nchown postgres:postgres /BSTablespace\n\nand then\n\nCREATE TABLESPACE magdat OWNER maggie LOCATION '/BSTablespace';\n\nshould work fine.\nRegarding the user maggie: database users are not the same as OS users. That isn't to say that you couldn't have a user in both places named maggie-- but you would need to create the user in both the database and the OS for that to happen.",
    "tag": "chown"
  },
  {
    "question": "github error: insufficient permission for adding an object to repository database",
    "answer": "You accidentally committed a file or folder to git using elevated permissions and now git can't modify those objects.   It is safe to recursively force ownership of all files under .git/objects/ to your current user to clear the problem.\n\nMake sure you're inside the repository where you're getting the error.\n\nGet your username by typing\nwhoami\n\nEnter this command\nsudo chown -R your_user_name .git/*\n\nFinally\ngit add .",
    "tag": "chown"
  },
  {
    "question": "Wordpress on Docker: Could not create directory on mounted volume",
    "answer": "Looking at your error message i come to the conclusion that you are trying to install a plugin or update wordpress itself\nThis issue is slightly tricky to figure out. \nexecuting chown -R www-data:www-data /var/www to set correct user:group permissions should technically solve it, however .. \nOn a new wordpress install the upload & plugins folder does not yet exist and therefore when installer trying to create a plugins/subfolder it will throw the error.    \nHow to Fix Wordpress / Docker Plugin install Permission issue\nFixing this issue is however quite easy once grasping it.\nOption A\nin your .Docker file add following towards the very end but before any [CMD] command.\nRUN mkdir /var/www/html/wp-content/plugins\nRUN mkdir /var/www/html/wp-content/uploads\nRUN chown -R www-data:www-data /var/www\nRUN find /var/www/ -type d -exec chmod 0755 {} \\;\nRUN find /var/www/ -type f -exec chmod 644 {} \\; \nOption B\nssh in to your docker container\ndocker exec -it <container_name> /bin/bash \nIf you don't know the container name find it with\ndocker ps \nSimply run same commands as in example above\n$ mkdir /var/www/html/wp-content/plugins\n$ mkdir /var/www/html/wp-content/uploads\n$ chown -R www-data:www-data /var/www\n$ find /var/www/ -type d -exec chmod 0755 {} \\;\n$ find /var/www/ -type f -exec chmod 644 {} \\;",
    "tag": "chown"
  },
  {
    "question": "Chown not working",
    "answer": "chown is used to change ownership of the file, not change permissions. \nls -al is not showing you who owns the file, just its permissions. \nIf root owns those files, you'll need to chown them properly, before you can change their permissions: \nchown -R yourname:yourname folderName\n\nThen as the owner you can change their permissions:\nchmod -R 776 folderName\n\nEdit: \nI double checked the syntax and it seems to be right, you'll likely need to use sudo to use them.",
    "tag": "chown"
  },
  {
    "question": "chown command returning Operation not permitted",
    "answer": "The reason is because the ownership and permissions are defined at mount time for the vfat FS.\n\nManual page mount(8):\nMount options for fat ..\n   uid=value and gid=value\n\n          Set the owner and group of all files.  (Default: the uid and gid\n          of the current process.)\n\n   umask=value\n\n          Set the umask (the bitmask  of  the  permissions  that  are  not\n          present).  The default is the umask of the current process.  The\n          value is given in octal.\n\n\nThere are at least three things you can do:\n(1) Give pi:pi access to the entire /media/USBHDD1 mount:\nmount -o remount,gid=<pi's gid>,uid=<pi's uid> /media/USBHDD1\nTo determine pi's uid:\ncat /etc/passwd |grep pi\nTo determine pi's gid:\ncat /etc/group |grep pi\n(2) Give everyone access to /media/USBHDD1 by changing the umask and dmask (not recommended):\nmount -o remount,umask=000,dmask=000 /media/USBHDD1\n(3) Change the partition to a different file system.  Only do this if you're not accessing the the external hard drive from a windows computer:\nYou won't be able to convert the file system from VFAT to a Unix-compatible FS, so you'll have to backup the contents of the drive, format as EXT3+ or reiserfs, then copy the contents back.  You can find tutorials for doing this on the web.",
    "tag": "chown"
  },
  {
    "question": "Using chown command in ansible?",
    "answer": "Assuming the file already exists and you just want to change permissions, you can retrieve user ID and group from Ansible facts and do something like:\n- name: Change kubeconfig file permission\n  file:\n    path: $HOME/.kube/config \n    owner: \"{{ ansible_effective_user_id }}\"\n    group: \"{{ ansible_effective_group_id }}\"\n\nYou can also use ansible_real_group_id / ansible_real_user_id or ansible_user_gid/ansible_user_uid depending on your need.\nPlease don't forget double quotes around ansible expression.\nSee this post for details on the difference between real and effective user\nSee Ansible docs on system variables for all available variables",
    "tag": "chown"
  },
  {
    "question": "Apache - Permissions are missing on a component of the path",
    "answer": "I finally found it! Thanks a ton Justin lurman to poiting out the .htaccess file. It made me see that Wordpress didn't have the right to edit my .htaccess file anymore. \nThat was even more weird because I was 100% sure that permissions were good (even way too permissive if you ask me). \nSo I looked into SElinux, as I know it can play tricks on me at times and I was right. \nIssuing the following command solved it : \nchcon -R --type=httpd_sys_rw_content_t wp-content/\n\nI hope that helps someone else :)",
    "tag": "chown"
  },
  {
    "question": "Changing Ownership of a directory in OS X",
    "answer": "Simple solution that worked for me:\n\nclick on your background to go to finder\nclick on go and go to folder /usr\nright click on local and do get info\nunlock the lock at the bottom\nclick + sign and add your user to the list and give read/write privileges\nclick on the gear sign at the bottom and choose apply to enclosed items to recurse under that directory and assign privileges too all directories beneath it.\n\ntype brew doctor from command prompt to test. My result:\n\nYour system is ready to brew.",
    "tag": "chown"
  },
  {
    "question": "How to fix permission denied for .git/ directory when performing git push?",
    "answer": "On the server I ran sudo chown -R git:git /srv/git/ - this fixed my problem but I am wondering if this was the correct thing to do?\n\nAbsolutely. The problem previously was that the git user, who you're logging in as via SSH, could not write to the repository.\nDepending on your needs, you may consider different combinations of users and SSH keys, or one of the many additional programs (gitolite etc) that can be used to more finely control access.",
    "tag": "chown"
  },
  {
    "question": "Change owner and group in C?",
    "answer": "To complete the answer, on Linux the following can be used (I've tested on Ubuntu):\n#include <sys/types.h>\n#include <pwd.h>\n#include <grp.h>\n\nvoid do_chown (const char *file_path,\n               const char *user_name,\n               const char *group_name) \n{\n  uid_t          uid;\n  gid_t          gid;\n  struct passwd *pwd;\n  struct group  *grp;\n\n  pwd = getpwnam(user_name);\n  if (pwd == NULL) {\n      die(\"Failed to get uid\");\n  }\n  uid = pwd->pw_uid;\n\n  grp = getgrnam(group_name);\n  if (grp == NULL) {\n      die(\"Failed to get gid\");\n  }\n  gid = grp->gr_gid;\n\n  if (chown(file_path, uid, gid) == -1) {\n      die(\"chown fail\");\n  }\n}",
    "tag": "chown"
  },
  {
    "question": "How to install NPM end-user packages on NixOS?",
    "answer": "Firstly, please undo the permissions change (chown) you made. You should NEVER change the permissions of files in the Nix store (/nix/store).\nTo install NPM packages on NixOS use the corresponding Nix package, instead of using npm -g .... NPM packages are under the nodePackages \"namespace\".\nFor example, to install typescript (tsc) edit /etc/nixos/configuration.nix:\n...\n\nenvironment.systemPackages = with pkgs; [\n  ...\n\n  nodePackages.typescript;\n]\n\n...\n\nThen use nixos-rebuild switch to \"install\" the package.\nYou can install Node.js the same way. Use nix search nodejs to see the various versions you can install.",
    "tag": "chown"
  },
  {
    "question": "MySQL LOAD_FILE returns NULL",
    "answer": "I found out that it has to do with AppArmor. I disabled AppArmor for MySQL and it worked.\nFor people having the same problem, please read here: http://www.cyberciti.biz/faq/ubuntu-linux-howto-disable-apparmor-commands/",
    "tag": "chown"
  },
  {
    "question": "fopen Creates File, But how to change permissions?",
    "answer": "You should be able to chmod it using only the following line:\nchmod($filename, 0777);\n\nNote the 0 before the 777.\nAlso do not change the ownership before it has been chmod'ed",
    "tag": "chown"
  },
  {
    "question": "Used chown for /var/lib/mysql to change owner from root, now getting Error 1049 (42000) in mysql",
    "answer": "The normal ownership of everything in /var/lib/mysql is mysql:mysql. So you should be able to fix it with:\nsudo chown -R mysql:mysql /var/lib/mysql",
    "tag": "chown"
  },
  {
    "question": "chmod 775 on a folder but not all files under that folder",
    "answer": "Remove the -R flag. That means it changes all files and folders in the subdirectory as well.\n# chown root:user1 /var/www/project/\n# chmod 775 /var/www/project/",
    "tag": "chown"
  },
  {
    "question": "Installing Mongo and chown /data/db yields \"illegal user name\" error",
    "answer": "You can look for the current username and then try setting the permissions.\n$ whoami\nusername\n$ sudo chown username /data/db\n\nHope that helps.",
    "tag": "chown"
  },
  {
    "question": "Sync file permissions *only*",
    "answer": "If you have the \"normal\" content of /etc available on the same system (like mounted in some other directory, let's say /mnt/correct/etc), you could use the --reference parameter to chmod and chown commands, and combine it with find that is started from the \"normal\" directory: \n$ cd /mnt/correct/etc\n$ find . ! -type l -exec chown -v --reference='{}' /etc/'{}' \\;\n$ find . ! -type l -exec chmod -v --reference='{}' /etc/'{}' \\;\n\n(I'm assuming you're on a UNIX system with GNU coreutils versions of chmod and chown.)\nThe \"! -type l\" condition in find excludes symbolic links, because otherwise chmod will use the link's permissions to change the file the link points to (and same applies to chown).",
    "tag": "chown"
  },
  {
    "question": "docker-node: Running as non-root user, file permissions",
    "answer": "It definitely does, however I would also remove the chown binary (as well as all admin tools). This would make it harder when someone accesses the container as e.g. root. See here for a related answer.\nAlso, see this Dockerfile for inspiration.",
    "tag": "chown"
  },
  {
    "question": "FATAL:credentials.cc(127) Check failed: . : Permission denied (13) Trace/breakpoint trap (core dumped)",
    "answer": "One other thing to watch out for is apparmor. Check your dmesg log for something like:\n[  283.250675] audit: type=1400 audit(1702938463.134:129): apparmor=\"DENIED\" operation=\"userns_create\" class=\"namespace\" info=\"User namespace creation restricted\" error=-13 profile=\"unconfined\" pid=4483 comm=\"your-app\" requested=\"userns_create\" denied=\"userns_create\"\n\nIf that's the case, try:\nsudo sysctl -w kernel.apparmor_restrict_unprivileged_userns=0\n\nIt took me a while to figure this one out, as most answers keep referring to the nvidia modules problems, while this was not the case in my situation.\nref: https://github.com/flatpak/flatpak/issues/5462\nTo make it persist after reboot, add a line to /etc/sysctl.conf",
    "tag": "chown"
  },
  {
    "question": "chown docker volumes on host (possibly through docker-compose)",
    "answer": "It's best to not try to access files inside /var/lib/docker directly. Those directories are meant to be managed by the docker daemon, and not to be messed with.\nTo access the data inside a volume, there's a number of options;\n\nuse a bind-mounted directory (you considered that, but didn't fit your use case).\nuse a \"service\" container that uses the same volume and makes it accessible through that container, for example a container running ssh (to use scp) or a SAMBA container (such as svendowideit/samba)\nuse a volume-driver plugin. there's various plugins around that offer all kind of options. For example, the local persist plugin is a really simple plug-in that allows you to specify where docker should store the volume data (so outside of /var/lib/docker)",
    "tag": "chown"
  },
  {
    "question": "/usr/bin/sudo must be owned by uid 0 and have the setuid bit set version .ubantu14.04 LTS",
    "answer": "First restart your pc, and press the ESC key while Ubuntu is booting.\nThis will bring you up the boot menu.\nSelect Advanced Options.\nSelect your OS version in (recovery mode), and press Enter Key.\nIt will bring you up another screen. Now select “Drop to root shell prompt” and press Enter.\nIt will load a command line at the bottom of the screen.\nNow run each of the following commands.\nmount -o remount,rw /\nmount --all\nchown root:root /usr/bin/sudo\nchmod 4755 /usr/bin/sudo\nshutdown -r now",
    "tag": "chown"
  },
  {
    "question": "List too long to chmod recursively",
    "answer": "Omit the * after xargs chown because it will try to add the list of all file names twice twice (once from ls and then again from *).\nTry\nchown -R apache:apache .\n\nThis changes the current folder (.) and everything in it and always works. If you need different permissions for the folder itself, write them down and restore them afterwards using chown without -R.\nIf you really want to process only the contents of the folder, this will work:\nfind . -maxdepth 1 -not -name \".\" -print0 | xargs --null chown -R apache:apache",
    "tag": "chown"
  },
  {
    "question": "Why do my setuid root bash shell scripts not work?",
    "answer": "There is a pretty comprehansive answer at https://unix.stackexchange.com/questions/364/allow-setuid-on-shell-scripts\nBottom line is that there are two main points against it:\n\nA race condition between when the Kernel opens the file to find which interpreter it should execute and when the interpreter opens the file to read the script.\nShell scripts which execute many external programs without proper checks can be fooled into executing the wrong program (e.g. using malicious PATH), or expand variables in a broken way (e.g. having white space in variable values), and generally it has less control on how well the external programs it executes handle the input.\n\nHistorically, there was a famous bug in the original Bourne shell (at least on 4.2BSD, which is where I saw this in action) which allowed anyone to get interactive root shell by creating a symlink called -i to a suid shell script. That's possibly the original trigger for this being prohibited.\nEDIT: To answer \"How do I fix it\" - configure sudo to allow users to execute only these scripts as user root, and perhaps use a trick like in https://stackoverflow.com/a/4598126/164137 to find the original user's name and force operation on their own home directory, instead of letting them pass in any arbitrary input (i.e. in their current state, nothing in the scripts you include in your question prevents user1 from executing the scripts and passing them users2's directory, or any directory for that matter)",
    "tag": "chown"
  },
  {
    "question": "chown: /usr/local: Operation not permitted - issue with brew update /usr/local is not writable - MacOS 10.13.1 high sierra",
    "answer": "Reinstalling Homebrew worked for me\n/usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\"",
    "tag": "chown"
  },
  {
    "question": "Granting user permissions using sudo chown -R $USER:$USER /dir/ectory/",
    "answer": "This worked:\nsudo chown -R username:usergroup /var/www/nameofdirectory",
    "tag": "chown"
  },
  {
    "question": "Should Vagrant require sudo for each command?",
    "answer": "I did my first vagrant up as a part of my own installation script, which is ran as sudo. Because of that, every file installed belonged to the root user, not my current account.\nRelated: \n\nhttps://github.com/Varying-Vagrant-Vagrants/VVV/issues/261\nVagrant Up by Non-Sudo Vagrant User fails\n\nIn short, when you arrive to similar problem, you can always \"reclaim\" the ownership of relevant files.\nchown -R <USERNAME> /<YOUR-WEBSITES-DIRECTORY>/.vagrant/machines/\n\nchown -R <USERNAME> /<YOUR-HOME-DIRECTORY>/.vagrant.d\n\nvagrant up and other command should work without sudo from now on.",
    "tag": "chown"
  },
  {
    "question": "Node.JS: Alternative to chown string",
    "answer": "You've probably solved it by now, but for future reference: the uid-number package is used by npm, so I think it can be safely assumed that it works (and it does for me).",
    "tag": "chown"
  },
  {
    "question": "Changing ownership of ‘/usr/bin/’: Operation not permitted",
    "answer": "Solution:- Get in to Ubuntu Recovery Console\nStart your computer and press and hold SHIFT key while booting. It will take you to the grub loader page as shown in image – 1.\n\nImage 1\nSelect and enter Advanced options for Ubuntu and from there select the kernel named as recovery mode as shown in image – 2.\n\nImage 2\nselect root – drop to root shell prompt as shown in image – 3\n\nImage 3\nNow the file system is read only to Remount to Read Write run below command\n# mount -o remount,rw /\n\nmount –all\nthen need to change the ownership for sudo\n# chown root:root /usr/bin/sudo\n\ngive permisson for sudo\n# chmod 4755 /usr/bin/sudo\n\nit’s done … let’s see by restarting the machine\n# shutdown -r now\n\nYou should have your Sudo back by now....",
    "tag": "chown"
  },
  {
    "question": "Change the ownership (chown) from 'root' to 'apache'",
    "answer": "In order to change the ownership, try the following line: \nsudo chown -R apache /var/www/html/\n\nor \nsudo chown apache /var/www/html/www.example-virtualhost1.com\n\nThe structure is as follows please note the parentheses as an attempt to explain each piece of the command: \nsudo(run the command as root) chown(command to change ownership) -R(recursively change everything within the folder) apache(who you want to be the new owner) /var/www/html/(the folder you would like to modify ownership) \nOnce you have ran this command, you should be able to type in the following command: \nls -lr \n\nThat command will show you who has ownership.\nI hope this helps!",
    "tag": "chown"
  },
  {
    "question": "Change owner of the root folder and subfolders (Ubuntu 13.04)",
    "answer": "I am guessing when you ran the first command you also ended up modifying the ownership of the /usr/bin/sudo executable. \nIt is saying that effective UID isn't 0 (since root has EUID equal to 0). \nSo try to change owner of /usr/bin/sudo, and then try change the ownership of other files.",
    "tag": "chown"
  },
  {
    "question": "PHP mkdir and apache ownership",
    "answer": "Safe_mode is turn on on your server. The function mkdir() creates folder with owner (\"apache\", \"none\", ..) that different of the current script owner. And scripts couldn't upload (move, copy) files into that folder with another owner (that is not like current script owner).\nDisable safe_mode and that would be work.\nSee http://php.net/manual/en/features.safe-mode.php for details. \nP.S. With enable safe_mode you can't use chmod() function in php.",
    "tag": "chown"
  },
  {
    "question": "Command /usr/sbin/chown failed with exit code 1",
    "answer": "This problem is caused because some of your files (the ones that throw the chmod in xcode) don't have the proper owner/group set. They probably are set with root permissions, which can be caused by installing e.g. PhoneGap modules with sudo.\nRun a sudo chown -R username:staff YourAppFolder command on your original files and you will be able to archive your code again.",
    "tag": "chown"
  },
  {
    "question": "Docker EACCES permission denied mkdir",
    "answer": "COPY normally copies things into the image owned by root, and it will create directories inside the image if they don't exist.  In particular, when you COPY ./api/package.json ./api/, it creates the api subdirectory owned by root, and when you later try to run yarn install, it can't create the node_modules subdirectory because you've switched users.\nI'd recommend copying files into the container and running the build process as root.  Don't chown anything; leave all of these files owned by root.  Switch to an alternate USER only at the very end of the Dockerfile, where you declare the CMD.  This means that the non-root user running the container won't be able to modify the code or libraries in the container, intentionally or otherwise, which is a generally good security practice.\nFROM node:alpine\n\n# Don't RUN mkdir; WORKDIR creates the directory if it doesn't exist\n\nWORKDIR /usr/src/node-app\n\n# All of these files and directories are owned by root\nCOPY package.json yarn.lock ./\nCOPY ./api/package.json ./api/\nCOPY ./iso/package.json  ./iso/\n# Run this installation command still as root\nRUN yarn install --pure-lockfile\n\n# Copy in the rest of the application, still as root\nCOPY . .\n# RUN yarn build\n\n# Declare how to run the container -- _now_ switch to a non-root user\nEXPOSE 3000\nUSER node\nCMD yarn start",
    "tag": "chown"
  },
  {
    "question": "How to preserve ownership and permissions when doing an atomic file replace?",
    "answer": "Is the only solution to ensure that the uid/gid of the file matches the current user and group of the process overwriting the file?\n\nNo.\nIn Linux, a process with the CAP_LEASE capability can obtain an exclusive lease on the file, which blocks other processes from opening the file for up to /proc/sys/fs/lease-break-time seconds. This means that technically, you can take the exclusive lease, replace the file contents, and release the lease, to modify the file atomically (from the perspective of other processes).\nAlso, a process with the CAP_CHOWN capability can change the file ownership (user and group) arbitrarily.\n\nIs there a safe way to [handle the case where the uid or gid does not match the current process], or do we necessarily lose the atomic guarantee?\n\nConsidering that in general, files may have ACLs and xattrs, it might be useful to create a helper program, that clones the ownership including ACLs, and extended attributes, from an existing file to a new file in the same directory (perhaps with a fixed name pattern, say .new-################, where # indicate random alphanumeric characters), if the real user (getuid(), getgid(), getgroups()) is allowed to modify the original file. This helper program would have at least the CAP_CHOWN capability, and would have to consider the various security aspects (especially the ways it could be exploited). (However, if the caller can overwrite the contents, and create new files in the target directory -- the caller must have write access to the target directory, so that they can do the rename/hardlink replacement --, creating a clone file on their behalf with empty contents ought to be safe. I would personally exclude target files owned by root user or group, though.)\nEssentially, the helper program would behave much like the mktemp command, except it would take the path to the existing target file as a parameter. It would then be relatively straightforward to wrap it into a library function, using e.g. fork()/exec() and pipes or sockets.\nI personally avoid this problem by using group-based access controls: dedicated (local) group for each set. The file owner field is basically just an informational field then, indicating the user that last recreated  (or was in charge of) said file, with access control entirely based on the group. This means that changing the mode and the group id to match the original file suffices. (Copying ACLs would be even better, though.) If the user is a member of the target group, they can do the fchown() to change the group of any file they own, as well as the fchmod() to set the mode, too.",
    "tag": "chown"
  },
  {
    "question": "Is www-data user the same as apache user on Centos system",
    "answer": "Excerpt from :http://www.linuxquestions.org/questions/linux-newbie-8/is-www-data-some-sort-of-generic-user-933894/\nOn many (not all) distributions, www-data is the user under which the Apache web server runs. This also means that everything done by Apache (especially including PHP scripts) will be done with the permissions of user www-data (and also group www-data) by default.",
    "tag": "chown"
  },
  {
    "question": "How to change permission of vmware's shared folder",
    "answer": "probably a little bit late, but anyway.\nFirstly, unmount your shared folder:\nsudo umount /mnt/hgfs\n\nthen run:\nvmhgfs-fuse .host:/ /mnt/hgfs -o uid=1000 -o gid=1000 -o umask=0033\n\nwhere you should consider change uid and gid to yours. Remember that:\nid -u\nid -g\n\nwill return your current user and group ID.\nTake a look to vmhgfs-fuse --help for more options ;)",
    "tag": "chown"
  },
  {
    "question": "GHCi - Haskell Compiler Error - /home/user/.ghci is owned by someone else, IGNORING",
    "answer": "You can try:\n$ chmod 600 ~/.ghci\nThis has removed the permission to use it for any groups other than me.",
    "tag": "chown"
  },
  {
    "question": "Changing owner and group of newly created directories using PHP script",
    "answer": "Not tested, but after creating the folder, you can run another line of code to change the owner/group\n\n// define user and group\n$owner = \"dropbox\";\n$group = \"dropbox\";\n$folder = \"/home/dropbox/New_Project_Name/new_folders\";\n\n// change the owner and group\nchown($folder, $owner);\nchgrp($folder, $group);\n\nKeep in mind, that it might throw an error, because there are subfolders and the operation fails. A while loop should solve the problem.\nThere might be an issues with the permissions, up to the server-config\nThere is another way to run it recursively with the \"exec\" command.\nyou can go like this:\nexec(\"chown -R \".$owner.\":\".$group.\" \".$folder);\n\nThis will change user and group for the folder and all sub-folders. But beware,\nusing system is \"dangerous\". You can run any shell-commands. Don't play around with it too much.",
    "tag": "chown"
  },
  {
    "question": "Getting error chown: invalid group: ‘nobody:nogroup’ while setting up an NFS server drive ownership permission",
    "answer": "sudo chown -R nobody:nogroup /var/lib/tftpboot\n\nthis is work me on ubuntu server",
    "tag": "chown"
  },
  {
    "question": "Ansible chown operation not permitted for non-root user",
    "answer": "Do I need to run this as root or is it something else?\nroot is needed. See for example Changing Ownership\n\n\"The super user, root, has the unrestricted capability to change the ownership of any file but normal users can change the ownership of only those files that they own.\"\n\nThis practically means that normal users are only able to change the group of a file they own to a group they are a member of.\nIf I do need to run it as root, does become work?\nYes. become works. Frequently used become plugin is sudo. Default value of become_user is root. \n- file:\n  become: yes\n  become_method: sudo\n  ...\n\nGenerally, enable remote/login user to become root. But in your specific case, because of delegate_to: localhost, enable the user who is running the play. For example change at localhost\n$ grep alex /etc/sudoers\nalex ALL=(ALL) NOPASSWD: ALL\n\nSee plugin list for other options.",
    "tag": "chown"
  },
  {
    "question": "How to give docker container write/chmod permissions on mapped volume?",
    "answer": "When you run the container, you should be able to use the --user=\"uid:gid\" flag to specify the user you wish to run the container as.\nSource: https://docs.docker.com/engine/reference/run/#user",
    "tag": "chown"
  },
  {
    "question": "Is there a way to diff chown/chmod between two servers' directories?",
    "answer": "Just use find on both directory servers with the -ls flag, like:\nfind directory_a -not ( test_for_ephemeral_files ) -ls > listing_a\nfind directory_b -not ( test_for_ephemeral_files ) -ls > listing_b\ndiff listing_a listing_b",
    "tag": "chown"
  },
  {
    "question": "MySQL local owner and permissions",
    "answer": "I had a similar problem where, in correcting a permissions issue with /user/local, I was no longer able to see any of my databases.\nThis fixed it:\nwhich mysql\n\nWhich returned\n$ /usr/local/mysql/bin/mysql\n\nI then changed into the root mysql directory and reset the ownership:\ncd /usr/local/mysql\nsudo chown -R mysql .\nsudo chgrp -R mysql .\n\nAfter restarting mysql, my databases were back.\nDocumentation:\nhttps://dev.mysql.com/doc/refman/5.6/en/data-directory-initialization.html",
    "tag": "chown"
  },
  {
    "question": "Is there an R function to change file ownership?",
    "answer": "As was already mentioned in the comments, the function Sys.chown() does not exist. I have written a function that does the job for me. Compared to other functions of the type Sys.*, this one has the disadvantage that chown requires sudo. Using sudo with R's system() does not work, but this answer suggests the use of gksudo, which works fine for me.\nSo this is the function definition:\nSys.chown <- function(paths, owner = NULL, group = NULL) {\n\n   # create string for user:owner\n   if (!is.null(group)) {\n      og <- paste0(owner, \":\", group)\n   } else {\n      og <- owner\n   }\n\n   # create string with files\n   files <- paste(paths, collapse = \" \")\n\n   # run command\n   system(paste(\"gksudo chown\", og, files))\n\n   invisible()\n}\n\nThe first part creates the string that sets the owner and the group, which should be of the form owner:group. However, it is possible to omit one or both of these argument and the function takes care of all the possibilities.\nNext comes the part where all the file names that have been supplied to paths are put into a single string.\nAnd finally, system() is used to do the actual  call of chown. gksudo will open a dialog window and ask for the password. Unfortunately, one has to type the password every time.\nThere are several useful options to chown and a better implementation of Sys.chown() should probably be able to handle some of them.\nExample\nsystem(\"ls -l\")\n## total 0\n## -rw-rw-r-- 1 user1 user1 0 Jan 21 19:32 test2.file\n## -rw-rw-r-- 1 user1 user1 0 Jan 21 19:32 test.file\nSys.chown(\"test.file\", owner = \"user2\")\nSys.chown(\"test2.file\", group = \"user2\")\nsystem(\"ls -l\")\n## total 0\n## -rw-rw-r-- 1 user1 user2 0 Jan 21 19:32 test2.file\n## -rw-rw-r-- 1 user2 user1 0 Jan 21 19:32 test.file\nSys.chown(\"test.file\", owner = \"user1\", group = \"user2\")\nsystem(\"ls -l\")\n## total 0\n## -rw-rw-r-- 1 user1 user2 0 Jan 21 19:32 test2.file\n## -rw-rw-r-- 1 user1 user2 0 Jan 21 19:32 test.file\nSys.chown(dir(), owner = \"user1\", group = \"user1\")\nsystem(\"ls -l\")\n## total 0\n## -rw-rw-r-- 1 user1 user1 0 Jan 21 19:32 test2.file\n## -rw-rw-r-- 1 user1 user1 0 Jan 21 19:32 test.file",
    "tag": "chown"
  },
  {
    "question": "Chown permission denied while Docker volume binding",
    "answer": "sudo chown \"$USER\":\"$USER\" /home/\"$USER\"/.docker -R\nsudo chmod g+rwx \"$HOME/.docker\" -R",
    "tag": "chown"
  },
  {
    "question": "Docker Entrypoint Script Root Permission",
    "answer": "In these two steps:\nRUN chown root:root /usr/local/bin/entrypoint.sh\nRUN chmod 4755 /usr/local/bin/entrypoint.sh\n\nYou are ensuring the your entrypoint.sh script is owned by root, and then you are attempting to set the setuid bit.  Unfortunately, this second step isn't going to work: you can't mark a shell script (or in fact any interpreted script) as setuid; the bit will simply be ignored. See, e.g, this question.\nBut event if that worked, you would have another problem: your ENTRYPOINT script is responsible for handling any command passed into your container, either on the docker run command line or via the CMD directive in the Dockerfile.  If that setuid attempt worked, your ENTRYPOINT script would be running as root...which means you would end up running catalina.sh run as root, which means your USER directive is completely pointless.\nYou could get to the same place by simply dropping the USER directive, allowing the entrypoint to run as root, and then have your ENTRYPOINT script run the CMD as the tomcat user (e.g., using su or sudo):\nFROM tomcat:8.5.23-jre8-alpine\n\n#### OTHER IMAGE COMMANDS ####\n\nCOPY ./entrypoint.sh /usr/local/bin/entrypoint.sh\nENTRYPOINT [\"sh\", \"/usr/local/bin/entrypoint.sh\"]\nCMD [\"catalina.sh\", \"run\"]\n\nAnd in entrypoint.sh:\n#!/bin/sh\n\n# Do stuff as root here.\n$JAVA_HOME/bin/keytool -import -alias ca_local \\\n  -keystore $JAVA_HOME/lib/security/cacerts \n  -storepass XXXXX -noprompt -trustcacerts \\\n  -file /usr/local/tomcat/certificates/ca-local.cer\n\n# Now run everything else as a non-root user\nexec su - tomcat -c \"$*\"\n\nThis is a very common mechanism for handling initial setup tasks that must be run as root while running everything else as a non-root user.\nThere are still a number of problems here: the underlying tomcat:8.5.23-jre8-alpine doesn't have a tomcat user, and the scripts in /usr/local/tomcat/bin are only executable by root. So if you want this particular image to work with a non-root user you probably have a bunch of configuration changes to make first.",
    "tag": "chown"
  },
  {
    "question": "How do I chown to inherit parent directory's owner and group or the current account user",
    "answer": "The simples think I could think of is\nchown -R `stat . -c %u:%g` *\n\nYou might also use $(...) instead of backticks here.",
    "tag": "chown"
  },
  {
    "question": "setting file ownership with mercurial",
    "answer": "You should be pulling as you not as root and then you wouldn't have that problem.  However, if that's infeasible for some reason you can always use an update hook to correct file permissions.  In your repository's .hg/hgrc file you'd put:\n[hooks]\nupdate = chown -R username:usergroup /home/username\n\nAnd that command will be automatically run after each hg update (or hg pull -u).",
    "tag": "chown"
  },
  {
    "question": "Unix: command for sudo-mv-chown combo?",
    "answer": "Try the install command (options -o, -g and -m).",
    "tag": "chown"
  },
  {
    "question": "Clone User, copy all files and folders and copy rights (Linux)",
    "answer": "You want to recursively copy the whole user folder, not the contents. That automatically includes all \"hidden\" files. Afterwards you then change the ownership of the new folder:\ncp -pPr /home/user1 /home/user2\nchown -R user2 /home/user2\n\nYou really want to start reading the manual pages of the tools you use. They contain a wealth of precise information to explore: man cp",
    "tag": "chown"
  },
  {
    "question": "Apache/CentOS 7: /var/www/html/ owned by root but created files owned by apache - how do I resolve this?",
    "answer": "Check your /etc/httpd/conf/httpd.conf file and search for user and group [e.g. User apache Group apache]. Those are the owners by default. In your website there is no need too add write permissions for files and folders assigned to user:group, but you can set readable by owner and others in order to be accessible via web.\nUpdated answer:\nThe main reason DirectoryRoot (/var/www/html) owned by root is security. You can leave root as owner of files and set group to apache. Regarding security you make sure apache group has read-only access to files [-> One first meassure]. The security is not an illusion. While files are owned by root and do not have rw access from others, it is hard for external attackers to gain write access to files [because this is the most common way to hijack a site].",
    "tag": "chown"
  },
  {
    "question": "UNIX How to copy entire directory (subdirectories and files) from one location to another and retain permissions",
    "answer": "Use cp -a to copy permissions and user/group.\nThe man page explains it:\n-a, --archive\n    same as -dR --preserve=all\n...\n-d     same as --no-dereference --preserve=links\n...\n-R, -r, --recursive\n   copy directories recursively\n...\n--preserve[=ATTR_LIST]\n   preserve the specified attributes (default: mode,ownership,timestamps), if possible additional attributes:\n   context, links, xattr, all",
    "tag": "chown"
  },
  {
    "question": "Mac OSX Apache Write Privileges",
    "answer": "Have the user own the files. Change the group to _www. Give write permission to group.\nchown -R <username> .\nchgrp -R _www .\nchmod -R g+w .",
    "tag": "chown"
  },
  {
    "question": "MongoDB on macOS: How do I resolve chown: data/db: No such file or directory",
    "answer": "On your Mac, if you used Homebrew to install MongoDB, the data directory is created on installation. However, it is not data/db. Instead, on Intel Mac it is /usr/local/var/mongodb and on M1 Mac it is /opt/homebrew/var/mongodb.\nTo correctly install MongoDB using Homebrew (of course, check what's the latest version):\n brew tap mongodb/brew\n brew install mongodb-community@6.0\n\nFirst, check that your MongoDB is running ok by listing the running services using brew:\nbrew services list\n\nYou should see something like:\nName              Status  User File\nMongoDB-community started root\n\nIf not, try to stop, start or restart the service:\n brew services stop mongodb/brew/mongodb-community\n brew services start mongodb/brew/mongodb-community\n brew services restart mongodb/brew/mongodb-community\n\nFor further information about the possible problems, check the log file:\n tail $(brew --prefix)/var/log/mongodb/mongo.log\n\nMore details in the official documentation.",
    "tag": "chown"
  },
  {
    "question": "Dired: how to do chown as superuser?",
    "answer": "Yes, just type /sudo:root@localhost/... when C-x C-f asks you for file name. This will open a trampified dired where dired-do-chown and other dired commands work just fine.\n(Note that you can press TAB after /sudo:, and minibuffer completion will insert the @localhost boilerplate.)",
    "tag": "chown"
  },
  {
    "question": "chown returns invalid user in Dockerfile",
    "answer": "Your useradd command is wrong as you are creating an user named 123. The last argument of the command is the username so you should use remote_user instead.",
    "tag": "chown"
  },
  {
    "question": "Correct permissions for Tomcat installation on CentOS",
    "answer": "The key reason for setting up all these permissions is security. The program Tomcat is executed as user 'tomcat' and group 'tomcat'. No real person can login as these users.\nGiving the minimal necessary permissions to these kind of users/processes prevents that a rouge or malicious process inside Tomcat can do things to your Linux environment which are not desired.",
    "tag": "chown"
  },
  {
    "question": "How to change file ownership using boost library?",
    "answer": "It doesn't exist. Boost's Filesystem doesn't concern itself with owners (or ACLs, for that matter).",
    "tag": "chown"
  },
  {
    "question": "Monitor directory and change ownership of files upon creation",
    "answer": "By default, sed buffers its output when it's writing to a pipe. Use the -u option to unbuffer it.\ninotifywait -mr -e create /opt/freeswitch/storage/ 2>&-| sed -u 's/ CREATE //g' |\n    while read file; do\n    chown apache.apache $file\n    done",
    "tag": "chown"
  },
  {
    "question": "Best practices in assigning permissions to web folders",
    "answer": "Read your command again. What you are saying is \"make everything executable\" below these directories. Does an HTML or gif to be executable? I don't think so.\nRegarding a directory which should not be writable by the webserver. Think of what you want to do. You want to revoke the right to write a directory from the webserver and the webserver group (and everybody else anyway). So it would translate to chmod -w theDir. What you did is to tell the system \"I want root to make changes to that directory which shall be readable by everybody and the root group\". I highly doubt that.\nSo I would suggest having the directory owned by a webserver user with only minimal read access, it should belong to a group (of users, that is) which is allowed to do the necessary of the modification. The webserver does not belong to that group, as you want the outside world to be prevented from making modifications. Another option would be to hand over all the directories to root and to the editor group and modify what the webserver can do via the \"others\" permission group. But what to use heavily depends on your environment.\nEdit:\nIn general, the \"least rights\" policy is considered good practice: give away as few rights as possible to get the job done. This means read access to static files and depending on your environment php files, read and execute rights for cgi executables and read and execute rights for directories. Execute rights for directories allow you to enter and read it. No directory in the document root should be writable by the webserver ever. It is a security risk, even though some developers of bigger CMS do not seem to care to much about that. For temporary folders I would set the user and groups to nobody:nogroup and set the sticky bit for both user and groups.",
    "tag": "chown"
  },
  {
    "question": "How do I copy a folder from remote to local using scp?",
    "answer": "scp -r user@your.server.example.com:/path/to/foo /home/user/Desktop/\n\nBy not including the trailing '/' at the end of foo, you will copy the directory itself (including contents), rather than only the contents of the directory.\nFrom man scp (See online manual)\n\n-r Recursively copy entire directories",
    "tag": "ssh"
  },
  {
    "question": "ssh \"permissions are too open\"",
    "answer": "The keys need to be read-writable only by you:\nchmod 600 ~/.ssh/id_rsa\n\nAlternatively, the keys can be only readable by you (this also blocks your write access):\nchmod 400 ~/.ssh/id_rsa\n\n600 appears to be better in most cases, because you don't need to change file permissions later to edit it. (See the comments for more nuances)\nThe relevant portion from the manpage (man ssh)\n\n ~/.ssh/id_rsa\n         Contains the private key for authentication.  These files contain sensitive \n         data and should be readable by the user but not\n         accessible by others (read/write/execute).  ssh will simply ignore a private \n         key file if it is              \n         accessible by others.  It is possible to specify a\n         passphrase when generating the key which will be used to encrypt the sensitive \n         part of this file using 3DES.\n\n ~/.ssh/identity.pub\n ~/.ssh/id_dsa.pub\n ~/.ssh/id_ecdsa.pub\n ~/.ssh/id_rsa.pub\n         Contains the public key for authentication.  These files are not sensitive and \n         can (but need not) be readable by anyone.",
    "tag": "ssh"
  },
  {
    "question": "Could not open a connection to your authentication agent",
    "answer": "Did You Start ssh-agent?\nYou might need to start ssh-agent before you run the ssh-add command:\neval `ssh-agent -s`\nssh-add\n\nNote that this will start the agent for msysgit Bash on Windows. If you're using a different shell or operating system, you might need to use a variant of the command, such as those listed in the other answers.\nSee the following answers:\n\nssh-add complains: Could not open a connection to your authentication agent\nGit push requires username and password (contains detailed instructions on how to use ssh-agent)\nHow to run (git/ssh) authentication agent?.\nCould not open a connection to your authentication agent\n\nTo automatically start ssh-agent and allow a single instance to work in multiple console windows, see Start ssh-agent on login.\nWhy do we need to use eval instead of just ssh-agent?\nSSH needs two things in order to use ssh-agent: an ssh-agent instance running in the background, and an environment variable set that tells SSH which socket it should use to connect to the agent (SSH_AUTH_SOCK IIRC). If you just run ssh-agent then the agent will start, but SSH will have no idea where to find it.\nfrom this comment.\nPublic vs Private Keys\nAlso, whenever I use ssh-add, I always add private keys to it. The file ~/.ssh/id_rsa.pub looks like a public key, I'm not sure if that will work.  Do you have a ~/.ssh/id_rsa file? If you open it in a text editor, does it say it's a private key?",
    "tag": "ssh"
  },
  {
    "question": "How to specify the private SSH-key to use when executing shell command on Git?",
    "answer": "None of these solutions worked for me.\nInstead, I elaborate on @Martin v. Löwis  's mention of setting a config file for SSH.\nSSH will look for the user's ~/.ssh/config file. I have mine setup as:\nHost gitserv\n    Hostname remote.server.com\n    IdentityFile ~/.ssh/id_rsa.github\n    IdentitiesOnly yes # see NOTES below\n    AddKeysToAgent yes\n\nAnd I add a remote git repository:\ngit remote add origin git@gitserv:myrepo.git\n\n(or clone a fresh copy of the repo with git@gitserv:myrepo.git as address)\nAnd then git commands work normally for me.\ngit push -v origin master\n\nIf you have submodules, you can also execute the following in the repo directory, to force the submodules to use the same key:\ngit config url.git@gitserv:.insteadOf https://remote.server.com\n\nNOTES\n\nThe IdentitiesOnly yes is required to prevent the SSH default behavior of sending the identity file matching the default filename for each protocol. If you have a file named ~/.ssh/id_rsa that will get tried BEFORE your ~/.ssh/id_rsa.github without this option.\n\nAddKeysToAgent yes lets you avoid reentering the key passphrase every time.\n\nYou can also add User git to avoid writing git@ every time.\n\n\nReferences\n\nBest way to use multiple SSH private keys on one client\nHow could I stop ssh offering a wrong key",
    "tag": "ssh"
  },
  {
    "question": "How do I remove the passphrase for the SSH key without having to create a new key?",
    "answer": "Short answer:\n$ ssh-keygen -p\n\nThis will then prompt you to enter the keyfile location, the old passphrase, and the new passphrase (which can be left blank to have no passphrase).\n\nIf you would like to do it all on one line without prompts do:\n$ ssh-keygen -p [-P old_passphrase] [-N new_passphrase] [-f keyfile]\n\nImportant: Beware that when executing commands they will typically be logged in your ~/.bash_history file (or similar) in plain text including all arguments provided (i.e. the passphrases in this case). It is, therefore, is recommended that you use the first option unless you have a specific reason to do otherwise.   \nNotice though that you can still use -f keyfile without having to specify -P nor -N, and that the keyfile defaults to ~/.ssh/id_rsa, so in many cases, it's not even needed.\nYou might want to consider using ssh-agent, which can cache the passphrase for a time. The latest versions of gpg-agent also support the protocol that is used by ssh-agent.",
    "tag": "ssh"
  },
  {
    "question": "How to use SSH to run a local shell script on a remote machine?",
    "answer": "If Machine A is a Windows box, you can use Plink (part of PuTTY) with the -m parameter, and it will execute the local script on the remote server.\nplink root@MachineB -m local_script.sh\n\nIf Machine A is a Unix-based system, you can use:\nssh root@MachineB 'bash -s' < local_script.sh\n\nYou shouldn't have to copy the script to the remote server to run it.",
    "tag": "ssh"
  },
  {
    "question": "ssh remote host identification has changed",
    "answer": "Here is the simplest solution:\nssh-keygen -R <host>\n\nFor example,\nssh-keygen -R 192.168.3.10\n\nFrom the ssh-keygen man page:\n\n-R hostname Removes all keys belonging to hostname from a known_hosts file.  This option is useful to delete hashed hosts (see the -H option above).\n\nPS: For windows, execute this command in git bash\nPS2：For specific port， use keygen -R [127.0.0.1]:3022",
    "tag": "ssh"
  },
  {
    "question": "How to Configure Multiple SSH Private Keys for Different Servers Efficiently?",
    "answer": "From my .ssh/config:\nHost myshortname realname.example.com\n    HostName realname.example.com\n    IdentityFile ~/.ssh/realname_rsa # private key for realname\n    User remoteusername\n\nHost myother realname2.example.org\n    HostName realname2.example.org\n    IdentityFile ~/.ssh/realname2_rsa  # different private key for realname2\n    User remoteusername\n\nThen you can use the following to connect:\nssh myshortname\nssh myother\nAnd so on.",
    "tag": "ssh"
  },
  {
    "question": "How to solve Permission denied (publickey) error when using Git?",
    "answer": "If the user has not generated a ssh public/private key pair set before\nThis info is working on theChaw but can be applied to all other git repositories which support SSH pubkey authentications. (See [gitolite][1], gitlab or github for example.)\n\nFirst start by setting up your own public/private key pair set. This\ncan use either DSA or RSA, so basically any key you setup will work.\nOn most systems you can use ssh-keygen.\n\n\n\nFirst you'll want to cd into your .ssh directory. Open up the terminal and run:\n\n\ncd ~/.ssh && ssh-keygen\n\n\nNext you need to copy this to your clipboard.\nOn OS X run: cat id_rsa.pub | pbcopy\nOn Linux run: cat id_rsa.pub | xclip\nOn Windows (via Cygwin/Git Bash) run: cat id_rsa.pub | clip\nOn Windows (Powershell) run: Get-Content id_rsa.pub | Set-Clipboard (Thx to @orion elenzil)\nAdd your key to your account via the website.\nFinally setup your .gitconfig.\ngit config --global user.name \"bob\"\ngit config --global user.email bob@...\n(don't forget to restart your command line to make sure the config is reloaded)\n\nThat's it you should be good to clone and checkout.\n\nFurther information can be found at https://help.github.com/articles/generating-ssh-keys (thanks to @Lee Whitney)\n[1]: https://github.com/sitaramc/gitolite\n-\nIf the user has generated a ssh public/private key pair set before\n\ncheck which key have been authorized on your github or gitlab account settings\ndetermine which corresponding private key must be associated from your local computer\n\neval $(ssh-agent -s)\n\ndefine where the keys are located\n\nssh-add ~/.ssh/id_rsa",
    "tag": "ssh"
  },
  {
    "question": "Calculate RSA key fingerprint",
    "answer": "Run the following command to retrieve the SHA256 fingerprint of your SSH key (-l means \"list\" instead of create a new key, -f means \"filename\"):\n$ ssh-keygen -lf /path/to/ssh/key\n\nSo for example, on my machine the command I ran was (using RSA public key):\n$ ssh-keygen -lf ~/.ssh/id_rsa.pub\n2048 00:11:22:33:44:55:66:77:88:99:aa:bb:cc:dd:ee:ff /Users/username/.ssh/id_rsa.pub (RSA)\n\nTo get the GitHub (MD5) fingerprint format with newer versions of ssh-keygen, run:\n$ ssh-keygen -E md5 -lf <fileName>\n\nBonus information:\nssh-keygen -lf also works on known_hosts and authorized_keys files.\nTo find most public keys on Linux/Unix/OS X systems, run\n$ find /etc/ssh /home/*/.ssh /Users/*/.ssh -name '*.pub' -o -name 'authorized_keys' -o -name 'known_hosts'\n\n(If you want to see inside other users' homedirs, you'll have to be root or sudo.)\nThe ssh-add -l is very similar, but lists the fingerprints of keys added to your agent. (OS X users take note that magic passwordless SSH via Keychain is not the same as using ssh-agent.)",
    "tag": "ssh"
  },
  {
    "question": "\"UNPROTECTED PRIVATE KEY FILE!\" Error using SSH into Amazon EC2 Instance (AWS)",
    "answer": "The problem is wrong set of permissions on the file.\nEasily solved by executing -\nchmod 400 mykey.pem\nTaken from AWS instructions -\n\nYour key file must not be publicly viewable for SSH to work. Use this\ncommand if needed:  chmod 400 mykey.pem\n\n400 protects it by making it read only and only for the owner.",
    "tag": "ssh"
  },
  {
    "question": "How do I access my SSH public key?",
    "answer": "cat ~/.ssh/id_rsa.pub or cat ~/.ssh/id_dsa.pub or cat ~/.ssh/id_ed25519.pub\nYou can list all the public keys you have by doing:\n$ ls ~/.ssh/*.pub",
    "tag": "ssh"
  },
  {
    "question": "SSH Key - Still asking for password and passphrase",
    "answer": "Add Identity without Keychain\nThere may be times in which you don't want the passphrase stored in the keychain, but don't want to have to enter the passphrase over and over again.\nYou can do that like this:\nssh-add ~/.ssh/id_rsa \n\nThis will ask you for the passphrase, enter it and it will not ask again until you restart.\nAdd Identity Using Keychain\nAs @dennis points out in the comments, to persist the passphrase through restarts by storing it in your keychain, you can use the --apple-use-keychain option (-k for Ubuntu) when adding the identity like this:\nssh-add --apple-use-keychain ~/.ssh/id_rsa\n\nOnce again, this will ask you for the passphrase, enter it and this time it will never ask again for this identity.",
    "tag": "ssh"
  },
  {
    "question": "How to download a file from server using SSH?",
    "answer": "In your terminal, type:\nscp your_username@remotehost.edu:foobar.txt /local/dir\n\nreplacing the username, host, remote filename, and local directory as appropriate.\nIf you want to access EC2 (or other service that requires authenticating with a private key), use the -i option:\nscp -i key_file.pem your_username@remotehost.edu:/remote/dir/foobar.txt /local/dir\n\nFrom: http://www.hypexr.org/linux_scp_help.php",
    "tag": "ssh"
  },
  {
    "question": "SSH Key: “Permissions 0644 for 'id_rsa.pub' are too open.” on mac",
    "answer": "I suggest you to do:\nchmod 400 ~/.ssh/id_rsa\nIt works fine for me.",
    "tag": "ssh"
  },
  {
    "question": "How to permanently add a private key with ssh-add on Ubuntu?",
    "answer": "A solution would be to force the key files to be kept permanently, by adding them in your ~/.ssh/config file:\nHost *\n    IdentityFile ~/.ssh/gitHubKey\n    IdentityFile ~/.ssh/id_rsa_buhlServer\n\nIf you do not have a 'config' file in the ~/.ssh directory, then you should create one. It does not need root rights, so simply:\nnano ~/.ssh/config\n\n...and enter the lines above as per your requirements.\nFor this to work the file needs to have chmod 600. You can use the command chmod 600 ~/.ssh/config.\nIf you want all users on the computer to use the key put these lines into /etc/ssh/ssh_config and the key in a folder accessible to all.\nAdditionally if you want to set the key specific to one host, you can do the following in your ~/.ssh/config :\nHost github.com\n    User git\n    IdentityFile ~/.ssh/githubKey\n\nThis has the advantage when you have many identities that a server doesn't reject you because you tried the wrong identities first. Only the specific identity will be tried.",
    "tag": "ssh"
  },
  {
    "question": "Why are connections to GitHub over SSH throwing an error \"Warning: Remote Host Identification Has Changed\"?",
    "answer": "This happened because on the 24th of March 2023, GitHub updated their RSA SSH host key used to secure Git operations for GitHub.com because the private key was briefly exposed in a public GitHub repository.  You will get that message if you had remembered GitHub’s previous key fingerprint in your SSH client before that date.\nAs per per the linked blog post, the solution is to remove the old key by running this command:\n$ ssh-keygen -R github.com\n\nNow the next git connection (pull, push or clone) should ask if you trust the new SSH key.  Before entering yes, ensure the shown new key is valid, using the list:\nhttps://docs.github.com/en/authentication/keeping-your-account-and-data-secure/githubs-ssh-key-fingerprints\nRefer to the blog post for other ways to fix the issue.",
    "tag": "ssh"
  },
  {
    "question": "How to SSH to a VirtualBox guest externally through a host?",
    "answer": "The best way to login to a guest Linux VirtualBox VM is port forwarding. By default, you should have one interface already which is using NAT. Then go to the Network settings and click the Port Forwarding button.  Add a new Rule. As the rule name, insert \"ssh\". As \"Host port\", insert 3022. As \"Guest port\", insert 22. Everything else of the rule can be left blank.\nor from the command line\nVBoxManage modifyvm myserver --natpf1 \"ssh,tcp,,3022,,22\"\n\nwhere 'myserver' is the name of the created VM. Check the added rules:\nVBoxManage showvminfo myserver | grep 'Rule'\n\nThat's all! Please be sure you don't forget to install an SSH server in the VM:\nsudo apt-get install openssh-server\n\nTo SSH into the guest VM, write:\nssh -p 3022 user@127.0.0.1\n\nWhere user is your username within the VM.",
    "tag": "ssh"
  },
  {
    "question": "mysql_config not found when installing mysqldb python interface",
    "answer": "mySQLdb is a python interface for mysql, but it is not mysql itself. And apparently mySQLdb needs the command 'mysql_config', so you need to install that first.\nCan you confirm that you did or did not install mysql itself, by running \"mysql\" from the shell? That should give you a response other than \"mysql: command not found\". \nWhich linux distribution are you using? Mysql is pre-packaged for most linux distributions. For example, for debian / ubuntu, installing mysql is as easy as \nsudo apt-get install mysql-server\n\nmysql-config is in a different package, which can be installed from (again, assuming debian / ubuntu):\nsudo apt-get install libmysqlclient-dev\n\nif you are using mariadb, the drop in replacement for mysql, then run\nsudo apt-get install libmariadbclient-dev\n\nReference:\nhttps://github.com/JudgeGirl/Judge-sender/issues/4#issuecomment-186542797",
    "tag": "ssh"
  },
  {
    "question": "Git error: \"Host Key Verification Failed\" when connecting to remote repository",
    "answer": "As I answered previously in Cloning git repo causes error - Host key verification failed. fatal: The remote end hung up unexpectedly, add GitHub to the list of known hosts:\nssh-keyscan -t rsa github.com >> ~/.ssh/known_hosts",
    "tag": "ssh"
  },
  {
    "question": "Is it possible to specify a different ssh port when using rsync?",
    "answer": "Your command line should look like this:\nrsync -rvz -e 'ssh -p 2222' --progress ./dir user@host:/path\n\nthis works fine - I use it all the time without needing any new firewall rules - just note the SSH command itself is enclosed in quotes.",
    "tag": "ssh"
  },
  {
    "question": "Change key pair for ec2 instance",
    "answer": "This answer is useful in the case you no longer have SSH access to the existing server (i.e. you lost your private key).\nIf you still have SSH access, please use one of the answers below.\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html#replacing-lost-key-pair\nHere is what I did, thanks to Eric Hammond's blog post:\n\nStop the running EC2 instance\nDetach its /dev/xvda1 volume (let's call it volume A) - see here\nStart new t1.micro EC2 instance, using my new key pair. Make sure you create it in the same subnet, otherwise you will have to terminate the instance and create it again. - see here\nAttach volume A to the new micro instance, as /dev/xvdf (or /dev/sdf)\nSSH to the new micro instance and mount volume A to /mnt/tmp\n\n$ sudo mkdir /mnt/tmp; sudo mount /dev/xvdf1 /mnt/tmp\n\n\nCopy ~/.ssh/authorized_keys to /mnt/tmp/home/ubuntu/.ssh/authorized_keys\nLogout\nTerminate micro instance\nDetach volume A from it\nAttach volume A back to the main instance as /dev/xvda\nStart the main instance\nLogin as before, using your new .pem file\n\nThat's it.",
    "tag": "ssh"
  },
  {
    "question": "Specify an SSH key for git push for a given domain",
    "answer": "Even if the user and host are the same, they can still be distinguished in ~/.ssh/config.  For example, if your configuration looks like this:\nHost gitolite-as-alice\n  HostName git.company.com\n  User git\n  IdentityFile /home/whoever/.ssh/id_rsa.alice\n  IdentitiesOnly yes\n\nHost gitolite-as-bob\n  HostName git.company.com\n  User git\n  IdentityFile /home/whoever/.ssh/id_dsa.bob\n  IdentitiesOnly yes\n\nThen you just use gitolite-as-alice and gitolite-as-bob instead of the hostname in your URL:\ngit remote add alice git@gitolite-as-alice:whatever.git\ngit remote add bob git@gitolite-as-bob:whatever.git\n\nNote\nYou want to include the option IdentitiesOnly yes to prevent the use of default ids. Otherwise, if you also have id files matching the default names, they will get tried first because unlike other config options (which abide by \"first in wins\") the IdentityFile option appends to the list of identities to try. See: https://serverfault.com/questions/450796/how-could-i-stop-ssh-offering-a-wrong-key/450807#450807",
    "tag": "ssh"
  },
  {
    "question": "Adding a public key to ~/.ssh/authorized_keys does not log me in automatically",
    "answer": "You need to verify the permissions of the authorized_keys file and the folder / parent folders in which it is located.\nchmod 700 ~/.ssh\nchmod 600 ~/.ssh/authorized_keys\n\nFor more information see this page.\nYou may also need to change/verify the permissions of your home directory to remove write access for the group and others.\nchmod go-w ~",
    "tag": "ssh"
  },
  {
    "question": "Pseudo-terminal will not be allocated because stdin is not a terminal",
    "answer": "Try ssh -t -t(or ssh -tt for short) to force pseudo-tty allocation even if stdin isn't a terminal.\nSee also: Terminating SSH session executed by bash script\nFrom ssh manpage:\n-T      Disable pseudo-tty allocation.\n\n-t      Force pseudo-tty allocation.  This can be used to execute arbitrary \n        screen-based programs on a remote machine, which can be very useful,\n        e.g. when implementing menu services.  Multiple -t options force tty\n        allocation, even if ssh has no local tty.",
    "tag": "ssh"
  },
  {
    "question": "Starting ssh-agent on Windows 10 fails: \"unable to start ssh-agent service, error :1058\"",
    "answer": "Yeah, as others have suggested, this error seems to mean that ssh-agent is installed but its service (on windows) hasn't been started.\nYou can check this by running in Windows PowerShell:\n> Get-Service ssh-agent\n\nAnd then check the output of status is not running.\nStatus   Name               DisplayName\n------   ----               -----------\nStopped  ssh-agent          OpenSSH Authentication Agent\n\nThen check that the service has been disabled by running\n> Get-Service ssh-agent | Select StartType\n\nStartType\n---------\nDisabled\n\nI suggest setting the service to start manually. This means that as soon as you run ssh-agent, it'll start the service. You can do this through the Services GUI or you can run the command in admin mode:\n > Get-Service -Name ssh-agent | Set-Service -StartupType Manual\n\nAlternatively, you can set it through the GUI if you prefer.",
    "tag": "ssh"
  },
  {
    "question": "scp (secure copy) to ec2 instance without password",
    "answer": "I figured it out.  I had the arguments in the wrong order.  This works:\nscp -i mykey.pem somefile.txt root@my.ec2.id.amazonaws.com:/",
    "tag": "ssh"
  },
  {
    "question": "Transferring files over SSH",
    "answer": "You need to scp something somewhere. You have scp ./styles/, so you're saying secure copy ./styles/, but not where to copy it to.\nGenerally, if you want to download, it will go:\n# download: remote -> local\nscp user@remote_host:remote_file local_file \n\nwhere local_file might actually be a directory to put the file you're copying in. To upload, it's the opposite:\n# upload: local -> remote\nscp local_file user@remote_host:remote_file\n\nIf you want to copy a whole directory, you will need -r. Think of scp as like cp, except you can specify a file with user@remote_host:file as well as just local files.\nEdit: As noted in a comment, if the usernames on the local and remote hosts are the same, then the user can be omitted when specifying a remote file.",
    "tag": "ssh"
  },
  {
    "question": "Keep SSH session alive",
    "answer": "The ssh daemon (sshd), which runs server-side, closes the connection from the server-side if the client goes silent (i.e., does not send information). To prevent connection loss, instruct the ssh client to send a sign-of-life signal to the server once in a while.\nThe configuration for this is in the file $HOME/.ssh/config, create the file if it does not exist (the config file must not be world-readable, so run chmod 600 ~/.ssh/config after creating the file). To send the signal every e.g. four minutes (240 seconds) to the remote host, put the following in that configuration file:\nHost remotehost\n    HostName remotehost.com\n    ServerAliveInterval 240\n\nTo enable sending a keep-alive signal for all hosts, place the following contents in the configuration file:\nHost *\n    ServerAliveInterval 240",
    "tag": "ssh"
  },
  {
    "question": ".bashrc at ssh login",
    "answer": ".bashrc is not sourced when you log in using SSH. You need to source it in your .bash_profile like this:\nif [ -f ~/.bashrc ]; then\n  . ~/.bashrc\nfi",
    "tag": "ssh"
  },
  {
    "question": "What is the cleanest way to ssh and run multiple commands in Bash?",
    "answer": "How about a Bash Here Document:\nssh otherhost << EOF\n  ls some_folder; \n  ./someaction.sh 'some params'\n  pwd\n  ./some_other_action 'other params'\nEOF\n\nTo avoid the problems mentioned by @Globalz in the comments, you may be able to (depending what you're doing on the remote site) get away with replacing the first line with\nssh otherhost /bin/bash << EOF\n\nNote that you can do variable substitution in the Here document, but you may have to deal with quoting issues. For instance, if you quote the \"limit string\" (ie. EOF in the above), then you can't do variable substitutions. But without quoting the limit string, variables are substituted. For example, if you have defined $NAME above in your shell script, you could do\nssh otherhost /bin/bash << EOF\ntouch \"/tmp/${NAME}\"\nEOF\n\nand it would create a file on the destination otherhost with the name of whatever you'd assigned to $NAME. Other rules about shell script quoting also apply, but are too complicated to go into here.",
    "tag": "ssh"
  },
  {
    "question": "git clone with HTTPS or SSH remote?",
    "answer": "GitHub have changed their recommendation several times (example).\nIt appears that they currently recommend HTTPS because it is the easiest to set up on the widest range of networks and platforms, and by users who are new to all this.\nThere is no inherent flaw in SSH (if there was they would disable it) -- in the links below, you will see that they still provide details about SSH connections too:\n\nHTTPS is less likely to be blocked by a firewall.\nhttps://docs.github.com/en/get-started/getting-started-with-git/about-remote-repositories#cloning-with-https-urls\n\nThe https:// clone URLs are available on all repositories, regardless of visibility. https:// clone URLs work even if you are behind a firewall or proxy.\n\n\nAn HTTPS connection allows credential.helper to cache your password.\nhttps://docs.github.com/en/get-started/quickstart/set-up-git#connecting-over-https-recommended\n\nIf you clone with HTTPS, you can cache your GitHub credentials in Git using a credential helper. For more information, see \"Cloning with HTTPS urls\" and \"Caching your GitHub credentials in Git.\"",
    "tag": "ssh"
  },
  {
    "question": "How do I verify/check/test/validate my SSH passphrase?",
    "answer": "ssh-keygen -y\n\nssh-keygen -y will prompt you for the passphrase (if there is one).\n    If you input the correct passphrase, it will show you the associated public key.\n    If you input the wrong passphrase, it will display load failed.\n    If the key has no passphrase, it will not prompt you for a passphrase and will immediately show you the associated public key.\nNote that the order of arguments is important. -y must come before -f input_keyfile, else you will get the error Too many arguments.\n\ne.g.,\nCreate a new public/private key pair, with or without a passphrase:\n$ ssh-keygen -f /tmp/my_key\n...\n\nNow see if you can access the key pair:\n$ ssh-keygen -y -f /tmp/my_key\n\n\nFollowing are extended examples, with example output.\n\n\n\nBehavior with a passphrase:\nFirst, create a new public/private key pair, with a passphrase:\n$ ssh-keygen -f /tmp/my_key_with_passphrase\nGenerating public/private rsa key pair.\nEnter passphrase (empty for no passphrase):\nEnter same passphrase again:\nYour identification has been saved in /tmp/my_key_with_passphrase.\nYour public key has been saved in /tmp/my_key_with_passphrase.pub.\nThe key fingerprint is:\nde:24:1b:64:06:43:ca:76:ba:81:e5:f2:59:3b:81:fe rob@Robs-MacBook-Pro.local\nThe key's randomart image is:\n+--[ RSA 2048]----+\n|     .+          |\n|   . . o         |\n|    = . +        |\n|   = + +         |\n|  o = o S .      |\n|   + = + *       |\n|    = o o .      |\n|     . .         |\n|      E          |\n+-----------------+\n\nNow attempt to access the key pair by inputting the correct passphrase.\nNote that the public key will be shown and the exit status ($?) will be 0 to indicate success:\n$ ssh-keygen -y -f /tmp/my_key_with_passphrase\nEnter passphrase:\nssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDBJhVYDYxXOvcQw0iJTPY64anbwSyzI58hht6xCGJ2gzGUJDIsr1NDQsclka6s0J9TNhUEBBzKvh9nTAYibXwwhIqBwJ6UwWIfA3HY13WS161CUpuKv2A/PrfK0wLFBDBlwP6WjwJNfi4NwxA21GUS/Vcm/SuMwaFid9bM2Ap4wZIahx2fxyJhmHugGUFF9qYI4yRJchaVj7TxEmquCXgVf4RVWnOSs9/MTH8YvH+wHP4WmUzsDI+uaF1SpCyQ1DpazzPWAQPgZv9R8ihOrItLXC1W6TPJkt1CLr/YFpz6vapdola8cRw6g/jTYms00Yxf2hn0/o8ORpQ9qBpcAjJN\n$ echo $?\n0\n\nNow attempt to access the key pair by inputting an incorrect passphrase.\nNote that the load failed error message will be displayed (message may differ depending on OS) and the exit status ($?) will be 1 to indicate an error:\n$ ssh-keygen -y -f /tmp/my_key_with_passphrase\nEnter passphrase:\nload failed\n$ echo $?\n1\n\n\nBehavior with no passphrase:\nFirst, create a new public/private key pair, without a passphrase:\n$ ssh-keygen -f /tmp/my_key_with_no_passphrase\nGenerating public/private rsa key pair.\nEnter passphrase (empty for no passphrase):\nEnter same passphrase again:\nYour identification has been saved in /tmp/my_key_with_no_passphrase\nYour public key has been saved in /tmp/my_key_with_no_passphrase.pub\nThe key fingerprint is:\nSHA256:wx2N90FB68dUM9DnFXkUm/2RoiWV0hjF3N29D5TBeDo robbednark@Robs-MacBook-Pro.local\nThe key's randomart image is:\n+---[RSA 3072]----+\n|           .BBXXX|\n|           =o*==/|\n|          o.+*o*B|\n|       . . oEoo=+|\n|        S .. .o.=|\n|         .     ..|\n|                 |\n|                 |\n|                 |\n+----[SHA256]-----+\n\nNow attempt to access that key pair.  Note that there is no prompt for the passphrase, the public key will be displayed, and the exit status ($?) will be 0 to indicate success:\n$ ssh-keygen -y -f /tmp/my_key_with_no_passphrase\nssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDHr9DfECzng5+cSq2ZnuGMHU2c5j106bJlPt4a4SFWagQJJD9FyeC53sfzprTyuUnM7DYZ8y/QZ6xqI6aWqD7FGyrsvUktPoFShz7CyXZM85SriLJeV2ORRlUeszjK0hDk942nnKcu7SDugaVCQHmuPk9HrE/z4ikaLcD3N/MUNAwZGtyGI5yHQN9o3H064c6gESu/WyjaF2WNWcD8OoqiLXMj2gAWSNVCu43nFsdaW2UmVxiW8oI8ZoPKd5gndyPFOSQk4atYxyO4fhDoMqlwf4dPYNR3Gi6Bfc/8sHPRUiJGSSPSUOSlP/GPjFMY/J8lRex7745eGn3Tm98gtyeto1wTgom5O0H46LxjA3IEeeD4Nn4sQw/EF3lVr1RR7N8uTK4lrHUPKOFa26BpbIoNi6Omhmjgj5GoerMUIB+7aG7mUsQgxy6cTAUefIxVF6RG9GquS/NozC7KSJKfgKq73BROSJIf/9XMQhyK7LFnFFwzHOy6IxXPIFsPef1k69U= robbednark@Robs-MacBook-Pro.local\n$ echo $?\n0",
    "tag": "ssh"
  },
  {
    "question": "Copying files using rsync from remote server to local machine",
    "answer": "From your local machine:\nrsync -chavzP --stats user@remote.host:/path/to/copy /path/to/local/storage\n\nFrom your local machine with a non standard ssh port:\nrsync -chavzP -e \"ssh -p $portNumber\" user@remote.host:/path/to/copy /local/path\n\nOr from the remote host, assuming you really want to work this way and your local machine is listening on SSH:\nrsync -chavzP --stats /path/to/copy user@host.remoted.from:/path/to/local/storage\n\nSee man rsync for an explanation of my usual switches.",
    "tag": "ssh"
  },
  {
    "question": "How to fix \"ssh: connect to host github.com port 22: Connection timed out\" for git push/pull/... commands?",
    "answer": "For me none of the suggested solutions worked so I tried to fix it myself and I resolved it. I was getting this error on my AWS EC2 UBUNTU instance. I edited the ssh config (or add it if it does not exist).\nnano ~/.ssh/config\nAnd I added the following\nHost github.com\n Hostname ssh.github.com\n Port 443\n\nThen, run the command ssh -T git@github.com to confirm if the issue is fixed.\nAccording to this\n\nSometimes, firewalls refuse to allow SSH connections entirely. If using HTTPS cloning with credential caching is not an option, you can attempt to clone using an SSH connection made over the HTTPS port. Most firewall rules should allow this, but proxy servers may interfere\n\nHopefully this helps anyone else who's having the same issue I did.",
    "tag": "ssh"
  },
  {
    "question": "Start ssh-agent on login",
    "answer": "Please go through this article. You may find this very useful:\nhttps://web.archive.org/web/20210506080335/https://mah.everybody.org/docs/ssh\nJust in case the above link vanishes some day, I am capturing the main piece of the solution below:\n\nThis solution from Joseph M. Reagle by way of Daniel Starin:\nAdd this following to your .bash_profile\nSSH_ENV=\"$HOME/.ssh/agent-environment\"\n\nfunction start_agent {\n    echo \"Initialising new SSH agent...\"\n    /usr/bin/ssh-agent | sed 's/^echo/#echo/' >\"$SSH_ENV\"\n    echo succeeded\n    chmod 600 \"$SSH_ENV\"\n    . \"$SSH_ENV\" >/dev/null\n    /usr/bin/ssh-add; \n}\n\n# Source SSH settings, if applicable\n\nif [ -f \"$SSH_ENV\" ]; then\n    . \"$SSH_ENV\" >/dev/null\n    #ps $SSH_AGENT_PID doesn't work under Cygwin\n    ps -ef | grep $SSH_AGENT_PID | grep ssh-agent$ >/dev/null || {\n        start_agent\n    } \nelse\n    start_agent \nfi \n\nThis version is especially nice since it will see if you've already started ssh-agent  and, if it can't find it, will start it up and store the settings so that they'll be usable the next time you start up a shell.",
    "tag": "ssh"
  },
  {
    "question": "git remote add with other SSH port",
    "answer": "You can just do this:\ngit remote add origin ssh://user@host:1234/srv/git/example\n\n1234 is the ssh port being used",
    "tag": "ssh"
  },
  {
    "question": "Vagrant stuck connection timeout retrying",
    "answer": "I solved this problem, and will answer in case anyone else has a similar issue.\nWhat I did was: I enabled the GUI of Virtual box to see that it was waiting for input on startup to select whether I wanted to boot directly to ubuntu or safemode etc.\nTo turn on the GUI you have to put this in your vagrant config Vagrantfile:\nconfig.vm.provider :virtualbox do |vb|\n  vb.gui = true\nend",
    "tag": "ssh"
  },
  {
    "question": "Is it possible to create a remote repo on GitHub from the CLI without opening browser?",
    "answer": "CLI commands for github API v3 (replace all CAPS keywords):\ncurl -u 'USER' https://api.github.com/user/repos -d '{\"name\":\"REPO\"}'\n# Remember replace USER with your username and REPO with your repository/application name!\ngit remote add origin git@github.com:USER/REPO.git\ngit push origin master",
    "tag": "ssh"
  },
  {
    "question": "How do I get git to default to ssh and not https for new repositories",
    "answer": "Set up a repository's origin branch to be SSH\nThe GitHub repository setup page is just a suggested list of commands (and GitHub now suggests using the HTTPS protocol). Unless you have administrative access to GitHub's site, I don't know of any way to change their suggested commands.\nIf you'd rather use the SSH protocol, simply add a remote branch like so (i.e. use this command in place of GitHub's suggested command). To modify an existing branch, see the next section.\n$ git remote add origin git@github.com:nikhilbhardwaj/abc.git\n\n\nModify a pre-existing repository\nAs you already know,  to switch a pre-existing repository to use SSH instead of HTTPS, you can change the remote url within your .git/config file.\n[remote \"origin\"]\n    fetch = +refs/heads/*:refs/remotes/origin/*\n    -url = https://github.com/nikhilbhardwaj/abc.git\n    +url = git@github.com:nikhilbhardwaj/abc.git\n\nA shortcut is to use the set-url command:\n$ git remote set-url origin git@github.com:nikhilbhardwaj/abc.git\n\n\nMore information about the SSH-HTTPS switch\n\n\"Why is Git always asking for my password?\" - GitHub help page.\nGitHub's switch to Smart HTTP - relevant StackOverflow question\nCredential Caching for Wrist-Friendly Git Usage - GitHub blog post about HTTPS, and how to avoid re-entering your password",
    "tag": "ssh"
  },
  {
    "question": "Getting ssh to execute a command in the background on target machine",
    "answer": "This should solve your problem:\nnohup myprogram > foo.log 2> foo.err < /dev/null &\n\n\nThe syntax and unusual use of < /dev/null are explained especially well in this answer, quoted here for your convenience.\n\n< /dev/null is used to instantly send EOF to the program, so that it\ndoesn't wait for input (/dev/null, the null device, is a special file\nthat discards all data written to it, but reports that the write\noperation succeeded, and provides no data to any process that reads\nfrom it, yielding EOF immediately).\nSo the command:\nnohup myscript.sh >myscript.log 2>&1 </dev/null &\n#\\__/             \\___________/ \\__/ \\________/ ^\n# |                    |          |      |      |\n# |                    |          |      |  run in background\n# |                    |          |      |\n# |                    |          |   don't expect input\n# |                    |          |   \n# |                    |        redirect stderr to stdout\n# |                    |           \n# |                    redirect stdout to myscript.log\n# |\n# keep the command running \n# no matter whether the connection is lost or you logout\n\nwill move to background the command, outputting both stdout and\nstderr to myscript.log without waiting for any input.\n\n\nSee also the wikipedia artcle on nohup, also quoted here for your convenience.\n\nNohuping backgrounded jobs is for\nexample useful when logged in via SSH,\nsince backgrounded jobs can cause the\nshell to hang on logout due to a race\ncondition. This problem can also\nbe overcome by redirecting all three\nI/O streams.",
    "tag": "ssh"
  },
  {
    "question": "Multiple GitHub Accounts & SSH Config",
    "answer": "Andy Lester's response is accurate but I found an important extra step I needed to make to get this to work. In trying to get two profiles set up, one for personal and one for work, my ~/.ssh/config was roughly as follows:\nHost me.github.com\n    HostName github.com\n    PreferredAuthentications publickey\n    IdentityFile ~/.ssh/me_rsa\n\nHost work.github.com\n    HostName github.com\n    PreferredAuthentications publickey\n    IdentityFile ~/.ssh/work_rsa\n\nMy work profile didn't take until I did a ssh-add ~/.ssh/work_rsa. After that connections to github used the correct profile. Previously they defaulted to the first public key.\n\nIf you encounter problems, they are likely from using the wrong account. You can troubleshoot things from the command line with a command like this (use ssh -vvv for max verbosity):\n$ GIT_SSH_COMMAND='ssh -v' git pull --verbose\n\nFor Could not open a connection to your authentication agent when using ssh-add,check https://stackoverflow.com/a/17695338/1760313\nFor  defaulted to the first public key, add IdentitiesOnly to the Host * section at the bottom of your ~/.ssh/config file. This tells ssh to only use specified identities rather than all unlocked keys in your agent.\nIf your Host * section has an IdentityFile line, this will not work. Remove that line and add it to a new Host !work.github.com * secion.\nIf you use ControlMaster and you try to use different Github accounts before the ControlPersist timeout expires, it will use the persisted connection, which may be for the other account. Change your ControlPath to include %k (the host key alias if specified, otherwise this is identical to %h, the hostname). If you prefer, you can limit this to just one Github account's path with e.g. ControlPath ~/.ssh/control-%C-2 solely in the Host work.github.com stanza.",
    "tag": "ssh"
  },
  {
    "question": "SSH to Elastic Beanstalk instance",
    "answer": "I found it to be a 2-step process. This assumes that you've already set up a keypair to access EC2 instances in the relevant region.\nConfigure Security Group\n\nIn the AWS console, open the EC2 tab.\n\nSelect the relevant region and click on Security Group.\n\nYou should have an elasticbeanstalk-default security group if you have launched an Elastic Beanstalk instance in that region.\n\nEdit the security group to add a rule for SSH access. The below will lock it down to only allow ingress from a specific IP address.\nSSH | tcp | 22 | 22 | 192.168.1.1/32\n\n\n\nConfigure the environment of your Elastic Beanstalk Application\n\nIf you haven't made a key pair yet, make one by clicking Key Pairs below Security Group in the ec2 tab.\nIn the AWS console, open the Elastic Beanstalk tab.\nSelect the relevant region.\nSelect relevant Environment\nSelect Configurations in left pane.\nSelect Security.\nUnder \"EC2 key pair:\", select the name of your keypair in the Existing Key Pair field.\n\nIf after these steps you see that the Health is set Degraded\n\nthat's normal and it just means that the EC2 instance is being updated. Just wait on a few seconds it'll be Ok again\n\nOnce the instance has relaunched, you need to get the host name from the AWS Console EC2 instances tab, or via the API. You should then be able to ssh onto the server.\n$ ssh -i path/to/keypair.pub ec2-user@ec2-an-ip-address.compute-1.amazonaws.com\n\nNote: For adding a keypair to the environment configuration, the instances' termination protection must be off as Beanstalk would try to terminate the current instances and start new instances with the KeyPair.\nNote: If something is not working, check the \"Events\" tab in the Beanstalk application / environments and find out what went wrong.",
    "tag": "ssh"
  },
  {
    "question": "How can I ssh directly to a particular directory?",
    "answer": "You can do the following:\nssh -t xxx.xxx.xxx.xxx \"cd /directory_wanted ; bash --login\"\n\nThis way, you will get a login shell right on the directory_wanted.\n\nExplanation\n\n-t Force pseudo-terminal allocation.  This can be used to execute arbitrary screen-based programs on a remote machine, which can be very useful, e.g. when implementing menu services.\nMultiple -t options force tty allocation, even if ssh has no local tty.\n\n\nIf you don't use -t then no prompt will appear.\nIf you don't add ; bash then the connection will get closed and return control to your local machine\nIf you don't add bash --login then it will not use your configs because it's not a login shell",
    "tag": "ssh"
  },
  {
    "question": "How to solve \"sign_and_send_pubkey: signing failed: agent refused operation\"?",
    "answer": "Run ssh-add on the client machine, that will add the SSH key to the agent. \nConfirm with ssh-add -l (again on the client) that it was indeed added.",
    "tag": "ssh"
  },
  {
    "question": "Permission denied (publickey) when SSH Access to Amazon EC2 instance",
    "answer": "This error message means you failed to authenticate. \nThese are common reasons that can cause that:\n\nTrying to connect with the wrong key. Are you sure this instance is using this keypair?\nTrying to connect with the wrong username. ubuntu is the username for the ubuntu based AWS distribution, but on some others it's ec2-user (or admin on some Debians, according to Bogdan Kulbida's answer)(can also be root, fedora, see below) \nTrying to connect the wrong host. Is that the right host you are trying to log in to?\n\nNote that 1. will also happen if you have messed up the /home/<username>/.ssh/authorized_keys file on your EC2 instance. \nAbout 2., the information about which username you should use is often lacking from the AMI Image description. But you can find some in AWS EC2 documentation, bullet point 4. : \nhttp://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AccessingInstancesLinux.html\n\nUse the ssh command to connect to the instance. You'll specify the private key (.pem) file and user_name@public_dns_name. For Amazon Linux, the user name is ec2-user. For RHEL5, the user name is either root or ec2-user. For Ubuntu, the user name is ubuntu. For Fedora, the user name is either fedora or ec2-user. For SUSE Linux, the user name is root. Otherwise, if ec2-user and root don't work, check with your AMI provider.\n\nFinally, be aware that there are many other reasons why authentication would fail. SSH is usually pretty explicit about what went wrong if you care to add the -v option to your SSH command and read the output, as explained in many other answers to this question.",
    "tag": "ssh"
  },
  {
    "question": "SSH configuration: override the default username",
    "answer": "Create a file called config inside ~/.ssh. Inside the file you can add:\nHost *\n    User buck\n\nOr add\nHost example\n    HostName example.net\n    User buck\n\nThe second example will set a username and is hostname specific, while the first example sets a username only. And when you use the second one you don't need to use ssh example.net; ssh example will be enough.",
    "tag": "ssh"
  },
  {
    "question": "vim: how to delete a newline/linefeed character(s)?",
    "answer": "If you are on the first line, pressing (upper case) J will join that line and the next line together, removing the newline. You can also combine this with a count, so pressing 3J will combine all 3 lines together.",
    "tag": "ssh"
  },
  {
    "question": "How to prevent a background process from being stopped after closing SSH client in Linux",
    "answer": "Check out the \"nohup\" program.",
    "tag": "ssh"
  },
  {
    "question": "How do I add a password to an OpenSSH private key that was generated without a password?",
    "answer": "Try the following command:\nssh-keygen -p -f keyfile\n\nFrom the ssh-keygen man page\n -p      Requests changing the passphrase of a private key file instead of\n         creating a new private key.  The program will prompt for the file\n         containing the private key, for the old passphrase, and twice for\n         the new passphrase.\n\n -f filename\n         Specifies the filename of the key file.\n\nExample:\nssh-keygen -p -f ~/.ssh/id_rsa",
    "tag": "ssh"
  },
  {
    "question": "Repository access denied. access via a deployment key is read-only",
    "answer": "First confusion on my side was about where exactly to set SSH Keys in BitBucket.\nI am new to BitBucket and I was setting a Deployment Key which gives read-access only.\nSo make sure you are setting your rsa pub key in your BitBucket Account Settings. \nClick your BitBucket avatar and select Bitbucket Settings(Manage account). There you'll be able to set SSH Keys.\nI simply deleted the Deployment Key, I don't need any for now. And it worked",
    "tag": "ssh"
  },
  {
    "question": "Getting permission denied (public key) on gitlab",
    "answer": "I found this after searching a lot. It will work perfectly fine for me.\n\nGo to \"Git Bash\" just like cmd. Right click and \"Run as Administrator\".\nType ssh-keygen\nPress enter.\nIt will ask you to save the key to the specific directory.\nPress enter. It will prompt you to type password or enter without password.\nThe public key will be created to the specific directory.\nNow go to the directory and open .ssh folder.\nYou'll see a file id_rsa.pub. Open it on notepad. Copy all text from it.\nGo to https://gitlab.com/-/profile/keys or\nPaste here in the \"key\" textfield.\nNow click on the \"Title\" below. It will automatically get filled.\nThen click \"Add key\".\n\nNow give it a shot and it will work for sure.",
    "tag": "ssh"
  },
  {
    "question": "'heroku' does not appear to be a git repository",
    "answer": "To add a Heroku app as a Git remote, you need to execute heroku git:remote -a yourapp. \nSource: Deploying with Git",
    "tag": "ssh"
  },
  {
    "question": "How to set ssh timeout?",
    "answer": "ssh -o ConnectTimeout=10  <hostName>\n\nWhere 10 is time in seconds.  This Timeout applies only to the creation of the connection.",
    "tag": "ssh"
  },
  {
    "question": "Add Keypair to existing EC2 instance",
    "answer": "You can't apply a keypair to a running instance. You can only use the new keypair to launch a new instance.\nFor recovery, if it's an EBS boot AMI, you can stop it, make a snapshot of the volume. Create a new volume based on it. And be able to use it back to start the old instance, create a new image, or recover data. \nThough data at ephemeral storage will be lost.\n\nDue to the popularity of this question and answer, I wanted to capture the information in the link that Rodney posted on his comment.\nCredit goes to Eric Hammond for this information.\nFixing Files on the Root EBS Volume of an EC2 Instance\nYou can examine and edit files on the root EBS volume on an EC2 instance even if you are in what you considered a disastrous situation like:\n\nYou lost your ssh key or forgot your password\nYou made a mistake editing the /etc/sudoers file and can no longer\ngain root access with sudo to fix it\nYour long running instance is hung for some reason, cannot be\ncontacted, and fails to boot properly\nYou need to recover files off of the instance but cannot get to it\n\nOn a physical computer sitting at your desk, you could simply boot the system with a CD or USB stick, mount the hard drive, check out and fix the files, then reboot the computer to be back in business.\nA remote EC2 instance, however, seems distant and inaccessible when you are in one of these situations. Fortunately, AWS provides us with the power and flexibility to be able to recover a system like this, provided that we are running EBS boot instances and not instance-store.\nThe approach on EC2 is somewhat similar to the physical solution, but we’re going to move and mount the faulty “hard drive” (root EBS volume) to a different instance, fix it, then move it back.\nIn some situations, it might simply be easier to start a new EC2 instance and throw away the bad one, but if you really want to fix your files, here is the approach that has worked for many:\nSetup\nIdentify the original instance (A) and volume that contains the broken root EBS volume with the files you want to view and edit.\ninstance_a=i-XXXXXXXX\n\nvolume=$(ec2-describe-instances $instance_a |\n  egrep '^BLOCKDEVICE./dev/sda1' | cut -f3)\n\nIdentify the second EC2 instance (B) that you will use to fix the files on the original EBS volume. This instance must be running in the same availability zone as instance A so that it can have the EBS volume attached to it. If you don’t have an instance already running, start a temporary one.\ninstance_b=i-YYYYYYYY\n\nStop the broken instance A (waiting for it to come to a complete stop), detach the root EBS volume from the instance (waiting for it to be detached), then attach the volume to instance B on an unused device.\nec2-stop-instances $instance_a\nec2-detach-volume $volume\nec2-attach-volume --instance $instance_b --device /dev/sdj $volume\n\nssh to instance B and mount the volume so that you can access its file system.\nssh ...instance b...\n\nsudo mkdir -p 000 /vol-a\nsudo mount /dev/sdj /vol-a\n\nFix It\nAt this point your entire root file system from instance A is available for viewing and editing under /vol-a on instance B. For example, you may want to:\n\nPut the correct ssh keys in /vol-a/home/ubuntu/.ssh/authorized_keys\nEdit and fix /vol-a/etc/sudoers\nLook for error messages in /vol-a/var/log/syslog\nCopy important files out of /vol-a/…\n\nNote: The uids on the two instances may not be identical, so take care if you are creating, editing, or copying files that belong to non-root users. For example, your mysql user on instance A may have the same UID as your postfix user on instance B which could cause problems if you chown files with one name and then move the volume back to A.\nWrap Up\nAfter you are done and you are happy with the files under /vol-a, unmount the file system (still on instance-B):\nsudo umount /vol-a\nsudo rmdir /vol-a\n\nNow, back on your system with ec2-api-tools, continue moving the EBS volume back to it’s home on the original instance A and start the instance again:\nec2-detach-volume $volume\nec2-attach-volume --instance $instance_a --device /dev/sda1 $volume\nec2-start-instances $instance_a\n\nHopefully, you fixed the problem, instance A comes up just fine, and you can accomplish what you originally set out to do. If not, you may need to continue repeating these steps until you have it working.\nNote: If you had an Elastic IP address assigned to instance A when you stopped it, you’ll need to reassociate it after starting it up again.\nRemember! If your instance B was temporarily started just for this process, don’t forget to terminate it now.",
    "tag": "ssh"
  },
  {
    "question": "How can I remove an SSH key?",
    "answer": "Note that there are at least two bug reports for ssh-add -d/-D not removing keys:\n\n\"Debian Bug report #472477: ssh-add -D does not remove SSH key from gnome-keyring-daemon memory\"\n\"Ubuntu: ssh-add -D deleting all identities does not work. Also, why are all identities auto-added?\"\n\nThe exact issue is:\n\nssh-add -d/-D deletes only manually added keys from gnome-keyring.\nThere is no way to delete automatically added keys.\nThis is the original bug, and it's still definitely present.\nSo, for example, if you have two different automatically-loaded ssh identities associated with two different GitHub accounts -- say for work and for home -- there's no way to switch between them. GitHubtakes the first one which matches, so you always appear as your 'home' user to GitHub, with no way to upload things to work projects.\nAllowing ssh-add -d to apply to automatically-loaded keys (and ssh-add -t X to change the lifetime of automatically-loaded keys), would restore the behavior most users expect.\n\n\nMore precisely, about the issue:\n\nThe culprit is gpg-keyring-daemon:\n\nIt subverts the normal operation of ssh-agent, mostly just so that it can pop up a pretty box into which you can type the passphrase for an encrypted ssh key.\nAnd it paws through your .ssh directory, and automatically adds any keys it finds to your agent.\nAnd it won't let you delete those keys.\n\nHow do we hate this? Let's not count the ways -- life's too short.\nThe failure is compounded because newer ssh clients automatically try all the keys in your ssh-agent when connecting to a host.\nIf there are too many, the server will reject the connection.\nAnd since gnome-keyring-daemon has decided for itself how many keys you want your ssh-agent to have, and has autoloaded them, AND WON'T LET YOU DELETE THEM, you're toast.\n\nThis bug is still confirmed in Ubuntu 14.04.4, as recently as two days ago (August 21st, 2014)\n\nA possible workaround:\n\n\nDo ssh-add -D to delete all your manually added keys. This also locks\nthe automatically added keys, but is not much use since gnome-keyring will ask you to unlock them anyways when you try doing a git push.\nNavigate to your ~/.ssh folder and move all your key files except the one you want to identify with into a separate folder called backup. If necessary you can also open seahorse and delete the keys from there.\nNow you should be able to do git push without a problem.\n\n\n\nAnother workaround:\n\nWhat you really want to do is to turn off gpg-keyring-daemon altogether.\nGo to System --> Preferences --> Startup Applications, and unselect the \"SSH Key Agent (Gnome Keyring SSH Agent)\" box -- you'll need to scroll down to find it.\nYou'll still get an ssh-agent, only now it will behave sanely: no keys autoloaded, you run ssh-add to add them, and if you want to delete keys, you can. Imagine that.\n\nThis comments actually suggests:\n\nThe solution is to keep gnome-keyring-manager from ever starting up, which was strangely difficult by finally achieved by removing the program file's execute permission.\n\n\nRyan Lue adds another interesting corner case in the comments:\n\nIn case this helps anyone: I even tried deleting the id_rsa and id_rsa.pub files altogether, and the key was still showing up.\nTurns out gpg-agent was caching them in a ~/.gnupg/sshcontrol file; I had to manually delete them from there.\n\nThat is the case when the keygrip has been added as in here.",
    "tag": "ssh"
  },
  {
    "question": "Why does an SSH remote command get fewer environment variables then when run manually?",
    "answer": "There are different types of shells. The SSH command execution shell is a non-interactive shell, whereas your normal shell is either a login shell or an interactive shell. Description follows, from man bash:\n\n       A  login  shell  is  one whose first character of argument\n       zero is a -, or one started with the --login option.\n\n       An interactive shell is  one  started  without  non-option\n       arguments  and  without the -c option whose standard input\n       and error are both connected to terminals  (as  determined\n       by  isatty(3)), or one started with the -i option.  PS1 is\n       set and $- includes i if bash is interactive,  allowing  a\n       shell script or a startup file to test this state.\n\n       The  following  paragraphs  describe how bash executes its\n       startup files.  If any of the files exist  but  cannot  be\n       read,  bash reports an error.  Tildes are expanded in file\n       names as described below  under  Tilde  Expansion  in  the\n       EXPANSION section.\n\n       When  bash is invoked as an interactive login shell, or as\n       a non-interactive shell with the --login option, it  first\n       reads and executes commands from the file /etc/profile, if\n       that file exists.  After reading that file, it  looks  for\n       ~/.bash_profile,  ~/.bash_login,  and  ~/.profile, in that\n       order, and reads and executes commands from the first  one\n       that  exists  and is readable.  The --noprofile option may\n       be used when the shell is started to inhibit  this  behav­\n       ior.\n\n       When a login shell exits, bash reads and executes commands\n       from the file ~/.bash_logout, if it exists.\n\n       When an interactive shell that is not  a  login  shell  is\n       started,  bash reads and executes commands from ~/.bashrc,\n       if that file exists.  This may be inhibited by  using  the\n       --norc  option.   The --rcfile file option will force bash\n       to  read  and  execute  commands  from  file  instead   of\n       ~/.bashrc.\n\n       When  bash  is  started  non-interactively, to run a shell\n       script, for example, it looks for the variable BASH_ENV in\n       the  environment,  expands  its value if it appears there,\n       and uses the expanded value as the name of a file to  read\n       and  execute.   Bash  behaves  as if the following command\n       were executed:\n              if [ -n \"$BASH_ENV\" ]; then . \"$BASH_ENV\"; fi\n       but the value of the PATH variable is not used  to  search\n       for the file name.",
    "tag": "ssh"
  },
  {
    "question": "Github permission denied: ssh add agent has no identities",
    "answer": "Full details in this answer. \nIn summary, when ssh-add -l returns “The agent has no identities”, it means that keys used by ssh (stored in files such as ~/.ssh/id_rsa, ~/.ssh/id_dsa, etc.) are either missing, they are not known to ssh-agent, which is the authentication agent, or that their permissions are set incorrectly (for example, world writable).\nIf your keys are missing or if you have not generated any, use ssh-keygen -t rsa, then ssh-add to add them.\nIf keys exist but are not known to ssh-agent (like if they are in a non-standard folder), use ssh-add /path/to/my-non-standard-ssh-folder/id_rsa to add them.\nSee this answer if you are having trouble with ssh-add or ssh-agent.",
    "tag": "ssh"
  },
  {
    "question": "Automatically enter SSH password with script",
    "answer": "First you need to install sshpass.\n\nUbuntu/Debian: apt-get install sshpass \nFedora/CentOS: yum install sshpass \nArch: pacman -S sshpass\n\n\nExample:\nsshpass -p \"YOUR_PASSWORD\" ssh -o StrictHostKeyChecking=no YOUR_USERNAME@SOME_SITE.COM\n\nCustom port example:\nsshpass -p \"YOUR_PASSWORD\" ssh -o StrictHostKeyChecking=no YOUR_USERNAME@SOME_SITE.COM:2400\n\nNotes:\n\nsshpass can also read a password from a file when the -f flag is passed.\n\n\nUsing -f prevents the password from being visible if the ps command is executed.\nThe file that the password is stored in should have secure permissions.",
    "tag": "ssh"
  },
  {
    "question": "ssh -L forward multiple ports",
    "answer": "The -L option can be specified multiple times within the same command. Every time with different ports. I.e. ssh -L localPort0:ip:remotePort0 -L localPort1:ip:remotePort1 ...",
    "tag": "ssh"
  },
  {
    "question": "Is there a way to continue broken scp (secure copy) command process in Linux?",
    "answer": "If you need to resume an scp transfer from local to remote, try with rsync:\nrsync --partial --progress --rsh=ssh local_file user@host:remote_file\n\nShort version, as pointed out by @aurelijus-rozenas: \nrsync -P -e ssh local_file user@host:remote_file\n\nIn general the order of args for rsync is\nrsync [options] SRC DEST",
    "tag": "ssh"
  },
  {
    "question": "Run ssh and immediately execute command",
    "answer": "ssh destination -t 'command; bash -l'\n\nwill execute the command and then start up a login shell when it completes. For example:\nssh -t user@domain.example 'cd /some/path; bash -l'",
    "tag": "ssh"
  },
  {
    "question": "WARNING: UNPROTECTED PRIVATE KEY FILE! when trying to SSH into Amazon EC2 Instance",
    "answer": "I've chmoded my keypair to 600 in order to get into my personal instance last night,\n\nAnd this is the way it is supposed to be.  \nFrom the EC2 documentation we have \"If you're using OpenSSH (or any reasonably paranoid SSH client) then you'll probably need to set the permissions of this file so that it's only readable by you.\"  The Panda documentation you link to links to Amazon's documentation but really doesn't convey how important it all is.\nThe idea is that the key pair files are like passwords and need to be protected.  So, the ssh client you are using requires that those files be secured and that only your account can read them.\nSetting the directory to 700 really should be enough, but 777 is not going to hurt as long as the files are 600.\nAny problems you are having are client side, so be sure to include local OS information with any follow up questions!",
    "tag": "ssh"
  },
  {
    "question": "SSH Private Key Permissions using Git GUI or ssh-keygen are too open",
    "answer": "You changed the permissions on the whole directory, which I agree with Splash is a bad idea.  If you can remember what the original permissions for the directory are, I would try to set them back to that and then do the following\ncd ~/.ssh\nchmod 700 id_rsa\n\ninside the .ssh folder.  That will set the id_rsa file to rwx (read, write, execute) for the owner (you) only, and zero access for everyone else.\nIf you can't remember what the original settings are, add a new user and create a set of SSH keys for that user, thus creating a new .ssh folder which will have default permissions.  You can use that new .ssh folder as the reference for permissions to reset your .ssh folder and files to.\nIf that doesn't work, I would try doing an uninstall of msysgit, deleting ALL .ssh folders on the computer (just for safe measure), then reinstalling msysgit with your desired settings and try starting over completely (though I think you told me you tried this already).\nEdited: Also just found this link via Google -- Fixing \"WARNING: UNPROTECTED PRIVATE KEY FILE!\" on Linux While it's targeted at linux, it might help since we're talking liunx permissions and such.",
    "tag": "ssh"
  },
  {
    "question": "How to ignore ansible SSH authenticity checking?",
    "answer": "Two options - the first, as you said in your own answer, is setting the environment variable ANSIBLE_HOST_KEY_CHECKING to False.\nThe second way to set it is to put it in an ansible.cfg file, and that's a really useful option because you can either set that globally (at system or user level, in /etc/ansible/ansible.cfg or ~/.ansible.cfg), or in an config file in the same directory as the playbook you are running.\nTo do that, make an ansible.cfg file in one of those locations, and include this:\n[defaults]\nhost_key_checking = False\n\nYou can also set a lot of other handy defaults there, like whether or not to gather facts at the start of a play, whether to merge hashes declared in multiple places or replace one with another, and so on. There's a whole big list of options here in the Ansible docs.\n\nEdit: a note on security.\nSSH host key validation is a meaningful security layer for persistent hosts - if you are connecting to the same machine many times, it's valuable to accept the host key locally.\nFor longer-lived EC2 instances, it would make sense to accept the host key with a task run only once on initial creation of the instance:\n- name: Write the new ec2 instance host key to known hosts\n  connection: local\n  shell: \"ssh-keyscan -H {{ inventory_hostname }} >> ~/.ssh/known_hosts\"\n\nThere's no security value for checking host keys on instances that you stand up dynamically and remove right after playbook execution, but there is security value in checking host keys for persistent machines. So you should manage host key checking differently per logical environment.\n\nLeave checking enabled by default (in ~/.ansible.cfg)\nDisable host key checking in the working directory for playbooks you run against ephemeral instances (./ansible.cfg alongside the playbook for unit tests against vagrant VMs, automation for short-lived ec2 instances)",
    "tag": "ssh"
  },
  {
    "question": "Git on Bitbucket: Always asked for password, even after uploading my public SSH key",
    "answer": "Are you sure you cloned it using the ssh url?\nThe url for origin says url = https://Nicolas_Raoul@bitbucket.org/Nicolas_Raoul/therepo.git so if it is using https it will ask for password irrespective of your ssh keys.\nSo what you want to do is the following:\nopen your config file in your current repo ..\nvim .git/config\nand change the line with the url from\n[remote \"origin\"]\n        fetch = +refs/heads/*:refs/remotes/origin/*\n        url = https://Nicolas_Raoul@bitbucket.org/Nicolas_Raoul/therepo.git\n\nto \n[remote \"origin\"]\n        fetch = +refs/heads/*:refs/remotes/origin/*\n        url = git@bitbucket.org:Nicolas_Raoul/therepo.git",
    "tag": "ssh"
  },
  {
    "question": "How to use Sublime over SSH",
    "answer": "There are three ways:\n\nUse SFTP plugin (commercial) http://wbond.net/sublime_packages/sftp - I personally recommend this, as after settings public SSH keys with passphrase it is safe, easy and worth every penny http://opensourcehacker.com/2012/10/24/ssh-key-and-passwordless-login-basics-for-developers/\n\nMount the remote as local file system using osxfuse and sshfs as mentioned in the comments. This might be little difficult, depending on OSX version and your skills with UNIX file systems.\n\nHack together something like rmate which does file editing over remote tunneling using some kind of a local daemon (very difficult, cumbersome, but sudo compatible)  http://blog.macromates.com/2011/mate-and-rmate/\n\n\nAlso, in theory, you can install X11 on the remote server and run Sublime there over VNC or X11 forwarding, but this would be very slow.",
    "tag": "ssh"
  },
  {
    "question": "Seeing escape characters when pressing the arrow keys in python shell",
    "answer": "I've solved this issue by installing readline package:\npip install readline",
    "tag": "ssh"
  },
  {
    "question": "Why git can't remember my passphrase under Windows",
    "answer": "Every time I set up a new desktop I forget these instructions, so I'm adding another answer here since I stumble across it equally often!\n\nQuick Steps for Impatient Users Like Me\n\nEnable the OpenSSH Authentication Agent service and make it start automatically.\n\nWith a Windows update you may have to re-do this step! (It has only happened to me one time).\n\n\nAdd your SSH key to the agent with ssh-add on the command line.\nTest git integration, if it still asks for your passphrase, continue on.\nAdd the environment variable $ENV:GIT_SSH=C:\\Windows\\System32\\OpenSSH\\ssh.exe to your session, or permanently to your user environment.\n\n\nDetailed Steps: Overview\nWindows has been shipping with OpenSSH for some time now. It includes all the necessary bits for ssh to work alongside Git, but it still seems to need some TLC before it works 100% seamlessly. Here's the steps I've been following with success since Windows ver 10.0.18362.449 (you can see your Windows version by opening a cmd.exe shell and typing ver).\nI assume here that you already have your SSH key setup, and is located at ~/.ssh/id_rsa\nEnable the ssh-agent service on your Windows 10/11 box.\n\nStart-> Type 'Services' and click on the Services App that appears.\nFind the OpenSSH Authentication Agent service in the list.\nRight-click on the OpenSSH Authentication Agent service, and choose 'Properties'.\nChange the Startup type: to Automatic.\nClick the Start button to change the service status to Running.\nDismiss the dialog by clicking OK, and close the Services app.\n\nAdd your key to the ssh-agent\n\nOpen your shell of preference (I'll use Windows Powershell in this example, applies to Powershell Core too).\nAdd your SSH key to the ssh-agent: ssh-add (you can add the path to your key as the first argument if it differs from the default).\nEnter your passphrase if/when prompted to do so.\n\nTry Git + SSH\n\nOpen your shell (again, I'm using Powershell) and clone a repo. git clone git@github.com:octocat/Spoon-Knife\nIf you see this prompt, continue on to the next section:\n\nEnter passphrase for key '/c/Users/your_user_name/.ssh/id_rsa':\n\nSet your GIT_SSH Environment Variable\nIn any session you can simply set this environment variable and the prompt for your passphrase will stop coming up and ssh will use the ssh-agent on your behalf. Alternatively, you can set your passphrase into your user's environment permanently.\nTo set GIT_SSH in the current shell only:\n\nOpen your shell of preference. (Powershell for me)\nSet the environment variable GIT_SSH to the appropriate ssh.exe: $Env:GIT_SSH=$((Get-Command -Name ssh).Source)\nRetry the steps in Try Git + SSH above.\n\nTo set GIT_SSH permanently\n\nOpen File Explorer. Start-> type 'File Explorer' and click on it in the list.\nRight-click 'This PC' and click on 'Properties'.\nClick on 'Advanced system settings'.\nClick the 'Environment Variables...' button.\nUnder 'User variables for your_user_name' click New...\nSet Variable name: field to GIT_SSH\nSet the Variable value: field to path-to-ssh.exe (typically C:\\Windows\\System32\\OpenSSH\\ssh.exe).\nClick OK to dismiss the New User Variable dialog.\nClick OK to dismiss the Environment Variables dialog.\nRetry the steps in Try Git + SSH above.\n\n\nNote that this is likely going to change with new steps/procedures as Windows progresses and as I learn more. I will attempt to keep this updated, I look forward to feedback in the comments.",
    "tag": "ssh"
  },
  {
    "question": "Git clone / pull continually freezing at \"Store key in cache?\"",
    "answer": "I had this problem when cloning a repo on Windows 10 too. \nI got around it by using the Putty GUI to SSH to the server in question (in your case: bitbucket.org) then clicked 'Yes' when the prompt asks if you want to save the server key to the cache. Running the clone command again then worked for me!",
    "tag": "ssh"
  },
  {
    "question": "How to run the sftp command with a password from Bash script?",
    "answer": "You have a few options other than using public key authentication:\n\nUse keychain\nUse sshpass (less secured but probably that meets your requirement)\nUse expect (least secured and more coding needed)\n\nIf you decide to give sshpass a chance here is a working script snippet to do so:\nexport SSHPASS=your-password-here\nsshpass -e sftp -oBatchMode=no -b - sftp-user@remote-host << !\n   cd incoming\n   put your-log-file.log\n   bye\n!\n\n\nUpdate: However do understand that using environment variables is also insecure as using command line option -p for passing password.\nIt is better to store and read password from a file like this using -f option:\necho 'your-password-here' > ~/.passwd\nchmod 0400 ~/.passwd\n\nsshpass -f ~/.passwd -e sftp -oBatchMode=no -b - sftp-user@remote-host << !\n   cd incoming\n   put your-log-file.log\n   bye\n!",
    "tag": "ssh"
  },
  {
    "question": "How do I force detach Screen from another SSH session?",
    "answer": "As Jose answered, screen -d -r should do the trick. This is a combination of two commands, as taken from the man page.\nscreen -d detaches the already-running screen session, and screen -r reattaches the existing session. By running screen -d -r, you force screen to detach it and then resume the session.  \nIf you use the capital -D -RR, I quote the man page because it's too good to pass up.\n\nAttach here and now. Whatever that means, just do it.\n\nNote: It is always a good idea to check the status of your sessions by means of \"screen -list\".",
    "tag": "ssh"
  },
  {
    "question": "Git error no matching host key type found. Their offer: ssh-rsa",
    "answer": "With SSH, there are several different types of keys and RSA keys (the ssh-rsa) kind can support multiple kinds of signatures.  The signature type ssh-rsa refers to RSA with SHA-1, whereas the signature type rsa-sha2-256 is RSA with SHA-256 and rsa-sha2-512 is RSA with SHA-512.\nIn the case of Azure DevOps, it only supports the kind of RSA with SHA-1, and SHA-1 is considered very weak.  This essentially means that there are no secure ways to connect to it over SSH, and until they fix that, you're better off using HTTPS or a different hosting service.  GitHub, GitLab, and Bitbucket all support secure methods of authentication.\nIf you really need to use SSH with Azure DevOps at the moment, you can add an entry to your ~/.ssh/config file to work around this:\nHost ssh.dev.azure.com\n    User git\n    PubkeyAcceptedAlgorithms +ssh-rsa\n    HostkeyAlgorithms +ssh-rsa\n\nHowever, be aware that this is a workaround and it's known to be insecure, so you should contact Azure DevOps about this problem and switch to HTTPS until they do, or move elsewhere.",
    "tag": "ssh"
  },
  {
    "question": "Extract public/private key from PKCS12 file for later use in SSH-PK-Authentication",
    "answer": "You can use following commands to extract public/private key from a PKCS#12 container:\n\nPKCS#1 Private key\nopenssl pkcs12 -in yourP12File.pfx -nocerts -out privateKey.pem\n\nCertificates:\nopenssl pkcs12 -in yourP12File.pfx -clcerts -nokeys -out publicCert.pem",
    "tag": "ssh"
  },
  {
    "question": "Git on custom SSH port",
    "answer": "git clone ssh://git@mydomain.example:[port]/gitolite-admin\n\nNote that the port number should be there without the square brackets: []",
    "tag": "ssh"
  },
  {
    "question": "Copying files from server to local computer using SSH",
    "answer": "It depends on what your local OS is.\nIf your local OS is Unix-like, then try:\nscp username@remoteHost:/remote/dir/file.txt /local/dir/\n\nIf your local OS is Windows ,then you should use pscp.exe utility.\nFor example, below command will download file.txt from remote to D: disk of local machine.\npscp.exe username@remoteHost:/remote/dir/file.txt d:\\\n\nIt seems your Local OS is Unix, so try the former one.\n\nFor those who don't know what pscp.exe is and don't know where it is, you can always go to putty official website to download it. And then open a CMD prompt, go to the pscp.exe directory where you put it. Then execute the command as provided above\nEDIT\nif you are using Windows OS above Windows 10, then you can use scp directly from its terminal, just like how Unix-like OS does.\nThanks to @gijswijs @jaunt @icanfathom",
    "tag": "ssh"
  },
  {
    "question": "Configuring Git over SSH to login once",
    "answer": "Had a similar problem with the GitHub because I was using HTTPS protocol. To check what protocol you're using just run\ngit config -l\n\nand look at the line starting with remote.origin.url. To switch your protocol\ngit config remote.origin.url git@github.com:your_username/your_project.git",
    "tag": "ssh"
  },
  {
    "question": "Git says \"Warning: Permanently added to the list of known hosts\"",
    "answer": "Create a ~/.ssh/config file and insert the line:\nUserKnownHostsFile ~/.ssh/known_hosts\n\nYou will then see the message the next time you access Github, but after that you'll not see it anymore because the host is added to the known_hosts file.  This fixes the issue, rather than just hiding the log message.\nThis problem was bugging me for quite some time.  The problem occurs because the OpenSSH client compiled for Windows doesn't check the known_hosts file in ~/.ssh/known_hosts\nssh -vvv git@github.com\ndebug3: check_host_in_hostfile: filename /dev/null\ndebug3: check_host_in_hostfile: filename /etc/ssh/ssh_known_hosts\ndebug3: check_host_in_hostfile: filename /dev/null\ndebug3: check_host_in_hostfile: filename /etc/ssh/ssh_known_hosts\nWarning: Permanently added 'github.com,207.97.227.239' (RSA) to the list of known hosts.",
    "tag": "ssh"
  },
  {
    "question": "ssh: Could not resolve hostname [hostname]: nodename nor servname provided, or not known",
    "answer": "If you're on Mac, restarting the DNS responder fixed the issue for me.\nsudo killall -HUP mDNSResponder",
    "tag": "ssh"
  },
  {
    "question": "Connect over SSH using a .pem file",
    "answer": "Use the -i option:\nssh -i mykey.pem myusername@mydomain.example\n\nAs noted in this answer, this file needs to have correct permissions set. The ssh man page says:\n\nSSH will simply ignore a private key file if it is accessible by others.\n\nYou can change the permissions with this command:\nchmod go= mykey.pem\n\nThat is, set permissions for group and others equal to the empty list of permissions.\n\nIf you're going to connect to this server with this key many times in the future, it might be worth configuring it once and for all. Open the file ~/.ssh/config (create it if it doesn't exist), and add:\nHost mydomain.example\n    IdentityFile /path/to/mykey.pem\n    User myusername\n\nNow you can just type ssh mydomain.example, and it will use the specified key file and user name.  See the ssh_config man page for more details.",
    "tag": "ssh"
  },
  {
    "question": "How do I copy a folder from remote to local using scp?",
    "answer": "scp -r user@your.server.example.com:/path/to/foo /home/user/Desktop/\n\nBy not including the trailing '/' at the end of foo, you will copy the directory itself (including contents), rather than only the contents of the directory.\nFrom man scp (See online manual)\n\n-r Recursively copy entire directories",
    "tag": "scp"
  },
  {
    "question": "scp with port number specified",
    "answer": "Unlike ssh, scp uses the uppercase P switch to set the port instead of the lowercase p:\nscp -P 80 ... # Use port 80 to bypass the firewall, instead of the scp default\n\nThe lowercase p switch is used with scp for the preservation of times and modes.\nHere is an excerpt from scp's man page with all of the details concerning the two switches, as well as an explanation of why uppercase P was chosen for scp:\n\n-P port   Specifies the port to connect to on the remote host. Note that this option is written with a capital 'P', because -p is already\nreserved for preserving the times and modes of the file in rcp(1).\n-p           Preserves modification times, access times, and modes from the original file.\n\nBonus Tip: How can I determine the port being used by the/an SSH daemon to accept SSH connections?\nThis question can be answered by using the netstat utility, as follows:\nsudo netstat -tnlp | grep sshd\n\nOr, using the far more readable word based netstat option names:\nsudo netstat --tcp --numeric-ports --listening --program | grep sshd\n\nThe output you will see, assuming your ssh daemon is configured with default values its listening ports, is shown below (with a little trimming of the whitespace in between columns, in order to get the entire table to be visible without having to scroll):\nActive Internet connections (only servers)\nProto Recv-Q Send-Q Local Address  Foreign Address  State       ID/Program name\ntcp      0      0   0.0.0.0:22     0.0.0.0:*        LISTEN      888/sshd: /usr/sbin \ntcp6     0      0   :::22          :::*             LISTEN      888/sshd: /usr/sbin \n\nImportant Note\nFor the above examples, sudo was used to run netstat with administrator privs, in order to be able to see all of the Program Names. If you run netstat as a regular user (i.e., without sudo and assuming you don't have admin rights granted to you, via some other method), you will only see program names shown for sockets that have your UID as the owner. The Program Names for sockets belonging to other users will not be shown (i.e., will be hidden and a placeholder hyphen will be displayed, instead):\nProto Recv-Q Send-Q Local Address   Foreign Address  State       ID/Program name\ntcp        0      0 127.0.0.1:46371 0.0.0.0:*        LISTEN      4489/code\n...\ntcp        0      0 0.0.0.0:111     0.0.0.0:*        LISTEN      -                   \ntcp        0      0 127.0.0.53:53   0.0.0.0:*        LISTEN      -                   \ntcp        0      0 0.0.0.0:22      0.0.0.0:*        LISTEN      -                   \n...\n\nUpdate and aside to address one of the (heavily upvoted) comments:\nWith regard to Abdull's comment about scp option order, what he suggests:\nscp -r some_directory -P 80 ...\n\n..., intersperses options and parameters, since the -r switch takes no additional arguments and some_directory is treated as the first parameter to the command, making -P and all subsequent command line arguments look like additional parameters to the command (i.e., hyphen prefixed arguments are no longer considered as switches).\ngetopt(1) clearly defines that parameters must come after options (i.e., switches) and not be interspersed with them, willy-nilly:\n\nThe parameters getopt is called with can be divided into two parts: options which modify the way getopt will do the parsing (the options and the optstring in the SYNOPSIS), and the parameters which are to be parsed (parameters in the SYNOPSIS).  The second part will start at\nthe first non-option parameter that is not an option argument, or after the first occurrence of '--'.  If no '-o' or '--options' option is found in the first part, the first parameter of the second part is used as the short options string.\n\nSince the -r command line option takes no further arguments, some_directory is \"the first non-option parameter that is not an option argument.\" Therefore, as clearly spelled out in the getopt(1) man page, all succeeding command line arguments that follow it (i.e., -P 80 ...) are assumed to be non-options (and non-option arguments).\nSo, in effect, this is how getopt(1) sees the example presented with the end of the options and the beginning of the parameters demarcated by gray text:\nscp -r some_directory -P 80 ...\nThis has nothing to do with scp behavior and everything to do with how POSIX standard applications parse command line options using the getopt(3) set of C functions.\nFor more details with regard to command line ordering and processing, please read the getopt(1) manpage using:\nman 1 getopt",
    "tag": "scp"
  },
  {
    "question": "scp or sftp copy multiple files with single command",
    "answer": "Copy multiple files from remote to local:\n$ scp your_username@remote.edu:/some/remote/directory/\\{a,b,c\\} ./\n\nCopy multiple files from local to remote:\n$ scp foo.txt bar.txt your_username@remotehost.edu:~\n$ scp {foo,bar}.txt your_username@remotehost.edu:~\n$ scp *.txt your_username@remotehost.edu:~\n\nCopy multiple files from remote to remote:\n$ scp your_username@remote1.edu:/some/remote/directory/foobar.txt \\\nyour_username@remote2.edu:/some/remote/directory/\n\nSource: http://www.hypexr.org/linux_scp_help.php",
    "tag": "scp"
  },
  {
    "question": "scp (secure copy) to ec2 instance without password",
    "answer": "I figured it out.  I had the arguments in the wrong order.  This works:\nscp -i mykey.pem somefile.txt root@my.ec2.id.amazonaws.com:/",
    "tag": "scp"
  },
  {
    "question": "Transferring files over SSH",
    "answer": "You need to scp something somewhere. You have scp ./styles/, so you're saying secure copy ./styles/, but not where to copy it to.\nGenerally, if you want to download, it will go:\n# download: remote -> local\nscp user@remote_host:remote_file local_file \n\nwhere local_file might actually be a directory to put the file you're copying in. To upload, it's the opposite:\n# upload: local -> remote\nscp local_file user@remote_host:remote_file\n\nIf you want to copy a whole directory, you will need -r. Think of scp as like cp, except you can specify a file with user@remote_host:file as well as just local files.\nEdit: As noted in a comment, if the usernames on the local and remote hosts are the same, then the user can be omitted when specifying a remote file.",
    "tag": "scp"
  },
  {
    "question": "How to pass password to scp?",
    "answer": "Use sshpass:\nsshpass -p \"password\" scp -r user@example.com:/some/remote/path /some/local/path\n\nor so the password does not show in the bash history\nsshpass -f \"/path/to/passwordfile\" scp -r user@example.com:/some/remote/path /some/local/path\n\nThe above copies contents of path from the remote host to your local.\nInstall :\n\nubuntu/debian\napt install sshpass\ncentos/fedora\nyum install sshpass\nmac w/ macports\nport install sshpass\nmac w/ homebrew\nbrew install sshpass",
    "tag": "scp"
  },
  {
    "question": "How do I escape spaces in path for scp copy in Linux?",
    "answer": "Basically you need to escape it twice, because it's escaped locally and then on the remote end.\nThere are a couple of options you can do (in bash):\nscp user@example.com:\"web/tmp/Master\\ File\\ 18\\ 10\\ 13.xls\" .\nscp user@example.com:web/tmp/Master\\\\\\ File\\\\\\ 18\\\\\\ 10\\\\\\ 13.xls .\n\nAnother option only works with older versions of OpenSSH and possibly other clients:\nscp user@example.com:\"'web/tmp/Master File 18 0 13.xls'\" .",
    "tag": "scp"
  },
  {
    "question": "How does `scp` differ from `rsync`?",
    "answer": "The major difference between these tools is how they copy files.\nscp basically reads the source file and writes it to the destination. It performs a plain linear copy, locally, or over a network.\nrsync also copies files locally or over a network. But it employs a special delta transfer algorithm and a few optimizations to make the operation a lot faster. Consider the call.\nrsync A host:B\n\n\nrsync will check files sizes and modification timestamps of both A and B, and skip any further processing if they match.\n\nIf the destination file B already exists, the delta transfer algorithm will make sure only differences between A and B are sent over the wire.\n\nrsync will write data to a temporary file T, and then replace the destination file B with T to make the update look \"atomic\" to processes that might be using B.\n\n\nAnother difference between them concerns invocation. rsync has a plethora of command line options, allowing the user to fine tune its behavior. It supports complex filter rules, runs in batch mode, daemon mode, etc. scp has only a few switches.\nIn summary, use scp for your day to day tasks. Commands that you type once in a while on your interactive shell. It's simpler to use, and in those cases rsync optimizations won't help much.\nFor recurring tasks, like cron jobs, use rsync. As mentioned, on multiple invocations it will take advantage of data already transferred, performing very quickly and saving on resources. It is an excellent tool to keep two directories synchronized over a network.\nAlso, when dealing with large files, use rsync with the -P option. If the transfer is interrupted, you can resume it where it stopped by reissuing the command. See Sid Kshatriya's answer.\nFinally, note that rsync:// the protocol is similar to plain HTTP: unencrypted and no integrity checks. Be sure to always use rsync via SSH (as in the examples from the question above), not via the rsync protocol, unless you really know what you're doing. scp will always use SSH as underlying transport mechanism which has both integrity and confidentiality guarantees, so that is another difference between the two utilities.",
    "tag": "scp"
  },
  {
    "question": "Using scp to copy a file to Amazon EC2 instance?",
    "answer": "Try specifying the user to be ec2-user, e.g.\nscp -i myAmazonKey.pem phpMyAdmin-3.4.5-all-languages.tar.gz ec2-user@mec2-50-17-16-67.compute-1.amazonaws.com:~/.\n\nSee Connecting to Linux/UNIX Instances Using SSH.",
    "tag": "scp"
  },
  {
    "question": "Is there a WinSCP equivalent for Linux?",
    "answer": "If you're using GNOME, you can go to: Places → Connect to Server in Nautilus and choose SSH. If you have an SSH agent running and configured, no password will be asked!\n(This is the same as sftp://root@servername/directory in Nautilus)\nIn Konqueror, you can simply type: fish://servername.\nPer Mike R: In Ubuntu 14.04 (with Unity) it’s under Files → Connect to Server in the menu or Network → Connect to Server in the sidebar.",
    "tag": "scp"
  },
  {
    "question": "Copying files from server to local computer using SSH",
    "answer": "It depends on what your local OS is.\nIf your local OS is Unix-like, then try:\nscp username@remoteHost:/remote/dir/file.txt /local/dir/\n\nIf your local OS is Windows ,then you should use pscp.exe utility.\nFor example, below command will download file.txt from remote to D: disk of local machine.\npscp.exe username@remoteHost:/remote/dir/file.txt d:\\\n\nIt seems your Local OS is Unix, so try the former one.\n\nFor those who don't know what pscp.exe is and don't know where it is, you can always go to putty official website to download it. And then open a CMD prompt, go to the pscp.exe directory where you put it. Then execute the command as provided above\nEDIT\nif you are using Windows OS above Windows 10, then you can use scp directly from its terminal, just like how Unix-like OS does.\nThanks to @gijswijs @jaunt @icanfathom",
    "tag": "scp"
  },
  {
    "question": "How to scp in Python?",
    "answer": "Try the Python scp module for Paramiko. It's very easy to use. See the following example:\nimport paramiko\nfrom scp import SCPClient\n\ndef createSSHClient(server, port, user, password):\n    client = paramiko.SSHClient()\n    client.load_system_host_keys()\n    client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n    client.connect(server, port, user, password)\n    return client\n\nssh = createSSHClient(server, port, user, password)\nscp = SCPClient(ssh.get_transport())\n\nThen call scp.get() or scp.put() to do SCP operations.\n(SCPClient code)",
    "tag": "scp"
  },
  {
    "question": "How Can I Download a File from EC2",
    "answer": "Use scp:\nscp -i ec2key.pem username@ec2ip:/remote/path/to/file /local/path/to/file\n\nwhere:\n\nec2key.pem is your PEM key\nusername is the username you log in with into your EC2 instance\nec2ip is the IP or DNS alias of your EC2 instance\n/remote/path/to/file is the location where the file is stored on your EC2 instance\n/local/path/to/file is where you want to put the file on your local machine\n\nYou can use . to put it into the current folder:\nscp -i ec2key.pem username@ec2ip:/remote/path/to/file .\n\nYou can read more here on how to access your instance with ssh if you haven't done already:\n\nhttp://docs.aws.amazon.com/gettingstarted/latest/computebasics-linux/getting-started-deploy-app-connect-linux.html\n\nWhen you are able to ssh as in the above doc, you will be able to use scp to copy the file.\nAnother option is to bring up some Web server on your instance, configure HTTPS if your file is sensitive and then download using your browser, here are some tutorials:\n\nhttp://flurdy.com/docs/ec2/apache_tomcat/\nhttp://www.robotmedia.net/2011/04/how-to-create-an-amazon-ec2-instance-with-apache-php-and-mysql-lamp/",
    "tag": "scp"
  },
  {
    "question": "How to resume scp with partially copied files?",
    "answer": "You should use rsync over ssh\nrsync -P -e ssh remoteuser@remotehost:/remote/path /local/path\n\nThe key option is -P, which is the same as --partial --progress\n\nBy default, rsync will delete any partially transferred file if the transfer is interrupted. In some circumstances it is more desirable to keep partially transferred files. Using the --partial option tells rsync to keep the partial file which should make a subsequent transfer of the rest of the file much faster.\n\nOther options, such -a (for archive mode), and -z (to enable compression) can also be used.\nThe manual: https://download.samba.org/pub/rsync/rsync.html",
    "tag": "scp"
  },
  {
    "question": "scp from Linux to Windows",
    "answer": "This one worked for me.\nscp /home/ubuntu/myfile username@IP_of_windows_machine:/C:/Users/Anshul/Desktop",
    "tag": "scp"
  },
  {
    "question": "How to copy a file to a remote server in Python using SCP or SSH?",
    "answer": "To do this in Python (i.e. not wrapping scp through subprocess.Popen or similar) with the Paramiko library, you would do something like this:\nimport os\nimport paramiko\n\nssh = paramiko.SSHClient() \nssh.load_host_keys(os.path.expanduser(os.path.join(\"~\", \".ssh\", \"known_hosts\")))\nssh.connect(server, username=username, password=password)\nsftp = ssh.open_sftp()\nsftp.put(localpath, remotepath)\nsftp.close()\nssh.close()\n\n(You would probably want to deal with unknown hosts, errors, creating any directories necessary, and so on).",
    "tag": "scp"
  },
  {
    "question": "How can I fix \"kex_exchange_identification: read: Connection reset by peer\"?",
    "answer": "kex_exchange_identification: read: Connection reset by peer\n\nWhen an SSH client connects to an SSH server, the server starts by sending a version string to the client. The error that you're getting means that the TCP connection from the client to the server was \"abnormally closed\" while the client was waiting for this data from the server, in other words immediately after the TCP connection was opened.\nAs a practical matter, it's likely to mean one of two things:\n\nThe SSH server process malfunctioned (crashed), or perhaps it detected some serious issue causing it to exit immediately.\nSome firewall is interfering with connections to the ssh server.\n\nIt looks like the ssh-keyscan program was able to connect to the server and  get a version string without an error. So the SSH server process is apparently able to talk to a client without crashing.\nYou should talk the administrators of this x.x.x.x host and the network that it's attached to, to see if they can identify the problem from their end. It's possible that something—a firewall, or the ssh server process itself—is seeing the multiple connections, first from the ssh-keyscan process, then by the scp program, as an intrusion attempt. And it's blocking the second connection attempt.",
    "tag": "scp"
  },
  {
    "question": "Transfer files to/from session I'm logged in with PuTTY",
    "answer": "This is probably not a direct answer to what you're asking, but when I need to transfer files over a SSH session I use WinSCP, which is an excellent file transfer program over SCP or SFTP. Of course this assumes you're on Windows.",
    "tag": "scp"
  },
  {
    "question": "Single line sftp from terminal",
    "answer": "Update Sep 2017 - tl;dr\nDownload a single file from a remote ftp server to your machine:\nsftp {user}@{host}:{remoteFileName} {localFileName}\n\nUpload a single file from your machine to a remote ftp server:\nsftp {user}@{host}:{remote_dir} <<< $'put {local_file_path}'\n\n\nOriginal answer:\nOk, so I feel a little dumb. But I figured it out. I almost had it at the top with:\nsftp user@host remoteFile localFile\n\nThe only documentation shown in the terminal is this: \nsftp [user@]host[:file ...]\nsftp [user@]host[:dir[/]]\n\nHowever, I came across this site which shows the following under the synopsis: \nsftp [-vC1 ] [-b batchfile ] [-o ssh_option ] [-s subsystem | sftp_server ] [-B buffer_size ] [-F ssh_config ] [-P sftp_server path ] [-R num_requests ] [-S program ] host \nsftp [[user@]host[:file [file]]] \nsftp [[user@]host[:dir[/]]]\n\nSo the simple answer is you just do : after your user and host then the remote file and local filename. Incredibly simple! \nSingle line, sftp copy remote file:\nsftp username@hostname:remoteFileName localFileName\nsftp kyle@kylesserver:/tmp/myLogFile.log /tmp/fileNameToUseLocally.log\n\nUpdate Feb 2016\nIn case anyone is looking for the command to do the reverse of this and push a file from your local computer to a remote server in one single line sftp  command, user @Thariama below posted the solution to accomplish that. Hat tip to them for the extra code.\nsftp {user}@{host}:{remote_dir} <<< $'put {local_file_path}'",
    "tag": "scp"
  },
  {
    "question": "How to filter files when using scp to copy dir recursively?",
    "answer": "I'd probably recommend using something like rsync for this due to its include and exclude flags, e.g:-\nrsync -rav -e ssh --include '*/' --include='*.class' --exclude='*' \\\nserver:/usr/some/unknown/number/of/sub/folders/ \\ \n/usr/project/backup/some/unknown/number/of/sub/folders/\n\nSome other useful flags:\n\n-r for recursive\n-a for archive (mostly all files)\n-v for verbose output\n-e to specify ssh instead of the default (which should be ssh, actually)",
    "tag": "scp"
  },
  {
    "question": "recursively use scp but excluding some folders",
    "answer": "Although scp supports recursive directory copying with the -r option, it does not support filtering of the files. There are several ways to accomplish your task, but I would probably rely on find, xargs, tar, and ssh instead of scp.\nfind . -type d -wholename '*bench*/image' \\\n| xargs tar cf - \\\n| ssh user@remote tar xf - -C /my/dir\n\nThe rsync solution can be made to work, but you are missing some arguments. rsync also needs the r switch to recurse into subdirectories. Also, if you want the same security of scp, you need to do the transfer under ssh. Something like:\nrsync -avr -e \"ssh -l user\" --exclude 'fl_*' ./bench* remote:/my/dir",
    "tag": "scp"
  },
  {
    "question": "Copying a local file from Windows to a remote server using scp",
    "answer": "If your drive letter is C, you should be able to use \nscp -r \\desktop\\myfolder\\deployments\\ user@host:/path/to/whereyouwant/thefile \nwithout drive letter and backslashes instead of forward slashes. \nYou are using putty, so you can use pscp. It is better adapted to Windows.",
    "tag": "scp"
  },
  {
    "question": "Git fetch/pull/clone hangs on receiving objects",
    "answer": "Ran into this problem on a fresh Git install on Windows 11. None of the other answers helped. Was finally able to fix it by switching Git to using the built-in ssh.exe instead of the one packaged with Git for Windows by running:\ngit config --global core.sshCommand \"C:/Windows/System32/OpenSSH/ssh.exe\"",
    "tag": "scp"
  },
  {
    "question": "SSH SCP Local file to Remote in Terminal Mac Os X",
    "answer": "At first, you need to add : after the IP address to indicate the path is following:\nscp magento.tar.gz user@xx.x.x.xx:/var/www\n\nI don't think you need to sudo the scp. In this case it doesn't affect the remote machine, only the local command.\nThen if your user@xx.x.x.xx doesn't have write access to /var/www then you need to do it in 2 times:\nCopy to remote server in your home folder (: represents your remote home folder, use :subfolder/ if needed, or :/home/user/ for full path):\nscp magento.tar.gz user@xx.x.x.xx:\n\nThen SSH and move the file:\nssh user@xx.x.x.xx\nsudo mv magento.tar.gz /var/www",
    "tag": "scp"
  },
  {
    "question": "Automate scp file transfer using a shell script",
    "answer": "Instead of hardcoding password in a shell script, use SSH keys, its easier and secure.\n$ scp -i ~/.ssh/id_rsa *.derp devops@myserver.org:/path/to/target/directory/\n\nassuming your private key is at ~/.ssh/id_rsa and the files you want to send can be filtered with *.derp\nTo generate a public / private key pair :\n$ ssh-keygen -t rsa\n\nThe above will generate 2 files, ~/.ssh/id_rsa (private key) and ~/.ssh/id_rsa.pub (public key)\nTo setup the SSH keys for usage (one time task) :\nCopy the contents of ~/.ssh/id_rsa.pub and paste in a new line of ~devops/.ssh/authorized_keys in myserver.org server. If ~devops/.ssh/authorized_keys doesn't exist, feel free to create it.\nA lucid how-to guide is available here.",
    "tag": "scp"
  },
  {
    "question": "How to scp with a second remote host",
    "answer": "I don't know of any way to copy the file directly in one single command, but if you can concede to running an SSH instance in the background to just keep a port forwarding tunnel open, then you could copy the file in one command.\nLike this:\n# First, open the tunnel\nssh -L 1234:remote2:22 -p 45678 user1@remote1\n# Then, use the tunnel to copy the file directly from remote2\nscp -P 1234 user2@localhost:file .\n\nNote that you connect as user2@localhost in the actual scp command, because it is on port 1234 on localhost that the first ssh instance is listening to forward connections to remote2. Note also that you don't need to run the first command for every subsequent file copy; you can simply leave it running.",
    "tag": "scp"
  },
  {
    "question": "How to download a file from my server using SSH (using PuTTY on Windows)",
    "answer": "There's no way to initiate a file transfer back to/from local Windows from a SSH session opened in PuTTY window.\nBut you can use a separate SFTP/SCP client.\n\nIf you prefer commandline, you can use PuTTY pscp or OpenSSH scp (it's built-in in recent versions of Windows 10/11) from local Windows commandline (not from PuTTY console):\npscp user@host:/remote/source/path/file C:\\target\\local\\path\\file\n\nThe OpenSSH scp has the same syntax. Note that in recent versions the OpenSSH scp uses SFTP protocol.\n\nIf you prefer GUI, you can use any GUI SFTP/SCP client. For example my WinSCP.\nWinSCP integrates closely with PuTTY. While you are browsing the remote site, you can anytime open SSH terminal to the same site using Open in PuTTY command.\nSee Opening Session in PuTTY.\nWith an additional setup, you can even make PuTTY automatically navigate to the same directory you are browsing with WinSCP.\nSee Opening PuTTY in the same directory.\n\n\n\nAnother option is use of PuTTY connection-sharing.\nWhile you still need to run a compatible file transfer client (pscp or psftp), no new login is required, it automatically (if enabled) makes use of an existing PuTTY session.\nTo enable the sharing see:\nSharing an SSH connection between PuTTY tools.",
    "tag": "scp"
  },
  {
    "question": "Is it possible to make SCP ignore symbolic links during copy?",
    "answer": "I knew that it was possible, I just took wrong tool. I did it with rsync\nrsync --progress -avhe ssh /usr/local/  XXX.XXX.XXX.XXX:/BackUp/usr/local/",
    "tag": "scp"
  },
  {
    "question": "SCP doesn't work when echo in .bashrc?",
    "answer": "Using echo in a .bashrc will break scp, as scp expects to see its protocol data over the stdin/stdout channels. See https://bugzilla.redhat.com/show_bug.cgi?id=20527 for more discussion on this issue.\nThere's a few workarounds available:\n\nCondition on the 'interactive' flag (e.g. case $- in *i* as suggested by tripleee)\nUse the tty utility to detect an interactive shell (e.g. if tty > /dev/null or if [ -t 0 ])\nCheck the value of $SSH_TTY\n\nI suppose you should use whichever one works for you. I don't know what the best (most portable/most reliable) option is, unfortunately.",
    "tag": "scp"
  },
  {
    "question": "SCP Permission denied (publickey). on EC2 only when using -r flag on directories",
    "answer": "The -i flag specifies the private key (.pem file) to use. If you don't specify that flag (as in your first command) it will use your default ssh key (usually under ~/.ssh/).\nSo in your first command, you are actually asking scp to upload the .pem file itself using your default ssh key. I don't think that is what you want.\nTry instead with:\nscp -r -i /Applications/XAMPP/htdocs/keypairfile.pem uploads/* ec2-user@publicdns:/var/www/html/uploads",
    "tag": "scp"
  },
  {
    "question": "Why is copying a directory with Ansible so slow?",
    "answer": "TLDR: use synchronize instead of copy.\nHere's the copy command I'm using:\n- copy: src=testdata dest=/tmp/testdata/\n\nAs a guess, I assume the sync operations are slow. The files module documentation implies this too:\n\nThe \"copy\" module recursively copy facility does not scale to lots (>hundreds) of files. For alternative, see synchronize module, which is a wrapper around rsync.\n\nDigging into the source shows each file is processed with SHA1. That's implemented using hashlib.sha1. A local test implies that only takes 10 seconds for 900 files (that happen to take 400mb of space).\nSo, the next avenue. The copy is handled with module_utils/basic.py's atomic_move method. I'm not sure if accelerated mode helps (it's a mostly-deprecated feature), but I tried pipelining, putting this in a local ansible.cfg:\n[ssh_connection]\npipelining=True\n\nIt didn't appear to help; my sample took 24 minutes to run . There's obviously a loop that checks a file, uploads it, fixes permissions, then starts on the next file. That's a lot of commands, even if the ssh connection is left open. Reading between the lines it makes a little bit of sense- the \"file transfer\" can't be done in pipelining, I think.\nSo, following the hint to use the synchronize command:\n- synchronize: src=testdata dest=/tmp/testdata/\n\nThat took 18 seconds, even with pipeline=False. Clearly, the synchronize command is the way to go in this case.\nKeep in mind synchronize uses rsync, which defaults to mod-time and file size. If you want or need checksumming, add checksum=True to the command. Even with checksumming enabled the time didn't really change- still 15-18 seconds. I verified the checksum option was on by running ansible-playbook with -vvvv, that can be seen here:\nok: [testhost] => {\"changed\": false, \"cmd\": \"rsync --delay-updates -FF --compress --checksum --archive --rsh 'ssh  -o StrictHostKeyChecking=no' --out-format='<<CHANGED>>%i %n%L' \\\"testdata\\\" \\\"user@testhost:/tmp/testdata/\\\"\", \"msg\": \"\", \"rc\": 0, \"stdout_lines\": []}",
    "tag": "scp"
  },
  {
    "question": "scp transfer via java",
    "answer": "I ended up using Jsch- it was pretty straightforward, and seemed to scale up pretty well (I was grabbing a few thousand files every few minutes).",
    "tag": "scp"
  },
  {
    "question": "Using putty to scp from windows to Linux",
    "answer": "You need to tell scp where to send the file. In your command that is not working:\nscp C:\\Users\\Admin\\Desktop\\WMU\\5260\\A2.c ~\n\nYou have not mentioned a remote server. scp uses : to delimit the host and path, so it thinks you have asked it to download a file at the path \\Users\\Admin\\Desktop\\WMU\\5260\\A2.c from the host C to your local home directory.\nThe correct upload command, based on your comments, should be something like:\nC:\\> pscp C:\\Users\\Admin\\Desktop\\WMU\\5260\\A2.c ckg8221@thor.cs.wmich.edu:\n\nIf you are running the command from your home directory, you can use a relative path:\nC:\\Users\\Admin> pscp Desktop\\WMU\\5260\\A2.c ckg8221@thor.cs.wmich.edu:\n\nYou can also mention the directory where you want to this folder to be downloaded to at the remote server. i.e by just adding a path to the folder as below: \nC:/> pscp C:\\Users\\Admin\\Desktop\\WMU\\5260\\A2.c ckg8221@thor.cs.wmich.edu:/home/path_to_the_folder/",
    "tag": "scp"
  },
  {
    "question": "rsync - create all missing parent directories?",
    "answer": "As of version 3.2.3 (6 Aug 2020), rynsc has a flag for this purpose.\nFrom the rsync manual page (man rsync):\n--mkpath                 create the destination's path component",
    "tag": "scp"
  },
  {
    "question": "scp files from local to remote machine error: no such file or directory",
    "answer": "Looks like you are trying to copy to a local machine with that command. \nAn example scp looks more like the command below:\nCopy the file \"foobar.txt\" from the local host to a remote host\n$ scp foobar.txt your_username@remotehost.edu:/some/remote/directory\n\nscp \"the_file\"  your_username@the_remote_host:the/path/to/the/directory\n\nto send a directory:\nCopy the directory \"foo\" from the local host to a remote host's directory \"bar\"\n$ scp -r foo your_username@remotehost.edu:/some/remote/directory/bar\n\nscp -r \"the_directory_to_copy\" your_username@the_remote_host:the/path/to/the/directory/to/copy/to\n\nand to copy from remote host to local:\nCopy the file \"foobar.txt\" from a remote host to the local host\n$ scp your_username@remotehost.edu:foobar.txt /your/local/directory\n\nscp your_username@the_remote_host:the_file /your/local/directory \n\nand to include port number:\nCopy the file \"foobar.txt\" from a remote host with port 8080 to the local host\n$ scp -P 8080 your_username@remotehost.edu:foobar.txt /your/local/directory\n\nscp -P port_number your_username@the_remote_host:the_file /your/local/directory\n\nFrom a windows machine to linux machine using putty\npscp -r <directory_to_copy> username@remotehost:/path/to/directory/on/remote/host",
    "tag": "scp"
  },
  {
    "question": "Error when using scp command \"bash: scp: command not found\"",
    "answer": "Make sure the scp command is available on both sides - both on the client and on the server.\nIf this is Fedora or Red Hat Enterprise Linux and clones (CentOS), make sure this package is installed:\n    yum -y install openssh-clients\n\nIf you work with Debian or Ubuntu and clones, install this package:\n    apt-get install openssh-client\n\nAgain, you need to do this both on the server and the client, otherwise you can encounter \"weird\" error messages on your client: scp: command not found or similar although you have it locally. This already confused thousands of people, I guess :)",
    "tag": "scp"
  },
  {
    "question": "using ssh keys with scp and ssh",
    "answer": "Assume your case for scp from 192.168.1.1 try below command.\nscp -i ~/.ssh/mytest.key root@192.168.1.1:/<filepath on host> <path on client>\n\nmake sure the key file should have permission 600 or 400.",
    "tag": "scp"
  },
  {
    "question": "scp gives \"not a regular file\"",
    "answer": "I just tested this and found at least 3 situations in which scp will return not a regular file:\n\nFile is actually a directory\nFile is a named pipe (a.k.a. FIFO)\nFile is a device file\n\nCase #1 seems most likely. If you meant to transfer an entire directory structure with scp use the -r option to indicate recursive copy.",
    "tag": "scp"
  },
  {
    "question": "How to scp back to local when I've already sshed into remote machine?",
    "answer": "Given that you have an sshd running on your local machine, it's possible and you don't need to know your outgoing IP address. If SSH port forwarding is enabled, you can open a secure tunnel even when you already have an ssh connection opened, and without terminating it.\nAssume you have an ssh connection to some server:\nlocal $ ssh user@example.com\nPassword:\nremote $ echo abc > abc.txt  # now we have a file here\n\nOK now we need to copy that file back to our local server, and for some reason we don't want to open a new connection. OK, let's get the ssh command line by pressing Enter ~C (Enter, then tilde, then capital C):\nssh> help\nCommands:\n      -L[bind_address:]port:host:hostport    Request local forward\n      -R[bind_address:]port:host:hostport    Request remote forward\n      -D[bind_address:]port                  Request dynamic forward\n      -KR[bind_address:]port                 Cancel remote forward\n\nThat's just like the regular -L/R/D options. We'll need -R, so we hit Enter ~C again and type:\nssh> -R 127.0.0.1:2222:127.0.0.1:22\nForwarding port.\n\nHere we forward remote server's port 2222 to local machine's port 22 (and here is where you need the local SSH server to be started on port 22; if it's listening on some other port, use it instead of 22).\nNow just run scp on a remote server and copy our file to remote server's port 2222 which is mapped to our local machine's port 22 (where our local sshd is running).\nremote $ scp -P2222 abc.txt user@127.0.0.1:\nuser@127.0.0.1's password:\nabc.txt                   100%    4     0.0KB/s   00:00\n\nWe are done!\nremote $ exit\nlogout\nConnection to example.com closed.\nlocal $ cat abc.txt\nabc\n\nTricky, but if you really cannot just run scp from another terminal, could help.",
    "tag": "scp"
  },
  {
    "question": "subsystem request failed on channel 0 scp: Connection closed",
    "answer": "Try -O as an option in your scp cmd, i.e.\nscp -p -O -P 29418 michealvern.genzola@192.168.0.122:hooks/commit-msg \"jyei-erp/.git/hooks/\"\n\nHere is the documentation for the -O option:\n\n-O      Use the legacy SCP protocol for file transfers instead of the SFTP protocol.  Forcing the use of the SCP protocol may be necessary for servers that do not implement SFTP, for backwards‐compatibility for particular filename wildcard patterns and for expanding paths with a ~ prefix for older SFTP servers.\n\nFaced similar issue. This solution is from comments in github regarding this problem. For more details - https://github.com/PowerShell/Win32-OpenSSH/issues/1945",
    "tag": "scp"
  },
  {
    "question": "How can I scp a file with a colon in the file name?",
    "answer": "Not quite a bash escaping problem, it's scp treating x: as a [user@]host prefix, try:\nscp ./file:\\ name.mp4 user@host:\"/path/to/dest\"\n\nUsing relative (e.g. ./) or fully qualified paths (/path/to/source) prevents this behaviour - the presence of / before a : causes OpenSSH to stop checking for a possible host: or user@host: prefix). \nOpenSSH's scp only special-cases filenames that start with a colon allowing those to work without problems, it has no support for escaping a : in the normal sense, and has no other notion of valid hostnames so almost any filename with a : can cause this (or equivalent IPv6 behaviour if [ ] are found before :).\nThis can also affect other programs, e.g. rsync, the same workaround applies there.\n(Due to OpenSSH's simplistic parsing of [] enclosed IPv6 addresses, you can successfully scp files containing : which start with [, or contain @[ before the : and do not contain ]: , but that's not generally useful ;-)\n\n(The below text was written when the original question was How do I escape a colon in bash? It applies to that situation, but not to scp as no amount of shell escaping will help there.)\nTo answer the question about how to escape :, you don't need to, but \"\\:\" works. Places that a : is used:\n\nthe null command :, no need to escape, though you can, just like \\e\\c\\h\\o foo it has no effect on the command (\"no effect\" is not completely true, if you escape one or more characters it will prevent an alias being matched, and you can alias :)\nPATH (and others, CDPATH, MAILPATH) escaping the values has no useful effect (I have been unable to run a program in my PATH from a directory containing a :, which is a little unexpected)\nparameter expansion ${name:-x} and more, name must be [a-zA-Z_][a-zA-Z0-9_], so no need to escape variables names, and since there's no ambiguity, no need to escape subsequent : in the other variations of parameter expansion\n? : trinary operates only on variables and numbers, no need to escape\n== and =~ with classes in the pattern like [[:digit:]], you can escape with \\: but I'm at a loss as to how that might ever be useful...\nwithin command or function names, no need to escape, \\: has no useful effect\n\n(Note that the null command is just :, you can have a command or function named like \":foo\" and it can be invoked without escaping, in this respect it's different to # where a command named #foo would need to be escaped.)",
    "tag": "scp"
  },
  {
    "question": "scp stalled while copying large files",
    "answer": "An attempt at a comprehensive solution, as there could be several problems and limitations depending on your situation.\nrsync\nMy preferred option: using rsync doesn't give this problem and is a bit more versatile in my opinion, e.g. it keeps track of which files are already there, so if the connection ever does break it can pick up from where it left off - try the --partial flag too - among other things. \nInstead of \nscp local/path/some_file usr@server.com:\"/some/path/\"\n\nyou can just do\nrsync -avz --progress local/path/some_file usr@server.com:\"/some/path/\"\n\nI've tested this on several occasions when scp would give me the same problem it gave you - and now I just use rsync by default.\nLimit speed\nNot a solution for OP as the MTU is fixed in this situation (and probably not the issue here), but if the culprit is a slow/unreliable connection between the two drives, setting a speed limit reduces the delays which make the TCP connection stall - at the expense of a slower transfer of course. \nThis is because scp grabs all the bandwidth it can get unless you specify the maximum data rate in kilobits, like so:\nscp -l 8192 local/path/some_file usr@server.com:\"/some/path/\"\n\nThis doesn't always work though. \nCompression option\nscp's -C option can speed up the transfer, reducing the probability that the transfer stalls. \nDisabling TCP SACK\nAs mentioned by the OP, and here. \nsudo sysctl -w net.ipv4.tcp_sack=0\n\n(or similar)\nLAN card MTU\nAgain an MTU fix, not necessarily of the transfer specifically though: \nifconfig eth0 mtu 1492\n\nor on newer (Linux) systems: \nip link set dev eth0 mtu 1492\n\nOther\nIf all else fails, this lists another handful of potential solutions not included here. \nThe more exotic hpn bug may be at fault too.",
    "tag": "scp"
  },
  {
    "question": "scp copy directory to another server with private key auth",
    "answer": "or you can also do ( for pem file )\n scp -r -i file.pem user@192.10.10.10:/home/backup /home/user/Desktop/",
    "tag": "scp"
  },
  {
    "question": "How do I create a directory on remote host if it doesn't exist without ssh-ing in?",
    "answer": "You can use rsync.\nFor example,\nrsync -ave ssh fileToCopy ssh.myhost.net:/some/nonExisting/dirToCopyTO\n\nNote about rsync:\nrsync is utility software and network protocol for Unix which synchronizes files and directories from one location to another. It minimizes data transfer sizes by using delta encoding when appropriate using the rsync algorithm which is faster than other tools.",
    "tag": "scp"
  },
  {
    "question": "Getting stty: standard input: Inappropriate ioctl for device when using scp through an ssh tunnel",
    "answer": "I got the exact same problem when I included the following line on my ~/.bashrc:\nstty -ixon\n\nThe purpose of this line was to allow the use of Ctrl-s in reverse search of bash.\nThis gmane link has a solution: (original link dead) => Web Archive version of gmane link\n\n'stty' applies to ttys, which you have for interactive login sessions.\n.kshrc is executed for all sessions, including ones where stdin isn't\na  tty.  The solution, other than moving it to your .profile, is to\nmake  execution conditional on it being an interactive shell.\n\nThere are several ways to check for interactive shell. The following solves the problem for bash:\n[[ $- == *i* ]] && stty -ixon",
    "tag": "scp"
  },
  {
    "question": "File location if target path not specified with scp command",
    "answer": "When you use user@server without a ':' character, scp interprets user@server as the file name on the local machine to which you would like to copy your file.  So, you should find a file (or in this case a directory) called user@server in the directory from which you issued the scp -r local_folder user@server command.",
    "tag": "scp"
  },
  {
    "question": "Run local python script on remote server",
    "answer": "It is possible using ssh. Python accepts hyphen(-) as argument to execute the standard input,\ncat hello.py | ssh user@192.168.1.101 python -\n\nRun python --help for more info.",
    "tag": "scp"
  },
  {
    "question": "How to copy a directory from local machine to remote machine",
    "answer": "Easiest way is scp\nscp -r /path/to/local/storage user@remote.host:/path/to/copy\n\nrsync is best for when you want to update versions where it has been previously copied.\nIf that doesn't work, rerun with -v and see what the error is.",
    "tag": "scp"
  },
  {
    "question": "scp fails with \"protocol error: filename does not match request\"",
    "answer": "I ended up having a look through the source code and found the commit where this error is thrown:\nGitHub Commit\n\nremote->local directory copies satisfy the wildcard specified by the\n  user.\nThis checking provides some protection against a malicious server\n  sending unexpected filenames, but it comes at a risk of rejecting\n  wanted files due to differences between client and server wildcard\n  expansion rules.\nFor this reason, this also adds a new -T flag to disable the check.\n\nThey have added a new flag -T that will ignore this new check they've added so it is backwards compatible. However, I suppose we should look and find out why the filenames we're using are flagged as restricted.",
    "tag": "scp"
  },
  {
    "question": "How to copy a file from remote server to local machine?",
    "answer": "For example, your remote host is example.com and remote login name is user1:\nscp user1@example.com:/path/to/file /path/to/store/file",
    "tag": "scp"
  },
  {
    "question": "Use ssh from Windows command prompt",
    "answer": "New, resurrected project site (Win7 compability and more!): http://sshwindows.sourceforge.net\n\n1st January 2012 \n\nOpenSSH for Windows 5.6p1-2 based release created!!\nHappy New Year all! Since COpSSH has started charging I've resurrected this project \nUpdated all binaries to current releases\nAdded several new supporting DLLs as required by all executables in package\nRenamed switch.exe to bash.exe to remove the need to modify and compile mkpasswd.exe each build\nPlease note there is a very minor bug in this release, detailed in the docs. I'm working on fixing this, anyone who can code in C and can offer a bit of help it would be much appreciated",
    "tag": "scp"
  },
  {
    "question": "How do I download SCP and ssh on Cygwin?",
    "answer": "When you run the Cygwin installer again to install new packages, you might notice that searching for \"scp\" produces no results.\nIt's actually part of the \"openssh\" package.",
    "tag": "scp"
  },
  {
    "question": "Does scp create the target folder if it does not exist",
    "answer": "To achieve the task with ssh and scp (instead of rsync): lets break the task into 2 steps:\n1. Create directory if missing:\nssh user@ftpserver.com \"mkdir -p /data/install/somefolder\"\n\n2. Copy to it:\nscp -r /data/install/somefolder user@ftpserver.com:/data/install/somefolder\n\n\nPut them together\nserver=\"user@ftpserver.com\"\ndestiny=\"/data/install/somefolder\"\nsrc=\"/data/install/somefolder\"\nssh \"$server\" \"mkdir -p $destiny\" && scp -r \"$src\" \"$server:$destiny\"",
    "tag": "scp"
  },
  {
    "question": "sending a large file with SCP to a certain server stalls at exactly 2112 kB",
    "answer": "I seemed to have found the fix. I had to set the LAN card's mtu setting to 1492 by:\n# ifconfig eth0 mtu 1492",
    "tag": "scp"
  },
  {
    "question": "How to copy entire folder from Amazon EC2 Linux instance to local Linux machine?",
    "answer": "another way to do it is\nscp -i \"insert key file here\" -r \"insert ec2 instance here\" \"your local directory\"\n\nOne mistake I made was scp -ir. The key has to be after the -i, and the -r after that. \nso\nscp -i amazon.pem -r ec2-user@ec2-##-##-##:/source/dir /destination/dir",
    "tag": "scp"
  },
  {
    "question": "Prevent overwriting of files when using Scp",
    "answer": "rsync seems to be the solution to your problem. Here's an example I got from here:\nrsync -avz foo:src/bar /data/tmp\n\nThe -a option will preserve permissions, directory structure, ownership, and symlinks. You can also specify any of those options individually as well.\n-v and -z mean verbose and compress respectively. You don't really need them although -z is nice if you are copying large files.",
    "tag": "scp"
  },
  {
    "question": "scp from remote host to local host",
    "answer": "You need the ip of the other pc and do:\nscp user@ip_of_remote_pc:/home/user/stuff.php /Users/djorge/Desktop\n\nit will ask you for 'user's password on the other pc.",
    "tag": "scp"
  },
  {
    "question": "How to copy file from a Vagrant machine to localhost",
    "answer": "Another option is cat the files to something local:\nvagrant ssh -c \"sudo cat /home/vagrant/devstack/local.conf\" > local.conf\n\nThis should also work for files that require root permissions (something the vagrant SCP plugin doesn't seem to support).",
    "tag": "scp"
  },
  {
    "question": "Scp command syntax for copying a folder from local machine to a remote server",
    "answer": "scp -r C:/site user@server_ip:path\n\npath is the place, where site will be copied into the remote server\n\nEDIT: As I said in my comment, try pscp, as you want to use scp using PuTTY. \nThe other option is WinSCP",
    "tag": "scp"
  },
  {
    "question": "scp host key verification failed for remote to remote copy",
    "answer": "Check out the option:\n-3 : Copies between two remote hosts are transferred through the local host. Without this option the data is copied directly between the two remote hosts. Note that this option disables the progress meter.\nThis option became available in OpenSSH 5.7",
    "tag": "scp"
  },
  {
    "question": "Transferring Files between two EC2 Instances in the same region",
    "answer": "Actually, I figured it out ... I just needed to replace the Elastic IP with the private IP and configure the security groups properly to allow instances to communicate!\nTransferring from Machine A to Machine B\nI am running this code on machine A\nscp -i ~/Path-To-Key-File/AAA.pem /path/file  ec2-user@<Private IP of Machine B>:/path/file\n\nFor security groups, I had to allow SSH protocol over the private IP (from Machine B)!!",
    "tag": "scp"
  },
  {
    "question": "scp not working saying its a directory error",
    "answer": "Amazingly enough in my case it was that the directory didn't exists!! :|\nIs the error message a bug?... or it's me. Tempted for the latter.",
    "tag": "scp"
  },
  {
    "question": "Timeout in paramiko (python)",
    "answer": "The connection timeout can be set with the timeout parameter (that indicated the number of seconds for the time out as described here) of the connect function.\nssh = paramiko.SSHClient()\nssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\nssh.connect(host, username=username, password=password, timeout=10)\nsftp = ssh.open_sftp()\nsftp.get(remotepath, localpath)\nsftp.close()",
    "tag": "scp"
  },
  {
    "question": "How to scp to Amazon s3?",
    "answer": "As of 2015, SCP/SSH is not supported (and probably never will be for the reasons mentioned in the other answers).\nOfficial AWS tools for copying files to/from S3\n\ncommand line tool (pip3 install awscli) - note credentials need to be specified, I prefer via environment variables rather than a file: AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY.\naws s3 cp /tmp/foo/ s3://bucket/ --recursive --exclude \"*\" --include \"*.jpg\"\n\n\nhttp://docs.aws.amazon.com/cli/latest/reference/s3/index.html\n\nand an rsync-like command:\naws s3 sync . s3://mybucket\n\n\nhttp://docs.aws.amazon.com/cli/latest/reference/s3/sync.html\n\nWeb interface:\n\nhttps://console.aws.amazon.com/s3/home?region=us-east-1\n\n\nNon-AWS methods\nAny other solutions depend on third-party executables (e.g. botosync, jungledisk...) which can be great as long as they are supported. But third party tools come and go as years go by and your scripts will have a shorter shelf life.\n\nhttps://github.com/ncw/rclone\n\n\nEDIT: Actually, AWS CLI is based on botocore:\nhttps://github.com/boto/botocore\nSo botosync deserves a bit more respect as an elder statesman than I perhaps gave it.",
    "tag": "scp"
  },
  {
    "question": "Shellscript action if two files are different",
    "answer": "if ! cmp -s test.py test.py~\nthen\n  # restart service\nfi\n\nBreaking that down:\n\ncmp -s test.py test.py~ returns true (0) if test.py and test.py~ are identical, else false (1).  You can see this in man cmp. The -s options makes cmp silent, so it doesn't give any output (except errors), but only an exit code.\n! inverts that result, so the if statement translates to \"if test.py and test.py~ are different\".\n\nps: If you are not sure the 2nd file exists, you may want check that too. (cmp still works in this case, but gives an error message, suppressing error message may be enough too (cmp ... 2>/dev/null)",
    "tag": "scp"
  },
  {
    "question": "scp a folder to a remote system keeping the directory layout",
    "answer": "You need a two-pass solution. First, ensure the target directory exists on the remote host:\nssh me@my-system 'mkdir -p ~/test/sub1/subsub1' \n\nThen, you can copy your files. I recommend using rsync instead of scp, since it's designed for syncing directories. Example usage: \nrsync -r -e ssh ./test/sub1/subsub1/ me@my-system:~/test/sub1/subsub1\n\nThe -e flag accepts a remote shell to use to carry out the transfer. Trailing slashes are very important with rsync, so make sure yours match the example above.",
    "tag": "scp"
  },
  {
    "question": "Transferring a file to an amazon ec2 instance using scp always gives me permission denied (publickey,gssapi-with-mic)",
    "answer": "The example amazon provided is correct. It sounds like a folder permissions issue. If you created the folder you are trying to copy to with another user or another user created it, chances are you don't have permissions to copy to it or edit it. \nIf you have sudo abilities, you can try opening access for yourself. Though not recommended to be left this way, you could try this command:\nsudo chmod 777 /folderlocation\n\nThat gives complete read/write/executable permissions to anyone (hence why you shouldn't leave it at 777) but it will give you the chance to test your scp command to rule out permissions. \nAfterwards if you aren't familiar with permissions, I suggest you read up on it. this is an example: http://www.tuxfiles.org/linuxhelp/filepermissions.html It is generally suggested you lock down the folder as much as possible depending on the type of information held within. \nIf that was not the cause some other things you might want to check:\n\nare you in the directory of your key when executing the 'scp -i keyname' command?\ndo you have permissions to use the folder you are transferring from?\n\nBest of luck.",
    "tag": "scp"
  },
  {
    "question": "How to best capture and log scp output?",
    "answer": "scp prints its progress bar to the terminal using control codes. It will detect if you redirect output and thus omit the progress bar.\nYou can get around that by tricking scp into thinking it runs in a terminal using the \"script\" command which is installed on most distros by default:\nscript -q -c \"scp server:/file /tmp/\" > /tmp/test.txt\n\nThe content of test.txt will be:\nfile    0%    0     0.0KB/s   --:-- ETA\nfile   18%   11MB  11.2MB/s   00:04 ETA\nfile   36%   22MB  11.2MB/s   00:03 ETA\nfile   54%   34MB  11.2MB/s   00:02 ETA\nfile   73%   45MB  11.2MB/s   00:01 ETA\nfile   91%   56MB  11.2MB/s   00:00 ETA\nfile  100%   61MB  10.2MB/s   00:06\n\n...which is probably what you want.\nI stumbled over this problem while redirecting the output of an interactive script into a log file. Not having the results in the log wasn't a problem as you can always evaluate exit codes. But I really wanted the interactive user to see the progress bar. This answer solves both problems.",
    "tag": "scp"
  },
  {
    "question": "passing yes argument while scp command",
    "answer": "scp -o StrictHostKeyChecking=no root@IP:/root/K \n\nObviously, this isn't a very secure solution. Works for one-shots where you're not concerned about man in the middle, though.",
    "tag": "scp"
  },
  {
    "question": "How to enable sshpass output to console",
    "answer": "After\nsudo apt-get install expect\n\nthe file send-files.exp works as desired:\n#!/usr/bin/expect -f\n\nspawn scp -r $FILES $DEST\nmatch_max 100000\nexpect \"*?assword:*\"\nsend -- \"12345\\r\"\nexpect eof",
    "tag": "scp"
  },
  {
    "question": "How to transfer a file between two remote servers using scp from a third, local machine?",
    "answer": "If you can ssh to both remote servers from your local (local -> remote1 & local -> remote2), then you could try:\nssh -A -t user1@remote1 scp srcpath user2@remote2:destpath\n\nThis will transfer straight from remote1 to remote2 using your local credentials all the way.\nIf you do not want to be asked for passwords, then you should set up the authorized_keys file on remotes.",
    "tag": "scp"
  },
  {
    "question": "SCP File from local to Heroku Server",
    "answer": "As @tamas7 said it's firewalled, but your local machine is probably also firewalled. So unless you have a private server with SSH accessible from the Internet, you won't be able to scp.\nI'm personally using transfer.sh free and open source service.\nUpload your config.yml to it:\n$ curl --upload-file ./config.yml https://transfer.sh/\nhttps://transfer.sh/66nb8/config.yml\n\nThen download it back from wherever you want:\n$ wget https://transfer.sh/66nb8/config.yml",
    "tag": "scp"
  },
  {
    "question": "mtime.sec is not present",
    "answer": "I've got the same issue. It worked for me if I scp to ubuntu 12.04. When I did that to ubuntu 14.04, it failed with same message.\nI wrote the scp code based on this example http://www.jcraft.com/jsch/examples/ScpTo.java.html. I guess you too.\nIf I remove the whitespace after the \"T\" in the following line, it'll work.  Change:\ncommand=\"T \"+(_lfile...\n\nto\ncommand=\"T\"+(_lfile...\n\nEither do that or set ptimestamp = false",
    "tag": "scp"
  },
  {
    "question": "How can I get a folder from remote machine to local machine?",
    "answer": "The -r flag should work. In your example you seem to be forgetting the name of the folder you want to copy. Try:\nscp -r nameOfFolderToCopy username@ipaddress:/path/to/copy/\n\nto copy a folder from your local computer to a remote one. Or\nscp -r username@ipaddress:/path/of/folder/to/copy /target/local/directory\n\nto copy a folder from a remote machine to your local one.",
    "tag": "scp"
  },
  {
    "question": "Is there a SCP alternative for PowerShell?",
    "answer": "There is a handy little tool that comes with Putty called pscp.exe that will do this and can be called in powershell easily.\nExample below copies from windows to a CentOS box (logging in as the usercode \"bill\") and you use the -pw switch in pscp to pass in a password (otherwise the command window that is spawned will prompt for the Linux password):\nStart-Process 'C:\\Program Files (x86)\\PuTTY\\pscp.exe' -ArgumentList (\"-scp -pw password C:\\Document.rtf bill@192.168.0.28:/home/bill/\")  \n\n \nPuTTY Secure Copy client\nRelease 0.62\nUsage: pscp [options] [user@]host:source target\n       pscp [options] source [source...] [user@]host:target\n       pscp [options] -ls [user@]host:filespec\nOptions:\n  -V        print version information and exit\n  -pgpfp    print PGP key fingerprints and exit\n  -p        preserve file attributes\n  -q        quiet, don't show statistics\n  -r        copy directories recursively\n  -v        show verbose messages\n  -load sessname  Load settings from saved session\n  -P port   connect to specified port\n  -l user   connect with specified username\n  -pw passw login with specified password\n  -1 -2     force use of particular SSH protocol version\n  -4 -6     force use of IPv4 or IPv6\n  -C        enable compression\n  -i key    private key file for authentication\n  -noagent  disable use of Pageant\n  -agent    enable use of Pageant\n  -batch    disable all interactive prompts\n  -unsafe   allow server-side wildcards (DANGEROUS)\n  -sftp     force use of SFTP protocol\n  -scp      force use of SCP protocol",
    "tag": "scp"
  },
  {
    "question": "ssh config name alias not working for scp",
    "answer": "When you use scp with no additional options like you here, remote h1 tries to directly connect to h2.\nh1 -> h2\n\nSince h1 need to know who h2 is, h1 needs the definition of h2. But you could also route it over your PC like\nh1 -> your pc -> h2\n\nusing the option -3\nscp -r -3 h1:/dir1 h2:/dir2",
    "tag": "scp"
  },
  {
    "question": "Library to do SCP for C#",
    "answer": "Have you tried SharpSSH? (last update was 2013)",
    "tag": "scp"
  },
  {
    "question": "Unable to connect to remote host using paramiko?",
    "answer": "Maybe you are missing the missing_host_key_policy\nWhat about this one:\nproxy = None\nclient = paramiko.SSHClient()\nclient.load_system_host_keys()\nclient.set_missing_host_key_policy(paramiko.AutoAddPolicy())\nclient.connect(host['hostname'], username=host['user'], sock=proxy)\n\nmore examples here: www.programcreek.com",
    "tag": "scp"
  },
  {
    "question": "Copying files with scp: connection timed out",
    "answer": "After spending way too long on this, scp reports this error any time the syntax of the command line is wrong. If ssh works to the host that you are trying to reach, but scp returns this error, the scp command line is not understandable by scp - ie despite the error message, the error is not Connection related.\nThe mistake may be an invisible character. This can happen if you try to paste a long file name using ^v for example, but the character is entered into the command line instead.\nRetype your request and insure that you don't insert invisible characters.",
    "tag": "scp"
  },
  {
    "question": "How to get expect -c to work in single line rather than script",
    "answer": "Got it:\nThe following code scps a file called Sean_Lilly.zip from my box to another box without entering a password:\nexpect -c \"spawn /usr/bin/scp Sean_Lilly.zip adaptive@10.10.12.17:/opt/ams/epf_3_4/Sean_Lilly.zip; sleep 5; expect -re \\\"password\\\"; send \\\"ad\\r\\n\\\"; set timeout -1; expect -re \\\"100%\\\";\"\n\nI know this can be done by setting passwordless ssh access between the two boxes but I wanted to do it in one command line using expect. Thanks fuzzy lollipop for the inspiration. Note if you run expect -d -c \"spawn ... you get excellent debug on what is happening including whether your regex is good enough",
    "tag": "scp"
  },
  {
    "question": "What is the purpose of .PHONY in a Makefile?",
    "answer": "By default, Makefile targets are \"file targets\" - they are used to build files from other files. Make assumes its target is a file, and this makes writing Makefiles relatively easy:\nfoo: bar\n  create_one_from_the_other foo bar\n\nHowever, sometimes, you want your Makefile to run commands that do not represent physical files in the file system. Good examples of this are the common targets \"clean\" and \"all\". Chances are this isn't the case, but you may potentially have a file named clean in your main directory. In such a case Make will be confused because by default the clean target would be associated with this file and Make will only run it when the file doesn't appear to be up-to-date with regards to its dependencies.\nThese special targets are called phony and you can explicitly tell Make they're not associated with files, e.g.:\n.PHONY: clean\nclean:\n  rm -rf *.o\n\nNow make clean will run as expected even if you do have a file named clean.\nIn terms of Make, a phony target is simply a target that is always out-of-date, so whenever you ask make <phony_target>, it will run, independent from the state of the file system. Some common make targets that are often phony are: all, install, clean, distclean, TAGS, info, check.\nSee also: GNU make manual: Phony Targets",
    "tag": "makefile"
  },
  {
    "question": "What is the difference between the GNU Makefile variable assignments =, ?=, := and +=?",
    "answer": "Lazy Set\nVARIABLE = value\n\nNormal setting of a variable, but any other variables mentioned with the value field are recursively expanded with their value at the point at which the variable is used, not the one it had when it was declared\nImmediate Set\nVARIABLE := value\n\nSetting of a variable with simple expansion of the values inside - values within it are expanded at declaration time.\nLazy Set If Absent\nVARIABLE ?= value\n\nSetting of a variable only if it doesn't have a value. value is always evaluated when VARIABLE is accessed. It is equivalent to\nifeq ($(origin VARIABLE), undefined)\n  VARIABLE = value\nendif\n\nSee the documentation for more details.\nAppend\nVARIABLE += value\n\nAppending the supplied value to the existing value (or setting to that value if the variable didn't exist)",
    "tag": "makefile"
  },
  {
    "question": "Passing additional variables from command line to make",
    "answer": "You have several options to set up variables from outside your makefile:\n\nFrom environment - each environment variable is transformed into a makefile variable with the same name and value.\nYou may also want to set -e option (aka --environments-override) on, and your environment variables will override assignments made into makefile (unless these assignments themselves use the override directive .  However, it's not recommended, and it's much better and flexible to use ?= assignment (the conditional variable assignment operator, it only has an effect if the variable is not yet defined):\n  FOO?=default_value_if_not_set_in_environment\n\nNote that certain variables are not inherited from environment:\n\nMAKE is gotten from name of the script\nSHELL is either set within a makefile, or defaults to /bin/sh (rationale: commands are specified within the makefile, and they're shell-specific).\n\n\nFrom command line - make can take variable assignments as part of its command line, mingled with targets:\n  make target FOO=bar\n\nBut then all assignments to FOO variable within the makefile will be ignored unless you use the override directive in assignment. (The effect is the same as with -e option for environment variables).\n\nExporting from the parent Make - if you call Make from a Makefile, you usually shouldn't explicitly write variable assignments like this:\n  # Don't do this!\n  target:\n          $(MAKE) -C target CC=$(CC) CFLAGS=$(CFLAGS)\n\nInstead, better solution might be to export these variables.  Exporting a variable makes it into the environment of every shell invocation, and Make calls from these commands pick these environment variable as specified above.\n  # Do like this\n  CFLAGS=-g\n  export CFLAGS\n  target:\n          $(MAKE) -C target\n\nYou can also export all variables by using export without arguments.",
    "tag": "makefile"
  },
  {
    "question": "makefile:4: *** missing separator. Stop",
    "answer": "make defines a tab is required to start each recipe.  All actions of every rule are identified by tabs.  If you prefer to prefix your recipes with a character other than tab, you can set the .RECIPEPREFIX variable to an alternate character.\nTo check, I use the command cat -e -t -v makefile_name.\nIt shows the presence of tabs with ^I and line endings with $.  Both are vital to ensure that dependencies end properly and tabs mark the action for the rules so that they are easily identifiable to the make utility.\nExample:\nKaizen ~/so_test $ cat -e -t -v  mk.t\nall:ll$      ## here the $ is end of line ...                   \n$\nll:ll.c   $\n^Igcc  -c  -Wall -Werror -02 c.c ll.c  -o  ll  $@  $<$ \n## the ^I above means a tab was there before the action part, so this line is ok .\n $\nclean :$\n   \\rm -fr ll$\n## see here there is no ^I which means , tab is not present .... \n## in this case you need to open the file again and edit/ensure a tab \n## starts the action part",
    "tag": "makefile"
  },
  {
    "question": "What do the makefile symbols $@ and $< mean?",
    "answer": "$@ is the name of the target being generated, and $< the first prerequisite (usually a source file). You can find a list of all these special variables in the GNU Make manual.\nFor example, consider the following declaration:\nall: library.cpp main.cpp\n\nIn this case:\n\n$@ evaluates to all\n$< evaluates to library.cpp\n$^ evaluates to library.cpp main.cpp",
    "tag": "makefile"
  },
  {
    "question": "What is the difference between using a Makefile and CMake to compile the code?",
    "answer": "Make (or rather a Makefile) is a buildsystem - it drives the compiler and other build tools to build your code.\nCMake is a generator of buildsystems. It can produce Makefiles, it can produce Ninja build files, it can produce KDEvelop or Xcode projects, it can produce Visual Studio solutions. From the same starting point, the same CMakeLists.txt file. So if you have a platform-independent project, CMake is a way to make it buildsystem-independent as well.\nIf you have Windows developers used to Visual Studio and Unix developers who swear by GNU Make, CMake is (one of) the way(s) to go.\nI would always recommend using CMake (or another buildsystem generator, but CMake is my personal preference) if you intend your project to be multi-platform or widely usable. CMake itself also provides some nice features like dependency detection, library interface management, or integration with CTest, CDash and CPack.\nUsing a buildsystem generator makes your project more future-proof. Even if you're GNU-Make-only now, what if you later decide to expand to other platforms (be it Windows or something embedded), or just want to use an IDE?",
    "tag": "makefile"
  },
  {
    "question": "Passing arguments to \"make run\"",
    "answer": "I don't know a way to do what you want exactly, but a workaround might be:\nrun: ./prog\n    ./prog $(ARGS)\n\nThen:\nmake ARGS=\"asdf\" run\n# or\nmake run ARGS=\"asdf\"",
    "tag": "makefile"
  },
  {
    "question": "How do I write the 'cd' command in a makefile?",
    "answer": "It is actually executing the command, changing the directory to some_directory, however, this is performed in a sub-process shell, and affects neither make nor the shell you're working from.\nIf you're looking to perform more tasks within some_directory, you need to add a semi-colon and append the other commands as well. Note that you cannot use new lines as they are interpreted by make as the end of the rule, so any new lines you use for clarity need to be escaped by a backslash.\nFor example:\nall:\n        cd some_dir; echo \"I'm in some_dir\"; \\\n          gcc -Wall -o myTest myTest.c\n\nNote also that the semicolon is necessary between every command even though you add a backslash and a newline. This is due to the fact that the entire string is parsed as a single line by the shell. As noted in the comments, you should use '&&' to join commands, which means they only get executed if the preceding command was successful.\nall:\n        cd some_dir && echo \"I'm in some_dir\" && \\\n          gcc -Wall -o myTest myTest.c\n\nThis is especially crucial when doing destructive work, such as clean-up, as you'll otherwise destroy the wrong stuff, should the cd fail for whatever reason.\nA common usage, though, is to call make in the subdirectory, which you might want to look into. There's a command-line option for this, so you don't have to call cd yourself, so your rule would look like this\nall:\n        $(MAKE) -C some_dir all\n\nwhich will change into some_dir and execute the Makefile in that directory, with the target \"all\". As a best practice, use $(MAKE) instead of calling make directly, as it'll take care to call the right make instance (if you, for example, use a special make version for your build environment), as well as provide slightly different behavior when running using certain switches, such as -t.\nFor the record, make always echos the command it executes (unless explicitly suppressed), even if it has no output, which is what you're seeing.",
    "tag": "makefile"
  },
  {
    "question": "gcc makefile error: \"No rule to make target ...\"",
    "answer": "That's usually because you don't have a file called vertex.cpp available to make. Check that:\n\nthat file exists.\nyou're in the right directory when you make.\n\nOther than that, I've not much else to suggest. Perhaps you could give us a directory listing of that directory.",
    "tag": "makefile"
  },
  {
    "question": "What's the opposite of 'make install', i.e. how do you uninstall a library in Linux?",
    "answer": "make clean removes any intermediate or output files from your source / build tree.  However, it only affects the source / build tree; it does not touch the rest of the filesystem and so will not remove previously installed software.\nIf you're lucky, running make uninstall will work.  It's up to the library's authors to provide that, however; some authors provide an uninstall target, others don't.\nIf you're not lucky, you'll have to manually uninstall it.  Running make -n install can be helpful, since it will show the steps that the software would take to install itself but won't actually do anything.  You can then manually reverse those steps.",
    "tag": "makefile"
  },
  {
    "question": "How to install and use \"make\" in Windows?",
    "answer": "make is a GNU command so the only way you can get it on Windows is installing a Windows version like the one provided by GNUWin32. Anyway, there are several options for getting that:\n\nDirectly download from Make for Windows\n\nUsing Chocolatey. First you need to install this package manager. Once installed you simply need to install make (you may need to run it in an elevated/admin command prompt) :\nchoco install make\n\n\nOther recommended option is installing a Windows Subsystem for Linux (WSL/WSL2), so you'll have a Linux distribution of your choice embedded in Windows 10 where you'll be able to install make, gccand all the tools you need to build C programs.\n\nFor older Windows versions (MS Windows 2000 / XP / 2003 / Vista / 2008 / 7 with msvcrt.dll) you can use GnuWin32.\n\n\nAn outdated alternative was MinGw, but the project seems to be abandoned so it's better to go for one of the previous choices.",
    "tag": "makefile"
  },
  {
    "question": "How to discover number of *logical* cores on Mac OS X?",
    "answer": "You can do this using the sysctl utility:\nsysctl -n hw.ncpu",
    "tag": "makefile"
  },
  {
    "question": "What are Makefile.am and Makefile.in?",
    "answer": "Makefile.am is a programmer-defined file and is used by automake to generate the Makefile.in file (the .am stands for automake).\nThe configure script typically seen in source tarballs will use the Makefile.in to generate a Makefile.\nThe configure script itself is generated from a programmer-defined file named either configure.ac or configure.in (deprecated). I prefer .ac (for autoconf) since it differentiates it from the generated Makefile.in files and that way I can have rules such as make dist-clean which runs rm -f *.in. Since it is a generated file, it is not typically stored in a revision system such as Git, SVN, Mercurial or CVS, rather the .ac file would be.\nRead more on GNU Autotools.\nRead about make and Makefile first, then learn about automake, autoconf, libtool, etc.",
    "tag": "makefile"
  },
  {
    "question": "How do I force make/GCC to show me the commands?",
    "answer": "To invoke a dry run:\nmake -n\n\nThis will show what make is attempting to do.",
    "tag": "makefile"
  },
  {
    "question": "How to assign the output of a command to a Makefile variable",
    "answer": "Use the Make shell builtin like in MY_VAR=$(shell echo whatever)\nme@Zack:~$make\nMY_VAR IS whatever\n\n\nme@Zack:~$ cat Makefile \nMY_VAR := $(shell echo whatever)\n\nall:\n    @echo MY_VAR IS $(MY_VAR)",
    "tag": "makefile"
  },
  {
    "question": "How to print out a variable in makefile",
    "answer": "As per the GNU Make manual and also pointed by 'bobbogo' in the below answer,\nyou can use info / warning / error to display text.\n$(error   text…)\n$(warning text…)\n$(info    text…)\n\nTo print variables,\n$(error   VAR is $(VAR))\n$(warning VAR is $(VAR))\n$(info    VAR is $(VAR))\n\n'error' would stop the make execution, after showing the error string",
    "tag": "makefile"
  },
  {
    "question": "Why does make think the target is up to date?",
    "answer": "Maybe you have a file/directory named test in the directory. If this directory exists, and has no dependencies that are more recent, then this target is not rebuild.\nTo force rebuild on these kind of not-file-related targets, you should make them phony as follows:\n.PHONY: all test clean\n\nNote that you can declare all of your phony targets there.\nA phony target is one that is not really the name of a file; rather it is just a name for a recipe to be executed when you make an explicit request.",
    "tag": "makefile"
  },
  {
    "question": "How to run a makefile in Windows?",
    "answer": "You can install GNU make with chocolatey or scoop, a well-maintained package managers, which will add make to the global path and runs on all CLIs (powershell, git bash, cmd, etc…) saving you a ton of time in both maintenance and initial setup to get make running.\n\nInstall the chocolatey package manager for Windows\ncompatible to Windows 7+ / Windows Server 2003+\nRun choco install make\n\nI am not affiliated with choco, but I highly recommend it, so far it has never let me down and I do have a talent for breaking software unintentionally.\nedit in 2025: There is an alternative package manager for Windows called scoop that allows the same workflow, released a few years later.",
    "tag": "makefile"
  },
  {
    "question": "Make error: missing separator",
    "answer": "As indicated in the online manual, the most common cause for that error is that lines are indented with spaces  when make expects tab characters.\nCorrect\ntarget: \n\\tcmd\n\nwhere \\t is TAB (U+0009)\nWrong\ntarget:\n....cmd\n\nwhere each . represents a SPACE (U+0020).",
    "tag": "makefile"
  },
  {
    "question": "How do you get the list of targets in a makefile?",
    "answer": "Under Bash (at least), this can be done automatically with tab completion:\nmake spacetabtab\n\nIf you're using a bare-bones distribution (maybe a container) you might need to install a package.\n$ apt-get install bash-completion ## Debian/Ubuntu/etc.\n$ . /etc/bash_completion ## or just launch bash again",
    "tag": "makefile"
  },
  {
    "question": "OS detecting makefile",
    "answer": "There are many good answers here already, but I wanted to share a more complete example that both:\n\ndoesn't assume uname exists on Windows\nalso detects the processor\n\nThe CCFLAGS defined here aren't necessarily recommended or ideal; they're just what the project to which I was adding OS/CPU auto-detection happened to be using.\nifeq ($(OS),Windows_NT)\n    CCFLAGS += -D WIN32\n    ifeq ($(PROCESSOR_ARCHITEW6432),AMD64)\n        CCFLAGS += -D AMD64\n    else\n        ifeq ($(PROCESSOR_ARCHITECTURE),AMD64)\n            CCFLAGS += -D AMD64\n        endif\n        ifeq ($(PROCESSOR_ARCHITECTURE),x86)\n            CCFLAGS += -D IA32\n        endif\n    endif\nelse\n    UNAME_S := $(shell uname -s)\n    ifeq ($(UNAME_S),Linux)\n        CCFLAGS += -D LINUX\n    endif\n    ifeq ($(UNAME_S),Darwin)\n        CCFLAGS += -D OSX\n    endif\n    UNAME_P := $(shell uname -p)\n    ifeq ($(UNAME_P),x86_64)\n        CCFLAGS += -D AMD64\n    endif\n    ifneq ($(filter %86,$(UNAME_P)),)\n        CCFLAGS += -D IA32\n    endif\n    ifneq ($(filter arm%,$(UNAME_P)),)\n        CCFLAGS += -D ARM\n    endif\nendif",
    "tag": "makefile"
  },
  {
    "question": "How to make a SIMPLE C++ Makefile",
    "answer": "Since this is for Unix, the executables don't have any extensions.\nOne thing to note is that root-config is a utility which provides the right compilation and linking flags; and the right libraries for building applications against root. That's just a detail related to the original audience for this document.\nMake Me Baby\nor You Never Forget The First Time You Got Made\nAn introductory discussion of make, and how to write a simple makefile\nWhat is Make? And Why Should I Care?\nThe tool called Make is a build dependency manager. That is, it takes care of knowing what commands need to be executed in what order to take your software project from a collection of source files, object files, libraries, headers, etc., etc.---some of which may have changed recently---and turning them into a correct up-to-date version of the program.\nActually, you can use Make for other things too, but I'm not going to talk about that.\nA Trivial Makefile\nSuppose that you have a directory containing: tool tool.cc tool.o support.cc support.hh, and  support.o which depend on root and are supposed to be compiled into a program called tool, and suppose that you've been hacking on the source files (which means the existing tool is now out of date) and want to compile the program.\nTo do this yourself you could\n\nCheck if either support.cc or support.hh is newer than support.o, and if so run a command like\ng++ -g -c -pthread -I/sw/include/root support.cc\n\nCheck if either support.hh or tool.cc are newer than tool.o, and if so run a command like\ng++ -g  -c -pthread -I/sw/include/root tool.cc\n\nCheck if tool.o is newer than tool, and if so run a command like\ng++ -g tool.o support.o -L/sw/lib/root -lCore -lCint -lRIO -lNet -lHist -lGraf -lGraf3d -lGpad -lTree -lRint \\\n-lPostscript -lMatrix -lPhysics -lMathCore -lThread -lz -L/sw/lib -lfreetype -lz -Wl,-framework,CoreServices \\\n-Wl,-framework,ApplicationServices -pthread -Wl,-rpath,/sw/lib/root -lm -ldl\n\n\nPhew! What a hassle! There is a lot to remember and several chances to make mistakes. (BTW-- the particulars of the command lines exhibited here depend on our software environment. These ones work on my computer.)\nOf course, you could just run all three commands every time. That would work, but it doesn't scale well to a substantial piece of software (like DOGS which takes more than 15 minutes to compile from the ground up on my MacBook).\nInstead you could write a file called makefile like this:\ntool: tool.o support.o\n    g++ -g -o tool tool.o support.o -L/sw/lib/root -lCore -lCint -lRIO -lNet -lHist -lGraf -lGraf3d -lGpad -lTree -lRint \\\n        -lPostscript -lMatrix -lPhysics -lMathCore -lThread -lz -L/sw/lib -lfreetype -lz -Wl,-framework,CoreServices \\\n        -Wl,-framework,ApplicationServices -pthread -Wl,-rpath,/sw/lib/root -lm -ldl\n\ntool.o: tool.cc support.hh\n    g++ -g  -c -pthread -I/sw/include/root tool.cc\n\nsupport.o: support.hh support.cc\n    g++ -g -c -pthread -I/sw/include/root support.cc\n\nand just type make at the command line. Which will perform the three steps shown above automatically.\nThe unindented lines here have the form \"target: dependencies\" and tell Make that the associated commands (indented lines) should be run if any of the dependencies are newer than the target. That is, the dependency lines describe the logic of what needs to be rebuilt to accommodate changes in various files. If support.cc changes that means that support.o must be rebuilt, but tool.o can be left alone. When support.o changes tool must be rebuilt.\nThe commands associated with each dependency line are set off with a tab (see below) should modify the target (or at least touch it to update the modification time).\nVariables, Built In Rules, and Other Goodies\nAt this point, our makefile is simply remembering the work that needs doing, but we still had to figure out and type each and every needed command in its entirety. It does not have to be that way: Make is a powerful language with variables, text manipulation functions, and a whole slew of built-in rules which can make this much easier for us.\nMake Variables\nThe syntax for accessing a make variable is $(VAR).\nThe syntax for assigning to a Make variable is: VAR = A text value of some kind\n(or VAR := A different text value but ignore this for the moment).\nYou can use variables in rules like this improved version of our makefile:\nCPPFLAGS=-g -pthread -I/sw/include/root\nLDFLAGS=-g\nLDLIBS=-L/sw/lib/root -lCore -lCint -lRIO -lNet -lHist -lGraf -lGraf3d -lGpad -lTree -lRint \\\n       -lPostscript -lMatrix -lPhysics -lMathCore -lThread -lz -L/sw/lib -lfreetype -lz \\\n       -Wl,-framework,CoreServices -Wl,-framework,ApplicationServices -pthread -Wl,-rpath,/sw/lib/root \\\n       -lm -ldl\n\ntool: tool.o support.o\n    g++ $(LDFLAGS) -o tool tool.o support.o $(LDLIBS)\n\ntool.o: tool.cc support.hh\n    g++ $(CPPFLAGS) -c tool.cc\n\nsupport.o: support.hh support.cc\n    g++ $(CPPFLAGS) -c support.cc\n\nwhich is a little more readable, but still requires a lot of typing\nMake Functions\nGNU make supports a variety of functions for accessing information from the filesystem or other commands on the system. In this case we are interested in $(shell ...) which expands to the output of the argument(s), and $(subst opat,npat,text) which replaces all instances of opat with npat in text.\nTaking advantage of this gives us:\nCPPFLAGS=-g $(shell root-config --cflags)\nLDFLAGS=-g $(shell root-config --ldflags)\nLDLIBS=$(shell root-config --libs)\n\nSRCS=tool.cc support.cc\nOBJS=$(subst .cc,.o,$(SRCS))\n\ntool: $(OBJS)\n    g++ $(LDFLAGS) -o tool $(OBJS) $(LDLIBS)\n\ntool.o: tool.cc support.hh\n    g++ $(CPPFLAGS) -c tool.cc\n\nsupport.o: support.hh support.cc\n    g++ $(CPPFLAGS) -c support.cc\n\nwhich is easier to type and much more readable.\nNotice that\n\nWe are still stating explicitly the dependencies for each object file and the final executable\nWe've had to explicitly type the compilation rule for both source files\n\nImplicit and Pattern Rules\nWe would generally expect that all C++ source files should be treated the same way, and Make provides three ways to state this:\n\nsuffix rules (considered obsolete in GNU make, but kept for backwards compatibility)\nimplicit rules\npattern rules\n\nImplicit rules are built in, and a few will be discussed below. Pattern rules are specified in a form like\n%.o: %.c\n    $(CC) $(CFLAGS) $(CPPFLAGS) -c $<\n\nwhich means that object files are generated from C source files by running the command shown, where the \"automatic\" variable $< expands to the name of the first dependency.\nBuilt-in Rules\nMake has a whole host of built-in rules that mean that very often, a project can be compile by a very simple makefile, indeed.\nThe GNU make built in rule for C source files is the one exhibited above. Similarly we create object files from C++ source files with a rule like $(CXX) -c $(CPPFLAGS) $(CFLAGS).\nSingle object files are linked using $(LD) $(LDFLAGS) n.o $(LOADLIBES) $(LDLIBS), but this won't work in our case, because we want to link multiple object files.\nVariables Used By Built-in Rules\nThe built-in rules use a set of standard variables that allow you to specify local environment information (like where to find the ROOT include files) without re-writing all the rules. The ones most likely to be interesting to us are:\n\nCC -- the C compiler to use\nCXX -- the C++ compiler to use\nLD -- the linker to use\nCFLAGS -- compilation flag for C source files\nCXXFLAGS -- compilation flags for C++ source files\nCPPFLAGS -- flags for the c-preprocessor (typically include file paths and symbols defined on the command line), used by C and C++\nLDFLAGS -- linker flags\nLDLIBS -- libraries to link\n\nA Basic Makefile\nBy taking advantage of the built-in rules we can simplify our makefile to:\nCC=gcc\nCXX=g++\nRM=rm -f\nCPPFLAGS=-g $(shell root-config --cflags)\nLDFLAGS=-g $(shell root-config --ldflags)\nLDLIBS=$(shell root-config --libs)\n\nSRCS=tool.cc support.cc\nOBJS=$(subst .cc,.o,$(SRCS))\n\nall: tool\n\ntool: $(OBJS)\n    $(CXX) $(LDFLAGS) -o tool $(OBJS) $(LDLIBS)\n\ntool.o: tool.cc support.hh\n\nsupport.o: support.hh support.cc\n\nclean:\n    $(RM) $(OBJS)\n\ndistclean: clean\n    $(RM) tool\n\nWe have also added several standard targets that perform special actions (like cleaning up the source directory).\nNote that when make is invoked without an argument, it uses the first target found in the file (in this case all), but you can also name the target to get which is what makes make clean remove the object files in this case.\nWe still have all the dependencies hard-coded.\nSome Mysterious Improvements\nCC=gcc\nCXX=g++\nRM=rm -f\nCPPFLAGS=-g $(shell root-config --cflags)\nLDFLAGS=-g $(shell root-config --ldflags)\nLDLIBS=$(shell root-config --libs)\n\nSRCS=tool.cc support.cc\nOBJS=$(subst .cc,.o,$(SRCS))\n\nall: tool\n\ntool: $(OBJS)\n    $(CXX) $(LDFLAGS) -o tool $(OBJS) $(LDLIBS)\n\ndepend: .depend\n\n.depend: $(SRCS)\n    $(RM) ./.depend\n    $(CXX) $(CPPFLAGS) -MM $^>>./.depend;\n\nclean:\n    $(RM) $(OBJS)\n\ndistclean: clean\n    $(RM) *~ .depend\n\ninclude .depend\n\nNotice that\n\nThere are no longer any dependency lines for the source files!?!\nThere is some strange magic related to .depend and depend\nIf you do make then ls -A you see a file named .depend which contains things that look like make dependency lines\n\nOther Reading\n\nGNU make manual\nRecursive Make Considered Harmful on a common way of writing makefiles that is less than optimal, and how to avoid it.\n\nKnow Bugs and Historical Notes\nThe input language for Make is whitespace sensitive. In particular, the action lines following dependencies must start with a tab. But a series of spaces can look the same (and indeed there are editors that will silently convert tabs to spaces or vice versa), which results in a Make file that looks right and still doesn't work. This was identified as a bug early on, but (the story goes) it was not fixed, because there were already 10 users.\n(This was copied from a wiki post I wrote for physics graduate students.)",
    "tag": "makefile"
  },
  {
    "question": "How to get current relative directory of your Makefile?",
    "answer": "The shell function.\nYou can use shell function: current_dir = $(shell pwd).\nOr shell in combination with notdir, if you need not absolute path:\ncurrent_dir = $(notdir $(shell pwd)).\nUpdate.\nGiven solution only works when you are running make from the Makefile's current directory.\nAs @Flimm noted:\n\nNote that this returns the current working directory, not the parent directory of the Makefile. For example, if you run cd /; make -f /home/username/project/Makefile, the current_dir variable will be /, not /home/username/project/.\n\nCode below will work for Makefiles invoked from any directory:\nmkfile_path := $(abspath $(lastword $(MAKEFILE_LIST)))\ncurrent_dir := $(notdir $(patsubst %/,%,$(dir $(mkfile_path))))",
    "tag": "makefile"
  },
  {
    "question": "What is makeinfo, and how do I get it?",
    "answer": "In (at least) Ubuntu when using bash, it tells you what package you need to install if you type in a command and its not found in your path. My terminal says you need to install 'texinfo' package.\nsudo apt-get install texinfo",
    "tag": "makefile"
  },
  {
    "question": "Define make variable at rule execution time",
    "answer": "In your example, the TMP variable is set (and the temporary directory created) whenever the rules for out.tar are evaluated. In order to create the directory only when out.tar is actually fired, you need to move the directory creation down into the steps:\nout.tar : \n    $(eval TMP := $(shell mktemp -d))\n    @echo hi $(TMP)/hi.txt\n    tar -C $(TMP) cf $@ .\n    rm -rf $(TMP)\n\nThe eval function evaluates a string as if it had been typed into the makefile manually. In this case, it sets the TMP variable to the result of the shell function call.\nedit (in response to comments):\nTo create a unique variable, you could do the following:\nout.tar : \n    $(eval $@_TMP := $(shell mktemp -d))\n    @echo hi $($@_TMP)/hi.txt\n    tar -C $($@_TMP) cf $@ .\n    rm -rf $($@_TMP)\n\nThis would prepend the name of the target (out.tar, in this case) to the variable, producing a variable with the name out.tar_TMP. Hopefully, that is enough to prevent conflicts.",
    "tag": "makefile"
  },
  {
    "question": "How does \"make\" app know default target to build if no target is specified?",
    "answer": "By default, it begins by processing the first target that does not begin with a . aka the default goal; to do that, it may have to process other targets - specifically, ones the first target depends on.\nThe GNU Make Manual covers all this stuff, and is a surprisingly easy and informative read.",
    "tag": "makefile"
  },
  {
    "question": "Suppress echo of command invocation in makefile?",
    "answer": "Add @ to the beginning of command to tell gmake not to print the command being executed. Like this:\nrun:\n     @java myprogram\n\nAs Oli suggested, this is a feature of Make and not of Bash.\nOn the other hand, Bash will never echo commands being executed unless you tell it to do so explicitly (i.e. with -x option).",
    "tag": "makefile"
  },
  {
    "question": "How to write loop in a Makefile?",
    "answer": "The following will do it if, as I assume by your use of ./a.out, you're on a UNIX-type platform.\nfor number in 1 2 3 4 ; do \\\n    ./a.out $$number ; \\\ndone\n\nTest as follows:\ntarget:\n    for number in 1 2 3 4 ; do \\\n        echo $$number ; \\\n    done\n\nproduces:\n1\n2\n3\n4\n\nFor bigger ranges, use:\ntarget:\n    number=1 ; while [[ $$number -le 10 ]] ; do \\\n        echo $$number ; \\\n        ((number = number + 1)) ; \\\n    done\n\nThis outputs 1 through 10 inclusive, just change the while terminating condition from 10 to 1000 for a much larger range as indicated in your comment.\nNested loops can be done thus:\ntarget:\n    num1=1 ; while [[ $$num1 -le 4 ]] ; do \\\n        num2=1 ; while [[ $$num2 -le 3 ]] ; do \\\n            echo $$num1 $$num2 ; \\\n            ((num2 = num2 + 1)) ; \\\n        done ; \\\n        ((num1 = num1 + 1)) ; \\\n    done\n\nproducing:\n1 1\n1 2\n1 3\n2 1\n2 2\n2 3\n3 1\n3 2\n3 3\n4 1\n4 2\n4 3",
    "tag": "makefile"
  },
  {
    "question": "How can I use Bash syntax in Makefile targets?",
    "answer": "From the GNU Make documentation,\n5.3.2 Choosing the Shell\n------------------------\n\nThe program used as the shell is taken from the variable `SHELL'.  If\nthis variable is not set in your makefile, the program `/bin/sh' is\nused as the shell.\n\nSo put SHELL := /bin/bash at the top of your makefile, and you should be good to go.\nBTW: You can also do this for one target, at least for GNU Make. Each target can have its own variable assignments, like this:\nall: a b\n\na:\n    @echo \"a is $$0\"\n\nb: SHELL:=/bin/bash   # HERE: this is setting the shell for b only\nb:\n    @echo \"b is $$0\"\n\nThat'll print:\na is /bin/sh\nb is /bin/bash\n\nSee \"Target-specific Variable Values\" in the documentation for more details. That line can go anywhere in the Makefile, it doesn't have to be immediately before the target.",
    "tag": "makefile"
  },
  {
    "question": "What do @, - and + do as prefixes to recipe lines in Make?",
    "answer": "They control the behaviour of make for the tagged command lines:\n\n@ suppresses the normal 'echo' of the command that is executed.\n- means ignore the exit status of the command that is executed (normally, a non-zero exit status would stop that part of the build).\n+ means 'execute this command under make -n' (or 'make -t' or 'make -q') when commands are not normally executed. See also the POSIX specification for make and also §9.3 of the GNU Make manual.\n\nThe + notation is a (POSIX-standardized) generalization of the de facto  (non-standardized) mechanism whereby a command line containing ${MAKE} or $(MAKE) is executed under make -n.\n(@ is discussed in §5.2 of the GNU Make manual; - is described in §5.5; and §5.7.1 mentions the use of +.)",
    "tag": "makefile"
  },
  {
    "question": "How do you force a makefile to rebuild a target?",
    "answer": "The -B switch to make, whose long form is --always-make, tells make to disregard timestamps and make the specified targets. This may defeat the purpose of using make, but it may be what you need.",
    "tag": "makefile"
  },
  {
    "question": "Using G++ to compile multiple .cpp and .h files",
    "answer": "list all the other cpp files after main.cpp.\nie \ng++ main.cpp other.cpp etc.cpp\n\nand so on.\nOr you can compile them all individually.   You then link all the resulting \".o\" files together.",
    "tag": "makefile"
  },
  {
    "question": "In Unix, can I run 'make' in a directory without cd'ing to that directory first?",
    "answer": "make -C /path/to/dir",
    "tag": "makefile"
  },
  {
    "question": "What does @: (at symbol colon) mean in a Makefile?",
    "answer": "It means \"don't echo this command on the output.\"  So this rule is saying \"execute the shell command : and don't echo the output.\nOf course the shell command : is a no-op, so this is saying \"do nothing, and don't tell.\"\nWhy?\nThe trick here is that you've got an obscure combination of two different syntaxes.  The make(1) syntax is the use of an action starting with @, which is simply not to echo the command.  So a rule like \nalways:\n       @echo this always happens\n\nwon't emit\n   echo this always happens\n   this always happens\n\nNow, the action part of a rule can be any shell command, including :.  Bash help explains this as well as anywhere:\n$ help :\n:: :\n    Null command.\n\n    No effect; the command does nothing.\n\n    Exit Status:\n    Always succeeds.",
    "tag": "makefile"
  },
  {
    "question": "How to set child process' environment variable in Makefile",
    "answer": "Make variables are not exported into the environment of processes make invokes... by default.  However you can use make's export to force them to do so.  Change:\ntest: NODE_ENV = test\n\nto this:\ntest: export NODE_ENV = test\n\n(assuming you have a sufficiently modern version of GNU make >= 3.77 ).",
    "tag": "makefile"
  },
  {
    "question": "How to get a shell environment variable in a makefile?",
    "answer": "If you've exported the environment variable:\nexport demoPath=/usr/local/demo\n\nyou can simply refer to it by name in the makefile (make imports all the environment variables you have set):\nDEMOPATH = ${demoPath}    # Or $(demoPath) if you prefer.\n\nIf you've not exported the environment variable, it is not accessible until you do export it, or unless you pass it explicitly on the command line:\nmake DEMOPATH=\"${demoPath}\" …\n\nIf you are using a C shell derivative, substitute setenv demoPath /usr/local/demo for the export command.",
    "tag": "makefile"
  },
  {
    "question": "How to abort makefile if variable not set?",
    "answer": "TL;DR: Use the error function:\nifndef MY_FLAG\n$(error MY_FLAG is not set)\nendif\n\nNote that the lines must not be indented. More precisely, no tabs must precede these lines.\n\nGeneric solution\nIn case you're going to test many variables, it's worth defining an auxiliary function for that:\n# Check that given variables are set and all have non-empty values,\n# die with an error otherwise.\n#\n# Params:\n#   1. Variable name(s) to test.\n#   2. (optional) Error message to print.\ncheck_defined = \\\n    $(strip $(foreach 1,$1, \\\n        $(call __check_defined,$1,$(strip $(value 2)))))\n__check_defined = \\\n    $(if $(value $1),, \\\n      $(error Undefined $1$(if $2, ($2))))\n\nAnd here is how to use it:\n\n$(call check_defined, MY_FLAG)\n\n$(call check_defined, OUT_DIR, build directory)\n$(call check_defined, BIN_DIR, where to put binary artifacts)\n$(call check_defined, \\\n            LIB_INCLUDE_DIR \\\n            LIB_SOURCE_DIR, \\\n        library path)\n\n\nThis would output an error like this:\nMakefile:17: *** Undefined OUT_DIR (build directory).  Stop.\n\nNotes:\nThe real check is done here:\n$(if $(value $1),,$(error ...))\n\nThis reflects the behavior of the ifndef conditional, so that a variable defined to an empty value is also considered \"undefined\". But this is only true for simple variables and explicitly empty recursive variables:\n# ifndef and check_defined consider these UNDEFINED:\nexplicitly_empty =\nsimple_empty := $(explicitly_empty)\n\n# ifndef and check_defined consider it OK (defined):\nrecursive_empty = $(explicitly_empty)\n\nAs suggested by @VictorSergienko in the comments, a slightly different behavior may be desired:\n\n$(if $(value $1) tests if the value is non-empty. It's sometimes OK if the variable is defined with an empty value. I'd use $(if $(filter undefined,$(origin $1)) ...\n\nAnd:\n\nMoreover, if it's a directory and it must exist when the check is run, I'd use $(if $(wildcard $1)). But would be another function.\n\nTarget-specific check\nIt is also possible to extend the solution so that one can require a variable only if a certain target is invoked.\n$(call check_defined, ...) from inside the recipe\nJust move the check into the recipe:\n\nfoo :\n    @:$(call check_defined, BAR, baz value)\n\n\nThe leading @ sign turns off command echoing and : is the actual command, a shell no-op stub.\nShowing target name\nThe check_defined function can be improved to also output the target name (provided through the $@ variable):\ncheck_defined = \\\n    $(strip $(foreach 1,$1, \\\n        $(call __check_defined,$1,$(strip $(value 2)))))\n__check_defined = \\\n    $(if $(value $1),, \\\n        $(error Undefined $1$(if $2, ($2))$(if $(value @), \\\n                required by target `$@')))\n\nSo that, now a failed check produces a nicely formatted output:\nMakefile:7: *** Undefined BAR (baz value) required by target `foo'.  Stop.\n\ncheck-defined-MY_FLAG special target\nPersonally I would use the simple and straightforward solution above. However, for example, this answer suggests using a special target to perform the actual check. One could try to generalize that and define the target as an implicit pattern rule:\n# Check that a variable specified through the stem is defined and has\n# a non-empty value, die with an error otherwise.\n#\n#   %: The name of the variable to test.\n#   \ncheck-defined-% : __check_defined_FORCE\n    @:$(call check_defined, $*, target-specific)\n\n# Since pattern rules can't be listed as prerequisites of .PHONY,\n# we use the old-school and hackish FORCE workaround.\n# You could go without this, but otherwise a check can be missed\n# in case a file named like `check-defined-...` exists in the root \n# directory, e.g. left by an accidental `make -t` invocation.\n.PHONY : __check_defined_FORCE\n__check_defined_FORCE :\n\nUsage: \n\nfoo :|check-defined-BAR\n\n\nNotice that the check-defined-BAR is listed as the order-only (|...) prerequisite.\nPros:\n\n(arguably) a more clean syntax\n\nCons:\n\nOne can't specify a custom error message\nRunning make -t (see Instead of Executing Recipes) will pollute your root directory with lots of check-defined-... files. This is a sad drawback of the fact that pattern rules can't be declared .PHONY.\n\nI believe, these limitations can be overcome using some eval magic and secondary expansion hacks, although I'm not sure it's worth it.",
    "tag": "makefile"
  },
  {
    "question": "What's the difference between := and = in Makefile?",
    "answer": "Simple assignment :=\nA simple assignment expression is evaluated only once, at the very first occurrence. \nFor example, if CC :=${GCC} ${FLAGS} during the first encounter is evaluated to gcc -W then \neach time ${CC} occurs it will be replaced with gcc -W.\nRecursive assignment =\nA Recursive assignment expression is evaluated everytime the variable is encountered \nin the code. For example, a statement like CC = ${GCC} {FLAGS} will be evaluated only when\n an action like ${CC} file.c is executed. However, if the variable GCC is reassigned i.e\nGCC=c++ then the ${CC} will be converted to c++ -W after the reassignment. \nConditional assignment ?=\nConditional assignment assigns a value to a variable only if it does not have a value   \nAppending +=\nAssume that CC = gcc then the appending operator is used like CC += -w\nthen CC now has the value gcc -W\nFor more check out these tutorials",
    "tag": "makefile"
  },
  {
    "question": "makefile execute another target",
    "answer": "Actually you are right: it runs another instance of make.\nA possible solution would be:\n.PHONY : clearscr fresh clean all\n\nall :\n    compile executable\n\nclean :\n    rm -f *.o $(EXEC)\n\nfresh : clean clearscr all\n\nclearscr:\n    clear\n\nBy calling make fresh you get first the clean target, then the clearscreen which runs clear and finally all which does the job.\nEDIT Aug 4\nWhat happens in the case of parallel builds with make’s -j option?\nThere's a way of fixing the order. From the make manual, section 4.2:\n\nOccasionally, however, you have a situation where you want to impose a specific ordering on the rules to be invoked without forcing the target to be updated if one of those rules is executed. In that case, you want to define order-only prerequisites. Order-only prerequisites can be specified by placing a pipe symbol (|) in the prerequisites list: any prerequisites to the left of the pipe symbol are normal; any prerequisites to the right are order-only: targets : normal-prerequisites | order-only-prerequisites\nThe normal prerequisites section may of course be empty. Also, you may still declare multiple lines of prerequisites for the same target: they are appended appropriately. Note that if you declare the same file to be both a normal and an order-only prerequisite, the normal prerequisite takes precedence (since they are a strict superset of the behavior of an order-only prerequisite).\n\nHence the makefile becomes\n.PHONY : clearscr fresh clean all\n\nall :\n    compile executable\n\nclean :\n    rm -f *.o $(EXEC)\n\nfresh : | clean clearscr all\n\nclearscr:\n    clear\n\nEDIT Dec 5\nIt is not a big deal to run more than one makefile instance since each command inside the task will be a sub-shell anyways. But you can have reusable methods using the call function.\nlog_success = (echo \"\\x1B[32m>> $1\\x1B[39m\")\nlog_error = (>&2 echo \"\\x1B[31m>> $1\\x1B[39m\" && exit 1)\n\ninstall:\n  @[ \"$(AWS_PROFILE)\" ] || $(call log_error, \"AWS_PROFILE not set!\")\n  command1  # this line will be a subshell\n  command2  # this line will be another subshell\n  @command3  # Use `@` to hide the command line\n  $(call log_error, \"It works, yey!\")\n\nuninstall:\n  @[ \"$(AWS_PROFILE)\" ] || $(call log_error, \"AWS_PROFILE not set!\")\n  ....\n  $(call log_error, \"Nuked!\")",
    "tag": "makefile"
  },
  {
    "question": "Where can I find \"make\" program for Mac OS X Lion?",
    "answer": "You need to install Xcode from App Store.\nThen start Xcode, go to Xcode->Preferences->Downloads and install component named \"Command Line Tools\".\nAfter that all the relevant tools will be placed in /usr/bin folder and you will be able to use it just as it was in 10.6.",
    "tag": "makefile"
  },
  {
    "question": "How can I configure my makefile for debug and release builds?",
    "answer": "You can use Target-specific Variable Values. Example:\nCXXFLAGS = -g3 -gdwarf2\nCCFLAGS = -g3 -gdwarf2\n\nall: executable\n\ndebug: CXXFLAGS += -DDEBUG -g\ndebug: CCFLAGS += -DDEBUG -g\ndebug: executable\n\nexecutable: CommandParser.tab.o CommandParser.yy.o Command.o\n    $(CXX) -o output CommandParser.yy.o CommandParser.tab.o Command.o -lfl\n\nCommandParser.yy.o: CommandParser.l \n    flex -o CommandParser.yy.c CommandParser.l\n    $(CC) -c CommandParser.yy.c\n\nRemember to use $(CXX) or $(CC) in all your compile commands.\nThen, 'make debug' will have extra flags like -DDEBUG and -g where as 'make' will not.\nOn a side note, you can make your Makefile a lot more concise like other posts had suggested.",
    "tag": "makefile"
  },
  {
    "question": "Make install, but not to default directories?",
    "answer": "It depends on the package. If the Makefile is generated by GNU autotools (./configure) you can usually set the target location like so:\n./configure --prefix=/somewhere/else/than/usr/local\n\nIf the Makefile is not generated by autotools, but distributed along with the software, simply open it up in an editor and change it. The install target directory is probably defined in a variable somewhere.",
    "tag": "makefile"
  },
  {
    "question": "Make: how to continue after a command fails?",
    "answer": "Try the -i flag (or --ignore-errors). The documentation seems to suggest a more robust way to achieve this, by the way:\n\nTo ignore errors in a command line, write a - at the beginning of the line's text (after the initial tab). The - is discarded before the command is passed to the shell for execution.\nFor example,\nclean:\n  -rm -f *.o\n\nThis causes rm to continue even if it is unable to remove a file.\n\nAll examples are with rm, but are applicable to any other command you need to ignore errors from (i.e. mkdir).",
    "tag": "makefile"
  },
  {
    "question": "Why use make over a shell script?",
    "answer": "The general idea is that make supports (reasonably) minimal rebuilds -- i.e., you tell it what parts of your program depend on what other parts. When you update some part of the program, it only rebuilds the parts that depend on that. While you could do this with a shell script, it would be a lot more work (explicitly checking the last-modified dates on all the files, etc.) The only obvious alternative with a shell script is to rebuild everything every time. For tiny projects this is a perfectly reasonable approach, but for a big project a complete rebuild could easily take an hour or more -- using make, you might easily accomplish the same thing in a minute or two...\nI should probably also add that there are quite a few alternatives to make that have at least broadly similar capabilities. Especially in cases where only a few files in a large project are being rebuilt, some of them (e.g., Ninja) are often considerably faster than make.",
    "tag": "makefile"
  },
  {
    "question": "How do I check if file exists in Makefile so I can delete it?",
    "answer": "It's strange to see so many people using shell scripting for this. I was looking for a way to use native makefile syntax, because I'm writing this outside of any target. You can use the wildcard function to check if file exists:\n ifeq ($(UNAME),Darwin)\n     SHELL := /opt/local/bin/bash\n     OS_X  := true\n else ifneq (,$(wildcard /etc/redhat-release))\n     OS_RHEL := true\n else\n     OS_DEB  := true\n     SHELL := /bin/bash\n endif \n\nSee also:\n\nGNU make documentation: wildcard function\n\nUpdate:\nI found a way which I really like:\nifneq (\"$(wildcard $(PATH_TO_FILE))\",\"\")\n    FILE_EXISTS = 1\nelse\n    FILE_EXISTS = 0\nendif",
    "tag": "makefile"
  },
  {
    "question": "Compiling with g++ using multiple cores",
    "answer": "You can do this with make - with gnu make it is the -j flag (this will also help on a uniprocessor machine).\nFor example if you want 4 parallel jobs from make:\nmake -j 4\n\nYou can also run gcc in a pipe with \ngcc -pipe\n\nThis will pipeline the compile stages, which will also help keep the cores busy.\nIf you have additional machines available too, you might check out distcc, which will farm compiles out to those as well.",
    "tag": "makefile"
  },
  {
    "question": "How to ensure Makefile variable is set as a prerequisite?",
    "answer": "This will cause a fatal error if ENV is undefined and something needs it (in GNUMake, anyway).\n\n.PHONY: deploy check-env\n\ndeploy: check-env\n\t...\n\nother-thing-that-needs-env: check-env\n\t...\n\ncheck-env:\nifndef ENV\n\t$(error ENV is undefined)\nendif\n\n(Note that ifndef and endif are not indented - they control what make \"sees\", taking effect before the Makefile is run. \"$(error\" is indented with a tab so that it only runs in the context of the rule.)",
    "tag": "makefile"
  },
  {
    "question": "Compiling C++ on remote Linux machine - \"clock skew detected\" warning",
    "answer": "That message is usually an indication that some of your files have modification times later than the current system time. Since make decides which files to compile when performing an incremental build by checking if a source files has been modified more recently than its object file, this situation can cause unnecessary files to be built, or worse, necessary files to not be built.\nHowever, if you are building from scratch (not doing an incremental build) you can likely ignore this warning without consequence.",
    "tag": "makefile"
  },
  {
    "question": "Why is no one using make for Java?",
    "answer": "The fundamental issue with Make and Java is that Make works on the premise that you have specify a dependency, and then a rule to resolve that dependency.\nWith basic C, that typically \"to convert a main.c file to a main.o file, run \"cc main.c\".\nYou can do that in java, but you quickly learn something.\nMostly that the javac compiler is slow to start up.\nThe difference between:\njavac Main.java\njavac This.java\njavac That.java\njavac Other.java\n\nand \njavac Main.java This.java That.java Other.java\n\nis night and day.\nExacerbate that with hundreds of classes, and it just becomes untenable.\nThen you combine that with the fact that java tends to be organized as groups of files in directories, vs C and others which tend towards a flatter structure. Make doesn't have much direct support to working with hierarchies of files.\nMake also isn't very good at determining what files are out of date, at a collection level.\nWith Ant, it will go through and sum up all of the files that are out of date, and then compile them in one go. Make will simply call the java compiler on each individual file. Having make NOT do this requires enough external tooling to really show that Make is not quite up to the task.\nThat's why alternatives like Ant and Maven rose up.",
    "tag": "makefile"
  },
  {
    "question": "How to pass argument to Makefile from command line?",
    "answer": "You probably shouldn't do this; you're breaking the basic pattern of how Make works. But here it is:\naction:\n        @echo action $(filter-out $@,$(MAKECMDGOALS))\n\n%:      # thanks to chakrit\n    @:    # thanks to William Pursell\n\nEDIT:\nTo explain the first command,\n$(MAKECMDGOALS) is the list of \"targets\" spelled out on the command line, e.g. \"action value1 value2\".\n$@ is an automatic variable for the name of the target of the rule, in this case \"action\".\nfilter-out is a function that removes some elements from a list. So $(filter-out bar, foo bar baz) returns foo baz (it can be more subtle, but we don't need subtlety here).\nPut these together and $(filter-out $@,$(MAKECMDGOALS)) returns the list of targets specified on the command line other than \"action\", which might be \"value1 value2\".",
    "tag": "makefile"
  },
  {
    "question": "What is ?= in Makefile",
    "answer": "?= indicates to set the KDIR variable only if it's not set/doesn't have a value.\nFor example:\nKDIR ?= \"foo\"\nKDIR ?= \"bar\"\n\ntest:\n    echo $(KDIR)\n\nWould print \"foo\"\nGNU manual: http://www.gnu.org/software/make/manual/html_node/Setting.html",
    "tag": "makefile"
  },
  {
    "question": "Multi-line bash commands in makefile",
    "answer": "You can use backslash for line continuation. However note that the shell receives the whole command concatenated into a single line, so you also need to terminate some of the lines with a semicolon:\nfoo:\n    for i in `find`;     \\\n    do                   \\\n        all=\"$$all $$i\"; \\\n    done;                \\\n    gcc $$all\n\nBut if you just want to take the whole list returned by the find invocation and pass it to gcc, you actually don't necessarily need a multiline command:\nfoo:\n    gcc `find`\n\nOr, using a more shell-conventional $(command) approach (notice the $ escaping though):\nfoo:\n    gcc $$(find)",
    "tag": "makefile"
  },
  {
    "question": "What's the difference between parenthesis $() and curly bracket ${} syntax in Makefile?",
    "answer": "There's no difference – they mean exactly the same (in GNU Make and in POSIX make).\nI think that $(round brackets) look tidier, but that's just personal preference.\n(Other answers point to the relevant sections of the GNU Make documentation, and note that you shouldn't mix the syntaxes within a single expression)",
    "tag": "makefile"
  },
  {
    "question": "DESTDIR and PREFIX of make",
    "answer": "./configure --prefix=***\n\nNumber 1 determines where the package will go when it is installed, and where it will look for its associated files when it is run. It's what you should use if you're just compiling something for use on a single host.\n\n\nmake install DESTDIR=***\n\nNumber 2 is for installing to a temporary directory which is not where the package will be run from. For example this is used when building deb packages. The person building the package doesn't actually install everything into its final place on their own system. They may have a different version installed already and not want to disturb it, or they may not even be root. So they use\n./configure --prefix=/usr\n\nso the program will expect to be installed in /usr when it runs, then\nmake install DESTDIR=debian/tmp\n\nto actually create the directory structure.\n\n\nmake install prefix=***\n\nNumber 3 is going to install it to a different place but not create all the directories as DESTDIR=/foo/bar/baz would. It's commonly used with GNU stow via\n./configure --prefix=/usr/local && make && sudo make install prefix=/usr/local/stow/foo\n\n, which would install binaries in /usr/local/stow/foo/bin. By comparison,\nmake install DESTDIR=/usr/local/stow/foo\n\nwould install binaries in /usr/local/stow/foo/usr/local/bin.",
    "tag": "makefile"
  },
  {
    "question": "How to call Makefile from another Makefile?",
    "answer": "I'm not really too clear what you are asking, but using the -f command line option just specifies a file - it doesn't tell make to change directories. If you want to do the work in another directory, you need to cd to the directory:\nclean:\n    cd gtest-1.4.0 && $(MAKE) clean\n\nNote that each line in Makefile runs in a separate shell, so there is no need to change the directory back.",
    "tag": "makefile"
  },
  {
    "question": "Using local makefile for CLion instead of CMake",
    "answer": "Update: If you are using CLion 2020.2, then it already supports Makefiles. If you are using an older version, read on.\n\nEven though currently only CMake is supported, you can instruct CMake to call make with your custom Makefile. Edit your CMakeLists.txt adding one of these two commands:\n\nadd_custom_target\nadd_custom_command\n\nWhen you tell CLion to run your program, it will try to find an executable with the same name of the target in the directory pointed by PROJECT_BINARY_DIR. So as long as your make generates the file where CLion expects, there will be no problem.\nHere is a working example:\nTell CLion to pass its $(PROJECT_BINARY_DIR) to make\nThis is the sample CMakeLists.txt:\ncmake_minimum_required(VERSION 2.8.4)\nproject(mytest)\n\nset(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -std=c++11\")\n\nadd_custom_target(mytest COMMAND make -C ${mytest_SOURCE_DIR}\n                         CLION_EXE_DIR=${PROJECT_BINARY_DIR})\n\nTell make to generate the executable in CLion's directory\nThis is the sample Makefile:\nall:\n    echo Compiling $(CLION_EXE_DIR)/$@ ...\n    g++ mytest.cpp -o $(CLION_EXE_DIR)/mytest\n\nThat is all, you may also want to change your program's working directory so it executes as it is when you run make from inside your directory. For this edit: Run -> Edit Configurations ... -> mytest -> Working directory",
    "tag": "makefile"
  },
  {
    "question": "Check if a program exists from a Makefile",
    "answer": "Sometimes you need a Makefile to be able to run on different target OS's and you want the build to fail early if a required executable is not in PATH rather than to run for a possibly long time before failing.\nThe excellent solution provided by engineerchuan requires making a target. However, if you have many executables to test and your Makefile has many independent targets, each of which requires the tests, then each target requires the test target as a dependency. That makes for a lot of extra typing as well as processing time when you make more than one target at a time.\nThe solution provided by 0xf can test for an executable without making a target. That saves a lot of typing and execution time when there are multiple targets that can be built either separately or together.\nMy improvement to the latter solution is to use the which executable (where in Windows), rather than to rely on there being a --version option in each executable, directly in the GNU Make ifeq directive, rather than to define a new variable, and to use the GNU Make error function to stop the build if a required executable is not in ${PATH}. For example, to test for the lzop executable:\n ifeq (, $(shell which lzop))\n $(error \"No lzop in $(PATH), consider doing apt-get install lzop\")\n endif\n\nIf you have several executables to check, then you might want to use a foreach function with the which executable: \nEXECUTABLES = ls dd dudu lxop\nK := $(foreach exec,$(EXECUTABLES),\\\n        $(if $(shell which $(exec)),some string,$(error \"No $(exec) in PATH\")))\n\nNote the use of the := assignment operator that is required in order to force immediate evaluation of the RHS expression. If your Makefile changes the PATH, then instead of the last line above you will need:\n        $(if $(shell PATH=$(PATH) which $(exec)),some string,$(error \"No $(exec) in PATH\")))\n\nThis should give you output similar to:\nads$ make\nMakefile:5: *** \"No dudu in PATH.  Stop.",
    "tag": "makefile"
  },
  {
    "question": "How to use shell commands in Makefile",
    "answer": "With:\nFILES = $(shell ls)\n\nindented underneath all like that, it's a build command.  So this expands $(shell ls), then tries to run the command FILES ....\nIf FILES is supposed to be a make variable, these variables need to be assigned outside the recipe portion, e.g.:\nFILES = $(shell ls)\nall:\n        echo $(FILES)\n\nOf course, that means that FILES will be set to \"output from ls\" before running any of the commands that create the .tgz files.  (Though as Kaz notes the variable is re-expanded each time, so eventually it will include the .tgz files; some make variants have FILES := ... to avoid this, for efficiency and/or correctness.1)\nIf FILES is supposed to be a shell variable, you can set it but you need to do it in shell-ese, with no spaces, and quoted:\nall:\n        FILES=\"$(shell ls)\"\n\nHowever, each line is run by a separate shell, so this variable will not survive to the next line, so you must then use it immediately:\n        FILES=\"$(shell ls)\"; echo $$FILES\n\nThis is all a bit silly since the shell will expand * (and other shell glob expressions) for you in the first place, so you can just:\n        echo *\n\nas your shell command.\nFinally, as a general rule (not really applicable to this example): as esperanto notes in comments, using the output from ls is not completely reliable (some details depend on file names and sometimes even the version of ls; some versions of ls attempt to sanitize output in some cases).  Thus, as l0b0 and idelic note, if you're using GNU make you can use $(wildcard) and $(subst ...) to accomplish everything inside make itself (avoiding any \"weird characters in file name\" issues).  (In sh scripts, including the recipe portion of makefiles, another method is to use find ... -print0 | xargs -0 to avoid tripping over blanks, newlines, control characters, and so on.)\n\n1The GNU Make documentation notes further that POSIX make added ::= assignment in 2012.  I have not found a quick reference link to a POSIX document for this, nor do I know off-hand which make variants support ::= assignment, although GNU make does today, with the same meaning as :=, i.e., do the assignment right now with expansion.\nNote that VAR := $(shell command args...) can also be spelled VAR != command args... in several make variants, including all modern GNU and BSD variants as far as I know.  These other variants do not have $(shell) so using VAR != command args... is superior in both being shorter and working in more variants.",
    "tag": "makefile"
  },
  {
    "question": "Makefiles with source files in different directories",
    "answer": "The traditional way is to have a Makefile in each of the subdirectories (part1, part2, etc.) allowing you to build them independently. Further, have a Makefile in the root directory of the project which builds everything.  The \"root\" Makefile would look something like the following: \nall:\n    +$(MAKE) -C part1\n    +$(MAKE) -C part2\n    +$(MAKE) -C part3\n\nSince each line in a make target is run in its own shell, there is no need to worry about traversing back up the directory tree or to other directories.\nI suggest taking a look at the GNU make manual section 5.7; it is very helpful.",
    "tag": "makefile"
  },
  {
    "question": "Is it possible to create a multi-line string variable in a Makefile",
    "answer": "Yes, you can use the define keyword to declare a multi-line variable, like this:\ndefine ANNOUNCE_BODY\nVersion $(VERSION) of $(PACKAGE_NAME) has been released.\n\nIt can be downloaded from $(DOWNLOAD_URL).\n\netc, etc.\nendef\n\nThe tricky part is getting your multi-line variable back out of the makefile.  If you just do the obvious thing of using \"echo $(ANNOUNCE_BODY)\", you'll see the result that others have posted here -- the shell tries to handle the second and subsequent lines of the variable as commands themselves.\nHowever, you can export the variable value as-is to the shell as an environment variable, and then reference it from the shell as an environment variable (NOT a make variable).  For example:\nexport ANNOUNCE_BODY\nall:\n    @echo \"$$ANNOUNCE_BODY\"\n\nNote the use of $$ANNOUNCE_BODY, indicating a shell environment variable reference, rather than $(ANNOUNCE_BODY), which would be a regular make variable reference.  Also be sure to use quotes around your variable reference, to make sure that the newlines aren't interpreted by the shell itself.\nOf course, this particular trick may be platform and shell sensitive.  I tested it on Ubuntu Linux with GNU bash 3.2.13; YMMV.",
    "tag": "makefile"
  },
  {
    "question": "Difference between CPPFLAGS and CXXFLAGS in GNU Make",
    "answer": "CPPFLAGS is supposed to be for flags for the C PreProcessor; CXXFLAGS is for flags for the C++ compiler.\nThe default rules in make (on my machine, at any rate) pass CPPFLAGS to just about everything, CFLAGS is only passed when compiling and linking C, and CXXFLAGS is only passed when compiling and linking C++.",
    "tag": "makefile"
  },
  {
    "question": "What is the difference between gmake and make?",
    "answer": "'gmake' refers specifically to GNU make. 'make' refers to the system's default make implementation; on most Linux distros this is GNU make, but on other unixes, it could refer to some other implementation of make, such as BSD make, or the make implementations of various commercial unixes.\nThe language accepted by GNU make is a superset of the one supported by the traditional make utility.\nBy using 'gmake' specifically you can use GNU make extensions without worrying about them being misinterpreted by some other make implementation.",
    "tag": "makefile"
  },
  {
    "question": "How do I make a simple makefile for gcc on Linux?",
    "answer": "Interesting, I didn't know make would default to using the C compiler given rules regarding source files.\nAnyway, a simple solution that demonstrates simple Makefile concepts would be:\nHEADERS = program.h headers.h\n\ndefault: program\n\nprogram.o: program.c $(HEADERS)\n    gcc -c program.c -o program.o\n\nprogram: program.o\n    gcc program.o -o program\n\nclean:\n    -rm -f program.o\n    -rm -f program\n\n(bear in mind that make requires tab instead of space indentation, so be sure to fix that when copying)\nHowever, to support more C files, you'd have to make new rules for each of them.  Thus, to improve:\nHEADERS = program.h headers.h\nOBJECTS = program.o\n\ndefault: program\n\n%.o: %.c $(HEADERS)\n    gcc -c $< -o $@\n\nprogram: $(OBJECTS)\n    gcc $(OBJECTS) -o $@\n\nclean:\n    -rm -f $(OBJECTS)\n    -rm -f program\n\nI tried to make this as simple as possible by omitting variables like $(CC) and $(CFLAGS) that are usually seen in makefiles.  If you're interested in figuring that out, I hope I've given you a good start on that.\nHere's the Makefile I like to use for C source.  Feel free to use it:\nTARGET = prog\nLIBS = -lm\nCC = gcc\nCFLAGS = -g -Wall\n\n.PHONY: default all clean\n\ndefault: $(TARGET)\nall: default\n\nOBJECTS = $(patsubst %.c, %.o, $(wildcard *.c))\nHEADERS = $(wildcard *.h)\n\n%.o: %.c $(HEADERS)\n    $(CC) $(CFLAGS) -c $< -o $@\n\n.PRECIOUS: $(TARGET) $(OBJECTS)\n\n$(TARGET): $(OBJECTS)\n    $(CC) $(OBJECTS) -Wall $(LIBS) -o $@\n\nclean:\n    -rm -f *.o\n    -rm -f $(TARGET)\n\nIt uses the wildcard and patsubst features of the make utility to automatically include .c and .h files in the current directory, meaning when you add new code files to your directory, you won't have to update the Makefile.  However, if you want to change the name of the generated executable, libraries, or compiler flags, you can just modify the variables.\nIn either case, don't use autoconf, please.  I'm begging you!  :)",
    "tag": "makefile"
  },
  {
    "question": "How to add multi line comments in makefiles",
    "answer": "No, there is nothing like C-style /* */ comments in makefiles.  As somebody else suggested, you can make a multi-line comment by using line continuations.  For example:\n# This is the first line of a comment \\\nand this is still part of the comment \\\nas is this, since I keep ending each line \\\nwith a backslash character\n\nHowever, I imagine that you are probably looking to temporarily comment out a chunk of your makefile for debugging reasons, and adding a backslash on every line is not really practical.  If you are using GNU make, I suggest you use the ifeq directive with a deliberately false expression.  For example:\nifeq (\"x\",\"y\")\n# here's all your 'commented' makefile content...\nendif",
    "tag": "makefile"
  },
  {
    "question": "make: Nothing to be done for `all'",
    "answer": "Sometimes \"Nothing to be done for all\" error can be caused by spaces before command in makefile rule instead of tab. Please ensure that you use tabs instead of spaces inside of your rules.\nall:\n<\\t>$(CC) $(CFLAGS) ...\n\ninstead of\nall:\n    $(CC) $(CFLAGS) ...\n\nPlease see the GNU make manual for the rule syntax description: https://www.gnu.org/software/make/manual/make.html#Rule-Syntax",
    "tag": "makefile"
  },
  {
    "question": "What does \"all\" stand for in a makefile?",
    "answer": "A build, as Makefile understands it, consists of a lot of targets.  For example, to build a project you might need\n\nBuild file1.o out of file1.c\nBuild file2.o out of file2.c\nBuild file3.o out of file3.c\nBuild executable1 out of file1.o and file3.o\nBuild executable2 out of file2.o\n\nIf you implemented this workflow with makefile, you could make each of the targets separately.  For example, if you wrote\nmake file1.o\n\nit would only build that file, if necessary.\nThe name of all is not fixed.  It's just a conventional name;  all target denotes that if you invoke it, make will build all what's needed to make a complete build.  This is usually a dummy target, which doesn't create any files, but merely depends on the other files.  For the example above, building all necessary is building executables, the other files being pulled in as dependencies.  So in the makefile it looks like this:\nall: executable1 executable2\n\nall target is usually the first in the makefile, since if you just write make in command line, without specifying the target, it will build the first target.  And you expect it to be all.\nall is usually also a .PHONY target.  Learn more here.",
    "tag": "makefile"
  },
  {
    "question": "Define a Makefile variable using a ENV variable or a default value",
    "answer": "To follow up on my comments above, here's an example:\nT ?= foo\nall:\n        $(info T is $(T))\n\nNow if I run the Makefile in various ways, it behaves as we expect (I get foo only if I don't set T either on the command line or environment):\n$ make\nT is foo\n\n$ make T=bar\nT is bar\n\n$ T=bar make\nT is bar",
    "tag": "makefile"
  },
  {
    "question": "Makefile ifeq logical or",
    "answer": "As found on the mailing list archive,\n\nhttp://osdir.com/ml/gnu.make.windows/2004-03/msg00063.html\nhttp://osdir.com/ml/gnu.make.general/2005-10/msg00064.html\n\none can use the filter function.\nFor example\nifeq ($(GCC_MINOR),$(filter $(GCC_MINOR),4 5))\n\nfilter X, A B will return those of A,B that are equal to X.\nNote, while this is not relevant in the above example, this is a XOR operation. I.e. if you instead have something like:\nifeq (4, $(filter 4, $(VAR1) $(VAR2)))\n\nAnd then do e.g. make VAR1=4 VAR2=4, the filter will return 4 4, which is not equal to 4.\nA variation that performs an OR operation instead is:\nifneq (,$(filter $(GCC_MINOR),4 5))\n\nwhere a negative comparison against an empty string is used instead (filter will return en empty string if GCC_MINOR doesn't match the arguments). Using the VAR1/VAR2 example it would look like this:\nifneq (, $(filter 4, $(VAR1) $(VAR2)))\n\nThe downside to those methods is that you have to be sure that these arguments will always be single words. For example, if VAR1 is 4 foo, the filter result is still 4, and the ifneq expression is still true. If VAR1 is 4 5, the filter result is 4 5 and the ifneq expression is true.\nOne easy alternative is to just put the same operation in both the ifeq and else ifeq branch, e.g. like this:\nifeq ($(GCC_MINOR),4)\n    @echo Supported version\nelse ifeq ($(GCC_MINOR),5)\n    @echo Supported version\nelse\n    @echo Unsupported version\nendif",
    "tag": "makefile"
  },
  {
    "question": "How can I pass a macro definition from \"make\" command line arguments (-D) to C source code?",
    "answer": "Call the make command this way:\nmake CFLAGS=-Dvar=42\n\nAnd be sure to use $(CFLAGS) in your compile command in the Makefile. As @jørgensen mentioned, putting the variable assignment after the make command will override the CFLAGS value already defined in the Makefile.\nAlternatively, you could set -Dvar=42 in another variable than CFLAGS and then reuse this variable in CFLAGS to avoid completely overriding CFLAGS.",
    "tag": "makefile"
  },
  {
    "question": "Can I compile all .cpp files in src/ to .o's in obj/, then link to binary in ./?",
    "answer": "Makefile part of the question\nThis is pretty easy, unless you don't need to generalize\ntry something like the code below (but replace space indentation with tabs near g++)\nSRC_DIR := .../src\nOBJ_DIR := .../obj\nSRC_FILES := $(wildcard $(SRC_DIR)/*.cpp)\nOBJ_FILES := $(patsubst $(SRC_DIR)/%.cpp,$(OBJ_DIR)/%.o,$(SRC_FILES))\nLDFLAGS := ...\nCPPFLAGS := ...\nCXXFLAGS := ...\n\nmain.exe: $(OBJ_FILES)\n   g++ $(LDFLAGS) -o $@ $^\n\n$(OBJ_DIR)/%.o: $(SRC_DIR)/%.cpp\n   g++ $(CPPFLAGS) $(CXXFLAGS) -c -o $@ $<\n\n\nAutomatic dependency graph generation\nA \"must\" feature for most make systems. With GCC in can be done in a single pass as a side effect of the compilation by adding -MMD flag to CXXFLAGS and  -include $(OBJ_FILES:.o=.d)  to the end of the makefile body:\nCXXFLAGS += -MMD\n-include $(OBJ_FILES:.o=.d)\n\nAnd as guys mentioned already, always have GNU Make Manual around, it is very helpful.",
    "tag": "makefile"
  },
  {
    "question": "Can you make valid Makefiles without tab characters?",
    "answer": "This is a syntax oddity/requirement of make, it has nothing to do with Mac OS X.  Unfortunately, there's nothing you can do about it if you are going to use make.\nEdit: GNU Make now supports a custom recipe prefix.  See this answer.\nYou are not the first one to dislike this aspect of make. To quote Unix Haters' Handbook:\n\nThe problem with Dennis’s Makefile is that when he added the comment line, he inadvertently inserted a space before the tab character at the beginning of line 2. The tab character is a very important part of the syntax of Makefiles. All command lines (the lines beginning with cc in our example) must start with tabs. After he made his change, line 2 didn’t, hence the error.\n“So what?” you ask, “What’s wrong with that?”\nThere is nothing wrong with it, by itself. It’s just that when you consider how other programming tools work in Unix, using tabs as part of the syntax is like one of those pungee stick traps in The Green Berets: the poor kid from Kansas is walking point in front of John Wayne and doesn’t see the trip wire. After all, there are no trip wires to watch out for in Kansas corn fields. WHAM!",
    "tag": "makefile"
  },
  {
    "question": "Create directories using make file",
    "answer": "In my opinion, directories should not be considered targets of your makefile, either in technical or in design sense.  You should create files and if a file creation needs a new directory then quietly create the directory within the rule for the relevant file.\nIf you're targeting a usual or \"patterned\" file, just use make's internal variable $(@D), that means \"the directory the current target resides in\" (cmp. with $@ for the target).  For example,\n$(OUT_O_DIR)/%.o: %.cpp\n        @mkdir -p $(@D)\n        @$(CC) -c $< -o $@\n\ntitle: $(OBJS)\n\nThen, you're effectively doing the same: create directories for all $(OBJS), but you'll do it in a less complicated way.\nThe same policy (files are targets, directories never are) is used in various applications.  For example, git revision control system doesn't store directories.\n\nNote: If you're going to use it, it might be useful to introduce a convenience variable and utilize make's expansion rules.\ndir_guard=@mkdir -p $(@D)\n\n$(OUT_O_DIR)/%.o: %.cpp\n        $(dir_guard)\n        @$(CC) -c $< -o $@\n\n$(OUT_O_DIR_DEBUG)/%.o: %.cpp\n        $(dir_guard)\n        @$(CC) -g -c $< -o $@\n\ntitle: $(OBJS)",
    "tag": "makefile"
  },
  {
    "question": "How to overcome \"'aclocal-1.15' is missing on your system\" warning?",
    "answer": "Before running ./configure try running autoreconf -f -i.  The autoreconf program automatically runs autoheader, aclocal, automake, autopoint and libtoolize as required.\nEdit to add: This is usually caused by checking out code from Git instead of extracting it from a .zip or .tar.gz archive.  In order to trigger rebuilds when files change, Git does not preserve files' timestamps, so the  configure script might appear to be out of date.  As others have mentioned, there are ways to get around this if you don't have a sufficiently recent version of autoreconf.\nAnother edit: This error can also be caused by copying the source folder extracted from an archive with scp to another machine. The timestamps can be updated, suggesting that a rebuild is necessary. To avoid this, copy the archive and extract it in place.",
    "tag": "makefile"
  },
  {
    "question": "Cmake vs make sample codes?",
    "answer": "The following Makefile builds an executable named prog from the sources\nprog1.c, prog2.c, prog3.c and main.c. prog is linked against libmystatlib.a\nand libmydynlib.so which are both also built from source. Additionally, prog uses\nthe library libstuff.a in stuff/lib and its header in stuff/include. The\nMakefile by default builds a release target, but offers also a debug target:\n#Makefile    \nCC = gcc\nCPP = g++\nRANLIB = ar rcs\nRELEASE = -c -O3 \nDEBUG = -c -g -D_DEBUG\nINCDIR = -I./stuff/include\nLIBDIR = -L./stuff/lib -L.\nLIBS = -lstuff -lmystatlib -lmydynlib\nCFLAGS = $(RELEASE)\n\nPROGOBJS = prog1.o prog2.o prog3.o\n\nprog: main.o $(PROGOBJS) mystatlib mydynlib\n    $(CC) main.o $(PROGOBJS) $(LIBDIR) $(LIBS) -o prog \ndebug: CFLAGS=$(DEBUG)\ndebug: prog\n\nmystatlib: mystatlib.o\n    $(RANLIB) libmystatlib.a mystatlib.o\nmydynlib: mydynlib.o\n    $(CPP) -shared mydynlib.o -o libmydynlib.so\n\n%.o: %.c\n    $(CC) $(CFLAGS) $(INCDIR) $< -o $@ \n%.o: %.cpp\n    $(CPP) $(CFLAGS) $(INCDIR) -fPIC  $< -o $@ \n\nHere is a CMakeLists.txtthat does (almost) exactly the same, with some comments to underline the\nsimilarities to the Makefile:\n#CMakeLists.txt     \ncmake_minimum_required(VERSION 2.8)                    # stuff not directly\nproject(example)                                       # related to building\n\ninclude_directories(${CMAKE_SOURCE_DIR}/stuff/include) # -I flags for compiler\nlink_directories(${CMAKE_SOURCE_DIR}/stuff/lib)        # -L flags for linker\n\nset(PROGSRC prog1.c prog2.c prog3.c)                   # define variable \n\nadd_executable(prog main.c ${PROGSRC})                 # define executable target prog, specify sources\ntarget_link_libraries(prog mystatlib mydynlib stuff)   # -l flags for linking prog target\n\nadd_library(mystatlib STATIC mystatlib.c)              # define static library target mystatlib, specify sources\n\nadd_library(mydynlib SHARED mydynlib.cpp)              # define shared library target mydynlib, specify sources\n#extra flags for linking mydynlib\nset_target_properties(mydynlib PROPERTIES POSITION_INDEPENDENT_CODE TRUE) \n#alternatively:\n#set_target_properties(mydynlib PROPERTIES COMPILE_FLAGS \"-fPIC\")\n\nIn this simple example, the most important differences are:\n\nCMake recognizes which compilers to use for which kind of source. Also, it\ninvokes the right sequence of commands for each type of target. Therefore, there\nis no explicit specification of commands like $(CC) ..., $(RANLIB) ... and so on.\n\nAll usual compiler/linker flags dealing with inclusion of header files, libraries, etc.\nare replaced by platform independent / build system independent commands.\n\nDebugging flags are included by either setting the variable CMAKE_BUILD_TYPE to \"Debug\",\nor by passing it to CMake when invoking the program: cmake -DCMAKE_BUILD_TYPE:STRING=Debug.\n\nCMake offers also the platform independent inclusion of the '-fPIC' flag (via\nthe POSITION_INDEPENDENT_CODE property) and many others. Still, more obscure settings can be implemented by hand in CMake just as well as in a Makefile (by using COMPILE_FLAGS\nand similar properties). Of course CMake really starts to shine when third party\nlibraries (like OpenGL) are included in a portable manner.\n\nThe build process has one step if you use a Makefile, namely typing  make at the command line. For CMake, there are two steps: First, you need to setup your build environment (either by typing cmake <source_dir> in your build directory or by running some GUI client). This creates a Makefile or something equivalent, depending on the build system of your choice (e.g. make on Unixes or VC++ or MinGW + Msys on Windows). The build system can be passed to CMake as a parameter; however, CMake makes reasonable default choices depending on your system configuration. Second, you perform the actual build in the selected build system.\n\n\nSources and build instructions are available at https://github.com/rhoelzel/make_cmake.",
    "tag": "makefile"
  },
  {
    "question": "CFLAGS vs CPPFLAGS",
    "answer": "The implicit make rule for compiling a C program is\n%.o:%.c\n    $(CC) $(CPPFLAGS) $(CFLAGS) -c -o $@ $<\n\nwhere the $() syntax expands the variables. As both CPPFLAGS and CFLAGS are used in the compiler call, which you use to define include paths is a matter of personal taste. For instance if foo.c is a file in the current directory\nmake foo.o CPPFLAGS=\"-I/usr/include\"\nmake foo.o CFLAGS=\"-I/usr/include\"\n\nwill both call your compiler in exactly the same way, namely\ngcc -I/usr/include -c -o foo.o foo.c\n\nThe difference between the two comes into play when you have multiple languages which need the same include path, for instance if you have bar.cpp then try\nmake bar.o CPPFLAGS=\"-I/usr/include\"\nmake bar.o CFLAGS=\"-I/usr/include\"\n\nthen the compilations will be\ng++ -I/usr/include -c -o bar.o bar.cpp\ng++ -c -o bar.o bar.cpp\n\nas the C++ implicit rule also uses the CPPFLAGS variable.\nThis difference gives you a good guide for which to use - if you want the flag to be used for all languages put it in CPPFLAGS, if it's for a specific language put it in CFLAGS, CXXFLAGS etc. Examples of the latter type include standard compliance or warning flags - you wouldn't want to pass -std=c99 to your C++ compiler!\nYou might then end up with something like this in your makefile\nCPPFLAGS=-I/usr/include\nCFLAGS=-std=c99\nCXXFLAGS=-Weffc++",
    "tag": "makefile"
  },
  {
    "question": "How I could add dir to $PATH in Makefile?",
    "answer": "Did you try export directive of Make itself (assuming that you use GNU Make)?\nexport PATH := bin:$(PATH)\n\ntest all:\n    x\n\nAlso, there is a bug in you example:\ntest all:\n    PATH=bin:${PATH}\n    @echo $(PATH)\n    x\n\nFirst, the value being echoed is an expansion of PATH variable performed by Make, not the shell. If it prints the expected value then, I guess, you've set PATH variable somewhere earlier in your Makefile, or in a shell that invoked Make. To prevent such behavior you should escape dollars:\ntest all:\n    PATH=bin:$$PATH\n    @echo $$PATH\n    x\n\nSecond, in any case this won't work because Make executes each line of the recipe in a separate shell. This can be changed by writing the recipe in a single line:\ntest all:\n    export PATH=bin:$$PATH; echo $$PATH; x",
    "tag": "makefile"
  },
  {
    "question": "Why always ./configure; make; make install; as 3 separate steps?",
    "answer": "Because each step does different things\nPrepare(setup) environment for building\n./configure\n\nThis script has lots of options that you should change. Like --prefix or --with-dir=/foo. That means every system has a different configuration. Also ./configure checks for missing libraries that should be installed. Anything wrong here causes not to build your application. That's why distros have packages that are installed on different places, because every distro thinks it's better to install certain libraries and files to certain directories. It is said to run ./configure, but in fact you should change it always. \nFor example have a look at the Arch Linux packages site. Here you'll see that any package uses a different configure parameter (assume they are using autotools for the build system). \nBuilding the system\nmake\n\nThis is actually make all by default. And every make has different actions to do. Some do building, some do tests after building, some do checkout from external SCM repositories. Usually you don't have to give any parameters, but again some packages execute them differently. \nInstall to the system\nmake install\n\nThis installs the package in the place specified with configure. If you want you can specify ./configure to point to your home directory. However, lots of configure options are pointing to /usr or /usr/local. That means then you have to use actually sudo make install because only root can copy files to /usr and /usr/local.\n\nNow you see that each step is a pre-requirement for next step. Each step is a preparation to make things work in a problemless flow. Distros use this metaphor to build packages (like RPM, deb, etc.). \nHere you'll see that each step is actually a different state. That's why package managers have different wrappers. Below is an example of a wrapper that lets you build the whole package in one step. But remember that each application has a different wrapper (actually these wrappers have a name like spec, PKGBUILD, etc.):\ndef setup:\n... #use ./configure if autotools is used\n\ndef build:\n... #use make if autotools is used\n\ndef install:\n... #use make all if autotools is used\n\nHere one can use autotools, that means ./configure, make and make install. But another one can use SCons, Python related setup or something different. \nAs you see splitting each state makes things much easier for maintaining and deployment, especially for package maintainers and distros.",
    "tag": "makefile"
  },
  {
    "question": "how to prevent \"directory already exists error\" in a makefile when using mkdir",
    "answer": "On UNIX Just use this:\nmkdir -p $(OBJDIR)\n\nThe -p option to mkdir prevents the error message if the directory exists.",
    "tag": "makefile"
  },
  {
    "question": "GNU Makefile rule generating a few targets from a single source file",
    "answer": "The trick is to use a pattern rule with multiple targets. In that case make will assume that both targets are created by a single invocation of the command.\n\nall: file-a.out file-b.out\nfile-a%out file-b%out: input.in\n    foo-bin input.in file-a$*out file-b$*out\n\nThis difference in interpretation between pattern rules and normal rules doesn't exactly make sense, but it's useful for cases like this, and it is documented in the manual.\nThis trick can be used for any number of output files as long as their names have some common substring for the % to match. (In this case the common substring is \".\")",
    "tag": "makefile"
  },
  {
    "question": "Difference in details between \"make install\" and \"make altinstall\"",
    "answer": "TLDR: altinstall skips creating the python link and the manual pages links, install will hide the system binaries and manual pages.\nLet's take a look at the generated Makefile!\nFirst, the install target:\ninstall:         altinstall bininstall maninstall\n\nIt does everything altinstall does, along with bininstall and maninstall\nHere's bininstall; it just creates the python and other symbolic links.\n# Install the interpreter by creating a symlink chain:\n#  $(PYTHON) -> python2 -> python$(VERSION))\n# Also create equivalent chains for other installed files\nbininstall:     altbininstall\n        -if test -f $(DESTDIR)$(BINDIR)/$(PYTHON) -o -h $(DESTDIR)$(BINDIR)/$(PYTHON); \\\n        then rm -f $(DESTDIR)$(BINDIR)/$(PYTHON); \\\n        else true; \\\n        fi\n        (cd $(DESTDIR)$(BINDIR); $(LN) -s python2$(EXE) $(PYTHON))\n        -rm -f $(DESTDIR)$(BINDIR)/python2$(EXE)\n        (cd $(DESTDIR)$(BINDIR); $(LN) -s python$(VERSION)$(EXE) python2$(EXE))\n        ... (More links created)\n\nAnd here's maninstall, it just creates \"unversioned\" links to the Python manual pages.\n# Install the unversioned manual pages\nmaninstall:     altmaninstall\n        -rm -f $(DESTDIR)$(MANDIR)/man1/python2.1\n        (cd $(DESTDIR)$(MANDIR)/man1; $(LN) -s python$(VERSION).1 python2.1)\n        -rm -f $(DESTDIR)$(MANDIR)/man1/python.1\n        (cd $(DESTDIR)$(MANDIR)/man1; $(LN) -s python2.1 python.1)",
    "tag": "makefile"
  },
  {
    "question": "How is Docker different from a virtual machine?",
    "answer": "Docker originally used LinuX Containers (LXC), but later switched to runC (formerly known as libcontainer), which runs in the same operating system as its host. This allows it to share a lot of the host operating system resources. Also, it uses a layered filesystem (AuFS) and manages networking.\nAuFS is a layered file system, so you can have a read only part and a write part which are merged together. One could have the common parts of the operating system as read only (and shared amongst all of your containers) and then give each container its own mount for writing.\nSo, let's say you have a 1 GB container image; if you wanted to use a full VM, you would need to have 1 GB x number of VMs you want. With Docker and AuFS you can share the bulk of the 1 GB between all the containers and if you have 1000 containers you still might only have a little over 1 GB of space for the containers OS (assuming they are all running the same OS image).\nA full virtualized system gets its own set of resources allocated to it, and does minimal sharing. You get more isolation, but it is much heavier (requires more resources). With Docker you get less isolation, but the containers are lightweight (require fewer resources). So you could easily run thousands of containers on a host, and it won't even blink. Try doing that with Xen, and unless you have a really big host, I don't think it is possible.\nA full virtualized system usually takes minutes to start, whereas Docker/LXC/runC containers take seconds, and often even less than a second.\nThere are pros and cons for each type of virtualized system. If you want full isolation with guaranteed resources, a full VM is the way to go. If you just want to isolate processes from each other and want to run a ton of them on a reasonably sized host, then Docker/LXC/runC seems to be the way to go.\nFor more information, check out this set of blog posts which do a good job of explaining how LXC works.\n\nWhy is deploying software to a docker image (if that's the right term) easier than simply deploying to a consistent production environment?\n\nDeploying a consistent production environment is easier said than done. Even if you use tools like Chef and Puppet, there are always OS updates and other things that change between hosts and environments.\nDocker gives you the ability to snapshot the OS into a shared image, and makes it easy to deploy on other Docker hosts. Locally, dev, qa, prod, etc.: all the same image. Sure you can do this with other tools, but not nearly as easily or fast.\nThis is great for testing; let's say you have thousands of tests that need to connect to a database, and each test needs a pristine copy of the database and will make changes to the data. The classic approach to this is to reset the database after every test either with custom code or with tools like Flyway - this can be very time-consuming and means that tests must be run serially. However, with Docker you could create an image of your database and run up one instance per test, and then run all the tests in parallel since you know they will all be running against the same snapshot of the database. Since the tests are running in parallel and in Docker containers they could run all on the same box at the same time and should finish much faster. Try doing that with a full VM.\nFrom comments...\n\nInteresting! I suppose I'm still confused by the notion of \"snapshot[ting] the OS\". How does one do that without, well, making an image of the OS?\n\nWell, let's see if I can explain. You start with a base image, and then make your changes, and commit those changes using docker, and it creates an image. This image contains only the differences from the base. When you want to run your image, you also need the base, and it layers your image on top of the base using a layered file system: as mentioned above, Docker uses AuFS. AuFS merges the different layers together and you get what you want; you just need to run it. You can keep adding more and more images (layers) and it will continue to only save the diffs. Since Docker typically builds on top of ready-made images from a registry, you rarely have to \"snapshot\" the whole OS yourself.",
    "tag": "docker"
  },
  {
    "question": "From inside of a Docker container, how do I connect to the localhost of the machine?",
    "answer": "If you are using Docker-for-mac or Docker-for-Windows 18.03+, connect to your MySQL service using the host host.docker.internal (instead of the 127.0.0.1 in your connection string).\nIf you are using Docker-for-Linux 20.10.0+, you can also use the host host.docker.internal if you started your Docker container with the --add-host host.docker.internal:host-gateway option, or added the following snippet in your docker-compose.yml file:\nextra_hosts:\n    - \"host.docker.internal:host-gateway\"\n\nOtherwise, read below\n\nTLDR\nUse --network=\"host\" in your docker run command, then 127.0.0.1 in your Docker container will point to your Docker host.\nNote: This mode only works on Docker for Linux, per the documentation.\n\nNote on Docker container networking modes\nDocker offers different networking modes when running containers. Depending on the mode you choose you would connect to your MySQL database running on the Docker host differently.\ndocker run --network=\"bridge\" (default)\nDocker creates a bridge named docker0 by default. Both the Docker host and the Docker containers have an IP address on that bridge.\nOn the Docker host, type sudo ip addr show docker0 you will have an output looking like:\n[vagrant@docker:~] $ sudo ip addr show docker0\n4: docker0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default\n    link/ether 56:84:7a:fe:97:99 brd ff:ff:ff:ff:ff:ff\n    inet 172.17.42.1/16 scope global docker0\n       valid_lft forever preferred_lft forever\n    inet6 fe80::5484:7aff:fefe:9799/64 scope link\n       valid_lft forever preferred_lft forever\n\nSo here my Docker host has the IP address 172.17.42.1 on the docker0 network interface.\nNow start a new container and get a shell on it: docker run --rm -it ubuntu:trusty bash and within the container type ip addr show eth0 to discover how its main network interface is set up:\nroot@e77f6a1b3740:/# ip addr show eth0\n863: eth0: <BROADCAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000\n    link/ether 66:32:13:f0:f1:e3 brd ff:ff:ff:ff:ff:ff\n    inet 172.17.1.192/16 scope global eth0\n       valid_lft forever preferred_lft forever\n    inet6 fe80::6432:13ff:fef0:f1e3/64 scope link\n       valid_lft forever preferred_lft forever\n\nHere my container has the IP address 172.17.1.192. Now look at the routing table:\nroot@e77f6a1b3740:/# route\nKernel IP routing table\nDestination     Gateway         Genmask         Flags Metric Ref    Use Iface\ndefault         172.17.42.1     0.0.0.0         UG    0      0        0 eth0\n172.17.0.0      *               255.255.0.0     U     0      0        0 eth0\n\nSo the IP address of the Docker host 172.17.42.1 is set as the default route and is accessible from your container.\nroot@e77f6a1b3740:/# ping 172.17.42.1\nPING 172.17.42.1 (172.17.42.1) 56(84) bytes of data.\n64 bytes from 172.17.42.1: icmp_seq=1 ttl=64 time=0.070 ms\n64 bytes from 172.17.42.1: icmp_seq=2 ttl=64 time=0.201 ms\n64 bytes from 172.17.42.1: icmp_seq=3 ttl=64 time=0.116 ms\n\ndocker run --network=\"host\"\nAlternatively you can run a Docker container with network settings set to host. Such a container will share the network stack with the Docker host and from the container point of view, localhost (or 127.0.0.1) will refer to the Docker host.\nBe aware that any port opened in your Docker container would be opened on the Docker host. And this without requiring the -p or -P docker run option.\nIP configuration on my Docker host:\n[vagrant@docker:~] $ ip addr show eth0\n2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000\n    link/ether 08:00:27:98:dc:aa brd ff:ff:ff:ff:ff:ff\n    inet 10.0.2.15/24 brd 10.0.2.255 scope global eth0\n       valid_lft forever preferred_lft forever\n    inet6 fe80::a00:27ff:fe98:dcaa/64 scope link\n       valid_lft forever preferred_lft forever\n\nAnd from a Docker container in host mode:\n[vagrant@docker:~] $ docker run --rm -it --network=host ubuntu:trusty ip addr show eth0\n2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000\n    link/ether 08:00:27:98:dc:aa brd ff:ff:ff:ff:ff:ff\n    inet 10.0.2.15/24 brd 10.0.2.255 scope global eth0\n       valid_lft forever preferred_lft forever\n    inet6 fe80::a00:27ff:fe98:dcaa/64 scope link\n       valid_lft forever preferred_lft forever\n\nAs you can see, both the Docker host and Docker container share the exact same network interface and as such have the same IP address.\n\nConnecting to MySQL from containers\nBridge mode\nTo access MySQL running on the Docker host from containers in bridge mode, you need to make sure the MySQL service is listening for connections on the 172.17.42.1 IP address.\nTo do so, make sure you have either bind-address = 172.17.42.1 or bind-address = 0.0.0.0 in your MySQL configuration file (my.cnf).\nIf you need to set an environment variable with the IP address of the gateway, you can run the following code in a container:\nexport DOCKER_HOST_IP=$(route -n | awk '/UG[ \\t]/{print $2}')\n\nThen in your application, use the DOCKER_HOST_IP environment variable to open the connection to MySQL.\nNote: if you use bind-address = 0.0.0.0, your MySQL server will listen for connections on all network interfaces. That means your MySQL server could be reached from the Internet; make sure to set up firewall rules accordingly.\nNote 2: if you use bind-address = 172.17.42.1 your MySQL server won't listen for connections made to 127.0.0.1. Processes running on the Docker host that would want to connect to MySQL would have to use the 172.17.42.1 IP address.\nHost mode\nTo access MySQL running on the docker host from containers in host mode, you can keep bind-address = 127.0.0.1 in your MySQL configuration and connect to 127.0.0.1 from your containers:\n[vagrant@docker:~] $ docker run --rm -it --network=host mysql mysql -h 127.0.0.1 -uroot -p\nEnter password:\nWelcome to the MySQL monitor.  Commands end with ; or \\g.\nYour MySQL connection id is 36\nServer version: 5.5.41-0ubuntu0.14.04.1 (Ubuntu)\n\nCopyright (c) 2000, 2014, Oracle and/or its affiliates. All rights reserved.\n\nOracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners.\n\nType 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.\n\nmysql>\n\nNote: Do use mysql -h 127.0.0.1 and not mysql -h localhost; otherwise the MySQL client would try to connect using a Unix socket.",
    "tag": "docker"
  },
  {
    "question": "What is the difference between the 'COPY' and 'ADD' commands in a Dockerfile?",
    "answer": "You should check the ADD and COPY documentation for a more detailed description of their behaviors, but in a nutshell, the major difference is that ADD can do more than COPY:\n\nADD allows <src> to be a URL\nReferring to comments below, the ADD documentation states that:\n\n\nIf  is a local tar archive in a recognized compression format (identity, gzip, bzip2 or xz) then it is unpacked as a directory. Resources from remote URLs are not decompressed.\n\nNote that the Best practices for writing Dockerfiles suggests using COPY where the magic of ADD is not required. Otherwise, you (since you had to look up this answer) are likely to get surprised someday when you mean to copy keep_this_archive_intact.tar.gz into your container, but instead, you spray the contents onto your filesystem.",
    "tag": "docker"
  },
  {
    "question": "Copying files from Docker container to host",
    "answer": "In order to copy a file from a container to the host, you can use the command\ndocker cp <containerId>:/file/path/within/container /host/path/target\n\nHere's an example:\n$ sudo docker cp goofy_roentgen:/out_read.jpg .\n\nHere goofy_roentgen is the container name I got from the following command:\n$ sudo docker ps\n\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS                                            NAMES\n1b4ad9311e93        bamos/openface      \"/bin/bash\"         33 minutes ago      Up 33 minutes       0.0.0.0:8000->8000/tcp, 0.0.0.0:9000->9000/tcp   goofy_roentgen\n\nYou can also use (part of) the Container ID. The following command is equivalent to the first\n$ sudo docker cp 1b4a:/out_read.jpg .",
    "tag": "docker"
  },
  {
    "question": "What is the difference between CMD and ENTRYPOINT in a Dockerfile?",
    "answer": "Docker has a default entrypoint which is /bin/sh -c but does not have a default command.\nWhen you run docker like this:\ndocker run -i -t ubuntu bash\nthe entrypoint is the default /bin/sh -c, the image is ubuntu and the command is bash.\nThe command is run via the entrypoint. i.e., the actual thing that gets executed is /bin/sh -c bash. This allowed Docker to implement RUN quickly by relying on the shell's parser.\nLater on, people asked to be able to customize this, so ENTRYPOINT and --entrypoint were introduced.\nEverything after the image name, ubuntu in the example above, is the command and is passed to the entrypoint. When using the CMD instruction, it is exactly as if you were executing\ndocker run -i -t ubuntu <cmd>\nThe parameter of the entrypoint is <cmd>.\nYou will also get the same result if you instead type this command docker run -i -t ubuntu: a bash shell will start in the container because in the ubuntu Dockerfile a default CMD is specified:\nCMD [\"bash\"].\nAs everything is passed to the entrypoint, you can have a very nice behavior from your images. @Jiri example is good, it shows how to use an image as a \"binary\". When using [\"/bin/cat\"] as entrypoint and then doing docker run img /etc/passwd, you get it, /etc/passwd is the command and is passed to the entrypoint so the end result execution is simply /bin/cat /etc/passwd.\nAnother example would be to have any cli as entrypoint. For instance, if you have a redis image, instead of running docker run redisimg redis -H something -u toto get key, you can simply have ENTRYPOINT [\"redis\", \"-H\", \"something\", \"-u\", \"toto\"] and then run like this for the same result: docker run redisimg get key.",
    "tag": "docker"
  },
  {
    "question": "How to copy files from host to Docker container?",
    "answer": "The cp command can be used to copy files.\nOne specific file can be copied TO the container like:\ndocker cp foo.txt container_id:/foo.txt\n\nOne specific file can be copied FROM the container like:\ndocker cp container_id:/foo.txt foo.txt\n\nFor emphasis, container_id is a container ID, not an image ID. (Use docker ps to view listing which includes container_ids.)\nMultiple files contained by the folder src can be copied into the target folder using:\ndocker cp src/. container_id:/target\ndocker cp container_id:/src/. target\n\nReference: Docker CLI docs for cp\nIn Docker versions prior to 1.8 it was only possible to copy files from a container to the host. Not from the host to a container.",
    "tag": "docker"
  },
  {
    "question": "How do I get into a Docker container's shell?",
    "answer": "docker attach will let you connect to your Docker container, but this isn't really the same thing as ssh.  If your container is running a webserver, for example, docker attach will probably connect you to the stdout of the web server process.  It won't necessarily give you a shell.\nThe docker exec command is probably what you are looking for; this will let you run arbitrary commands inside an existing container.  For example, to run bash inside a container:\ndocker exec -it <mycontainer> sh\n\nOf course, whatever command you are running must exist in the container filesystem; if your container doesn't have sh, this will fail with something like:\nOCI runtime exec failed: exec failed: unable to start container process:\nexec: \"sh\": executable file not found in $PATH: unknown\n\n[If your container doesn't have sh -- which is a common case for minimal images -- you may need to investigate other ways to explore the container filesystem.]\nIn the above command <mycontainer> is the name or ID of the target container.  It doesn't matter whether or not you're using docker compose; just run docker ps and use either the ID (a hexadecimal string displayed in the first column) or the name (displayed in the final column).  E.g., given:\n$ docker ps\nd2d4a89aaee9        larsks/mini-httpd   \"mini_httpd -d /cont   7 days ago          Up 7 days                               web                 \n\nI can run:\n$ docker exec -it web ip addr\n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN \n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host \n       valid_lft forever preferred_lft forever\n18: eth0: <BROADCAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP \n    link/ether 02:42:ac:11:00:03 brd ff:ff:ff:ff:ff:ff\n    inet 172.17.0.3/16 scope global eth0\n       valid_lft forever preferred_lft forever\n    inet6 fe80::42:acff:fe11:3/64 scope link \n       valid_lft forever preferred_lft forever\n\nI could accomplish the same thing by running:\n$ docker exec -it d2d4a89aaee9 ip addr\n\nSimilarly, I could start a shell in the container;\n$ docker exec -it web sh\n/ # echo This is inside the container.\nThis is inside the container.\n/ # exit\n$\n\n\nIn commands shown in this answer, the -i and -t options (combined as -it) are necessary to get an interactive shell:\n\n-i keeps stdin connected; if you don't specify -i, the shell will simply exit.\n\n-t allocates a tty device; if you don't specify -t, you won't have a very pleasant interactive experience (there will be no shell prompt or job control, for example).\n\n\n\nIf you're specifically using docker compose, there is a convenience docker compose exec command that works very much like the docker exec command, except:\n\nIt defaults to the behavior of -i and -t\nIt allows you to refer to containers by their service name in your compose.yaml file.\n\nFor example, if you have a compose.yaml like this:\nservices:\n  web:\n    image: docker.io/alpinelinux/darkhttpd\n\nThen you can run:\ndocker compose exec web sh\n\nThe equivalent docker exec command would be something like:\ndocker exec -it myproject-web-1 sh",
    "tag": "docker"
  },
  {
    "question": "How to copy Docker images from one host to another without using a repository",
    "answer": "You will need to save the Docker image as a tar file:\ndocker save -o <path for generated tar file> <image name>\n\nThen copy your image to a new system with regular file transfer tools such as cp, scp, or rsync (preferred for big files). After that you will have to load the image into Docker:\ndocker load -i <path to image tar file>\n\nYou should add filename (not just directory) with -o, for example:\ndocker save -o c:/myfile.tar centos:16\n\nyour image syntax may need the repository prefix (:latest tag is default)\ndocker save -o C:\\path\\to\\file.tar repository/imagename\n\n\nPS: You may need to sudo all commands.",
    "tag": "docker"
  },
  {
    "question": "How to get a Docker container's IP address from the host",
    "answer": "This solution only works if the container is connected with a single network. The --format option of inspect comes to the rescue.\nModern Docker client syntax is:\ndocker inspect \\\n  -f '{{range.NetworkSettings.Networks}}{{.IPAddress}}{{end}}' container_name_or_id\n\nOld Docker client syntax is:\ndocker inspect \\\n  --format '{{ .NetworkSettings.IPAddress }}' container_name_or_id\n\nThese commands will return the Docker container's IP address.\nAs mentioned in the comments: if you are on Windows, use double quotes \" instead of single quotes ' around the curly braces.",
    "tag": "docker"
  },
  {
    "question": "Should I use Vagrant or Docker for creating an isolated environment?",
    "answer": "Disclaimer: I wrote Vagrant! But because I wrote Vagrant, I spend most of my time living in the DevOps world which includes software like Docker. I work with a lot of companies using Vagrant and many use Docker, and I see how the two interplay.\nBefore I talk too much, a direct answer: in your specific scenario (yourself working alone, working on Linux, using Docker in production), you can stick with Docker alone and simplify things. In many other scenarios (I discuss further), it isn't so easy.\nIt isn't correct to directly compare Vagrant to Docker. In some scenarios, they do overlap, and in the vast majority, they don't. Actually, the more apt comparison would be Vagrant versus something like Boot2Docker (minimal OS that can run Docker). Vagrant is a level above Docker in terms of abstractions, so it isn't a fair comparison in most cases.\nVagrant launches things to run apps/services for the purpose of development. This can be on VirtualBox, VMware. It can be remote like AWS, OpenStack. Within those, if you use containers, Vagrant doesn't care, and embraces that: it can automatically install, pull down, build, and run Docker containers, for example. With Vagrant 1.6, Vagrant has docker-based development environments, and supports using Docker with the same workflow as Vagrant across Linux, Mac, and Windows. Vagrant doesn't try to replace Docker here, it embraces Docker practices.\nDocker specifically runs Docker containers. If you're comparing directly to Vagrant: it is specifically a more specific (can only run Docker containers), less flexible (requires Linux or Linux host somewhere) solution. Of course if you're talking about production or CI, there is no comparison to Vagrant! Vagrant doesn't live in these environments, and so Docker should be used. \nIf your organization runs only Docker containers for all their projects and only has developers running on Linux, then okay, Docker could definitely work for you! \nOtherwise, I don't see a benefit to attempting to use Docker alone, since you lose a lot of what Vagrant has to offer, which have real business/productivity benefits:\n\nVagrant can launch VirtualBox, VMware, AWS, OpenStack, etc. machines. It doesn't matter what you need, Vagrant can launch it. If you are using Docker, Vagrant can install Docker on any of these so you can use them for that purpose.\nVagrant is a single workflow for all your projects. Or to put another way, it is just one thing people have to learn to run a project whether it is in a Docker container or not. If, for example, in the future, a competitor arises to compete directly with Docker, Vagrant will be able to run that too. \nVagrant works on Windows (back to XP), Mac (back to 10.5), and Linux (back to kernel 2.6). In all three cases, the workflow is the same. If you use Docker, Vagrant can launch a machine (VM or remote) that can run Docker on all three of these systems.\nVagrant knows how to configure some advanced or non-trivial things like networking and syncing folders. For example: Vagrant knows how to attach a static IP to a machine or forward ports, and the configuration is the same no matter what system you use (VirtualBox, VMware, etc.) For synced folders, Vagrant provides multiple mechanisms to get your local files over to the remote machine (VirtualBox shared folders, NFS, rsync, Samba [plugin], etc.). If you're using Docker, even Docker with a VM without Vagrant, you would have to manually do this or they would have to reinvent Vagrant in this case.\nVagrant 1.6 has first-class support for docker-based development environments. This will not launch a virtual machine on Linux, and will automatically launch a virtual machine on Mac and Windows. The end result is that working with Docker is uniform across all platforms, while Vagrant still handles the tedious details of things such as networking, synced folders, etc.\n\nTo address specific counter arguments that I've heard in favor of using Docker instead of Vagrant:\n\n\"It is less moving parts\" - Yes, it can be, if you use Docker exclusively for every project. Even then, it is sacrificing flexibility for Docker lock-in. If you ever decide to not use Docker for any project, past, present, or future, then you'll have more moving parts. If you had used Vagrant, you have that one moving part that supports the rest.\n\"It is faster!\" - Once you have the host that can run Linux containers, Docker is definitely faster at running a container than any virtual machine would be to launch. But launching a virtual machine (or remote machine) is a one-time cost. Over the course of the day, most Vagrant users never actually destroy their VM. It is a strange optimization for development environments. In production, where Docker really shines, I understand the need to quickly spin up/down containers.\n\nI hope now its clear to see that it is very difficult, and I believe not correct, to compare Docker to Vagrant. For dev environments, Vagrant is more abstract, more general. Docker (and the various ways you can make it behave like Vagrant) is a specific use case of Vagrant, ignoring everything else Vagrant has to offer. \nIn conclusion: in highly specific use cases, Docker is certainly a possible replacement for Vagrant. In most use cases, it is not. Vagrant doesn't hinder your usage of Docker; it actually does what it can to make that experience smoother. If you find this isn't true, I'm happy to take suggestions to improve things, since a goal of Vagrant is to work equally well with any system.\nHope this clears things up!",
    "tag": "docker"
  },
  {
    "question": "How to force Docker for a clean build of an image",
    "answer": "There's a --no-cache option:\ndocker build --no-cache -t u12_core -f u12_core .\n\nIn older versions of Docker you needed to pass --no-cache=true, but this is no longer the case.",
    "tag": "docker"
  },
  {
    "question": "How do I pass environment variables to Docker containers?",
    "answer": "You can pass environment variables to your containers with the -e (alias --env) flag.\ndocker run -e xx=yy\nAn example from a startup script:\nsudo docker run -d -t -i -e REDIS_NAMESPACE='staging' \\ \n-e POSTGRES_ENV_POSTGRES_PASSWORD='foo' \\\n-e POSTGRES_ENV_POSTGRES_USER='bar' \\\n-e POSTGRES_ENV_DB_NAME='mysite_staging' \\\n-e POSTGRES_PORT_5432_TCP_ADDR='docker-db-1.hidden.us-east-1.rds.amazonaws.com' \\\n-e SITE_URL='staging.mysite.com' \\\n-p 80:80 \\\n--link redis:redis \\  \n--name container_name dockerhub_id/image_name\n\nOr, if you don't want to have the value on the command-line where it will be displayed by ps, etc., -e can pull in the value from the current environment if you just give it without the =:\nsudo PASSWORD='foo' docker run  [...] -e PASSWORD [...]\n\nIf you have many environment variables and especially if they're meant to be secret, you can use an env-file:\n$ docker run --env-file ./env.list ubuntu bash\n\n\nThe --env-file flag takes a filename as an argument and expects each line to be in the VAR=VAL format, mimicking the argument passed to --env. Comment lines need only be prefixed with #",
    "tag": "docker"
  },
  {
    "question": "How to remove old Docker containers",
    "answer": "Since Docker 1.13.x you can use Docker container prune:\ndocker container prune\n\nThis will remove all stopped containers and should work on all platforms the same way.\nThere is also a Docker system prune:\ndocker system prune\n\nwhich will clean up all unused containers, networks, images (both dangling and unreferenced), and optionally, volumes, in one command.\n\nFor older Docker versions, you can string Docker commands together with other Unix commands to get what you need. Here is an example on how to clean up old containers that are weeks old:\n$ docker ps --filter \"status=exited\" | grep 'weeks ago' | awk '{print $1}' | xargs --no-run-if-empty docker rm\n\nTo give credit, where it is due, this example is from https://twitter.com/jpetazzo/status/347431091415703552.",
    "tag": "docker"
  },
  {
    "question": "How to fix Docker: Permission denied",
    "answer": "If you want to run Docker as a non-root user, then you need to add your user to the docker group.\n\nCreate the docker group if it does not exist:\n\n$ sudo groupadd docker\n\n\nAdd your user to the docker group:\n\n$ sudo usermod -aG docker $USER\n\n\nLog in to the new docker group (to avoid having to log out and log in again; but if not enough, try to reboot):\n\n$ newgrp docker\n\n\nCheck if Docker can be run without root:\n\n$ docker run hello-world\n\nReboot if you still get an error:\n$ reboot\n\n\nFrom the official Docker documentation \"Manage Docker as a non-root user\":\n⚠️ Warning\n\nThe docker group grants root-level privileges to the user. For details on how this impacts security in your system, see Docker Daemon Attack Surface.",
    "tag": "docker"
  },
  {
    "question": "What is the difference between a Docker image and a container?",
    "answer": "An instance of an image is called a container. You have an image, which is a set of layers as you describe. If you start this image, you have a running container of this image. You can have many running containers of the same image.\nYou can see all your images with docker images whereas you can see your running containers with docker ps (and you can see all containers with docker ps -a).\nSo a running instance of an image is a container.",
    "tag": "docker"
  },
  {
    "question": "How to remove old and unused Docker images",
    "answer": "(see below for original answer)\n\nUpdate Sept. 2016: Docker 1.13: PR 26108 and commit 86de7c0 introduce a few new commands to help facilitate visualizing how much space the docker daemon data is taking on disk and allowing for easily cleaning up \"unneeded\" excess.\ndocker system prune will delete all dangling data (containers, networks, and images). You can remove all unused volumes with the --volumes option and remove all unused images (not just dangling) with the -a option.\nYou also have:\n\ndocker container prune\ndocker image prune\ndocker network prune\ndocker volume prune\n\nFor unused images, use docker image prune -a (for removing dangling and ununsed images).\nWarning: 'unused' means \"images not referenced by any container\": be careful before using -a.\nAs illustrated in A L's answer, docker system prune --all will remove all unused images not just dangling ones... which can be a bit too much.\nCombining docker xxx prune with the --filter option can be a great way to limit the pruning (docker SDK API 1.28 minimum, so docker 17.04+)\n\nThe currently supported filters are:\n\n\nuntil (<timestamp>) - only remove containers, images, and networks created before given timestamp\nlabel (label=<key>, label=<key>=<value>, label!=<key>, or label!=<key>=<value>) - only remove containers, images, networks, and volumes with (or without, in case label!=... is used) the specified labels.\n\nSee \"Prune images\" for an example.\n\nWarning: there is no \"preview\" or \"--dry-run\" option for those docker xxx prune commands.\nThis is requested with moby/moby issue 30623 since 2017, but seems tricky to be implemented (Aug. 2022)\n\nHaving a more representative overview of what will be pruned will be quite complicated, for various reasons;\n\nrace conditions (can be resolved by documenting the limitations);\nA container/image/volume/network may not be in use at the time that \"dry run\" is used, but may be in use the moment the actual prune is executed (or vice-versa), so dry run will always be an \"approximation\" of what will be pruned.\nthe more difficult part is due to how objects (containers, images, networks etc.) depend on each other.\nFor example, an image can be deleted if it no longer has references to it (no more tags, no more containers using it); this is the reason that docker system prune deletes objects in a specific order (first remove all unused containers, then remove unused images).\nIn order to replicate the same flow for \"dry-run\", it will be needed to temporarily construct representation of all objects and where they're referenced based on that (basically; duplicate all reference-counters, and then remove references from that \"shadow\" representation).\nFinally; with the work being done on integrating the containerd snapshotter (image and layer store), things may change more;\nFor example, images can now be multi-arch, and (to be discussed), \"pruning\" could remove unused variants (architectures) from an image to clean up space, which brings another dimension to calculating \"what can be removed\".\n\n\n\nOriginal answer (Sep. 2016)\nI usually do:\ndocker rmi $(docker images --filter \"dangling=true\" -q --no-trunc)\n\nI have an [alias for removing those dangling images: drmi]13\n\nThe dangling=true filter finds unused images\n\nThat way, any intermediate image no longer referenced by a labelled image is removed.\nI do the same first for exited processes (containers)\nalias drmae='docker rm $(docker ps -qa --no-trunc --filter \"status=exited\")'\n\nAs haridsv points out in the comments:\n\nTechnically, you should first clean up containers before cleaning up images, as this will catch more dangling images and less errors.\n\n\nJess Frazelle (jfrazelle) has the bashrc function:\ndcleanup(){\n    docker rm -v $(docker ps --filter status=exited -q 2>/dev/null) 2>/dev/null\n    docker rmi $(docker images --filter dangling=true -q 2>/dev/null) 2>/dev/null\n}\n\n\nTo remove old images, and not just \"unreferenced-dangling\" images, you can consider docker-gc:\n\n\nA simple Docker container and image garbage collection script.\n\nContainers that exited more than an hour ago are removed.\nImages that don't belong to any remaining container after that are removed.",
    "tag": "docker"
  },
  {
    "question": "How can I delete all local Docker images?",
    "answer": "Unix\nTo delete all containers including its volumes use,\ndocker rm -vf $(docker ps -aq)\n\nTo delete all the images,\ndocker rmi -f $(docker images -aq)\n\nRemember, you should remove all the containers before removing all the images from which those containers were created.\nWindows - Powershell\ndocker images -a -q | % { docker image rm $_ -f }\n\nWindows - cmd.exe\nfor /F %i in ('docker images -a -q') do docker rmi -f %i",
    "tag": "docker"
  },
  {
    "question": "Exploring Docker container's file system",
    "answer": "Here are a couple different methods...\nA) Use docker exec (easiest)\nDocker version 1.3 or newer supports the command exec that behave similar to nsenter. This command can run new process in already running container (container must have PID 1 process running already). You can run /bin/bash to explore container state:\ndocker exec -t -i mycontainer /bin/bash\n\nsee Docker command line documentation\nB) Use Snapshotting\nYou can evaluate container filesystem this way:\n# find ID of your running container:\ndocker ps\n\n# create image (snapshot) from container filesystem\ndocker commit 12345678904b5 mysnapshot\n\n# explore this filesystem using bash (for example)\ndocker run -t -i mysnapshot /bin/bash\n\nThis way, you can evaluate filesystem of the running container in the precise time moment. Container is still running, no future changes are included.\nYou can later delete snapshot using (filesystem of the running container is not affected!):\ndocker rmi mysnapshot\n\nC) Use ssh\nIf you need continuous access, you can install sshd to your container and run the sshd daemon:\ndocker run -d -p 22 mysnapshot /usr/sbin/sshd -D\n \n# you need to find out which port to connect:\ndocker ps\n\nThis way, you can run your app using ssh (connect and execute what you want).\nD) Use nsenter\nUse nsenter, see Why you don't need to run SSHd in your Docker containers\n\nThe short version is: with nsenter, you can get a shell into an\nexisting container, even if that container doesn’t run SSH or any kind\nof special-purpose daemon",
    "tag": "docker"
  },
  {
    "question": "How to deal with persistent storage (e.g. databases) in Docker",
    "answer": "Docker 1.9.0 and above\nUse volume API\ndocker volume create --name hello\ndocker run -d -v hello:/container/path/for/volume container_image my_command\n\nThis means that the data-only container pattern must be abandoned in favour of the new volumes.\nActually the volume API is only a better way to achieve what was the data-container pattern.\nIf you create a container with a -v volume_name:/container/fs/path Docker will automatically create a named volume for you that can:\n\nBe listed through the docker volume ls\nBe identified through the docker volume inspect volume_name\nBacked up as a normal directory\nBacked up as before through a --volumes-from connection\n\nThe new volume API adds a useful command that lets you identify dangling volumes:\ndocker volume ls -f dangling=true\n\nAnd then remove it through its name:\ndocker volume rm <volume name>\n\nAs @mpugach underlines in the comments, you can get rid of all the dangling volumes with a nice one-liner:\ndocker volume rm $(docker volume ls -f dangling=true -q)\n# Or using 1.13.x\ndocker volume prune\n\nDocker 1.8.x and below\nThe approach that seems to work best for production is to use a data only container.\nThe data only container is run on a barebones image and actually does nothing except exposing a data volume.\nThen you can run any other container to have access to the data container volumes:\ndocker run --volumes-from data-container some-other-container command-to-execute\n\n\nHere you can get a good picture of how to arrange the different containers.\nHere there is a good insight on how volumes work.\n\nIn this blog post there is a good description of the so-called container as volume pattern which clarifies the main point of having data only containers.\nDocker documentation has now the DEFINITIVE description of the container as volume/s pattern.\nFollowing is the backup/restore procedure for Docker 1.8.x and below.\nBACKUP:\nsudo docker run --rm --volumes-from DATA -v $(pwd):/backup busybox tar cvf /backup/backup.tar /data\n\n\n--rm: remove the container when it exits\n--volumes-from DATA: attach to the volumes shared by the DATA container\n-v $(pwd):/backup: bind mount the current directory into the container; to write the tar file to\nbusybox: a small simpler image - good for quick maintenance\ntar cvf /backup/backup.tar /data: creates an uncompressed tar file of all the files in the /data directory\n\nRESTORE:\n# Create a new data container\n$ sudo docker run -v /data -name DATA2 busybox true\n# untar the backup files into the new container᾿s data volume\n$ sudo docker run --rm --volumes-from DATA2 -v $(pwd):/backup busybox tar xvf /backup/backup.tar\ndata/\ndata/sven.txt\n# Compare to the original container\n$ sudo docker run --rm --volumes-from DATA -v `pwd`:/backup busybox ls /data\nsven.txt\n\nHere is a nice article from the excellent Brian Goff explaining why it is good to use the same image for a container and a data container.",
    "tag": "docker"
  },
  {
    "question": "How to deal with persistent storage (e.g. databases) in Docker",
    "answer": "Docker 1.9.0 and above\nUse volume API\ndocker volume create --name hello\ndocker run -d -v hello:/container/path/for/volume container_image my_command\n\nThis means that the data-only container pattern must be abandoned in favour of the new volumes.\nActually the volume API is only a better way to achieve what was the data-container pattern.\nIf you create a container with a -v volume_name:/container/fs/path Docker will automatically create a named volume for you that can:\n\nBe listed through the docker volume ls\nBe identified through the docker volume inspect volume_name\nBacked up as a normal directory\nBacked up as before through a --volumes-from connection\n\nThe new volume API adds a useful command that lets you identify dangling volumes:\ndocker volume ls -f dangling=true\n\nAnd then remove it through its name:\ndocker volume rm <volume name>\n\nAs @mpugach underlines in the comments, you can get rid of all the dangling volumes with a nice one-liner:\ndocker volume rm $(docker volume ls -f dangling=true -q)\n# Or using 1.13.x\ndocker volume prune\n\nDocker 1.8.x and below\nThe approach that seems to work best for production is to use a data only container.\nThe data only container is run on a barebones image and actually does nothing except exposing a data volume.\nThen you can run any other container to have access to the data container volumes:\ndocker run --volumes-from data-container some-other-container command-to-execute\n\n\nHere you can get a good picture of how to arrange the different containers.\nHere there is a good insight on how volumes work.\n\nIn this blog post there is a good description of the so-called container as volume pattern which clarifies the main point of having data only containers.\nDocker documentation has now the DEFINITIVE description of the container as volume/s pattern.\nFollowing is the backup/restore procedure for Docker 1.8.x and below.\nBACKUP:\nsudo docker run --rm --volumes-from DATA -v $(pwd):/backup busybox tar cvf /backup/backup.tar /data\n\n\n--rm: remove the container when it exits\n--volumes-from DATA: attach to the volumes shared by the DATA container\n-v $(pwd):/backup: bind mount the current directory into the container; to write the tar file to\nbusybox: a small simpler image - good for quick maintenance\ntar cvf /backup/backup.tar /data: creates an uncompressed tar file of all the files in the /data directory\n\nRESTORE:\n# Create a new data container\n$ sudo docker run -v /data -name DATA2 busybox true\n# untar the backup files into the new container᾿s data volume\n$ sudo docker run --rm --volumes-from DATA2 -v $(pwd):/backup busybox tar xvf /backup/backup.tar\ndata/\ndata/sven.txt\n# Compare to the original container\n$ sudo docker run --rm --volumes-from DATA -v `pwd`:/backup busybox ls /data\nsven.txt\n\nHere is a nice article from the excellent Brian Goff explaining why it is good to use the same image for a container and a data container.",
    "tag": "docker"
  },
  {
    "question": "Where are Docker images stored on the host machine?",
    "answer": "The contents of the /var/lib/docker directory vary depending on the driver Docker is using for storage. \nBy default this will be aufs but can fall back to overlay, overlay2, btrfs, devicemapper or zfs depending on your kernel support. In most places this will be aufs but the RedHats went with devicemapper.\nYou can manually set the storage driver with the -s or --storage-driver= option to the Docker daemon. \n\n/var/lib/docker/{driver-name} will contain the driver specific storage for contents of the images. \n/var/lib/docker/graph/<id> now only contains metadata about the image, in the json and layersize files.\n\nIn the case of aufs:\n\n/var/lib/docker/aufs/diff/<id> has the file contents of the images.\n/var/lib/docker/repositories-aufs is a JSON file containing local image information. This can be viewed with the command docker images.\n\nIn the case of devicemapper:\n\n/var/lib/docker/devicemapper/devicemapper/data stores the images\n/var/lib/docker/devicemapper/devicemapper/metadata the metadata\nNote these files are thin provisioned \"sparse\" files so aren't as big as they seem.",
    "tag": "docker"
  },
  {
    "question": "What is the difference between ports and expose in docker-compose?",
    "answer": "According to the docker-compose reference,\nPorts is defined as:\n\nExpose ports. Either specify both ports (HOST:CONTAINER), or just the container port (a random host port will be chosen).\n\n\nPorts mentioned in docker-compose.yml will be shared among different services started by the docker-compose.\nPorts will be exposed to the host machine to a random port or a given port.\n\nMy docker-compose.yml looks like:\nmysql:\n  image: mysql:5.7\n  ports:\n    - \"3306\"\n\nIf I do docker-compose ps, it will look like:\n  Name                     Command               State            Ports\n-------------------------------------------------------------------------------------\n  mysql_1       docker-entrypoint.sh mysqld      Up      0.0.0.0:32769->3306/tcp\n\nExpose is defined as:\n\nExpose ports without publishing them to the host machine - they’ll only be accessible to linked services. Only the internal port can be specified.\n\nPorts are not exposed to host machines, only exposed to other services.\nmysql:\n  image: mysql:5.7\n  expose:\n    - \"3306\"\n\nIf I do docker-compose ps, it will look like:\n  Name                  Command             State    Ports\n---------------------------------------------------------------\n mysql_1      docker-entrypoint.sh mysqld   Up      3306/tcp\n\nEdit\nIn recent versions of Dockerfile, EXPOSE typically doesn't have any operational impact anymore, it is just informative. (see also)",
    "tag": "docker"
  },
  {
    "question": "Docker Compose - How to execute multiple commands?",
    "answer": "Figured it out, use bash -c.\nExample:\ncommand: bash -c \"python manage.py migrate && python manage.py runserver 0.0.0.0:8000\"\n\nSame example in multilines:\ncommand: >\n    bash -c \"python manage.py migrate\n    && python manage.py runserver 0.0.0.0:8000\"\n\nOr:\ncommand: bash -c \"\n    python manage.py migrate\n    && python manage.py runserver 0.0.0.0:8000\n  \"",
    "tag": "docker"
  },
  {
    "question": "Error \"The input device is not a TTY\"",
    "answer": "Remove the -it from your cli to make it non interactive and remove the TTY. If you don't need either, e.g. running your command inside of a Jenkins or cron script, you should do this.\nOr you can change it to -i if you have input piped into the docker command that doesn't come from a TTY. If you have something like xyz | docker ... or docker ... <input in your command line, do this.\nOr you can change it to -t if you want TTY support but don't have it available on the input device. Do this for apps that check for a TTY to enable color formatting of the output in your logs, or for when you later attach to the container with a proper terminal.\nOr if you need an interactive terminal and aren't running in a terminal on Linux or MacOS, use a different command line interface. PowerShell is reported to include this support on Windows.\n\nWhat is a TTY? It's a terminal interface that supports escape sequences, moving the cursor around, etc, that comes from the old days of dumb terminals attached to mainframes. Today it is provided by the Linux command terminals and ssh interfaces. See the wikipedia article for more details.\nTo see the difference of running a container with and without a TTY, run a container without one: docker run --rm -i ubuntu bash. From inside that container, install vim with apt-get update; apt-get install vim. Note the lack of a prompt. When running vim against a file, try to move the cursor around within the file.",
    "tag": "docker"
  },
  {
    "question": "Docker how to change repository name or rename image?",
    "answer": "docker image tag server:latest myname/server:latest\n\nor\ndocker image tag d583c3ac45fd myname/server:latest\n\nTags are just human-readable aliases for the full image name (d583c3ac45fd...). \nSo you can have as many of them associated with the same image as you like. If you don't like the old name you can remove it after you've retagged it:\ndocker rmi server\n\nThat will just remove the alias/tag. Since d583c3ac45fd has other names, the actual image won't be deleted.",
    "tag": "docker"
  },
  {
    "question": "How to list containers in Docker",
    "answer": "To show only running containers run:\ndocker ps\n\nTo show all containers run:\ndocker ps -a\n\nTo show the latest created container (includes all states) run:\ndocker ps -l\n\nTo show n last created containers (includes all states) run:\ndocker ps -n=-1\n\nTo display total file sizes run:\ndocker ps -s\n\nThe content presented above is from docker.com.\nIn the new version of Docker, commands are updated, and some management commands are added:\ndocker container ls\n\nIt is used to list all the running containers includes all states.\ndocker container ls -a\n\nAnd then, if you want to clean them all,\ndocker rm $(docker ps -aq)\n\nIt is used to list all the containers created irrespective of its state.\nAnd to stop all the Docker containers (force)\ndocker rm -f $(docker ps -a -q)  \n\nHere the container is the management command.",
    "tag": "docker"
  },
  {
    "question": "How to see docker image contents",
    "answer": "If the image contains a shell, you can run an interactive shell container using that image and explore whatever content that image has.  If sh is not available, the busybox ash shell might be.\nFor instance:\ndocker run -it image_name sh\n\nOr following for images with an entrypoint\ndocker run -it --entrypoint sh image_name\n\nOr if you want to see how the image was built, meaning the steps in its Dockerfile, you can:\ndocker image history --no-trunc image_name > image_history\n\nThe steps will be logged into the image_history file.",
    "tag": "docker"
  },
  {
    "question": "Cannot connect to the Docker daemon at unix:/var/run/docker.sock. Is the docker daemon running?",
    "answer": "You can try out this:\nsystemctl start docker\n\nIt worked fine for me.\nP.S.: after if there is commands that you can't do without sudo, try this:\ngpasswd -a $USER docker",
    "tag": "docker"
  },
  {
    "question": "How to mount a host directory in a Docker container",
    "answer": "There are a couple ways you can do this. The simplest way to do so is to use the dockerfile ADD command like so:\nADD . /path/inside/docker/container\n\nHowever, any changes made to this directory on the host after building the dockerfile will not show up in the container. This is because when building a container, docker compresses the directory into a .tar and uploads that context into the container permanently.\nThe second way to do this is the way you attempted, which is to mount a volume. Due to trying to be as portable as possible you cannot map a host directory to a docker container directory within a dockerfile, because the host directory can change depending on which machine you are running on. To map a host directory to a docker container directory you need to use the -v flag when using docker run, e.g.,:\n# Run a container using the `alpine` image, mount the `/tmp`\n# directory from your host into the `/container/directory`\n# directory in your container, and run the `ls` command to\n# show the contents of that directory.\ndocker run \\\n    -v /tmp:/container/directory \\\n    alpine \\\n    ls /container/directory",
    "tag": "docker"
  },
  {
    "question": "How to include files outside of Docker's build context?",
    "answer": "The best way to work around this is to specify the Dockerfile independently of the build context, using -f.\nFor instance, this command will give the ADD command access to anything in your current directory.\ndocker build -f docker-files/Dockerfile .\n\nUpdate: Docker now allows having the Dockerfile outside the build context (fixed in 18.03.0-ce). So you can also do something like\ndocker build -f ../Dockerfile .",
    "tag": "docker"
  },
  {
    "question": "How does one remove a Docker image?",
    "answer": "Try docker rmi node. That should work. \nSeeing all created containers is as simple as docker ps -a. \nTo remove all existing containers (not images!) run docker rm $(docker ps -aq)",
    "tag": "docker"
  },
  {
    "question": "How do I edit a file after I shell to a Docker container?",
    "answer": "As in the comments, there's no default editor set - strange - the $EDITOR environment variable is empty. You can log in into a container with:\ndocker exec -it <container> bash\n\nAnd run:\napt-get update\napt-get install vim\n\nOr use the following Dockerfile:\nFROM  confluent/postgres-bw:0.1\n\nRUN [\"apt-get\", \"update\"]\nRUN [\"apt-get\", \"install\", \"-y\", \"vim\"]\n\nDocker images are delivered trimmed to the bare minimum - so no editor is installed with the shipped container. That's why there's a need to install it manually.\nEDIT\nI also encourage you to read my post about the topic.",
    "tag": "docker"
  },
  {
    "question": "docker push error: denied: requested access to the resource is denied",
    "answer": "You may need to switch your docker repo to private before docker push.\nThanks to the answer provided by Dean Wu and this comment by ses, before pushing, remember to log out, then log in from the command line to your docker hub account\n# you may need log out first `docker logout` ref. https://stackoverflow.com/a/53835882/248616\ndocker login\n\nAccording to the docs:\nYou need to include the namespace for Docker Hub to associate it with your account.\nThe namespace is the same as your Docker Hub account name.\nYou need to rename the image to YOUR_DOCKERHUB_NAME/docker-whale.\n\nSo, this means you have to tag your image before pushing:\ndocker tag firstimage YOUR_DOCKERHUB_NAME/firstimage\n\nand then you should be able to push it.\ndocker push YOUR_DOCKERHUB_NAME/firstimage",
    "tag": "docker"
  },
  {
    "question": "Run a Docker image as a container",
    "answer": "The specific way to run it depends on whether you gave the image a tag/name or not.\n$ docker images\nREPOSITORY          TAG                 ID                  CREATED             SIZE\nubuntu              12.04               8dbd9e392a96        4 months ago        131.5 MB (virtual 131.5 MB)\n\nWith a name (let's use Ubuntu):\n$ docker run -i -t ubuntu:12.04 /bin/bash\n\nWithout a name, just using the ID:\n$ docker run -i -t 8dbd9e392a96 /bin/bash\n\nPlease see Docker run reference for more information.",
    "tag": "docker"
  },
  {
    "question": "How do I assign a port mapping to an existing Docker container?",
    "answer": "I'm also interested in this problem.\nAs @Thasmo mentioned, port forwardings can be specified ONLY with docker run (and docker create) command.\nOther commands, docker start does not have -p option and docker port only displays current forwardings.\nTo add port forwardings, I always follow these steps,\n\nstop running container\ndocker stop test01\n\ncommit the container\ndocker commit test01 test02\n\nNOTE: The above, test02 is a new image that I'm constructing from the test01 container.\nre-run from the commited image\ndocker run -p 8080:8080 -td test02\n\n\nWhere the first 8080 is the local port and the second 8080 is the container port.",
    "tag": "docker"
  },
  {
    "question": "What is the runtime performance cost of a Docker container?",
    "answer": "An excellent 2014 IBM research paper “An Updated Performance Comparison of Virtual Machines and Linux Containers” by Felter et al. provides a comparison between bare metal, KVM, and Docker containers. The general result is: Docker is nearly identical to native performance and faster than KVM in every category.\nThe exception to this is Docker’s NAT — if you use port mapping (e.g., docker run -p 8080:8080), then you can expect a minor hit in latency, as shown below. However, you can now use the host network stack (e.g., docker run --net=host) when launching a Docker container, which will perform identically to the Native column (as shown in the Redis latency results lower down). \n\nThey also ran latency tests on a few specific services, such as Redis. You can see that above 20 client threads, highest latency overhead goes Docker NAT, then KVM, then a rough tie between Docker host/native. \n\nJust because it’s a really useful paper, here are some other figures. Please download it for full access. \nTaking a look at Disk I/O:\n\nNow looking at CPU overhead: \n\nNow some examples of memory (read the paper for details, memory can be extra tricky):",
    "tag": "docker"
  },
  {
    "question": "Docker - Ubuntu - bash: ping: command not found",
    "answer": "Docker images are pretty minimal, but you can install ping in your official ubuntu docker image via:\napt-get update -y\napt-get install -y iputils-ping\n\nChances are you don't need ping on your image, and just want to use it for testing purposes. Above example will help you out.\nBut if you need ping to exist on your image, you can create a Dockerfile or commit the container you ran the above commands into a new image.\nCommit:\ndocker commit -m \"Installed iputils-ping\" --author \"Your Name <name@domain.com>\" ContainerNameOrId yourrepository/imagename:tag\n\nDockerfile:\nFROM ubuntu\nRUN apt-get update && apt-get install -y iputils-ping\nCMD bash\n\nPlease note there are best practices on creating docker images, like clearing apt cache files afterwards and etc.",
    "tag": "docker"
  },
  {
    "question": "How to access host port from docker container",
    "answer": "For all platforms\nDocker v 20.10 and above (since December 14th 2020)\nUse your internal IP address or connect to the special DNS name host.docker.internal which will resolve to the internal IP address used by the host.\nThis is for development purpose and does not work in a production environment outside of Docker Desktop.\nLinux caveats\nTo enable this in Docker on Linux, add --add-host=host.docker.internal:host-gateway to your docker command to enable the feature.\nTo enable this in Docker Compose on Linux, add the following lines to the container definition:\nextra_hosts:\n    - \"host.docker.internal:host-gateway\"\n\nAccording to some users the special DNS name only works within the Docker's default bridge network, not within custom networks.\n\nFor older macOS and Windows versions of Docker\nDocker v 18.03 and above (since March 21st 2018)\nUse your internal IP address or connect to the special DNS name host.docker.internal which will resolve to the internal IP address used by the host.\nLinux support pending https://github.com/docker/for-linux/issues/264\n\nFor older macOS versions of Docker\nDocker for Mac v 17.12 to v 18.02\nSame as above but use docker.for.mac.host.internal instead.\n\nDocker for Mac v 17.06 to v 17.11\nSame as above but use docker.for.mac.localhost instead.\n\nDocker for Mac 17.05 and below\nTo access host machine from the docker container you must attach an IP alias to your network interface. You can bind whichever IP you want, just make sure you're not using it to anything else.\nsudo ifconfig lo0 alias 123.123.123.123/24\nThen make sure that you server is listening to the IP mentioned above or 0.0.0.0. If it's listening on localhost 127.0.0.1 it will not accept the connection.\nThen just point your docker container to this IP and you can access the host machine!\n\nTo test you can run something like curl -X GET 123.123.123.123:3000 inside the container.\nThe alias will reset on every reboot so create a start-up script if necessary.\nSolution and more documentation here: https://docs.docker.com/desktop/networking/#use-cases-and-workarounds-for-all-platforms",
    "tag": "docker"
  },
  {
    "question": "In a Dockerfile, How to update PATH environment variable?",
    "answer": "You can use Environment Replacement in your Dockerfile as follows:\nENV PATH=\"$PATH:/opt/gtk/bin\"\n\n(please note that when using ${PATH}, this might use the host's PATH instead of the Container)",
    "tag": "docker"
  },
  {
    "question": "In a Dockerfile, How to update PATH environment variable?",
    "answer": "You can use Environment Replacement in your Dockerfile as follows:\nENV PATH=\"$PATH:/opt/gtk/bin\"\n\n(please note that when using ${PATH}, this might use the host's PATH instead of the Container)",
    "tag": "docker"
  },
  {
    "question": "What is the difference between \"expose\" and \"publish\" in Docker?",
    "answer": "Basically, you have three (four) options:\n\nNeither specify EXPOSE nor -p\nOnly specify EXPOSE\nSpecify EXPOSE and -p\nOnly specify -p which implicitly does EXPOSE\n\n\nIf you specify neither EXPOSE nor -p, the service in the container will only be accessible from inside the container itself.\n\nIf you EXPOSE a port, the service in the container is not accessible from outside Docker, but from inside other Docker containers. So this is good for inter-container communication.\n\nIf you EXPOSE and -p a port, the service in the container is accessible from anywhere, even outside Docker.\n\nIf you do -p, but do not EXPOSE, Docker does an implicit EXPOSE. This is because if a port is open to the public, it is automatically also open to other Docker containers. Hence -p includes EXPOSE. This is effectively same as 3).\n\n\nThe reason why both are separated is IMHO because:\n\nchoosing a host port depends on the host and hence does not belong to the Dockerfile (otherwise it would be depending on the host),\nand often it's enough if a service in a container is accessible from other containers.\n\nThe documentation explicitly states:\n\nThe EXPOSE instruction exposes ports for use within links.\n\nIt also points you to how to link containers (legacy feature), which basically is the inter-container communication I talked about.",
    "tag": "docker"
  },
  {
    "question": "Docker complains about \"no space left on device\", how to clean up?",
    "answer": "The current best practice is:\ndocker system prune\n\nNote the output from this command prior to accepting the consequences:\nWARNING! This will remove:\n  - all stopped containers\n  - all networks not used by at least one container\n  - all dangling images\n  - all dangling build cache\n\nAre you sure you want to continue? [y/N]\n\nIn other words, continuing with this command is permanent. Keep in mind that best practice is to treat stopped containers as ephemeral i.e. you should be designing your work with Docker to not keep these stopped containers around. You may want to consider using the --rm flag at runtime if you are not actively debugging your containers.\nMake sure you read this answer, re: Volumes\nYou may also be interested in this answer, if docker system prune does not work for you.",
    "tag": "docker"
  },
  {
    "question": "How can I expose more than 1 port with Docker?",
    "answer": "To expose just one port, this is what you need to do:\ndocker run -p <host_port>:<container_port>\n\nTo expose multiple ports, simply provide multiple -p arguments:\ndocker run -p <host_port1>:<container_port1> -p <host_port2>:<container_port2>",
    "tag": "docker"
  },
  {
    "question": "Docker - Name is already in use by container",
    "answer": "I got confused by this also. There are two commands relevant here:\ndocker run  # Run a command in a **new** container\n\ndocker start  # Start one or more stopped containers",
    "tag": "docker"
  },
  {
    "question": "How to push a docker image to a private repository",
    "answer": "You need to tag your image correctly first with your registryhost:\ndocker tag [OPTIONS] IMAGE[:TAG] [REGISTRYHOST/][USERNAME/]NAME[:TAG]\n\nThen docker push using that same tag.\ndocker push NAME[:TAG]\n\nExample:\ndocker tag 518a41981a6a myRegistry.com/myImage\ndocker push myRegistry.com/myImage",
    "tag": "docker"
  },
  {
    "question": "Difference between RUN and CMD in a Dockerfile",
    "answer": "RUN is an image build step, the state of the container after a RUN command will be committed to the container image. A Dockerfile can have many RUN steps that layer on top of one another to build the image.\nCMD is the command the container executes by default when you launch the built image. A Dockerfile will only use the final CMD defined. The CMD can be overridden when starting a container with docker run $image $other_command.\nENTRYPOINT is also closely related to CMD and can modify the way a CMD is interpreted when a container is started from an image.",
    "tag": "docker"
  },
  {
    "question": "How to enter in a Docker container already running with a new TTY",
    "answer": "With docker 1.3, there is a new command docker exec. This allows you to enter a running container:\ndocker exec -it [container-id] bash\n\nNote: this assumes bash is installed on your container. You may run sh or whatever interactive shell is installed on the container.",
    "tag": "docker"
  },
  {
    "question": "How to rebuild docker container in docker-compose.yml?",
    "answer": "docker-compose up\n$ docker-compose up -d --no-deps --build <service_name>\n\nor newer versions of docker\n$ docker compose up -d --no-deps --build <service_name>\n\n\n--no-deps - Don't start linked services.\n\n\n--build - Build images before starting containers.",
    "tag": "docker"
  },
  {
    "question": "Communication between multiple docker-compose projects",
    "answer": "You just need to make sure that the containers you want to talk to each other are on the same network. Networks are a first-class docker construct, and not specific to compose.\n# front/docker-compose.yml\nservices:\n  front:\n    ...\n    networks:\n      - some-net\nnetworks:\n  some-net:\n    driver: bridge\n\n...\n# api/docker-compose.yml\nservices:\n  api:\n    ...\n    networks:\n      - front_some-net\nnetworks:\n  front_some-net:\n    external: true\n\n\nNote: Your app’s network is given a name based on the “project name”, which is based on the name of the directory it lives in, in this case a prefix front_ was added\n\nThey can then talk to each other using the service name. From front you can do ping api and vice versa.",
    "tag": "docker"
  },
  {
    "question": "How can I use local Docker images with Minikube?",
    "answer": "As the handbook describes, you can reuse the Docker daemon from Minikube with eval $(minikube docker-env).\nSo to use an image without uploading it, you can follow these steps:\n\nSet the environment variables with eval $(minikube docker-env)\nBuild the image with the Docker daemon of Minikube (e.g., docker build -t my-image .)\nSet the image in the pod specification like the build tag (e.g., my-image)\nSet the imagePullPolicy to Never, otherwise Kubernetes will try to download the image.\n\nImportant note: You have to run eval $(minikube docker-env) on each terminal you want to use, since it only sets the environment variables for the current shell session.",
    "tag": "docker"
  },
  {
    "question": "Docker Compose wait for container X before starting Y",
    "answer": "Finally found a solution with a docker-compose method. Since docker-compose file format 2.1 you can define healthchecks.\nI did it in a example project\nyou need to install at least docker 1.12.0+.\nI also needed to extend the rabbitmq-management Dockerfile, because curl isn't installed on the official image.\nNow I test if the management page of the rabbitmq-container is available. If curl finishes with exitcode 0 the container app (python pika) will be started and publish a message to hello queue. Its now working (output).\ndocker-compose (version 2.1):\nversion: '2.1'\n\nservices:\n  app:\n    build: app/.\n    depends_on:\n      rabbit:\n        condition: service_healthy\n    links: \n        - rabbit\n\n  rabbit:\n    build: rabbitmq/.\n    ports: \n        - \"15672:15672\"\n        - \"5672:5672\"\n    healthcheck:\n        test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:15672\"]\n        interval: 30s\n        timeout: 10s\n        retries: 5\n\noutput:\nrabbit_1  | =INFO REPORT==== 25-Jan-2017::14:44:21 ===\nrabbit_1  | closing AMQP connection <0.718.0> (172.18.0.3:36590 -> 172.18.0.2:5672)\napp_1     |  [x] Sent 'Hello World!'\nhealthcheckcompose_app_1 exited with code 0\n\nDockerfile (rabbitmq + curl):\nFROM rabbitmq:3-management\nRUN apt-get update\nRUN apt-get install -y curl \nEXPOSE 4369 5671 5672 25672 15671 15672\n\nVersion 3 no longer supports the condition form of depends_on.\nSo i moved from depends_on to restart on-failure. Now my app container will restart 2-3 times until it is working, but it is still a docker-compose feature without overwriting the entrypoint.\ndocker-compose (version 3):\nversion: \"3\"\n\nservices:\n\n  rabbitmq: # login guest:guest\n    image: rabbitmq:management\n    ports:\n    - \"4369:4369\"\n    - \"5671:5671\"\n    - \"5672:5672\"\n    - \"25672:25672\"\n    - \"15671:15671\"\n    - \"15672:15672\"\n    healthcheck:\n        test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:15672\"]\n        interval: 30s\n        timeout: 10s\n        retries: 5\n\n  app:\n    build: ./app/\n    environment:\n      - HOSTNAMERABBIT=rabbitmq\n    restart: on-failure\n    depends_on:\n      - rabbitmq\n    links: \n        - rabbitmq",
    "tag": "docker"
  },
  {
    "question": "How to upgrade docker container after its image changed",
    "answer": "After evaluating the answers and studying the topic I'd like to summarize.\nThe Docker way to upgrade containers seems to be the following:\nApplication containers should not store application data. This way you can replace app container with its newer version at any time by executing something like this:\ndocker pull mysql\ndocker stop my-mysql-container\ndocker rm my-mysql-container\ndocker run --name=my-mysql-container --restart=always \\\n  -e MYSQL_ROOT_PASSWORD=mypwd -v /my/data/dir:/var/lib/mysql -d mysql\n\nYou can store data either on host (in directory mounted as volume) or in special data-only container(s). Read more about it \n\nAbout volumes (Docker docs)\nTiny Docker Pieces, Loosely Joined (by Tom Offermann)\nHow to deal with persistent storage (e.g. databases) in Docker (Stack Overflow question)\n\nUpgrading applications (eg. with yum/apt-get upgrade) within containers is considered to be an anti-pattern. Application containers are supposed to be immutable, which shall guarantee reproducible behavior. Some official application images (mysql:5.6 in particular) are not even designed to self-update (apt-get upgrade won't work).\nI'd like to thank everybody who gave their answers, so we could see all different approaches.",
    "tag": "docker"
  },
  {
    "question": "What's the difference between Docker Compose vs. Dockerfile",
    "answer": "Dockerfile\n\nA Dockerfile is a simple text file that contains the commands a user could call to assemble an image.\nExample,  Dockerfile\nFROM ubuntu:latest\nMAINTAINER john doe \n\nRUN apt-get update\nRUN apt-get install -y python python-pip wget\nRUN pip install Flask\n\nADD hello.py /home/hello.py\n\nWORKDIR /home\n\n\nDocker Compose\n\nDocker Compose\n\nis a tool for defining and running multi-container Docker applications.\n\ndefine the services that make up your app in docker-compose.yml so they can be run together in an isolated environment.\n\nget an app running in one command by just running  docker-compose up\n\n\nExample, docker-compose.yml\nversion: \"3\"\nservices:\n  web:\n    build: .\n    ports:\n    - '5000:5000'\n    volumes:\n    - .:/code\n    - logvolume01:/var/log\n    links:\n    - redis\n  redis:\n    image: redis\n    volumes:\n      logvolume01: {}",
    "tag": "docker"
  },
  {
    "question": "COPY with docker but with exclusion",
    "answer": "Create file .dockerignore in your docker build context directory (so in this case, most likely a directory that is a parent to node_modules) with one line in it: \n**/node_modules\n\nalthough you probably just want:\nnode_modules\n\nInfo about dockerignore: https://docs.docker.com/engine/reference/builder/#dockerignore-file",
    "tag": "docker"
  },
  {
    "question": "How to set image name in Dockerfile?",
    "answer": "Workaround using docker-compose\nTagging of the image isn't supported inside the Dockerfile. This needs to be done in your build command. As a workaround, you can do the build with a docker-compose.yml that identifies the target image name and then run a docker-compose build. A sample docker-compose.yml would look like\nversion: '2'\n\nservices:\n  man:\n    build: .\n    image: dude/man:v2\n\nThat said, there's a push against doing the build with compose since that doesn't work with swarm mode deploys. So you're back to running the command as you've given in your question:\ndocker build -t dude/man:v2 .\n\nPersonally, I tend to build with a small shell script in my folder (build.sh) which passes any args and includes the name of the image there to save typing. And for production, the build is handled by a ci/cd server that has the image name inside the pipeline script.",
    "tag": "docker"
  },
  {
    "question": "How do I make a comment in a Dockerfile?",
    "answer": "You can use # at the beginning of a line to start a comment (whitespaces before # are allowed):\n# do some stuff\nRUN apt-get update \\\n    # install some packages\n    && apt-get install -y cron\n\n#'s in the middle of a string are passed to the command itself, e.g.:\nRUN echo 'we are running some # of cool things'",
    "tag": "docker"
  },
  {
    "question": "How do I make a comment in a Dockerfile?",
    "answer": "You can use # at the beginning of a line to start a comment (whitespaces before # are allowed):\n# do some stuff\nRUN apt-get update \\\n    # install some packages\n    && apt-get install -y cron\n\n#'s in the middle of a string are passed to the command itself, e.g.:\nRUN echo 'we are running some # of cool things'",
    "tag": "docker"
  },
  {
    "question": "Interactive shell using Docker Compose",
    "answer": "You need to include the following lines in your docker-compose.yml:\nversion: \"3\"\nservices:\n  app:\n    image: app:1.2.3\n    stdin_open: true # docker run -i\n    tty: true        # docker run -t\n\nThe first corresponds to -i in docker run and the second to -t.",
    "tag": "docker"
  },
  {
    "question": "ps command doesn't work in docker container",
    "answer": "ps is not installed in the base wheezy image. Try this from within the container:\napt-get update && apt-get install procps\n\nor add the following line to the Dockerfile:\nRUN apt-get update && apt-get install -y procps && rm -rf /var/lib/apt/lists/*",
    "tag": "docker"
  },
  {
    "question": "Docker can't connect to docker daemon",
    "answer": "Linux\nThe Post-installation steps for Linux documentation reveals the following steps:\n\nCreate the docker group.\nsudo groupadd docker\nAdd the user to the docker group.\nsudo usermod -aG docker $(whoami)\nLog out and log back in to ensure docker runs with correct permissions or execute \nnewgrp docker\n\nStart docker.\nsudo service docker start\n\nMac OS X\nAs Dayel Ostraco says is necessary to add environments variables:\ndocker-machine start # Start virtual machine for docker\ndocker-machine env  # It's helps to get environment variables\neval \"$(docker-machine env default)\" # Set environment variables\n\nThe docker-machine start command outputs the comments to guide the process.",
    "tag": "docker"
  },
  {
    "question": "How do I run a command on an already existing Docker container?",
    "answer": "In October 2014 the Docker team introduced docker exec command: https://docs.docker.com/engine/reference/commandline/exec/\nSo now you can run any command in a running container just knowing its ID (or name):\ndocker exec -it <container_id_or_name> echo \"Hello from container!\"\n\nNote that exec command works only on already running container. If the container is currently stopped, you need to first run it with the following command:\ndocker run -it -d shykes/pybuilder /bin/bash\n\nThe most important thing here is the -d option, which stands for detached. It means that the command you initially provided to the container (/bin/bash) will be run in the background and the container will not stop immediately.",
    "tag": "docker"
  },
  {
    "question": "How to restart a single container with docker-compose",
    "answer": "It is very simple: Use the command:\ndocker-compose restart worker\n\nYou can set the time to wait for stop before killing the container (in seconds)\ndocker-compose restart -t 30 worker\n\nNote that this will restart the container but without rebuilding it. If you want to apply your changes and then restart, take a look at the other answers.",
    "tag": "docker"
  },
  {
    "question": "Docker: Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock",
    "answer": "If using jenkins\nThe user jenkins needs to be added to the group docker:\nsudo usermod -a -G docker jenkins\n\nThen restart Jenkins.\nOtherwise\nIf you arrive to this question of stack overflow because you receive this message from docker, but you don't use jenkins, most probably the error is the same: your unprivileged user does not belong to the docker group.\nYou can do:\nsudo usermod -a -G docker $USER\n\nYou can check it was successful by doing grep docker /etc/group and see something like this:\ndocker:x:998:[user]\n\nin one of the lines.\nThen change your users group ID to docker (to avoid having to log out and log in again):\nnewgrp docker",
    "tag": "docker"
  },
  {
    "question": "Copy directory to another directory using ADD command",
    "answer": "ADD go /usr/local/\n\nwill copy the contents of your local go directory into the /usr/local/ directory of your docker image.\nTo copy the go directory itself in /usr/local/ use:\nADD go /usr/local/go\n\nor\nCOPY go /usr/local/go",
    "tag": "docker"
  },
  {
    "question": "How to mount a single file in a volume",
    "answer": "TL;DR/Notice:\nIf you experience a directory being created in place of the file you are trying to mount, you have probably failed to supply a valid and absolute path. This is a common mistake with a silent and confusing failure mode.\n\nFile volumes are done this way in docker (absolute path example (can use env variables), and you need to mention the file name) :\nvolumes:\n  - /src/docker/myapp/upload:/var/www/html/upload\n  - /src/docker/myapp/upload/config.php:/var/www/html/config.php\n\nYou can also do:\nvolumes:\n  - ${PWD}/upload:/var/www/html/upload\n  - ${PWD}/upload/config.php:/var/www/html/config.php\n\nIf you fire the docker-compose from /src/docker/myapp folder",
    "tag": "docker"
  },
  {
    "question": "Connecting to Postgresql in a docker container from outside",
    "answer": "You can run Postgres this way (map a port):\ndocker run --name some-postgres -e POSTGRES_PASSWORD=mysecretpassword -d -p 5432:5432 postgres\n\nSo now you have mapped the port 5432 of your container to port 5432 of your server. -p <host_port>:<container_port>  .So now your postgres is accessible from your public-server-ip:5432\nTo test:\nRun the postgres database (command above)\ndocker ps\nCONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                     NAMES\n05b3a3471f6f        postgres            \"/docker-entrypoint.s\"   1 seconds ago       Up 1 seconds        0.0.0.0:5432->5432/tcp    some-postgres\n\nGo inside your container and create a database:\ndocker exec -it 05b3a3471f6f bash\nroot@05b3a3471f6f:/# psql -U postgres\npostgres-# CREATE DATABASE mytest;\npostgres-# \\q\n\nGo to your localhost (where you have some tool or the psql client).\npsql -h public-ip-server -p 5432 -U postgres\n\n(password mysecretpassword)\npostgres=# \\l\n\n                             List of databases\n   Name    |  Owner   | Encoding |  Collate   |   Ctype    |   Access privileges\n-----------+----------+----------+------------+------------+-----------------------\n mytest    | postgres | UTF8     | en_US.utf8 | en_US.utf8 |\n postgres  | postgres | UTF8     | en_US.utf8 | en_US.utf8 |\n template0 | postgres | UTF8     | en_US.utf8 | en_US.utf8 | =c/postgres   \n\nSo you're accessing the database (which is running in docker on a server) from your localhost.\nIn this post it's expained in detail.",
    "tag": "docker"
  },
  {
    "question": "Cannot connect to the Docker daemon on macOS",
    "answer": "On a supported Mac, run:\nbrew install --cask docker\n\nThen launch the Docker app. Click next. It will ask for privileged access. Confirm. A whale icon should appear in the top bar. Click it and wait for \"Docker is running\" to appear.\nYou should be able to run docker commands now:\ndocker ps\n\nBecause docker is a system-level package, you cannot install it using brew install, and must use --cask instead.\nNote: This solution only works for Macs whose CPUs support virtualization, which may not include old Macs.\nEdit 2022 - It was brought up in the comments section that there has been a licensing change for Docker Desktop. Please consider alternatives below, and check the comments for details.",
    "tag": "docker"
  },
  {
    "question": "Docker: How to clear the logs properly for a Docker container?",
    "answer": "First the bad answer. From this question there's a one-liner that you can run:\necho \"\" > $(docker inspect --format='{{.LogPath}}' <container_name_or_id>)\n\ninstead of echo, there's the simpler:\n: > $(docker inspect --format='{{.LogPath}}' <container_name_or_id>)\n\nor there's the truncate command:\ntruncate -s 0 $(docker inspect --format='{{.LogPath}}' <container_name_or_id>)\n\n\nI'm not a big fan of either of those since they modify Docker's files directly. The external log deletion could happen while docker is writing json formatted data to the file, resulting in a partial line, and breaking the ability to read any logs from the docker logs cli. For an example of that happening, see this comment on duketwo's answer:\n\nafter emptying the logfile, I get this error: error from daemon in stream: Error grabbing logs: invalid character '\\x00' looking for beginning of value\n\nInstead, you can have Docker automatically rotate the logs for you. This is done with additional flags to dockerd if you are using the default JSON logging driver:\ndockerd ... --log-opt max-size=10m --log-opt max-file=3\n\nYou can also set this as part of your daemon.json file instead of modifying your startup scripts:\n{\n  \"log-driver\": \"json-file\",\n  \"log-opts\": {\"max-size\": \"10m\", \"max-file\": \"3\"}\n}\n\nThese options need to be configured with root access. Make sure to run a systemctl reload docker after changing this file to have the settings applied. This setting will then be the default for any newly created containers. Note, existing containers need to be deleted and recreated to receive the new log limits.\n\nSimilar log options can be passed to individual containers to override these defaults, allowing you to save more or fewer logs on individual containers. From docker run this looks like:\ndocker run --log-driver json-file --log-opt max-size=10m --log-opt max-file=3 ...\n\nor in a compose file:\nversion: '3.7'\nservices:\n  app:\n    image: ...\n    logging:\n      options:\n        max-size: \"10m\"\n        max-file: \"3\"\n\n\nFor additional space savings, you can switch from the json log driver to the \"local\" log driver. It takes the same max-size and max-file options, but instead of storing in json it uses a binary syntax that is faster and smaller. This allows you to store more logs in the same sized file. The daemon.json entry for that looks like:\n{\n  \"log-driver\": \"local\",\n  \"log-opts\": {\"max-size\": \"10m\", \"max-file\": \"3\"}\n}\n\nThe downside of the local driver is external log parsers/forwarders that depended on direct access to the json logs will no longer work. So if you use a tool like filebeat to send to Elastic, or Splunk's universal forwarder, I'd avoid the \"local\" driver.\nI've got a bit more on this in my Tips and Tricks presentation.",
    "tag": "docker"
  },
  {
    "question": "How can I add a volume to an existing Docker container?",
    "answer": "You can commit your existing container (that creates a new image from container’s changes) and then run it with your new mounts.\nExample:\n$ docker ps  -a\n\nCONTAINER ID        IMAGE                 COMMAND                  CREATED              STATUS                          PORTS               NAMES\n5a8f89adeead        ubuntu:14.04          \"/bin/bash\"              About a minute ago   Exited (0) About a minute ago                       agitated_newton\n\n$ docker commit 5a8f89adeead newimagename\n$ docker run -ti -v \"$PWD/somedir\":/somedir newimagename /bin/bash\n\nIf it's all OK, stop your old container, and use this new one.\nYou can also commit a container using its name, for example:\ndocker commit agitated_newton newimagename\n\nThat's it :)",
    "tag": "docker"
  },
  {
    "question": "Dockerfile copy keep subdirectory structure",
    "answer": "Remove star from COPY, with this Dockerfile:\nFROM ubuntu\nCOPY files/ /files/\nRUN ls -la /files/*\n\nStructure is there:\n$ docker build .\nSending build context to Docker daemon 5.632 kB\nSending build context to Docker daemon \nStep 0 : FROM ubuntu\n ---> d0955f21bf24\nStep 1 : COPY files/ /files/\n ---> 5cc4ae8708a6\nRemoving intermediate container c6f7f7ec8ccf\nStep 2 : RUN ls -la /files/*\n ---> Running in 08ab9a1e042f\n/files/folder1:\ntotal 8\ndrwxr-xr-x 2 root root 4096 May 13 16:04 .\ndrwxr-xr-x 4 root root 4096 May 13 16:05 ..\n-rw-r--r-- 1 root root    0 May 13 16:04 file1\n-rw-r--r-- 1 root root    0 May 13 16:04 file2\n\n/files/folder2:\ntotal 8\ndrwxr-xr-x 2 root root 4096 May 13 16:04 .\ndrwxr-xr-x 4 root root 4096 May 13 16:05 ..\n-rw-r--r-- 1 root root    0 May 13 16:04 file1\n-rw-r--r-- 1 root root    0 May 13 16:04 file2\n ---> 03ff0a5d0e4b\nRemoving intermediate container 08ab9a1e042f\nSuccessfully built 03ff0a5d0e4b",
    "tag": "docker"
  },
  {
    "question": "Docker container will automatically stop after \"docker run -d\"",
    "answer": "The centos dockerfile has a default command bash.\nThat means, when run in background (-d), the shell exits immediately.\nUpdate 2017\nMore recent versions of docker authorize running a container both in detached mode and in foreground mode (-t, -i or -it)\nIn that case, you don't need any additional command and this is enough:\ndocker run -t -d centos\n\nThe bash will wait in the background.\nThat was initially reported in kalyani-chaudhari's answer and detailed in jersey bean's answer.\nvonc@voncvb:~$ d ps -a\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES\n4a50fd9e9189        centos              \"/bin/bash\"         8 seconds ago       Up 2 seconds                            wonderful_wright\n\nNote that for alpine, Marinos An reports in the comments:\n\ndocker run -t -d alpine/git does not keep the process up.\nHad to do: docker run --entrypoint \"/bin/sh\" -it alpine/git\n\n\nOriginal answer (2015)\nAs mentioned in this article:\n\nInstead of running with docker run -i -t image your-command, using -d is recommended because you can run your container with just one command and you don’t need to detach terminal of container by hitting Ctrl + P + Q.\nHowever, there is a problem with -d option. Your container immediately stops unless the commands keep running in foreground.\nDocker requires your command to keep running in the foreground. Otherwise, it thinks that your applications stops and shutdown the container.\nThe problem is that some application does not run in the foreground. How can we make it easier?\nIn this situation, you can add tail -f /dev/null to your command.\nBy doing this, even if your main command runs in the background, your container doesn’t stop because tail is keep running in the foreground.\n\nSo this would work:\ndocker run -d centos tail -f /dev/null\n\nOr in Dockerfile:\nENTRYPOINT [\"tail\"]\nCMD [\"-f\",\"/dev/null\"]\n\nA docker ps would show the centos container still running.\nFrom there, you can attach to it or detach from it (or docker exec some commands).\n\nHowever, as noted by bviktor in the comments:\n\nOverriding entrypoint will mess up with your Dockerfile if it has anything specified.\nsystemd is of particular interest.\nTailing /dev/null also doesn't sound very convincing: in essence you bombard your system with completely unnecessary system calls.\nThe other solution with sleep looks much better.\n\nThat relies on sleep infinity, a GNU coreutils extension not contemplated in POSIX.",
    "tag": "docker"
  },
  {
    "question": "Docker: adding a file from a parent directory",
    "answer": "cd to your parent directory instead\nbuild the image from the parent directory, specifying the path to your Dockerfile\n\ndocker build -t <some tag> -f <dir/dir/Dockerfile> .\n\nIn this case, the context of the docker will be switched to the parent directory and accessible for ADD and COPY",
    "tag": "docker"
  },
  {
    "question": "Why is docker build not showing any output from commands?",
    "answer": "The output you are showing is from buildkit, which is a replacement for the classic build engine that docker ships with. You can adjust output from this with the --progress option:\n  --progress string         Set type of progress output (auto, plain, tty). Use plain to show container output\n                            (default \"auto\")\n\nAdding --progress=plain will show the output of the run commands that were not loaded from the cache. This can also be done by setting the BUILDKIT_PROGRESS variable:\nexport BUILDKIT_PROGRESS=plain\n\n\nIf you are debugging a build, and the steps have already been cached, add --no-cache to your build to rerun the steps and redisplay the output:\ndocker build --progress=plain --no-cache ...\n\n\nIf you don't want to use buildkit, you can revert to the older build engine by exporting DOCKER_BUILDKIT=0 in your shell, e.g.:\nDOCKER_BUILDKIT=0 docker build ...\n\nor\nexport DOCKER_BUILDKIT=0\ndocker build ...",
    "tag": "docker"
  },
  {
    "question": "How to get the IP address of the docker host from inside a docker container",
    "answer": "/sbin/ip route|awk '/default/ { print $3 }'\n\nAs @MichaelNeale noticed, there is no sense to use this method in Dockerfile (except when we need this IP during build time only), because this IP will be hardcoded during build time.",
    "tag": "docker"
  },
  {
    "question": "How do you attach and detach from Docker's process?",
    "answer": "To detach the tty without exiting the shell, use the escape sequence Ctrl+P followed by Ctrl+Q. More details here.\nAdditional info from this source:\n\ndocker run -t -i → can be detached with ^P^Qand reattached with docker attach\ndocker run -i → cannot be detached with ^P^Q; will disrupt stdin\ndocker run → cannot be detached with ^P^Q; can SIGKILL client; can reattach with docker attach",
    "tag": "docker"
  },
  {
    "question": "How to apt-get install in a GitHub Actions workflow?",
    "answer": "The docs say:\n\nThe Linux and macOS virtual machines both run using passwordless sudo. When you need to execute commands or install tools that require more privileges than the current user, you can use sudo without needing to provide a password.\n\nSo simply doing the following should work:\n- name: Install xmllint\n  run: sudo apt install -y libxml2-utils",
    "tag": "apt"
  },
  {
    "question": "Install MySQL on Ubuntu without a password prompt",
    "answer": "sudo debconf-set-selections <<< 'mysql-server mysql-server/root_password password your_password'\nsudo debconf-set-selections <<< 'mysql-server mysql-server/root_password_again password your_password'\nsudo apt-get -y install mysql-server\n\nFor specific versions, such as mysql-server-5.6, you'll need to specify the version in like this:\nsudo debconf-set-selections <<< 'mysql-server-5.6 mysql-server/root_password password your_password'\nsudo debconf-set-selections <<< 'mysql-server-5.6 mysql-server/root_password_again password your_password'\nsudo apt-get -y install mysql-server-5.6\n\nFor mysql-community-server, the keys are slightly different:\nsudo debconf-set-selections <<< 'mysql-community-server mysql-community-server/root-pass password your_password'\nsudo debconf-set-selections <<< 'mysql-community-server mysql-community-server/re-root-pass password your_password'\nsudo apt-get -y install mysql-community-server\n\nReplace your_password with the desired root password. (it seems your_password can also be left blank for a blank root password.)\nIf you writing a script then your shell may be not bash but dash/ash or some basic unix shell. These shells, unlike zsh, ksh93 and bash, doesn't support here-strings <<<. So you should use:\necho ... | sudo debconf-set-selections \n\nOr a more readable print of multiline strings:\ncat << EOF | sudo debconf-set-selections\nmysql-server mysql-server/root_password password your_password\nmysql-server mysql-server/root_password_again password your_password\nEOF\n\nYou can check with debconf-get-selections command:\n$ sudo debconf-get-selections | grep ^mysql\nmysql-server    mysql-server/root_password_again    password    your_password\nmysql-server    mysql-server/root_password  password    your_password",
    "tag": "apt"
  },
  {
    "question": "Repository is not signed in docker build",
    "answer": "Apparently my root partition was full (maybe I've tried too many times to download packages through apt), and running sudo apt clean solved the issue\n\nIn addition, the following commands should help clean up space:\ndocker system df # which can show disk usage and size of 'Build Cache'\ndocker image prune # add -f or --force to not prompt for confirmation\ndocker container prune # add -f or --force to not prompt for confirmation",
    "tag": "apt"
  },
  {
    "question": "How can I check the version before installing a package using 'apt-get'?",
    "answer": "OK, I found it.\napt-cache policy <package name> will show the version details.\nIt also shows which version is currently installed and which versions are available to install.\nFor example, apt-cache policy hylafax+",
    "tag": "apt"
  },
  {
    "question": "What is the difference/usage of homebrew, macports or other package installation tools?",
    "answer": "MacPorts is the way to go.\n\nLike @user475443 pointed, MacPorts has many many more packages. With brew you'll find yourself trapped soon because the formula you need doesn't exist.\nMacPorts is a native application: C + TCL. You don't need Ruby at all. To install Ruby on Mac OS X you might need MacPorts, so just go with MacPorts and you'll be happy.\nMacPorts is really stable, in 8 years I never had a problem with it, and my entire Unix ecosystem relay on it.\nIf you are a PHP developer you can install the last version of Apache (Mac OS X uses 2.2), PHP and all the extensions you need, then upgrade all with one command. Forget to do the same with Homebrew.\nMacPorts support groups.\nfoo@macpro:~/ port select --summary\n\nName        Selected      Options\n====        ========      =======\ndb          none          db46 none\ngcc         none          gcc42 llvm-gcc42 mp-gcc48 none\nllvm        none          mp-llvm-3.3 none\nmysql       mysql56       mysql56 none\nphp         php55         php55 php56 none\npostgresql  postgresql94  postgresql93 postgresql94 none\npython      none          python24 python25-apple python26-apple python27 python27-apple none\n\nIf you have both PHP55 and PHP56 installed (with many different extensions), you can swap between them with just one command. All the relative extensions are part of the group and they will be activated within the chosen group: php55 or php56. I'm not sure Homebrew has this feature.\nRubists like to rewrite everything in Ruby, because the only thing they are at ease is Ruby itself.",
    "tag": "apt"
  },
  {
    "question": "Docker: Having issues installing apt-utils",
    "answer": "This is not actually an error and it is safe to ignore it. I have built a large number of container images without ever having apt-utils on any of them and regardless of this warning message, all package installs go through and work normally.\nAnyway, if you want to have apt-utils - install it. It will give you this warning once and then it will disappear for future invocations of apt-get (as you can see in your own log, curl got installed without that message).\nNOTE if you install apt-utils, you will get other warnings (because now the installer can run interactive config and will attempt that and fail). To suppress those and have packages that have interactive config with their defaults, run apt-get like this:\nDEBIAN_FRONTEND=noninteractive apt-get install -y pkgs....",
    "tag": "apt"
  },
  {
    "question": "Amazon Linux: \"apt-get: command not found\"",
    "answer": "If you're using Amazon Linux it's CentOS-based, which is RedHat-based. RH-based installs use yum not apt-get. Something like yum search httpd should show you the available Apache packages - you likely want yum install httpd24.\n\nNote: Amazon Linux 2 has diverged from CentOS since the writing of this answer, but still uses yum.",
    "tag": "apt"
  },
  {
    "question": "Warning: apt-key is deprecated. Manage keyring files in trusted.gpg.d instead",
    "answer": "Adding a key to /etc/apt/trusted.gpg.d is insecure because it adds the key for all repositories.\nThis is exactly why apt-key had to be deprecated.\nShort version\nDo similar to what Signal does.\nIf you want to use the key at https://example.com/EXAMPLE.gpg for a repository listed in /etc/apt/sources.list.d/EXAMPLE.sources, use:\nsudo mkdir -m 0755 -p /etc/apt/keyrings/\n\ncurl -fsSL https://example.com/EXAMPLE.gpg |\n    sudo gpg --dearmor -o /etc/apt/keyrings/EXAMPLE.gpg\n\necho \"Types: deb\nURIs: https://example.com/apt\nSuites: stable\nComponents: main\nSigned-By: /etc/apt/keyrings/EXAMPLE.gpg\" |\n    sudo tee /etc/apt/sources.list.d/EXAMPLE.sources > /dev/null\n\n# Optional (you can find the email address / ID using `apt-key list`)\nsudo apt-key del support@example.com\n\n# Optional (not necessary on most systems)\nsudo chmod 644 /etc/apt/keyrings/EXAMPLE.gpg\nsudo chmod 644 /etc/apt/sources.list.d/EXAMPLE.sources\n\n\nℹ️ You can also embed the key directly into the .sources file! See the section \"Embedding the key\" below.\n\n\nℹ️ We're using the .sources format here, not the old .list format. This is supported on basically all systems today. See the section \"Old one-line format\" below for more info.\n\nLong version\nWhile the deprecation notice recommends adding the key to /etc/apt/trusted.gpg.d, this is an insecure solution, and deprecated as of apt 2.9.24 (released January 2025). To quote this article from Linux Uprising:\n\nThe reason for this change is that when adding an OpenPGP key that's used to sign an APT repository to /etc/apt/trusted.gpg or /etc/apt/trusted.gpg.d, the key is unconditionally trusted by APT on all other repositories configured on the system that don't have a signed-by (see below) option, even the official Debian / Ubuntu repositories. As a result, any unofficial APT repository which has its signing key added to /etc/apt/trusted.gpg or /etc/apt/trusted.gpg.d can replace any package on the system. So this change was made for security reasons (your security).\n\nThe proper solution is explained in that Linux Uprising article and on the Debian Wiki: Store the key in /etc/apt/keyrings/ (or /usr/share/keyrings/ if keys are managed by a package), and then reference the key in the apt source list.\nTherefore, the appropriate method is as follows:\n\nCreate directory\nCreate the directory for PGP keys if it doesn't exist, and set its permissions.\nThis step explicitly sets the recommended permissions, just in case you've changed your umask using sudo's umask_override.\nCreating the directory is actually only necessary in releases older than Debian 12 and Ubuntu 22.04, but it can't hurt to run this line either way.\nsudo mkdir -m 0755 -p /etc/apt/keyrings/\n\n\nDownload key\nDownload the key from https://example.com/EXAMPLE.gpg and store it in /etc/apt/keyrings/EXAMPLE.gpg.\nBy giving options -fsSL to curl we enable error messages, ensure redirects are followed, and reduce output so you can see sudo's password prompt.\nThe Debian wiki explains that you should dearmor the key (i.e. convert it from base64 to binary) for compatibility with older software.\ncurl -fsSL https://example.com/EXAMPLE.gpg |\n    sudo gpg --dearmor -o /etc/apt/keyrings/EXAMPLE.gpg\n\nOptionally, you can verify that the file you downloaded is indeed a PGP key by running file /etc/apt/keyrings/EXAMPLE.gpg and inspecting the output.\n\nRegister repository\nA key has been added, but apt doesn't know about the repository yet.\nTo add the repository, you should create a .sources file in /etc/apt/sources.list.d/ that describes how to use the repository, and where to find the key.\n(You may also have .list files in that directory.\nSee the section \"Old one-line format\" below for more info.)\nThe contents of the created .sources file should look something like this:\nTypes: deb\nURIs: https://example.com/apt\nSuites: stable\nComponents: main\nSigned-By: /etc/apt/keyrings/EXAMPLE.gpg\n\nThe Signed-By field should link to the key you just downloaded.\nIf a repository wants you to specify an architecture, or you want to use multiple components (e.g. main contrib), the contents may instead be something like\nTypes: deb\nURIs: https://example.com/apt\nSuites: stable\nComponents: main universe\nArchitectures: amd64 i386\nSigned-By: /etc/apt/keyrings/EXAMPLE.gpg\n\nA \"flat\" repository doesn't work with suites and components, and instead specifies an exact path.\nIn the DEB822 format, this is represented by setting Suites: to that path, and omitting the Components: field entirely.\nIn this case, the Suites: field must end with a /.\nFor example:\nTypes: deb\nURIs: https://example.com/apt\nSuites: deb/\nSigned-By: /etc/apt/keyrings/EXAMPLE.gpg\n\nIf you are adapting the file from an existing repo, they may be using the old one-line format instead.\nSee the section \"Old one-line format instead\" below for more info.\nFor more examples, see the sources.list(5) man pages.\n\n(optional) Remove old key\nIf you previously added a third-party key with apt-key, you should remove it.\nRun sudo apt-key list to list all the keys, and find the one that was previously added.\nThen, using the key's email address or fingerprint, run sudo apt-key del support@example.com.\n\n(optional) Force-set permissions\nIf you have a custom umask_override set for sudo, or if you use ACLs, files will be created with different permissions than usual.\nIn those cases, explicitly set permissions for EXAMPLE.gpg and EXAMPLE.list to 644.\n\n\nEmbedding the key\napt 2.3.10 and newer support embedding the public key directly in the sources.list.\nYou can check your version of apt by running apt -v.\nDebian 11 \"Bullseye\" LTS (EOL: 2026-08-31) and Ubuntu 20.04 \"Focal Fossa\" (EOL: 2025-04-30) are too old, but Debian 12 \"Bookworm\" and Ubuntu 22.04 \"Jammy Jellyfish\" are good to go!\nTo embed a key, replace the path in Signed-By: /etc/apt/keyrings/EXAMPLE.gpg with the raw key, and delete the file /etc/apt/keyrings/EXAMPLE.gpg.\nImportantly, you must indent each line of the key block by (at least) one space, and you must put an indented . instead of an empty line.\n(Removing the empty line at the start of the key invalidates the key!)\nFor example, you may have a .sources file like below.\n(Real keys should be much longer than this. This one is too short to be secure.)\nTypes: deb\nURIs: https://example.com/apt\nSuites: stable\nComponents: main\nSigned-By:\n -----BEGIN PGP PUBLIC KEY BLOCK-----\n .\n mI0EZWiPbwEEANPyu6pUQEydxvf2uIsuuYOernFUsQdd8GjPE5yjlxP6pNhVlqNo\n 0fjB6yk91pWsoALOLM+QoBp1guC9IL2iZe0k7ENJp6o7q4ahCjJ7V/kO89mCAQ09\n yHGNHRBfbCo++bcdjOwkeITj/1KjYAfQnzH5VbfmgPfdWF4KqS/TmJP9ABEBAAG0\n G0phbmUgRG9lIDxqYW5lQGV4YW1wbGUub3JnPojMBBMBCgA2FiEEK8v49DttJG7D\n 35BwcvTpbeNfCTgFAmVoj28CGwMECwkIBwQVCgkIBRYCAwEAAh4BAheAAAoJEHL0\n 6W3jXwk4YLID/0arCzBy9utS8Q8g6FDtWyJVyifIvdloCvI7hqH51ZJ+Zb7ZLwwY\n /p08+Xnp4Ia0iliwqSHlD7j6M8eBy/JJORdypRKqRIbe0JQMBEcAOHbu2UCUR1jp\n jJTUnMHI0QHWQEeEkzH25og6ii8urtVGv1R2af3Bxi9k4DJwzzXc5Zch\n =8hwj\n -----END PGP PUBLIC KEY BLOCK-----\n\nThe script below will create a new .sources file at /etc/apt/sources.list.d/EXAMPLE.sources with the key at https://example.com/EXAMPLE.gpg embedded into it:\necho \"Types: deb\nURIs: https://example.com/apt\nSuites: stable\nComponents: main\nSigned-By:\n$(wget -O- https://example.com/EXAMPLE.gpg | sed -e 's/^$/./' -e 's/^/ /')\" | sudo tee /etc/apt/sources.list.d/EXAMPLE.sources > /dev/null\n\n# Optional (see above)\nsudo apt-key del support@example.com\nsudo chmod 644 /etc/apt/sources.list.d/EXAMPLE.sources\n\nOld one-line format\nThus far, this answer has used the \"new\" DEB822 format (.sources files) to specify repositories, instead of the old one-line format (.list files).\nThe DEB822 format has been supported since apt 1.1 (released in 2015).\nDebian and Ubuntu plan to use DEB822 as the default format starting late 2023.\nRepolib's documentation has a nice comparison and covers the motivation behind the new format.\nIf you are running Debian 9 \"Stretch\" or newer, or Ubuntu 16.04 \"Xenial Xerus\" or newer, you're good to go!\nThis section is intended for those who cannot use DEB822, or for those who want to migrate to DEB822.\nUsing the old one-line format\nThe following script replaces the DEB822 script from the section \"Short version\", at the top of this answer.\nsudo mkdir -m 0755 -p /etc/apt/keyrings/\n\ncurl -fsSL https://example.com/EXAMPLE.gpg |\n    sudo gpg --dearmor -o /etc/apt/keyrings/EXAMPLE.gpg\n\necho \"deb [signed-by=/etc/apt/keyrings/EXAMPLE.gpg] https://example.com/apt stable main\" |\n    sudo tee /etc/apt/sources.list.d/EXAMPLE.list > /dev/null\n\n# Optional (see above)\nsudo apt-key del support@example.com\nsudo chmod 644 /etc/apt/keyrings/EXAMPLE.gpg\nsudo chmod 644 /etc/apt/sources.list.d/EXAMPLE.list\n\nMigrating to DEB822\nIf you can, you should migrate to DEB822, because that will be the default format in Debian and Ubuntu.\nThat said, as of February 2025, there is no urgency to migrate, because the old one-line format is not deprecated.\nSince apt 2.9.24 (released January 2025), you can run apt modernize-sources to automatically migrate all your .list files to DEB822.\nNote, however, that some fields, such as arch=... are not migrated correctly, so you must add Architectures: ... to your .sources file manually.\nFields with multiple values are space-separated, so multiple architectures would be written as Architectures: amd64 i386.\nAdditional resources\n\nDebian wiki: Instructions to connect to a third-party repository\nAskUbuntu: What commands (exactly) should replace the deprecated apt-key?\nUnix SE: How to add a third-party repo. and key in Debian?\nman 5 sources.list, describes the old one-line format and the DEB822 formats in detail\nman 8 apt-secure, describes how signature checking works",
    "tag": "apt"
  },
  {
    "question": "How can I get a list of repositories 'apt-get' is checking?",
    "answer": "It seems the closest is:\napt-cache policy",
    "tag": "apt"
  },
  {
    "question": "Installing SciPy and NumPy using pip",
    "answer": "This worked for me on Ubuntu 14.04:\nsudo apt-get install libblas-dev liblapack-dev libatlas-base-dev gfortran\npip install scipy",
    "tag": "apt"
  },
  {
    "question": "E: Unable to locate package npm",
    "answer": "From the official Node.js documentation:\n\nA Node.js package is also available in the official repo for Debian Sid (unstable), Jessie (testing) and Wheezy (wheezy-backports) as \"nodejs\". It only installs a nodejs binary.\n\nSo, if you only type sudo apt-get install nodejs , it does not install other goodies such as npm.\nYou need to type:\ncurl -sL https://deb.nodesource.com/setup_14.x | sudo -E bash -\nsudo apt-get install -y nodejs\n\n\nOptional: install build tools\n\n\nTo compile and install native add-ons from npm you may also need to install build tools:\n\nsudo apt-get install -y build-essential\n\nMore info: Docs NodeJs",
    "tag": "apt"
  },
  {
    "question": "E: Repository 'http://dl.google.com/linux/chrome/deb stable Release' changed its 'Origin' value from 'Google, Inc.' to 'Google LLC'",
    "answer": "Solution\nRun:\n$ sudo apt update\n\nNote this is apt and not apt-get\nThen enter y to accept the change:\nE: Repository 'http://dl.google.com/linux/chrome/deb stable Release' changed its 'Origin' value from 'Google, Inc.' to 'Google LLC'\nN: This must be accepted explicitly before updates for this repository can be applied. See apt-secure(8) manpage for details.\nDo you want to accept these changes and continue updating from this repository? [y/N] y\n\nThe error will not show up again.  You need to run sudo apt-get update  and sudo apt-get upgrade as usual to get your updates.\nDetails\nAs pointed out in the question, this error message is meant to ensure that the new entity named Google LLC you are fetching Google Chrome updates from is the same as Google, Inc. that your system trusts and knows its signature.  By accepting the change, you are asking your system to trust Google LLC and accept Google Chrome updates from it in the future.",
    "tag": "apt"
  },
  {
    "question": "How to update-alternatives to Python 3 without breaking apt?",
    "answer": "replace \n[bash:~] $ sudo update-alternatives --install /usr/bin/python python \\\n/usr/bin/python2.7 2\n\n[bash:~] $ sudo update-alternatives --install /usr/bin/python python \\\n/usr/bin/python3.5 3\n\nwith\n[bash:~] $ sudo update-alternatives --install /usr/local/bin/python python \\\n/usr/bin/python2.7 2\n\n[bash:~] $ sudo update-alternatives --install /usr/local/bin/python python \\\n/usr/bin/python3.5 3\n\ne.g. installing into /usr/local/bin instead of /usr/bin.\nand ensure the /usr/local/bin is before /usr/bin in PATH.\ni.e. \n[bash:~] $ echo $PATH\n/usr/local/bin:/usr/bin:/bin\n\nEnsure this always is the case by adding\nexport PATH=/usr/local/bin:$PATH\n\nto the end of your ~/.bashrc file. Prefixing the PATH environment variable with custom bin folder such as /usr/local/bin or /opt/<some install>/bin is generally recommended to ensure that customizations are found before the default system ones.",
    "tag": "apt"
  },
  {
    "question": "What are the differences between node.js and node?",
    "answer": "The package node is not related to node.js.\nnodejs is what you want, however it is arguably better to have the command be called node for compatibility with scripts that use #!/usr/bin/env node.\nYou can either just create a symlink in your path:\nsudo ln -s `which nodejs` /usr/local/bin/node\n\nOr you could install nvm and then use it to install the latest version of node.js:\nnvm install stable\n\nI prefer the nvm method, as it allows you to sudo apt-get remove nodejs, and then manage which version of node you're using yourself. You can also have multiple versions of node.js installed and use nvm use <version> to easily switch between them.\nI also like to add a line to the bottom my .bashrc like: nvm use stable > /dev/null. That will automatically use the latest version you have installed.\nTo update your node version to the latest stable: nvm install stable. Every time you do this you will need to install any npm packages that you had installed globally if you want to continue using them.\nTo switch to an old version just run nvm use <version>, or, if you don't have the old version installed already: nvm install <version>.",
    "tag": "apt"
  },
  {
    "question": "Removing broken packages in Ubuntu",
    "answer": "I faced this problem with a broken package, rvm. I tried many things like sudo apt install -f and sudo dpkg --purge --force-all rvm but nothing worked. Finally I managed to find a blog post that explains how to remove broken packages on Debian/Ubuntu.\nHere are the steps.\n\nFind your package in /var/lib/dpkg/info, for example using: ls -l /var/lib/dpkg/info | grep <package>\n\nMove the package folder to another location, like suggested in the blog post I mentioned before.\n sudo mv /var/lib/dpkg/info/<package>.* /tmp/\n\n\nRun the following command:\n sudo dpkg --remove --force-remove-reinstreq <package>\n\n\n\nSo as an example I solved my problem by executing the following commands in a terminal:\nsudo mv /var/lib/dpkg/info/rvm.* /tmp/\nsudo dpkg --remove --force-remove-reinstreq rvm",
    "tag": "apt"
  },
  {
    "question": "Package php5 have no installation candidate (Ubuntu 16.04)",
    "answer": "Ubuntu 16.04 comes with PHP7 as the standard, so there are no PHP5 packages\nHowever if you like you can add a PPA to get those packages anyways:\nRemove all the stock php packages\nList installed php packages with dpkg -l | grep php| awk '{print $2}' |tr \"\\n\" \" \" then remove unneeded packages with sudo aptitude purge your_packages_here or if you want to directly remove them all use :\nsudo aptitude purge `dpkg -l | grep php| awk '{print $2}' |tr \"\\n\" \" \"`\n\nAdd the PPA\nsudo add-apt-repository ppa:ondrej/php\n\nInstall your PHP Version\nsudo apt-get update\nsudo apt-get install php5.6\n\nYou can install php5.6 modules too ..\nVerify your version\nsudo php -v\n\nBased on https://askubuntu.com/a/756186/532957 (thanks @AhmedJerbi)",
    "tag": "apt"
  },
  {
    "question": "How do you run `apt-get` in a dockerfile behind a proxy?",
    "answer": "UPDATE:\nYou have wrong capitalization of environment variables in ENV. Correct one is http_proxy. Your example should be:\nFROM ubuntu:13.10\nENV http_proxy <HTTP_PROXY>\nENV https_proxy <HTTPS_PROXY>\nRUN apt-get update && apt-get upgrade\n\nor\nFROM centos\nENV http_proxy <HTTP_PROXY>\nENV https_proxy <HTTPS_PROXY>\nRUN yum update \n\nAll variables specified in ENV are prepended to every RUN command. Every RUN command is executed in own container/environment, so it does not inherit variables from previous RUN commands!\nNote: There is no need to call docker daemon with proxy for this to work, although if you want to pull images etc. you need to set the proxy for docker deamon too. You can set proxy for daemon in /etc/default/docker in Ubuntu (it does not affect containers setting).\n\nAlso, this can happen in case you run your proxy on host (i.e. localhost, 127.0.0.1). Localhost on host differ from localhost in container. In such case, you need to use another IP (like 172.17.42.1) to bind your proxy to or if you bind to 0.0.0.0, you can use 172.17.42.1 instead of 127.0.0.1 for connection from container during docker build.\nYou can also look for an example here: How to rebuild dockerfile quick by using cache?",
    "tag": "apt"
  },
  {
    "question": "Installing R from CRAN Ubuntu repository: No Public Key Error",
    "answer": "The simplest solution that worked for me was from Emre Sahin in this thread:\nsudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys E084DAB9",
    "tag": "apt"
  },
  {
    "question": "/bin/sh: 1: apk: not found while creating docker image",
    "answer": "As larsks mentions, apk is for Alpine distributions and you selected FROM ubuntu:trusty which is Debian based with the apt-get command. Change your FROM line to FROM alpine:3.4 to switch to the Alpine based image with apk support.",
    "tag": "apt"
  },
  {
    "question": "How to trust a apt repository : Debian apt-get update error public key is not available: NO_PUBKEY <id>",
    "answer": "I found several posts telling me to run several gpg commands, but they didn't solve the problem because of two things.  First, I was missing the debian-keyring package on my system and second I was using an invalid keyserver.  Try different keyservers if you're getting timeouts!\nThus, the way I fixed it was:\napt-get install debian-keyring\ngpg --keyserver pgp.mit.edu --recv-keys 1F41B907\ngpg --armor --export 1F41B907 | apt-key add -\n\nThen running a new \"apt-get update\" worked flawlessly!",
    "tag": "apt"
  },
  {
    "question": "How to circumvent \"apt-key output should not be parsed\"?",
    "answer": "From apt-key sources, you can set APT_KEY_DONT_WARN_ON_DANGEROUS_USAGE to a non-empty value to disable this warning.\nYou can also use \"grep -q\" so you don't need to redirect stdout in /dev/null either.",
    "tag": "apt"
  },
  {
    "question": "Docker error: Unable to locate package git",
    "answer": "This is happening because the apt repository is not yet updated, it is common practice to clean your apt repositories and tmp files after creating an image, which your base image is probably doing.\nTo fix this, you are going to want to run apt-get update prior to installing git, it is good practice to combine the update and install command at the same time to bust cache on the update if the install line changes:\nRUN apt-get update && apt-get install -y git\n\nUsing -y is convenient to automatically answer yes to all the questions.",
    "tag": "apt"
  },
  {
    "question": "List directory /var/lib/apt/lists/partial is missing. - Acquire (20: Not a directory)",
    "answer": "This answer may be inappropriate here, but as I came here from a search engine - others may land here too.\nIf you're using Docker and you face the same issue you can do the following:\nUSER root\n# RUN commands\nUSER 1001\n\nReference: GitHub",
    "tag": "apt"
  },
  {
    "question": "Caching APT packages in GitHub Actions workflow",
    "answer": "The purpose of this answer is to show how caching can be done with github actions, not necessarily to show how to cache valgrind, (which it does). I also try to explain why not everything can/should be cached, because the cost (in terms of time) of caching and restoring a cache, vs reinstalling the dependency needs to be taken into account.\n\nYou will make use of the actions/cache action to do this.\nAdd it as a step (before you need to use valgrind):\n- name: Cache valgrind\n  uses: actions/cache@v2\n  id: cache-valgrind\n  with:\n      path: \"~/valgrind\"\n      key: ${{secrets.VALGRIND_VERSION}}\n\nThe next step should attempt to install the cached version if any or install from the repositories:\n- name: Install valgrind\n  env:\n    CACHE_HIT: ${{steps.cache-valgrind.outputs.cache-hit}}\n    VALGRIND_VERSION: ${{secrets.VALGRIND_VERSION}}\n  run: |\n      if [[ \"$CACHE_HIT\" == 'true' ]]; then\n        sudo cp --verbose --force --recursive ~/valgrind/* /\n      else\n        sudo apt-get install --yes valgrind=\"$VALGRIND_VERSION\"\n        mkdir -p ~/valgrind\n        sudo dpkg -L valgrind | while IFS= read -r f; do if test -f $f; then echo $f; fi; done | xargs cp --parents --target-directory ~/valgrind/\n      fi\n\nExplanation\nSet VALGRIND_VERSION secret to be the output of:\napt-cache policy valgrind | grep -oP '(?<=Candidate:\\s)(.+)'\n\nthis will allow you to invalidate the cache when a new version is released simply by changing the value of the secret.\ndpkg -L valgrind is used to list all the files installed when using sudo apt-get install valgrind.\nWhat we can now do with this command is to copy all the dependencies to our cache folder:\ndpkg -L valgrind | while IFS= read -r f; do if test -f $f; then echo $f; fi; done | xargs cp --parents --target-directory ~/valgrind/\n\n\nFurthermore\nIn addition to copying all the components of valgrind, it may also be necessary to copy the dependencies (such as libc in this case), but I don't recommend continuing along this path because the dependency chain just grows from there. To be precise, the dependencies needed to copy to finally have an environment suitable for valgrind to run in is as follows:\n\nlibc6\nlibgcc1\ngcc-8-base\n\nTo copy all these dependencies, you can use the same syntax as above:\nfor dep in libc6 libgcc1 gcc-8-base; do\n    dpkg -L $dep | while IFS= read -r f; do if test -f $f; then echo $f; fi; done | xargs cp --parents --target-directory ~/valgrind/\ndone\n\nIs all this work really worth the trouble when all that is required to install valgrind in the first place is to simply run sudo apt-get install valgrind? If your goal is to speed up the build process, then you also have to take into consideration the amount of time it is taking to restore (downloading, and extracting) the cache vs simply running the command again to install valgrind.\n\nAnd finally to restore the cache, assuming it is stored at /tmp/valgrind, you can use the command:\ncp --force --recursive /tmp/valgrind/* /\n\nWhich will basically copy all the files from the cache unto the root partition.\nIn addition to the process above, I also have an example of \"caching valgrind\" by installing and compiling it from source. The cache is now about 63MB (compressed) in size and one still needs to separately install libc which kind of defeats the purpose.\n\nNote: Another answer to this question proposes what I could consider to be a safer approach to caching dependencies, by using a container which comes with the dependencies pre-installed. The best part is that you can use actions to keep those containers up-to-date.\nReferences:\n\nhttps://askubuntu.com/a/408785\nhttps://unix.stackexchange.com/questions/83593/copy-specific-file-type-keeping-the-folder-structure",
    "tag": "apt"
  },
  {
    "question": "pip 10 and apt: how to avoid \"Cannot uninstall X\" errors for distutils packages",
    "answer": "This is the solution I ended up going with, and our apps have been running in production without any issues for close to a month with this fix in place:\nAll I had to do was to add\n--ignore-installed\nto the pip install lines in my dockerfile that were raising errors. Using the same dockerfile example from my original question, the fixed dockerfile would look something like:\nFROM ubuntu:14.04\n\nRUN apt-get -y update && apt-get -y install \\\n    python-pip \\\n    python-numpy # ...and many other packages\n\nRUN pip install -U pip\n\nRUN pip install -r /tmp/requirements1.txt --ignore-installed # don't try to uninstall existing packages, e.g., numpy\nRUN pip install -r /tmp/requirements2.txt\nRUN pip install -r /tmp/requirements3.txt\n\nThe documentation I could find for --ignore-installed was unclear in my opinion (pip install --help simply says \"Ignore the installed packages (reinstalling instead).\"), and I asked about the potential dangers of this flag here, but have yet to get satisfying answer. However, if there are any negative side effects, our production environment has yet to see the effects of them, and I think the risk is low/none (at least that has been our experience). I was able to confirm that in our case, when this flag was used, the existing installation was not uninstalled, but that the newer installation was always used. \nUpdate:\nI wanted to highlight this answer by @ivan_pozdeev. He provides some information that this answer does not include, and he also outlines some potential side-effects of my solution.",
    "tag": "apt"
  },
  {
    "question": "Getting apt-get on an alpine container",
    "answer": "Using multiple package systems is usually a very bad idea, for many reasons. Packages are likely to collide and break and you'll end up with much greater mess than you've started with.\nSee this excellent answer for more detail: Is there a pitfall of using multiple package managers?\nA more feasible approach would be troubleshooting and resolving the issues you are having with apk. apk is designed for simplicity and speed, and should take very little getting used to. It is really an excellent package manager, IMO.\nFor a good tutorial, I warmly recommend the apk introduction page at the Alpine Wiki site:\nhttps://wiki.alpinelinux.org/wiki/Alpine_Linux_package_management\nIf you're determined not to use apk, and for the sake of experiment want try bringing up apt instead, as a first step, you'll have first to build apt from source: https://github.com/Debian/apt. Then, if it is produces a functional build (not likely since it's probably not compatible with musl libc), you'll have to wire it to some repositories, but Alpine repositories are only fit  for apk, not apt. As you can see, this is not really feasible, and not the route you want to go to.",
    "tag": "apt"
  },
  {
    "question": "gcc: error trying to exec 'cc1plus': execvp: No such file or directory",
    "answer": "The following worked for me:\nsudo apt-get install g++",
    "tag": "apt"
  },
  {
    "question": "Benefits of repeated apt cache cleans",
    "answer": "The main reason people do this is to minimise the amount of data stored in that particular docker layer. When pulling a docker image, you have to pull the entire content of the layer.\nFor example, imagine the following two layers in the image:\nRUN apt-get update\nRUN rm -rf /var/lib/apt/lists/*\n\nThe first RUN command results in a layer containing the lists, which will ALWAYS be pulled by anyone using your image, even though the next command removes those files (so they're not accessible). Ultimately those extra files are just a waste of space and time.\nOn the other hand,\nRUN apt-get update && rm -rf /var/lib/apt/lists/*\n\nDoing it within a single layer, those lists are deleted before the layer is finished, so they are never pushed or pulled as part of the image.\nSo, why have multiple layers which use apt-get install? This is likely so that people can make better use of layers in other images, as Docker will share layers between images if they're identical in order to save space on the server and speed up builds and pulls.",
    "tag": "apt"
  },
  {
    "question": "How to fix: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY",
    "answer": "I was able to resolve this with:\napt-key adv --keyserver keyserver.ubuntu.com --recv-keys 605C66F00D6C9793 \\\n    0E98404D386FA1D9 648ACFD622F3D138\n\nLooks like the ubuntu servers also contain the debian keys.",
    "tag": "apt"
  },
  {
    "question": "Auto yes to the License Agreement on sudo apt-get -y install oracle-java7-installer",
    "answer": "try this out:\nsudo add-apt-repository -y ppa:webupd8team/java\nsudo apt-get update\necho debconf shared/accepted-oracle-license-v1-1 select true | sudo debconf-set-selections\necho debconf shared/accepted-oracle-license-v1-1 seen true | sudo debconf-set-selections\nsudo apt-get -y install oracle-java7-installer \n\nrunning 3rd and 4th command on my debian 7.1 helps, so I think the same can help on ubuntu as well",
    "tag": "apt"
  },
  {
    "question": "\"E: Unable to locate package python3-pip\"",
    "answer": "For Ubuntu 18.04 (Bionic Beaver), with Python 3.6.5, the python-pip package will install after:\nsudo add-apt-repository universe\nsudo apt-get update\n\nWhich is enabling the category universe\ndeb http://archive.ubuntu.com/ubuntu bionic main universe\ndeb http://archive.ubuntu.com/ubuntu bionic-security main universe\ndeb http://archive.ubuntu.com/ubuntu bionic-updates main universe",
    "tag": "apt"
  },
  {
    "question": "How to remove a snap application (docker) completely",
    "answer": "I had the same problem. This works for me.\nsudo snap remove docker\n\nsudo reboot \n\nthe point is to restart the instance or terminal.\nI hope this method can help",
    "tag": "apt"
  },
  {
    "question": "How can I get Chef to run apt-get update before running other recipes",
    "answer": "You can include the apt recipe in the very beginning:\ninclude_recipe 'apt'\n\nthis will run the update command.",
    "tag": "apt"
  },
  {
    "question": "How to compare Debian package versions?",
    "answer": "Perhaps because the title doesn't mention Python (though the tags do), Google brought me here when asking the same question but hoping for a bash answer.  That seems to be:\n$ dpkg --compare-versions 11a lt 100a && echo true\ntrue\n$ dpkg --compare-versions 11a gt 100a && echo true\n$ \n\nTo install a version of rubygems that's at least as new as the version from lenny-backports in a way that gives no errors on lenny and squeeze installations:\nsudo apt-get install rubygems &&\nVERSION=`dpkg-query --show --showformat '${Version}' rubygems` &&\ndpkg --compare-versions $VERSION lt 1.3.4-1~bpo50+1 &&\nsudo apt-get install -t lenny-backports rubygems\n\nPerhaps I should have asked how to do that in a separate question, in the hope of getting a less clunky answer.",
    "tag": "apt"
  },
  {
    "question": "When gcc-11 will appear in Ubuntu repositories?",
    "answer": "sudo apt install build-essential manpages-dev software-properties-common\nsudo add-apt-repository ppa:ubuntu-toolchain-r/test\nsudo apt update && sudo apt install gcc-11 g++-11\n\nThen use update-alternatives to set default gcc...\nsudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-9 90 --slave /usr/bin/g++ g++ /usr/bin/g++-9 --slave /usr/bin/gcov gcov /usr/bin/gcov-9 --slave /usr/bin/gcc-ar gcc-ar /usr/bin/gcc-ar-9 --slave /usr/bin/gcc-ranlib gcc-ranlib /usr/bin/gcc-ranlib-9  --slave /usr/bin/cpp cpp /usr/bin/cpp-9 && \\\n\nsudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-11 110 --slave /usr/bin/g++ g++ /usr/bin/g++-11 --slave /usr/bin/gcov gcov /usr/bin/gcov-11 --slave /usr/bin/gcc-ar gcc-ar /usr/bin/gcc-ar-11 --slave /usr/bin/gcc-ranlib gcc-ranlib /usr/bin/gcc-ranlib-11  --slave /usr/bin/cpp cpp /usr/bin/cpp-11;\n\nTo sample check settings to see which gcc is default you can run the following, if they show correct resuslts then the rest are fine...\ngcc --version;g++ --version;gcov --version;\n\nTo reconfigure to any previous gcc version...\nsudo update-alternatives --config gcc\n\nYou can do this on any version of ubuntu,... enjoy!\nHere are my 6 different gcc's living side by side with the default being gcc-11:\n$ sudo update-alternatives --config gcc\nThere are 6 choices for the alternative gcc (providing /usr/bin/gcc).\n\n  Selection    Path             Priority   Status\n------------------------------------------------------------\n* 0            /usr/bin/gcc-11   1010      auto mode\n  1            /usr/bin/gcc-10   1000      manual mode\n  2            /usr/bin/gcc-11   1010      manual mode\n  3            /usr/bin/gcc-5    40        manual mode\n  4            /usr/bin/gcc-7    700       manual mode\n  5            /usr/bin/gcc-8    800       manual mode\n  6            /usr/bin/gcc-9    900       manual mode\n\nPress <enter> to keep the current choice[*], or type selection number:",
    "tag": "apt"
  },
  {
    "question": "what is the function of /etc/apt/sources.list.d?",
    "answer": "The function of the /etc/apt/sources.list.d directory is as follows:\nUsing the directory you can easily add new repositories without the need to edit the central /etc/apt/sources.list file. I.e. you can just put a file with a unique name and the same format as /etc/apt/sources.list into this folder and it is used by apt.\nIn order to remove this source again you can just remove that specific file without the need for handling side effects, parsing  or mangling with /etc/apt/sources.list. It's mainly for scripts or other packages to put their repositories there automatically - if you manually add repositories you could add them to /etc/apt/sources.list manually.\nThis answers your question, however, it won't solve your problem. APT is complaining about a missing GPG key which you have to manually import before you can use your newly added repository (GPG verifies all data cryptographically and needs the public keys of the owners for this).\nThis can be done calling sudo apt-key add public-key-file or wget -qO - http://example.com/archive.key | sudo apt-key add - where http://example.com/archive.keyis the URL for the public key (which you should verify before using).\nIn case of llvm, you could issue wget -O - http://llvm.org/apt/llvm-snapshot.gpg.key|sudo apt-key add - (according to http://llvm.org/apt/)\nFore more details see\n\nHow to add a GPG key to the apt sources keyring?",
    "tag": "apt"
  },
  {
    "question": "Android project with Java and Kotlin files, kapt or annotationProcessor?",
    "answer": "First of all, the Kotlin Annotation processing tool (kapt) uses the Java compiler to run annotation processors. If your project contains any Java classes, kapt takes care of them by design. Kotlinlang recommends using kapt incase you used annotationProcessor from the Android Support before.\nJetBrains has a nice article about how kapt works in more detail, its from 2015 but UP-TO-DATE.",
    "tag": "apt"
  },
  {
    "question": "debian apt packages hash sum mismatch",
    "answer": "Try using apt-get:\napt-get clean\nrm -rf /var/lib/apt/lists/*\napt-get clean\napt-get update\napt-get upgrade",
    "tag": "apt"
  },
  {
    "question": "JDK is installed on mac but i'm getting \"The operation couldn’t be completed. Unable to locate a Java Runtime that supports apt.\" sudo apt update",
    "answer": "20 years ago, java shipped with a tool called apt: Annotation Processor Tool. This tool was obsolete not much later.\nWhat that update-node-js-version is talking about, is a completely and totally unrelated tool: It's the Advanced Package Tool, which is a tool to manage installations on debian and ubuntu - linux distros. You do not want to run this on a mac, and the instructions you found are therefore completely useless: That is how to update node-js on linux. Your machine isn't linux.\nSearch around for answers involving brew, which is the go-to equivalent of apt on mac. And completely forget about java - this has NOTHING to do with java - that was just a pure coincidence.",
    "tag": "apt"
  },
  {
    "question": "How can I manage keyring files in trusted.gpg.d with ansible playbook since apt-key is deprecated?",
    "answer": "In short, you need to put the GPG key with the correct extension into a separate directory that is not searched by default, and point your repository configuration at that specific file.\nFor more info on why you need a separate directory, see this answer to \"Warning: apt-key is deprecated. Manage keyring files in trusted.gpg.d instead\".\nWarning: apt will not accept ASCII GPG keys saved with .gpg extension.\nYou can verify whether you have the old ASCII GPG format(.asc) or the newer binary GPG format(.gpg) via file:\n# file elastic-old.gpg\nelastic-old.gpg: PGP public key block Public-Key (old)\n\n# file elastic.gpg    \nelastic.gpg: PGP/GPG key public ring (v4) created Mon Sep 16 17:07:54 2013 RSA (Encrypt or Sign) 2048 bits MPI=0xd70ed6cd267c5b3e...\n\nIf your key is the old format, you can either use the .asc extension, or you can optionally de-armor it via gpg --dearmor elastic.gpg into the new binary format and use the .gpg extension.\nThe de-armor step is annoying for ansible automation, so I suggest you just use whatever format upstream provides as is.\nOn Ubuntu 22.04, there's a folder you're expected to use that is not preloaded - /etc/apt/keyrings - or you can create your own directory and use that.\nAs for the Ansible part, you can use get_url or file to push the GPG key onto the system, and then use apt_repository like before to add the repo, with the addition of specifying the keyring.\nUsing the binary GPG format with .gpg\n- name: Add Example GPG key\n  ansible.builtin.get_url:\n    url: https://example.com/example.gpg\n    dest: /etc/apt/keyrings/example.gpg\n    mode: '0644'\n    force: true\n\nOr using the .asc extension if upstream still hasn't switched over yet\n- name: Add Example GPG key\n  ansible.builtin.get_url:\n    url: https://example.com/example.gpg\n    dest: /etc/apt/keyrings/example.asc\n    mode: '0644'\n    force: true\n\nThen you can define your repository via apt_repository module like before.\n- name: Add Example repo\n  ansible.builtin.apt_repository:\n    filename: example-repo\n    repo: 'deb [signed-by=/etc/apt/keyrings/example.gpg] https://example.com/packages/8.x/apt stable main'\n\nKeep in mind that apt_repository uses the old .list format instead of the new DEB822 compliant .sources format.\nIf you want/need to use the newer DEB822 format and you happen to be running Ansible 2.15 or newer, you should use the deb822_repository module.\nIf you are stuck using older Ansible core, you can template it yourself similarly to this:\ntasks/main.yaml\n- name: Add Elastic repo\n  notify: apt update force\n  ansible.builtin.template:\n    src: repo.j2\n    dest: /etc/apt/sources.list.d/elastic-8.x.sources\n    mode: '0644'\n  vars:\n    repo_name: Example PPA\n    repo_uris: https://example.com/packages/8.x/apt\n    repo_suites: stable\n    repo_components: main\n    repo_signed_by: /etc/apt/keyrings/example.gpg\n\ntemplates/repo.j2\nX-Repolib-Name: {{ repo_name }}\nTypes: deb\nURIs: {{ repo_uris }}\nSuites: {{ repo_suites }}\n{% if repo_components is defined %}\nComponents: {{ repo_components }}\n{% endif %}\nSigned-By: {{ repo_signed_by }}",
    "tag": "apt"
  },
  {
    "question": "Why cannot add PPA deadsnakes?",
    "answer": "You're probably behind a corporate proxy and to add -E to your sudo command to preserve the environment variables.\n$ sudo add-apt-repository -y 'ppa:deadsnakes/ppa'\nCannot add PPA: 'ppa:~deadsnakes/ubuntu/ppa'.\nERROR: '~deadsnakes' user or team does not exist.\n$ sudo -E add-apt-repository -y 'ppa:deadsnakes/ppa'\n This PPA contains more recent Python versions packaged for Ubuntu.\n\nDisclaimer: there's no guarantee of timely updates in case of security problems or other issues. If you want to use them in a security-or-otherwise-critical environment (say, on a production server), you do so at your own risk.\n\nUpdate Note\n===========\n...",
    "tag": "apt"
  },
  {
    "question": "Requirements file for apt-get, similar to pip",
    "answer": "Your question is that you want to have a list of system dependences in one file, for being able to install it with one command. \nI don't recomend you to include the version of a package in the system dependencies. In the soft system dependences like \"build-essential\" or \"uuid-dev\" you normally want the latest version of the package. In the \"hard dependeces\" like python, postgres or whatever, normally the version is specified in the name of the package itself, like \"python2.6-dev\" or \"postgresql-8.4\". Another problem you may have defining the exact version of the package is that maybe the version 8.4.11-1 of postgresql-8.4 will not be available in the repository in three months or in a year, and you will end up installing the current version in the repo.\nExample. You can create a file named \"requirements.system\" with the system packages you need for you project:\npython-virtualenv\npython2.6-dev\nuuid-dev\npython-pip\npostgresql-8.4\n\nThen, in your INSTALL file explain how to install the system packages.\n# Install system depencences by running\ncat ~/project/install/requirements.system | xargs sudo aptitude install\n\nWe have running this configuration for about two years, having to recreate the enviroment from the scrach a few times and we never had a problem.",
    "tag": "apt"
  },
  {
    "question": "The repository 'http://dl.google.com/linux/chrome/deb stable Release' is not signed",
    "answer": "You don't. You must wait for Google to renew their keys and for an update.\nThe important message is: \n\nThe following signatures were invalid: EXPKEYSIG 1397BC53640DB551 Google Inc. (Linux Packages Signing Authority) \n\nIt means that the cryptographic signature is invalid. The source of this can be an attack, a misconfiguration, or other kind of technical problem. Forcing your system to update will result in running an unverified version of your web browser, which can expose you to a lot of security troubles.",
    "tag": "apt"
  },
  {
    "question": "Doxygen - Could NOT find FLEX (missing: FLEX_EXECUTABLE)",
    "answer": "It seems like apt autoremove really removed the package I just installed in the previous step.\nSo what worked for me was:\ngit clone https://github.com/doxygen/doxygen.git\ncd doxygen\nmkdir build\ncd build\n\nThese are new:  \n\nsudo apt-get install flex\nsudo apt-get install bison\n\n\ncmake -G \"Unix Makefiles\" ..\nmake\n\n...but of course the horror wouldn't end there, see my next question :D",
    "tag": "apt"
  },
  {
    "question": "How to install \"make\" in ubuntu?",
    "answer": "I have no idea what linux distribution \"ubuntu centOS\" is. Ubuntu and CentOS are two different distributions.\nTo answer the question in the header:\nTo install make in ubuntu you have to install build-essentials\nsudo apt-get install build-essential",
    "tag": "apt"
  },
  {
    "question": "How to install build dependencies directly from the debian/control file?",
    "answer": "Try mk-build-deps from devscripts package.\nmk-build-deps --install <controfile>\n\nPS: Make sure you have package equivs installed also.",
    "tag": "apt"
  },
  {
    "question": "how to use kapt in androidTest scope",
    "answer": "As described in the documentation, it's kaptAndroidTest (and kaptTest for unit tests).",
    "tag": "apt"
  },
  {
    "question": "Debianzing a Python program to get a .deb",
    "answer": "I just tested stdeb (see https://pypi.python.org/pypi/stdeb), a Python package for turning any other Python package into a Debian package.\nFirst I installed stdeb:\napt-get install python-stdeb\n\nThen I made a simple script called myscript.py with the following contents:\ndef main():\n    print \"Hello world, says myscript!\"\n    # wait for input from the user\n    raw_input()\n\nif __name__ == '__main__':\n    main()\n\nImportantly, your directory structure should be:\nsomewhere/myscript/\n    setup.py\n    myscript/\n        __init__.py\n        myscript.py\n\nIn the setup.py file, you do something like:\nimport os\nfrom setuptools import setup\nfrom nvpy import nvpy\n\nsetup(\n    name = \"myscript\",\n    version = \"1.0\",\n    author = \"Charl P. Botha\",\n    author_email = \"cpbotha@vxlabs.com\",\n    description = \"Demo of packaging a Python script as DEB\",\n    license = \"BSD\",\n    url = \"https://github.com/cpbotha/nvpy\",\n    packages=['myscript'],\n    entry_points = {\n        'console_scripts' : ['myscript = myscript.myscript:main']\n    },\n    data_files = [\n        ('share/applications/', ['vxlabs-myscript.desktop'])\n    ],\n    classifiers=[\n        \"License :: OSI Approved :: BSD License\",\n    ],\n)\n\nThe console_scripts directive is important, and it'll create an executable script called my_script, which will be available system-wide after you install the resultant DEB. If your script uses something like tkinter or wxpython and has a graphical user interface, you should use gui_scripts instead of console_scripts.\nThe data_files directive will install a suitable desktop file into /usr/share/applications, so that you can also start myscript from your desktop environment. vxlabs-myscript.desktop looks like this:\n[Desktop Entry]\nVersion=1.0\nType=Application\nName=myscript\nComment=Minimal stdeb example\n# myscript should wait for user input at the end, else the terminal\n# window will disappear immediately.\nExec=myscript\nIcon=/usr/share/icons/gnome/48x48/apps/file-manager.png\nCategories=Utility;\n# desktop should run this in a terminal application\nTerminal=true\nStartupNotify=true\nStartupWMClass=myscript\n\nTo build the DEB, you do the following in the top-level myscript:\npython setup.py --command-packages=stdeb.command bdist_deb\n\nWhich will create a .deb in the deb_dist directory.\nAfter having installed the DEB I created like this, I could run myscript from the command-line, and I could also invoke it from my desktop environment.\nHere's a GitHub repository with the example code above: https://github.com/cpbotha/stdeb-minimal-example",
    "tag": "apt"
  },
  {
    "question": "After Ubuntu 18.04 upgrade php7.2-curl cannot be installed",
    "answer": "This can save you:\nsudo add-apt-repository ppa:ondrej/php\nsudo apt update\nsudo apt install php7.2-fpm php7.2-gd php7.2-curl php7.2-mysql php7.2-dev php7.2-cli php7.2-common php7.2-mbstring php7.2-intl php7.2-zip php7.2-bcmath\n\nThen sudo service apache2 restart",
    "tag": "apt"
  },
  {
    "question": "``apt-mark hold`` and ``apt-mark unhold`` with ansible modules",
    "answer": "You can use the ansible.builtin.dpkg_selections module for this.\nNote that unhold translates to install in this context.\n- name: Hold kubeadm\n  ansible.builtin.dpkg_selections:\n    name: kubeadm\n    selection: hold\n\n- name: Unhold kubeadm\n  ansible.builtin.dpkg_selections:\n    name: kubeadm\n    selection: install",
    "tag": "apt"
  },
  {
    "question": "Stuck with apt --fix-broken install (libc6:amd64 package post-installation)",
    "answer": "I ran into the same problem, and this procedure from this bug report fixed it:\n\nEdit /var/lib/dpkg/info/libc6\\:amd64.postinst\nand comment out the line\n# set -e\nso $ apt-get -f install can continue.",
    "tag": "apt"
  },
  {
    "question": "GPG error: https://apt.releases.hashicorp.com bionic InRelease: The following signatures couldn't be verified because the public key is not available",
    "answer": "This means that the gpg key for this HashiCorp repository is not available in the apt-key database.\nAs the fix, it can be re-added with the below commands.\n# GPG is required for the package signing key\nsudo apt install gpg\n\n# Download the signing key to a new keyring\nwget -O- https://apt.releases.hashicorp.com/gpg | gpg --dearmor | sudo tee /usr/share/keyrings/hashicorp-archive-keyring.gpg\n\n# Verify the key's fingerprint\ngpg --no-default-keyring --keyring /usr/share/keyrings/hashicorp-archive-keyring.gpg --fingerprint\n\n# The fingerprint must match 798A EC65 4E5C 1542 8C8E 42EE AA16 FCBC A621 E701, which can also be verified at https://www.hashicorp.com/security under \"Linux Package Checksum Verification\".\n\n# Add the HashiCorp repo\necho \"deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/hashicorp.list\n\n# apt update successfully\nsudo apt update\n\nNote that these commands were taken from Hashicorp's Official Packaging Guide.",
    "tag": "apt"
  },
  {
    "question": "Listing all user-installed packages in Debian",
    "answer": "This command may shorten your work:\napt-mark showmanual\n\nIt is supposed to show what packages were installed \"manually\". It is not 100% reliable though, as many automatically installed packages are flagged as manually installed (because of reasons too long to describe here).\nYou may also (if allowed) run security tools such as clamav and/or rkhunter to scan your computer for malicious programs.",
    "tag": "apt"
  },
  {
    "question": "Install package in running docker container",
    "answer": "You can use docker commit:\n\nStart your container sudo docker run IMAGE_NAME\nAccess your container using bash: sudo docker exec -it CONTAINER_ID bash\nInstall whatever you need inside the container\nExit container's bash\nCommit your changes: sudo docker commit CONTAINER_ID NEW_IMAGE_NAME\n\nIf you run now docker images, you will see NEW_IMAGE_NAME listed under your local images.\nNext time, when starting the docker container, use the new docker image  you just created:\nsudo docker run **NEW_IMAGE_NAME** - this one will include your additional installations.\nAnswer based on the following tutorial: How to commit changes to docker image",
    "tag": "apt"
  },
  {
    "question": "dpkg error: pycompile: not found",
    "answer": "(EDIT: works in Python 3.11 down to 3.5)\nAhhhh, yes.  The venerable \"Catch-22\" situation of Debian package management.  It boils down to Debian depending on py3compile as part of the libpython3.x package when that package provides the py3compile as well; it's a no-go, there.\nI too was bitten by this unable to get py3compile working again for I had too deleted the entire /usr/[/local]/lib/python3* directories.\nOnce done, nothing in Debian package management tool can help you get back to a working Python3 environment.  You must do meat-ball surgery.\nReconstruction of Python3 in Debian entails three critical things:\n\nRestoring py3compile script (for most of you, you already have this)\nRestoring libpython3.7\nRestoring python binary\n\nOne could do the RE-copying of /usr[/local]/lib directory from another working Debian host/system. But this time, I shall detail the steps from within the broken host in question (as if you do NOT have another working host).\nStep 1 - Download Packages\nDownload the impacted Debian packages:\ncd /tmp\napt-get download libpython3.7-minimal\napt-get download python3.7-minimal\napt-get download python3-minimal # (this is important)\napt-get download libpython3.7-stdlib\napt-get download python3.7\n\nStep 2 - Cleanup\nClean up old stuff\nrm -rf /usr[/local]/lib/python3.7*\nrm -rf /usr[/local]/bin/python3.7*\nupdate-alternatives --remove python3 /usr[/local]/bin/python3.7\nhash -r  # removes cached python3 binary path\n\nStep 3 - Extract files from packages\nLet us extract the missing py3compile\ncd /tmp\ndpkg-deb -x python3-minimal_3.7.3-1_amd64.deb missing\ndpkg-deb -x python3.7-minimal_3.7.3-2_amd64.deb missing\ndpkg-deb -x libpython3.7-minimal_3.7.3-2_amd64.deb missing\ndpkg-deb -x libpython3.7-stdlib_3.7.3-2_amd64.deb missing\ndpkg-deb -x python3.7_3.7.3-2_amd64.deb missing\n\nStep 4 - Restoring Python, et. al.\nManually install over your root filesystem\ncd /tmp/missing\nls -lR /tmp/missing  # if you are curious about overwriting your HD\nsudo cp -rpfv /tmp/missing/*  /\n\nStep 5 - Verification\nStart up Python3\npython3\nPython 3.7.3 (default, Apr  3 2019, 05:39:12) \n[GCC 8.3.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>>\n\nTest import and show version\n>>> import sys\n>>> print(sys.version_info)\nsys.version_info(major=3, minor=7, micro=3, releaselevel='final', serial=0)\n>>>\n>>> quit()\n\nStep 6 - Clean up ourselves\nrm -rf /tmp/missing\n\nStep 7 - Officially reinstall Python via Debian APT\ndpkg -s -a  | grep  reinstreq\n# Any listing also needs to be reinstalled along with python3\napt-get install --reinstall python3\n\nMost likely, you got MANY packages that are in that stuck state of \"reinstreq\" state.\napt-get autoclean\napt-get autoremove\n# (MANY PACKAGES FAILED TO BE INSTALLED)\n\nAt this point, you will have to manually reinstall each and every one of those listed by apt-get autoremove...\napt-get install --fix-broken --reinstall <list-of-many-failed-packages>\n\nLast Step - Reinstalling impacted half-state Debian packages\nLet me guess, you got the following error:\nE: Internal Error, No file name for XXXXXX\n\nI will tell you that you probably had a newer Debian release in your /etc/apt/sources.list for awhile, it went all down south (bad), and took that newer release out of the sources.list file (in effort to revert back to a 'stable' release): that's not an unrecoverable thing to me here, just that you jerked the Debian package database around a bit there ... rather brusquely.\nThe resolution of the last step entails a restoration and stabilization of the Debian package management database by reinstalling nearly everything.  I will detail it later but the link to follow is given here.",
    "tag": "apt"
  },
  {
    "question": "python add-apt-repository: ImportError: No module named 'apt_pkg'",
    "answer": "I had something quite different than this. Mine failed with\n\nNo module named 'softwareproperties'\n\nMy solution is:\nsudo vim /usr/bin/add-apt-repository\nChange package header from `python3` to `python3.4` (or lower)\n\nThis may happen when you recently upgraded or added another python3 package.",
    "tag": "apt"
  },
  {
    "question": "How to install a package using the python-apt API",
    "answer": "It's recommended to use the apt module from the python-apt Debian package.  This is a higher level wrapper around the underlying C/C++ libapt-xxx libraries and has a Pythonic interface.\nHere's an example script which will install the libjs-yui-doc package:\n#!/usr/bin/env python\n# aptinstall.py\n\nimport apt\nimport sys\n\npkg_name = \"libjs-yui-doc\"\n\ncache = apt.cache.Cache()\ncache.update()\ncache.open()\n\npkg = cache[pkg_name]\nif pkg.is_installed:\n    print \"{pkg_name} already installed\".format(pkg_name=pkg_name)\nelse:\n    pkg.mark_install()\n\n    try:\n        cache.commit()\n    except Exception, arg:\n        print >> sys.stderr, \"Sorry, package installation failed [{err}]\".format(err=str(arg))\n\nAs with the use of apt-get, this must be run with superuser privileges to access and modify the APT cache.\n$ sudo ./aptinstall.py\n\nIf you're attempting a package install as part of a larger script, it's probably a good idea to only raise to root privileges for the minimal time required.\nYou can find a small example in the /usr/share/pyshared/apt/progress/gtk2.py:_test() function showing how to install a package using a GTK front-end.",
    "tag": "apt"
  },
  {
    "question": "How to make apt-get accept New config files in a unattended install of debian from Repo",
    "answer": "I think the apt-get command line you're looking for is\napt-get -o Dpkg::Options::=\"--force-confnew\" install your-package\n\nAs for your sudo -S issue, maybe try adding some spaces around the '|' character.\nIs not, the question How to pass the password to su/sudo/ssh without overriding the TTY? may be useful.",
    "tag": "apt"
  },
  {
    "question": "Checking for installed packages and if not found install",
    "answer": "Try the following code :\nif ! rpm -qa | grep -qw glibc-static; then\n    yum install glibc-static\nfi\n\nor shorter :\nrpm -qa | grep -qw glibc-static || yum install glibc-static\n\n\nFor debian likes : \ndpkg -l | grep -qw package || apt-get install package\n\nFor archlinux :\npacman -Qq | grep -qw package || pacman -S package",
    "tag": "apt"
  },
  {
    "question": "\"InRelease: Clearsigned file isn't valid, got 'NODATA' (does the network require authentication?)\" when updating package list",
    "answer": "One way for that is to remove dl.google.com from your software update repositories, \nfor that, use System Settings -> Software & Update -> Other Software\nand uncheck dl.google.com \nthen run\nsudo apt-get update\n\nI could not find a way to keep google in circle because I live in Iran & google refuses service to users in Iran.",
    "tag": "apt"
  },
  {
    "question": "\"gpgkeys: key 7F0CEB10 not found on keyserver\" Response while try to install mongodb on Ubuntu",
    "answer": "A way to go around firewalls: query over typical port (80):\nsudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 7F0CEB10\nAnother problem you may have would be a network proxy. If you need to connect to the internet through proxy, export proxy settings to you bash configuration - add line with your proxy address (and if needed: username/password - without it's just http://proxyserver:port): \nexport http_proxy=http://username:password@proxyserver:port/\nat the end of file /etc/bash.bashrc",
    "tag": "apt"
  },
  {
    "question": "How do I completely clean-up or reset a Python installation on Ubuntu",
    "answer": "Helpful and cautionary tale, be a good idea not to delete it I think... I'll assume it was the profanity that got it deleted so I've edited it out.\n!!!! WARNING !!!!\nIf you're like me and you think ah shur I'll do an 'apt list --installed | grep -i python' and then 'apt purge -y' all that crap, well, maybe don't.\nNow my entire system is doo-doo. It all seemed fine until I rebooted and now there is no network connection, netplan and a bunch of other stuff is just gone. No recovery possible.\nActually it looks like it was an 'apt autoremove' AFTER I'd done the above that actually removed netplan.\nAnd all because I wanted to remove multiple python versions to get over those gosh darn import issues and messin' around with pip and pip3 and pip3432432 and what gosh flippin' version of python is tied to what oody doody version of pip...\nUPDATE - This video helped me recover me files https://youtu.be/tGIPeWkPkMc",
    "tag": "apt"
  },
  {
    "question": "apt-get install via tunnel proxy but ssh only from client side",
    "answer": "Setup:\nComputer A\n\nHas access to Internet\nHas access to Computer B\nSSH is installed\n\nComputer B\n\nDoesn't have access to Internet\nOpenSSH Server is installed\n\n\nSteps:\n\nssh into Computer B from Computer A\nsudo ssh -R <selected port>:us.archive.ubuntu.com:80 user@computerb.host\n\nEdit Computer B's /etc/apt/apt.conf to include the following lines:\nAcquire::http::Proxy \"http://localhost:<selected port>\";\nAcquire::https::Proxy \"https://localhost:<selected port>\";\n\nRun your apt-get update or install or upgrade on Computer B and it should work.\n\n\nA few notes:\n\nYou HAVE to keep the original session of ssh from Computer A to Computer B active while using Computer B to access apt-get repositories.\nYou DON'T have to use the same ssh connection to utilize the tunnel (meaning if you have multiple ssh connection into Computer B, they should all work)\n\n\nUsing Putty\nThis can also be achieved using Putty (assuming that Computer A is the Windows machine).  \n\nWhen starting the session, select SSH --> Tunnels\nSource Port: <selected port>\nDestination: us.archive.ubuntu.com:80\nSelect the \"Remote\" radio button\nSelect \"Add\" button\nConfigure your session as you normally would.  \nFollow steps 2 & 3 above",
    "tag": "apt"
  },
  {
    "question": "Dependency issue installing google-chrome-stable through Ubuntu docker",
    "answer": "wget http://archive.ubuntu.com/ubuntu/pool/main/libu/libu2f-host/libu2f-udev_1.1.4-1_all.deb\ndpkg -i libu2f-udev_1.1.4-1_all.deb\n\nand retry update chrome",
    "tag": "apt"
  },
  {
    "question": "wget command not found on linux server",
    "answer": "Given your kernel version, it looks like your Linux distribution is CentOS 6 or RHEL 6. Try installing wget with this command:\nyum install wget\n\nYou must be root when you run this command.",
    "tag": "apt"
  },
  {
    "question": "Invalid Signature for MySQL repository",
    "answer": "Was getting similar error\nErr:2 http://repo.mysql.com/apt/ubuntu bionic InRelease\n  The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 467B942D3A79BD29\nFetched 20.0 kB in 1s (26.1 kB/s)\n\nTry to remove the previous key and add it again:\n$ sudo apt-key list\n\n/etc/apt/trusted.gpg\n--------------------\npub   rsa4096 2021-12-14 [SC] [expires: 2023-12-14]\n      859B E8D7 C586 F538 430B  19C2 467B 942D 3A79 BD29\nuid           [ unknown] MySQL Release Engineering <mysql-build@oss.oracle.com>\nsub   rsa4096 2021-12-14 [E] [expires: 2023-12-14]\n\nsudo apt-key del A4A9 4068 76FC BD3C 4567  70C8 8C71 8D3B 5072 E1F5                                                                                                                                                                                                                                          \n\nsudo apt-key adv --keyserver pgp.mit.edu --recv-keys 467B942D3A79BD29\n\nAfter that sudo apt-get update is working fine.",
    "tag": "apt"
  },
  {
    "question": "Generating JPA2 Metamodel from a Gradle build script",
    "answer": "While I have no problem with the use gradle makes of Ant, I agree with the original poster that it is undesirable in this case.   I found a github project by Tom Anderson here that describes what I believe is a better approach.  I modified it a small amount to fit my needs (output to src/main/generated) so that it looks like:\nsourceSets {\n     generated\n}\n\nsourceSets.generated.java.srcDirs = ['src/main/generated']\n\nconfigurations {\n     querydslapt\n}\n\ndependencies {     \n    compile 'mine go here'\n    querydslapt 'com.mysema.querydsl:querydsl-apt:2.7.1'\n}\n\ntask generateQueryDSL(type: Compile, group: 'build', description: 'Generates the QueryDSL query types') {\n         source = sourceSets.main.java\n         classpath = configurations.compile + configurations.querydslapt\n         options.compilerArgs = [\n                \"-proc:only\",\n                \"-processor\", \"com.mysema.query.apt.jpa.JPAAnnotationProcessor\"\n         ]\n         destinationDir = sourceSets.generated.java.srcDirs.iterator().next()\n}\ncompileJava.dependsOn generateQueryDSL\n\nThis approach makes a lot more sense to me than the other, if it does to you too, then you have another option for querydsl generation.",
    "tag": "apt"
  },
  {
    "question": "Could not select 'OK' in mysql-apt-config [Ubuntu 14.04]",
    "answer": "When you can't do \napt-get purge mysql-apt-config\n\nbecause you're trapped: It won't execute because dpkg is interrupted, and \"sudo dpkg --configure -a\" backs you to the broken \"configuring mysql-apt-config\" screen...\nedit:  /var/lib/dpkg/info/mysql-apt-config.postinst\nadd exit at the beginning , then get rid of the package :\nsudo dpkg --configure mysql-apt-config\n\nsudo apt-get remove mysql-apt-config",
    "tag": "apt"
  },
  {
    "question": "Ansible Do Task If Apt Package Is Missing",
    "answer": "You can use the package_facts module (requires Ansible 2.5):\n- name: Gather package facts\n  package_facts:\n    manager: apt\n\n- name: Install debconf-utils if graphite-carbon is absent\n  apt:\n    name: debconf-utils\n    state: present\n  when: '\"graphite-carbon\" not in ansible_facts.packages'\n\n...",
    "tag": "apt"
  },
  {
    "question": "How can I publish my software to public linux repos to be available with \"apt\" installers etc",
    "answer": "First you need to create a DEB package, you can read more about it here.\nThe easiest way to get your package into huge repositories would be to submit your package to Debian, where Ubuntu is based off. A detailed explanation can be found here.\nAnother good method would be to host your OWN apt repository which is being explained here",
    "tag": "apt"
  },
  {
    "question": "How to make Debian package install dependencies?",
    "answer": "If you want to avoid creating a local APT repository, you can do:\ndpkg -i mypackage.deb\napt-get install --fix-missing\n\nIf you do want to create a local repository, you can use reprepro for this.",
    "tag": "apt"
  },
  {
    "question": "How to install software in CentOS",
    "answer": "CentOS uses yum (Yellowdog Updater, Modified)\nyum install devscripts",
    "tag": "apt"
  },
  {
    "question": "apt install unable to locate executable",
    "answer": "Hey I encountered this when trying to install libsndfile. Turns out I was reading their instructions for Debian/Ubuntu. apt isn't a thing on macosx. You'll want to use an alternative package installer like brew.\nhttps://unix.stackexchange.com/questions/359219/error-when-using-apt-on-macos-sierra",
    "tag": "apt"
  },
  {
    "question": "Package Python3.7 is not available",
    "answer": "I tried doing the below steps in an official docker image of Kali Linux. It should work on the desktop as well.\napt-get update\napt-get install -y build-essential openssl openssl-dev* wget curl\nwget https://www.python.org/ftp/python/3.7.8/Python-3.7.8.tgz\ntar -xvf Python-3.7.8.tgz\ncd Python-3.7.8\n./configure --enable-shared\nmake \nmake test\nmake install\n\n# Steps from here are to enable other libraries in linux to \n# access the shared python libraries.\n\ncd /usr/local/lib/\ncp libpython3.so /usr/lib64/\ncp libpython3.so /usr/lib\ncp libpython3.7m.so.1.0 /usr/lib64/\ncp libpython3.7m.so.1.0 /usr/lib/\ncd /usr/lib64\nln -s libpython3.7m.so.1.0 libpython3.7m.so\ncd /usr/lib\nln -s libpython3.7m.so.1.0 libpython3.7m.so\n\nDone, python3.7 is installed.\nroot@fe794c7ff15e:~# python3\nPython 3.7.8 (default, Aug 15 2020, 16:26:34)\n[GCC 9.3.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>>\n\nI tried creating a python virtual environment with this install. It worked properly. I was able to install pip packages as well.\n(testvirtual) root@fe794c7ff15e:~# pip install flask\nCollecting flask\n  Downloading Flask-1.1.2-py2.py3-none-any.whl (94 kB)\n     |████████████████████████████████| 94 kB 404 kB/s \nCollecting Jinja2>=2.10.1\n  Downloading Jinja2-2.11.2-py2.py3-none-any.whl (125 kB)\n     |████████████████████████████████| 125 kB 10.4 MB/s \nCollecting click>=5.1\n  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n     |████████████████████████████████| 82 kB 165 kB/s \nCollecting Werkzeug>=0.15\n  Downloading Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\n     |████████████████████████████████| 298 kB 11.9 MB/s \nCollecting itsdangerous>=0.24\n  Downloading itsdangerous-1.1.0-py2.py3-none-any.whl (16 kB)\nCollecting MarkupSafe>=0.23\n  Downloading MarkupSafe-1.1.1-cp37-cp37m-manylinux1_x86_64.whl (27 kB)\nInstalling collected packages: MarkupSafe, Jinja2, click, Werkzeug, itsdangerous, flask\nSuccessfully installed Jinja2-2.11.2 MarkupSafe-1.1.1 Werkzeug-1.0.1 click-7.1.2 flask-1.1.2 itsdangerous-1.1.0",
    "tag": "apt"
  },
  {
    "question": "E: Package 'python-certbot-nginx' has no installation candidate",
    "answer": "Since Python2 is no longer supported you just need to ask for Python3.\nSo\nsudo apt-get install python3-certbot-nginx\n\nshould solve your Problem.",
    "tag": "apt"
  },
  {
    "question": "How to get parameter type from javax.lang.model.VariableElement",
    "answer": "Element#asType() gets you the DeclaredType.\nFor enums, use Types#asElement() with the DeclaredType to get the enum type's element, and then iterate over the members using either an ElementVisitor or by using getEnclosedElements().",
    "tag": "apt"
  },
  {
    "question": "dependency problems - leaving unconfigured",
    "answer": "Try this out:\nsudo dpkg --configure -a\nsudo apt-get -f install\nsudo apt install libavahi-glib1 --reinstall\n\n\nsudo dpkg --configure -a\nInstruct dpkg to \"fix\" itself\nsudo apt-get -f install\nThis will instruct apt-get to correct dependencies and continue to configure your packages. \nsudo apt install libavahi-glib1 --reinstall\nreinstall the \"probelatic package\"",
    "tag": "apt"
  },
  {
    "question": "how to install debian package from unsigned repository",
    "answer": "In the sources.list entry for the untrusted repository, you can add \"[trusted=yes]\" (with the square brackets) between the first two elements of the entry, as in deb [trusted=yes] http://...\nThis will turn the error into a warning, as it has been default in past Debian releases, without having to completely disable signature verification",
    "tag": "apt"
  },
  {
    "question": "Java 6 annotation processing configuration with Ant",
    "answer": "This is not pretty, but it is what I do.  (Sources javac ant task javac man page) Using the compilerarg attribute I can pass in the annotation processing related arguments that are not directly supported by the javac ant task.\n<javac srcdir=\"${src}\" destdir=\"${classes}\" ... > \n     ....\n     <compilerarg line=\"-processorpath ${processorpath}\"/>\n     <compilerarg line=\"-processor ${processor}\"/>\n     <compilerarg line=\"-s ${whereToPutGeneratedClassFiles}\"/>\n</javac>\n\nI do not use the APT tool because the documentation states\n\nBe advised that the Apt tool does appear to be an unstable part of the JDK framework, so may change radically in future versions. In particular it is likely to be obsolete in JDK 6, which can run annotation processors as part of javac. \n\nIf you really don't care for compiler args, you can jar your annotation processors like this\n<jar destfile=\"${annotationprocessorjar}\" ... >\n     ...\n     <service type=\"javax.annotation.processing.Processor\" provider=\"${your.annotation.processor.fully.qualified.name}\"/>\n</jar>\n\nThen you can do\n <javac ... make sure ${annotationprocessorjar} is in classpath>\n </javac>",
    "tag": "apt"
  },
  {
    "question": "Java annotation processing with source code manipulation",
    "answer": "The standard annotation processing API does not support direct modification of source code. However, some of the effects of modifying source code can be had by generating either the superclass or subclass(es) of the annotated type. The blog entry below shows an example of this technique:\n\"Properties via Annotation Processing\"",
    "tag": "apt"
  },
  {
    "question": "How to list the contents of a package using YUM?",
    "answer": "There is a package called yum-utils that builds on YUM and contains a tool called repoquery that can do this.\n$ repoquery --help | grep -E \"list\\ files\" \n  -l, --list            list files in this package/group\n\nCombined into one example:\n$ repoquery -l time\n/usr/bin/time\n/usr/share/doc/time-1.7\n/usr/share/doc/time-1.7/COPYING\n/usr/share/doc/time-1.7/NEWS\n/usr/share/doc/time-1.7/README\n/usr/share/info/time.info.gz\n\nOn at least one RH system, with rpm v4.8.0, yum v3.2.29, and repoquery v0.0.11, repoquery -l rpm prints nothing.\nIf you are having this issue, try adding the --installed flag: repoquery --installed -l rpm.\n\nDNF Update:\nTo use dnf instead of yum-utils, use the following command:\n$ dnf repoquery -l time\n/usr/bin/time\n/usr/share/doc/time-1.7\n/usr/share/doc/time-1.7/COPYING\n/usr/share/doc/time-1.7/NEWS\n/usr/share/doc/time-1.7/README\n/usr/share/info/time.info.gz",
    "tag": "yum"
  },
  {
    "question": "How to yum install Node.js on Amazon Linux",
    "answer": "I stumbled onto this, and it was strangely hard to find again later. I am putting it here for posterity:\nsudo yum install nodejs npm --enablerepo=epel\n\nAs of July 2016, EDIT 1 no longer works for Node.js 4 (and EDIT 2 neither). This answer (https://stackoverflow.com/a/35165401/78935) gives a true one-liner.\nEDIT 1: If you're looking for Node.js 4, please try the EPEL testing repository:\nsudo yum install nodejs --enablerepo=epel-testing\n\nEDIT 2: To upgrade from Node.js 0.12 installed through the EPEL repository using the command above, to Node.js 4 from the EPEL testing repository, please follow these steps:\nsudo yum rm nodejs\nsudo rm -f /usr/local/bin/node\nsudo yum install nodejs --enablerepo=epel-testing\n\nThe newer packages put the node binaries in /usr/bin, instead of /usr/local/bin.\nAnd some background:\nThe option --enablerepo=epel causes yum to search for the packages in the EPEL repository.\n\nEPEL (Extra Packages for Enterprise Linux) is open source and free community based repository project from Fedora team which provides 100% high quality add-on software packages for Linux distribution including RHEL (Red Hat Enterprise Linux), CentOS, and Scientific Linux. Epel project is not a part of RHEL/Cent OS but it is designed for major Linux distributions by providing lots of open source packages like networking, sys admin, programming, monitoring and so on. Most of the epel packages are maintained by Fedora repo.\nVia http://www.tecmint.com/how-to-enable-epel-repository-for-rhel-centos-6-5/",
    "tag": "yum"
  },
  {
    "question": "How to install latest version of git on CentOS 8.x/7.x/6.x",
    "answer": "You can use WANDisco's CentOS repository to install Git 2.x: for CentOS 6, for CentOS 7\n\nInstall WANDisco repo package:\nyum install http://opensource.wandisco.com/centos/6/git/x86_64/wandisco-git-release-6-1.noarch.rpm\n- or -\nyum install http://opensource.wandisco.com/centos/7/git/x86_64/wandisco-git-release-7-1.noarch.rpm\n- or -\nyum install http://opensource.wandisco.com/centos/7/git/x86_64/wandisco-git-release-7-2.noarch.rpm\n\nInstall the latest version of Git 2.x:\nyum install git\n\nVerify the version of Git that was installed:\ngit --version\n\n\nAs of 02 Mar. 2020, the latest available version from WANDisco is 2.22.0.",
    "tag": "yum"
  },
  {
    "question": "How do I find which rpm package supplies a file I'm looking for?",
    "answer": "This is an old question, but the current answers are incorrect :)\nUse yum whatprovides, with the absolute path to the file you want (which may be wildcarded). For example:\nyum whatprovides '*bin/grep'\n\nReturns\ngrep-2.5.1-55.el5.x86_64 : The GNU versions of grep pattern matching utilities.\nRepo        : base\nMatched from:\nFilename    : /bin/grep\n\nYou may prefer the output and speed of the repoquery tool, available in the yum-utils package.\nsudo yum install yum-utils\nrepoquery --whatprovides '*bin/grep'\ngrep-0:2.5.1-55.el5.x86_64\ngrep-0:2.5.1-55.el5.x86_64\n\nrepoquery can do other queries such as listing package contents, dependencies, reverse-dependencies, etc.",
    "tag": "yum"
  },
  {
    "question": "Determining the path that a yum package installed to",
    "answer": "yum uses RPM, so the following command will list the contents of the installed package:\n$ rpm -ql package-name",
    "tag": "yum"
  },
  {
    "question": "How to make rpm auto install dependencies",
    "answer": "The link @gertvdijk provided shows a quick way to achieve the desired results without configuring a local repository:\n$ yum --nogpgcheck localinstall packagename.arch.rpm\n\nJust change packagename.arch.rpm to the RPM filename you want to install.\nEdit Just a clarification, this will automatically install all dependencies that are already available via system YUM repositories.\nIf you have dependencies satisfied by other RPMs that are not in the system's repositories, then this method will not work unless each RPM is also specified along with packagename.arch.rpm on the command line.",
    "tag": "yum"
  },
  {
    "question": "How do I install Maven with Yum?",
    "answer": "Icarus answered a very similar question for me. Its not using \"yum\", but should still work for your purposes. Try,\nwget http://mirror.olnevhost.net/pub/apache/maven/maven-3/3.0.5/binaries/apache-maven-3.0.5-bin.tar.gz\n\nbasically just go to the maven site. Find the version of maven you want. The file type and use the mirror for the wget statement above.\nAfterwards the process is easy\n\nRun the wget command from the dir you want to extract maven too. \nrun the following to extract the tar,\ntar xvf apache-maven-3.0.5-bin.tar.gz\n\nmove maven to /usr/local/apache-maven\nmv apache-maven-3.0.5  /usr/local/apache-maven\n\nNext add the env variables to your ~/.bashrc file \nexport M2_HOME=/usr/local/apache-maven\nexport M2=$M2_HOME/bin \nexport PATH=$M2:$PATH\n\nExecute these commands\nsource ~/.bashrc\n\n6:. Verify everything is working with the following command\n    mvn -version",
    "tag": "yum"
  },
  {
    "question": "yum error \"Cannot retrieve metalink for repository: epel. Please verify its path and try again\" updating ContextBroker",
    "answer": "You just needed to update ca-certificates package. Before that just disable all repos with https that are failing.\nThat's why solution with commenting mirrorlist or using http instead https would work also.\nFor example if you need to disable only epel repo:\nyum --disablerepo=epel -y update  ca-certificates\n\nThis will also help wget, curl, and anything else that uses SSL certificates.",
    "tag": "yum"
  },
  {
    "question": "Error: Failed to download metadata for repo 'appstream': Cannot prepare internal mirrorlist: No URLs in mirrorlist",
    "answer": "Try editing your dockerfile\nFROM centos\n\nRUN cd /etc/yum.repos.d/\nRUN sed -i 's/mirrorlist/#mirrorlist/g' /etc/yum.repos.d/CentOS-*\nRUN sed -i 's|#baseurl=http://mirror.centos.org|baseurl=http://vault.centos.org|g' /etc/yum.repos.d/CentOS-*\n\nRUN yum -y install java\n\nCMD /bin/bash\n\nRefer to this code\nfailed-metadata-repo-appstream-centos-8",
    "tag": "yum"
  },
  {
    "question": "How to Install gcc 5.3 with yum on CentOS 7.2?",
    "answer": "Update:\nOften people want the most recent version of gcc, and devtoolset is being kept up-to-date, so maybe you want devtoolset-N where N={4,5,6,7...}, check yum for the latest available on your system).  Updated the cmds below for N=7.\nThere is a package for gcc-7.2.1 for devtoolset-7 as an example.  First you need to enable the Software Collections, then it's available in devtoolset-7:\nsudo yum install centos-release-scl\nsudo yum install devtoolset-7-gcc*\nscl enable devtoolset-7 bash\nwhich gcc\ngcc --version",
    "tag": "yum"
  },
  {
    "question": "Yum crashed with Keyboard Interrupt error",
    "answer": "Because yum does not support Python3.\nYou can run command vi /usr/bin/yum, change /usr/bin/python to /usr/bin/python2 in first line.\nThus you can run the command yum by Python2 instead of Python3.\nNote however that this will make your setup unsupported and thus unmaintainable (as does what you did). You will likely have other similar problems in the future with other system packages.\nIf you want to use an alternative Python installation, consider installing it into /usr/local, /opt or using pyenv.",
    "tag": "yum"
  },
  {
    "question": "Amazon EC2 instance can't update or use yum",
    "answer": "Looks like the host is having trouble contacting the yum server. Make sure the instance has outbound internet access (check security groups etc). If the instance is in a VPC and the security groups look good you may need to use a nat appliance or attach an elastic IP. \nGood luck-",
    "tag": "yum"
  },
  {
    "question": "How to install packages in Linux (CentOS) without root user with automatic dependency handling?",
    "answer": "It is possible to use yum and rpm to install any package in the repository of the distribution. Here is the recipe:\nFind the package name\nUse yum search.\nDownload\nDownload the package and all of its dependencies using yumdownloader (which is available on CentOS by default). You'll need to pass it --resolve to get dependency resolution. yumdownloader downloads to the current directory unless you specify a --destdir.\nmkdir -p ~/rpm\nyumdownloader --destdir ~/rpm --resolve vim-common\n\nChoose a prefix location\nIt might be ~, ~/centos, or ~/y. If your home is slow because it is on a network file system, you can put it in /var/tmp/....\nmkdir ~/centos\n\nExtract all .rpm packages\nExtract all .rpm packages to your chosen prefix location.\ncd ~/centos && rpm2cpio ~/rpm/x.rpm | cpio -id\n\n\nrpm2cpio outputs the .rpm file as a .cpio archive on stdout.\ncpio reads it from from stdin\n-i means extract (to the current directory)\n-d means create missing directory\n\nYou can optionally use -v: verbose\nConfigure the environment\nYou will need to configure the environment variable PATH and LD_LIBRARY_PATH for the installed packages to work correctly. Here is the corresponding sample from my ~/.bashrc:\nexport PATH=\"$HOME/centos/usr/sbin:$HOME/centos/usr/bin:$HOME/centos/bin:$PATH\"\n\nexport MANPATH=\"$HOME/centos/usr/share/man:$MANPATH\"\n\nL='/lib:/lib64:/usr/lib:/usr/lib64'\nexport LD_LIBRARY_PATH=\"$HOME/centos/usr/lib:$HOME/centos/usr/lib64:$L\"\n\n\nEdited note (thanks to @AmitNaidu for pointing out my mistake):\nAccording to bash documentation about startup files, when connecting to a server via ssh, only .bashrc is sourced:\n\nInvoked by remote shell daemon\nBash attempts to determine when it is being run with its standard input connected to a network connection, as when executed by the remote shell daemon, usually rshd, or the secure shell daemon sshd. If Bash determines it is being run in this fashion, it reads and executes commands from ~/.bashrc, if that file exists and is readable.\n\n\nNow if you want to install a lot of packages that way, you might want to automate the process. If so, have a look at this repository.\n\nExtra note: if you are trying to install any of gcc, zlib, make, cmake, git, fish, zsh or tmux , you should really consider using conda, see my other answer.",
    "tag": "yum"
  },
  {
    "question": "How to list installed packages from a given repo using yum",
    "answer": "Try\nyum list installed | grep reponame\nOn one of my servers:\nyum list installed | grep remi\nImageMagick2.x86_64                       6.6.5.10-1.el5.remi          installed\nmemcache.x86_64                          1.4.5-2.el5.remi             installed\nmysql.x86_64                              5.1.54-1.el5.remi            installed\nmysql-devel.x86_64                        5.1.54-1.el5.remi            installed\nmysql-libs.x86_64                         5.1.54-1.el5.remi            installed\nmysql-server.x86_64                       5.1.54-1.el5.remi            installed\nmysqlclient15.x86_64                      5.0.67-1.el5.remi            installed\nphp.x86_64                                5.3.5-1.el5.remi             installed\nphp-cli.x86_64                            5.3.5-1.el5.remi             installed\nphp-common.x86_64                         5.3.5-1.el5.remi             installed\nphp-domxml-php4-php5.noarch               1.21.2-1.el5.remi            installed\nphp-fpm.x86_64                            5.3.5-1.el5.remi             installed\nphp-gd.x86_64                             5.3.5-1.el5.remi             installed\nphp-mbstring.x86_64                       5.3.5-1.el5.remi             installed\nphp-mcrypt.x86_64                         5.3.5-1.el5.remi             installed\nphp-mysql.x86_64                          5.3.5-1.el5.remi             installed\nphp-pdo.x86_64                            5.3.5-1.el5.remi             installed\nphp-pear.noarch                           1:1.9.1-6.el5.remi           installed\nphp-pecl-apc.x86_64                       3.1.6-1.el5.remi             installed\nphp-pecl-imagick.x86_64                   3.0.1-1.el5.remi.1           installed\nphp-pecl-memcache.x86_64                  3.0.5-1.el5.remi             installed\nphp-pecl-xdebug.x86_64                    2.1.0-1.el5.remi             installed\nphp-soap.x86_64                           5.3.5-1.el5.remi             installed\nphp-xml.x86_64                            5.3.5-1.el5.remi             installed\nremi-release.noarch                       5-8.el5.remi                 installed\n\nIt works.",
    "tag": "yum"
  },
  {
    "question": "Upgrading PHP on CentOS 6.5 (Final)",
    "answer": "As Jacob mentioned, the CentOS packages repo appears to only have PHP 5.3 available at the moment.  But these commands seemed to work for me...\nrpm -Uvh http://mirror.webtatic.com/yum/el6/latest.rpm\nyum remove php-common       # Need to remove this, otherwise it conflicts\nyum install php56w\nyum install php56w-mysql\nyum install php56w-common\nyum install php56w-pdo\nyum install php56w-opcache\nphp --version               # Verify version has been upgraded\n\nYou can alternatively use php54w or php55w if required.\nCAUTION!\nThis may potentially break your website if it doesn't fully resolve all your dependencies, so you may need a couple of extra packages in some cases. See here for a list of other PHP 5.6 modules that are available.\nIf you encounter a problem and need to reset back to the default, you can use these commands:\nsudo yum remove php56w\nsudo yum remove php56w-common\nsudo yum install php-common\nsudo yum install php-mysql\nsudo yum install php\n\n(Thanks Fabrizio Bartolomucci)",
    "tag": "yum"
  },
  {
    "question": "Upgrade python without breaking yum",
    "answer": "I have written a quick guide on how to install the latest versions of Python 2 and Python 3 on CentOS 6 and CentOS 7. It currently covers Python 2.7.13 and Python 3.6.0:\n# Start by making sure your system is up-to-date:\nyum update\n# Compilers and related tools:\nyum groupinstall -y \"development tools\"\n# Libraries needed during compilation to enable all features of Python:\nyum install -y zlib-devel bzip2-devel openssl-devel ncurses-devel sqlite-devel readline-devel tk-devel gdbm-devel db4-devel libpcap-devel xz-devel expat-devel\n# If you are on a clean \"minimal\" install of CentOS you also need the wget tool:\nyum install -y wget\n\nThe next steps depend on the version of Python you're installing.\nFor Python 2.7.14:\nwget http://python.org/ftp/python/2.7.14/Python-2.7.14.tar.xz\ntar xf Python-2.7.14.tar.xz\ncd Python-2.7.14\n./configure --prefix=/usr/local --enable-unicode=ucs4 --enable-shared LDFLAGS=\"-Wl,-rpath /usr/local/lib\"\nmake && make altinstall\n\n# Strip the Python 2.7 binary:\nstrip /usr/local/lib/libpython2.7.so.1.0\n\nFor Python 3.6.3:\nwget http://python.org/ftp/python/3.6.3/Python-3.6.3.tar.xz\ntar xf Python-3.6.3.tar.xz\ncd Python-3.6.3\n./configure --prefix=/usr/local --enable-shared LDFLAGS=\"-Wl,-rpath /usr/local/lib\"\nmake && make altinstall\n\n# Strip the Python 3.6 binary:\nstrip /usr/local/lib/libpython3.6m.so.1.0\n\nTo install Pip:\n# First get the script:\nwget https://bootstrap.pypa.io/get-pip.py\n\n# Then execute it using Python 2.7 and/or Python 3.6:\npython2.7 get-pip.py\npython3.6 get-pip.py\n\n# With pip installed you can now do things like this:\npip2.7 install [packagename]\npip2.7 install --upgrade [packagename]\npip2.7 uninstall [packagename]\n\nYou are not supposed to change the system version of Python because it will break the system (as you found out). Installing other versions works fine as long as you leave the original system version alone. This can be accomplished by using a custom prefix (for example /usr/local) when running configure, and using make altinstall (instead of the normal make install) when installing your build of Python.\nHaving multiple versions of Python available is usually not a big problem as long as you remember to type the full name including the version number (for example \"python2.7\" or \"pip2.7\"). If you do all your Python work from a virtualenv the versioning is handled for you, so make sure you install and use virtualenv!",
    "tag": "yum"
  },
  {
    "question": "Completely remove MariaDB or MySQL from CentOS 7 or RHEL 7",
    "answer": "These steps are working on CentOS 6.5 so they should work on CentOS 7 too:\n(EDIT - exactly the same steps work for MariaDB 10.3 on CentOS 8)\n\nyum remove mariadb mariadb-server\nrm -rf /var/lib/mysql If your datadir in /etc/my.cnf points to a different directory, remove that directory instead of /var/lib/mysql\nrm /etc/my.cnf the file might have already been deleted at step 1\nOptional step: rm ~/.my.cnf\nyum install mariadb mariadb-server\n\n[EDIT] - Update for MariaDB 10.1 on CentOS 7\nThe steps above worked for CentOS 6.5 and MariaDB 10. \nI've just installed MariaDB 10.1 on CentOS 7 and some of the steps are slightly different.\nStep 1 would become:\nyum remove MariaDB-server MariaDB-client\n\nStep 5 would become:\nyum install MariaDB-server MariaDB-client\n\nThe other steps remain the same.",
    "tag": "yum"
  },
  {
    "question": "/lib/ld-linux.so.2: bad ELF interpreter: No such file or directory",
    "answer": "yum install glibc.i686\n\ninstall this.",
    "tag": "yum"
  },
  {
    "question": "Can't use yum inside Docker container running on CentOS",
    "answer": "Turns out the user was set to jboss in the base image.\nWhen is switched to user root with the dockerfile command USER root everything worked.",
    "tag": "yum"
  },
  {
    "question": "How to install maven on redhat linux",
    "answer": "Go to mirror.olnevhost.net/pub/apache/maven/binaries/ and check what is the latest tar.gz file\nSupposing it is e.g. apache-maven-3.2.1-bin.tar.gz, from the command line; you should be able to simply do:\nwget http://mirror.olnevhost.net/pub/apache/maven/binaries/apache-maven-3.2.1-bin.tar.gz\n\nAnd then proceed to install it.\nUPDATE: Adding complete instructions (copied from the comment below)\n\nRun command above from the dir you want to extract maven to (e.g. /usr/local/apache-maven)\nrun the following to extract the tar:\ntar xvf apache-maven-3.2.1-bin.tar.gz\n\nNext add the env varibles such as \nexport M2_HOME=/usr/local/apache-maven/apache-maven-3.2.1 \nexport M2=$M2_HOME/bin \nexport PATH=$M2:$PATH \nVerify\nmvn -version",
    "tag": "yum"
  },
  {
    "question": "YumRepo Error: All mirror URLs are not using ftp, http[s] or file",
    "answer": "Be sure that you can ping vault.centos.org.\nThen edit /etc/yum.repos.d/CentOS-Base.repo\nComment out mirrorlist and uncomment baseurl\nChange all  \nbaseurl=http://mirror.centos.org/centos/$releasever/centosplus/$basearch/\n\nto\nbaseurl=http://vault.centos.org/$releasever/centosplus/$basearch/\n\nAnd enjoy your yum update!!! ;)",
    "tag": "yum"
  },
  {
    "question": "Can't install python-dev on centos 6.5",
    "answer": "On CentOS, the python development libraries are under the name python-devel, not python-dev.\nSo you should use\nsudo yum install python-devel\n\nto install them on your CentOS system.\nYou can search the repositories available to you using the yum search xxxxx command, where xxxxx is the name or part of the name of the package you are looking for.\nFurther, you may need to specify the architecture (such as python-devel.x86_64), although in my experience yum will automatically install the package that is appropriate for your system. Again, yum search will show you what is available in the repositories you currently have installed/enabled.",
    "tag": "yum"
  },
  {
    "question": "How to enable mysqlnd for php?",
    "answer": "The ./configure command is part of the compilation process from source code.\nYou can either compile from source or install via package manager. I guess in your case the package manager is preferable. \nAs the package manager complains, you can’t have both php-mysql and php-mysqlnd installed.\nSo you can\nyum remove php-mysql\n\nbefore \nyum install php-mysqlnd\n\nThen check for success via\nphp -m | grep mysqlnd\n\nor\nphp -i | grep mysqlnd",
    "tag": "yum"
  },
  {
    "question": "Duplicate Package - update / upgrade - Centos",
    "answer": "For me it looks like you rebooted  your computer (or it crashed) while you where in the process of upgrading packages. So new packages where installed, but old packages where not removed.\nFirst look if you have any uncomplete transactions with: yum-complete-transaction\nIf this doesn't help then take a look at the package-cleanup tool which is part of the yum-utils package.\npackage-cleanup --dupes  lists duplicate packages\npackage-cleanup --cleandupes  removes duplicate packages\nBut be carefull with the command and create a backup before removing duplicates.",
    "tag": "yum"
  },
  {
    "question": "Can yum tell me which of my repositories provide a particular package?",
    "answer": "yum list packagename\n\nThat will show from which repository the package is in the third column of the output.\nFor already installed packages, that won't work, as the third column shows just installed. In that case you can do e.g. rpm -qi packagename, typically the Vendor, Packager and Build Host tags will give an indication to which repository the package belongs. Also it's quite common for some repo symbol being appended to the package version number.",
    "tag": "yum"
  },
  {
    "question": "Install R packages using docker file",
    "answer": "As suggested by @Cameron Kerr's comment, Rscript does not give you a build failure.\nAs of now, the recommended way is to do as the question suggests.\nRUN R -e \"install.packages('methods',dependencies=TRUE, repos='http://cran.rstudio.com/')\"\nRUN R -e \"install.packages('jsonlite',dependencies=TRUE, repos='http://cran.rstudio.com/')\"\nRUN R -e \"install.packages('tseries',dependencies=TRUE, repos='http://cran.rstudio.com/')\" \n\nIf you're fairly certain of no package failures then use this one-liner -\nRUN R -e \"install.packages(c('methods', 'jsonlite', 'tseries'),\n                           dependencies=TRUE, \n                           repos='http://cran.rstudio.com/')\"\n\n\nEDIT: If you're don't use the Base-R image, you can use rocker-org's r-ver or r-studio or tidyverse images. Here's the repo. Here's an example Dockerfile -\nFROM rocker/tidyverse:latest\n\n# Install R packages\nRUN install2.r --error \\\n    methods \\\n    jsonlite \\\n    tseries\n\nThe --error flag is optional, it makes install.packages() throw an error if the package installation fails (which will cause the docker build command to fail). By default, install.packages() only throws a warning, which means that a Dockerfile can build successfully even if it has failed to install the package.\nAll rocker-org's basically installs the littler package for the install2.R functionality",
    "tag": "yum"
  },
  {
    "question": "Ansible: How can I update the system CentOS with Ansible",
    "answer": "The first task you're telling the system to only update the yum cache.\nOn the second you are effectively upgrading all packages to the latest version by using state=latest but you should also use update_cache=yes on the same task to be sure you're refreshing the cache with its latest package information.\nThe yum module documentation provides exactly this example:\n- name: upgrade all packages\n  yum: name=* state=latest\n\nAfter the execution of the task, the terminal should display a message in yellow meaning the status of the task is changed.",
    "tag": "yum"
  },
  {
    "question": "Python cannot find dateutil.relativedelta",
    "answer": "I also ran into this issue. The simple solution I ended up using was to add --upgrade to the end of the command. This forced it to install it even though Python thought it was installed. This resolved the issue. \nSo if you have this issue, try the following:\nsudo pip install python-dateutil --upgrade\n\nIt can't possibly hurt anything, so there is no harm in just forcing it to be reinstalled.",
    "tag": "yum"
  },
  {
    "question": "How do I install PHP intl extension on CentOS?",
    "answer": "As you have php-commom from remi repositories, you need to get php-intl from remi also. \nAdd --enable-repo option as follows:\nyum --enablerepo=remi install php-intl",
    "tag": "yum"
  },
  {
    "question": "Linux - Yum Install GCC - Missing Kernel-headers",
    "answer": "Your system is probably configured to exclude the kernel packages.\ntry: \nsudo vi /etc/yum.conf\n\nthen comment (or remove the 'kernel*' part):\n#exclude=kernel*\n\nThen you should be able to do: \nsudo yum install kernel-headers\n\nEdit: Or, as pointed by Andrew Beals, you can simply run:\nyum install kernel-headers --disableexcludes=all",
    "tag": "yum"
  },
  {
    "question": "Is there any way to retrieve a dependency tree from yum?",
    "answer": "Per the RHEL5 manual pages: \"repoquery is a program for querying information from YUM repositories similarly to rpm queries.\"\nFor your specific case of postgis:\n# repoquery --requires --recursive --resolve  postgis\npostgresql-libs-0:8.1.23-6.el5_8.i386\ngeos-0:2.2.3-3.el5.i386\nglibc-0:2.5-107.el5_9.5.i686\nproj-0:4.5.0-3.el5.i386\n\nYou can drop the \".i386\" and \".i686\" off of the package names if your system is 64-bit.\nThe output from repoquery is not perfect since, for example, it fails to list glibc-common in the above list. But your system would not be running if it did not have both glibc and glibc-common already installed.\nEDIT: Although it does not cause an error, the --recursive flag appears to do nothing in RHEL5.11 and can be omitted. Also, use the --pkgnarrow=all flag to ensure that all (installed, available, etc) packages are considered for the query. Lastly, for one step of recursion to get more of the dependency tree, in a bash shell, pass the output of the repoquery command to a second repoquery command using tee and xargs like so:\n# repoquery --requires  --resolve --pkgnarrow=all postgis.i386 | tee >(xargs -r -n 1 -- repoquery --requires  --resolve --pkgnarrow=all) | sort | uniq\nbasesystem-0:8.0-5.1.1.noarch\ngeos-0:2.2.3-3.el5.i386\nglibc-0:2.5-123.el5_11.3.i686\nglibc-common-0:2.5-123.el5_11.3.i386\nkrb5-libs-0:1.6.1-80.el5_11.i386\nlibgcc-0:4.1.2-55.el5.i386\nlibstdc++-0:4.1.2-55.el5.i386\nopenssl-0:0.9.8e-40.el5_11.i686\npostgresql-libs-0:8.1.23-10.el5_10.i386\nproj-0:4.5.0-3.el5.i386",
    "tag": "yum"
  },
  {
    "question": "Yum fails with - There are no enabled repos.",
    "answer": "ok,\nso my problem was that I tried to install the package with yum which is the primary tool for getting, installing, deleting, querying, and managing Red Hat Enterprise Linux RPM software packages from official Red Hat software repositories, as well as other third-party repositories.\nBut I'm using ubuntu and The usual way to install packages on the command line in Ubuntu is with apt-get. so the right command was:\nsudo apt-get install libstdc++.i686",
    "tag": "yum"
  },
  {
    "question": "CentOS 8 - yum/dnf error: Failed to download metadata for repo",
    "answer": "In my case \nsudo rm -r /var/cache/dnf \n\nsolved my problem.\nSource: https://access.redhat.com/discussions/4222851",
    "tag": "yum"
  },
  {
    "question": "How to install \"make\" in ubuntu?",
    "answer": "I have no idea what linux distribution \"ubuntu centOS\" is. Ubuntu and CentOS are two different distributions.\nTo answer the question in the header:\nTo install make in ubuntu you have to install build-essentials\nsudo apt-get install build-essential",
    "tag": "yum"
  },
  {
    "question": "Official Fedora package for 'xxd' command?",
    "answer": "xxd is in the vim-common package.\nYou can find that by using yum whatprovides '*bin/xxd'.",
    "tag": "yum"
  },
  {
    "question": "RHEL8/Fedora - yum/dnf causes cannot download repodata/repomd.xml for docker-ce",
    "answer": "Cause\nThe commands specified in the Docker documentations generates incorrect repository configurations.\nAs per Changes to dockerproject.org APT and YUM repositories, the repository for docker is now moved to download.docker.com. The document links to the Docker documentations such as Install Docker Engine on Fedora.\n\nWhat do I need to do?\nIf you are currently using the APT or YUM repositories from dockerproject.org or dockerproject.com, please update to use the repositories at download.docker.com.\nYou can find instructions for CentOS, Debian, Fedora and Ubuntu in the documentation.\n\nYou follow the instruction:\n$ sudo dnf config-manager \\\n    --add-repo \\\n    https://download.docker.com/linux/fedora/docker-ce.repo\n\nThe command creates /etc/yum.repos.d/docker-ce.repo which has wrong URL.\n[docker-ce-stable]\nname=Docker CE Stable - $basearch\nbaseurl=https://download.docker.com/linux/fedora/$releasever/$basearch/stable # <--- Wrong URL\nenabled=1\ngpgcheck=1\ngpgkey=https://download.docker.com/linux/fedora/gpg\n\nAs explained in Unable to install docker on rhel 8 linux, it should have been:\n[docker-ce-stable]\nname=Docker CE Stable - $basearch\nbaseurl=https://download.docker.com/linux/centos/$releasever/$basearch/stable # <--- Correct URL\nenabled=1\ngpgcheck=1\ngpgkey=https://download.docker.com/linux/centos/gpg\n\nNot sure who should fix this. Please advise who is in charge.\nSolution\nUse the command below for RHEL/Fedora instead of the one specified in the Docker documentation.\nsudo dnf config-manager \\\n--add-repo=https://download.docker.com/linux/centos/docker-ce.repo\n\nIf the incorrect configuration has been already created.\nsed -i -e 's/baseurl=https:\\/\\/download\\.docker\\.com\\/linux\\/\\(fedora\\|rhel\\)\\/$releasever/baseurl\\=https:\\/\\/download.docker.com\\/linux\\/centos\\/$releasever/g' /etc/yum.repos.d/docker-ce.repo\n\n\nNote\n\nSupported platforms\n\n\n\n\n\nInstall Docker Engine on RHEL\n\n\nWe currently only provide packages for RHEL on s390x (IBM Z). Other architectures are not yet supported for RHEL, but you may be able to install the CentOS packages on RHEL. Refer to the Install Docker Engine on CentOS page for details.\n\n\nIs the docker package available for Red Hat Enterprise Linux 8 and 9?\n\n\nThe docker package is not shipped or supported by Red Hat for Red Hat Enterprise Linux (RHEL) 8 and (RHEL) 9. The docker container engine is replaced by a suite of tools in the Container Tools module.\n\n\nTransitioning from Docker to Podman\n\n\nPodman is an excellent alternative to Docker containers when you need increased security, unique identifier (UID) separation using namespaces, and integration with systemd. In this article, I use real-world examples to show you how to install Podman, use its basic commands, and transition from the Docker command-line interface (CLI) to Podman.",
    "tag": "yum"
  },
  {
    "question": "Disable yum transaction check for file conflict",
    "answer": "Replacing files from another RPM package is bad idea in most cases and I strongly advise against what you're trying to do. That said, apply following at your own risk.\nYum does not provide an option to install conflicting files, I think. However, that does not prevent you from installing a RPM package with rpm(1) which does provide an option to override existing files from another package, namely --replacefiles.\nSo, first get the RPM of the package you want to install on a local filesystem (/usr/local/xenco... makes me suspect that is the case already). Next install the RPM with rpm -i --replacefiles <your_rpm_file>.",
    "tag": "yum"
  },
  {
    "question": "Install single package from Rawhide",
    "answer": "yum install fedora-repos-rawhide\nyum install --enablerepo rawhide bash",
    "tag": "yum"
  },
  {
    "question": "Why does configure say no C compiler found when GCC is installed?",
    "answer": "try yum groupinstall \"Development Tools\"\nif the installation is success then you will have a full set of development tools. Such as gcc, g++, make, ld ect. After that you can try the compilation of Code Blocks again.\nSince yum is deprecated you can use dnf instead:\ndnf groupinstall \"Development Tools\"",
    "tag": "yum"
  },
  {
    "question": "What is the difference between two \"state\" option values, \"present\" and \"installed\", available in Ansible's yum module?",
    "answer": "They do the same thing, i.e. they are aliases to each other, see this comment in the source code of the yum module:\n# removed==absent, installed==present, these are accepted as aliases\nAnd how they are used in the code:\nif state in ['installed', 'present']:\n    if disable_gpg_check:\n        yum_basecmd.append('--nogpgcheck')\n    res = install(module, pkgs, repoq, yum_basecmd, conf_file, en_repos, dis_repos)\nelif state in ['removed', 'absent']:\n    res = remove(module, pkgs, repoq, yum_basecmd, conf_file, en_repos, dis_repos)\nelif state == 'latest':\n    if disable_gpg_check:\n        yum_basecmd.append('--nogpgcheck')\n    res = latest(module, pkgs, repoq, yum_basecmd, conf_file, en_repos, dis_repos)\nelse:\n    # should be caught by AnsibleModule argument_spec\n    module.fail_json(msg=\"we should never get here unless this all\"\n            \" failed\", changed=False, results='', errors='unexpected state')\n\nreturn res\n\nhttps://github.com/ansible/ansible-modules-core/blob/devel/packaging/os/yum.py",
    "tag": "yum"
  },
  {
    "question": "Listing yum group",
    "answer": "Try yum groupinfo\nFrom the groupinfo section of the manpage:\n\nIs used to give the description and package list of a group (and which type those packages are marked as). Note that you can use the yum-filter-data and yum-list-data plugins  to  get/use  the  data  the other  way  around (Ie. what groups own packages need updating). If you pass the -v option, to enable  verbose mode, then the package names are matched against installed/available packages similar to  the list command.",
    "tag": "yum"
  },
  {
    "question": "Bash Centos7 \"which\" command",
    "answer": "To find a package in CentOS, use  yum whatprovides:\nyum whatprovides *bin/which\n\nIn this particular case, the package is called which, so\nyum install which\n\nshould pull it in.",
    "tag": "yum"
  },
  {
    "question": "yum on Centos stuck at \"loaded plugins: fastestmirror\"",
    "answer": "For me what ended up fixing it was this:\nrm -f /var/lib/rpm/__*\nrpm --rebuilddb -v -v\n\nThen rerunning yum command I was trying to run in the first place.\nIt got stuck for about a minute on:\nDetermining fastest mirrors\n\n...but then it completed without errors\nSuggested Here",
    "tag": "yum"
  },
  {
    "question": "RHEL 6 - how to install 'GLIBC_2.14' or 'GLIBC_2.15'?",
    "answer": "This often occurs when you build software in RHEL 7 and try to run on RHEL 6.\nTo update GLIBC to any version, simply download the package from\nhttps://ftp.gnu.org/gnu/libc/\nFor example glibc-2.14.tar.gz in your case.\n1. tar xvfz glibc-2.14.tar.gz\n2. cd glibc-2.14\n3. mkdir build\n4. cd build\n5. ../configure --prefix=/opt/glibc-2.14\n6. make\n7. sudo make install\n8. export LD_LIBRARY_PATH=/opt/glibc-2.14/lib:$LD_LIBRARY_PATH\n\nThen try to run your software, glibc-2.14 should be linked.",
    "tag": "yum"
  },
  {
    "question": "DNF missing config-manager command",
    "answer": "sudo yum install dnf-plugins-core did the trick for me.\nUPDATE: on el7, el8, el9 there is a better way\n\nwhen you try to execute dnf config-manager -h it will tell you to try dnf install 'dnf-command(config-manager)'\n\n# centos7\n# rocky8\n# rocky9\nsudo dnf install 'dnf-command(config-manager)'\n\np.s. the high level solution is actually \"try to execute the dnf config-manager -h and if the command fails then read the error message and try installing what the error message recommends\".",
    "tag": "yum"
  },
  {
    "question": "Checking for installed packages and if not found install",
    "answer": "Try the following code :\nif ! rpm -qa | grep -qw glibc-static; then\n    yum install glibc-static\nfi\n\nor shorter :\nrpm -qa | grep -qw glibc-static || yum install glibc-static\n\n\nFor debian likes : \ndpkg -l | grep -qw package || apt-get install package\n\nFor archlinux :\npacman -Qq | grep -qw package || pacman -S package",
    "tag": "yum"
  },
  {
    "question": "Installing tshark on RHEL",
    "answer": "The problem seems to be solved. To install tshark(CLI of wireshark) just do following:\nsudo yum install wireshark\n\nThis will install tshark in /usr/sbin/tshark \nTo install wireshark with gui, do the following:\nsudo yum install wireshark-gnome",
    "tag": "yum"
  },
  {
    "question": "What's the difference between rpm and yum?",
    "answer": "to expand on the Udo's answer, there is the program, \"rpm\", which manipulates specifically the packages it is asked to manipulate, and there is \"yum\", which is a more intelligent management system that can find dependencies and download .rpm files even if they're not in the system.\nwith the \"rpm\" command, you need to know the exact location of the .rpm package, but with \"yum\", you just need to know the name of it, and as long as it's available through your repositories list, it will be installed along with its dependencies",
    "tag": "yum"
  },
  {
    "question": "How to install nginx 1.9.15 on amazon linux disto",
    "answer": "If you're using AWS Linux2, you have to install nginx from the AWS \"Extras Repository\". To see a list of the packages available:\n# View list of packages to install\namazon-linux-extras list\n\nYou'll see a list similar to:\n0  ansible2   disabled  [ =2.4.2 ]\n1  emacs   disabled  [ =25.3 ]\n2  memcached1.5   disabled  [ =1.5.1 ]\n3  nginx1.12   disabled  [ =1.12.2 ]\n4  postgresql9.6   disabled  [ =9.6.6 ]\n5  python3   disabled  [ =3.6.2 ]\n6  redis4.0   disabled  [ =4.0.5 ]\n7  R3.4   disabled  [ =3.4.3 ]\n8  rust1   disabled  [ =1.22.1 ]\n9  vim   disabled  [ =8.0 ]\n10  golang1.9   disabled  [ =1.9.2 ]\n11  ruby2.4   disabled  [ =2.4.2 ]\n12  nano   disabled  [ =2.9.1 ]\n13  php7.2   disabled  [ =7.2.0 ]\n14  lamp-mariadb10.2-php7.2   disabled  [ =10.2.10_7.2.0 ]\n\nUse the amazon-linux-extras install command to install it, like:\nsudo amazon-linux-extras install nginx1.12\n\nMore details are here: https://aws.amazon.com/amazon-linux-2/faqs/.",
    "tag": "yum"
  },
  {
    "question": "How to instruct yum to install a specific package (rpm) from a specific repo",
    "answer": "you can tell yum which repositories he can use:\nyum --disablerepo=\"*\" --enablerepo=\"<desired-repo-id>\" install package-name\n\nThis does not permanently enable/disable repositories; just for this command execution.",
    "tag": "yum"
  },
  {
    "question": "How can I install xclip on an EC2 instance?",
    "answer": "I needed this today for a file larger than a ssh-key, and cat was not enough. You need to enable the EPEL repo in EC2 in order to get xclip:\nwget https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm\nsudo rpm -ivh epel-release-latest-7.noarch.rpm\nsudo yum-config-manager --enable epel\nsudo yum install xclip -y",
    "tag": "yum"
  },
  {
    "question": "docker-runc not installed on system",
    "answer": "In reference to the top answer, introduce the sym link in /usr/bin to enable it in global path\nsudo ln -s /usr/libexec/docker/docker-runc-current /usr/bin/docker-runc",
    "tag": "yum"
  },
  {
    "question": "PostgreSQL on Elastic Beanstalk (Amazon Linux 2)",
    "answer": "The following works:\npackages:\n    yum:\n        amazon-linux-extras: []\n\ncommands:\n    01_postgres_activate:\n        command: sudo amazon-linux-extras enable postgresql10\n    02_postgres_install:\n        command: sudo yum install -y postgresql-devel",
    "tag": "yum"
  },
  {
    "question": "CentOS 7 and Puppet unable to install nc",
    "answer": "Nc is a link to nmap-ncat.\nIt would be nice to use nmap-ncat in your puppet, because NC is a virtual name of nmap-ncat.\nPuppet cannot understand the links/virtualnames\nyour puppet should be:\npackage {\n  'nmap-ncat':\n    ensure => installed;\n}",
    "tag": "yum"
  },
  {
    "question": "What is the exact command to install yum through brew?",
    "answer": "yum is a package manager for Red Hat Linux. It will not work on macOS. brew is a package manager for macOS\nSo instead of trying to install yum to install another piece of software, you might want to try to install this software using brew directly. But beware: the package names might not be the same. Use brew search to search for packages in brew, or maybe, even better, try and find specific instructions for macOS for the software you're trying to install.",
    "tag": "yum"
  },
  {
    "question": "Trouble Installing Git on CentOS",
    "answer": "Figured it out, it was because of cpanel.\nThis fixes it:\nyum install git --disableexcludes=main",
    "tag": "yum"
  },
  {
    "question": "How to install Maven into Red Hat Enterprise Linux 6?",
    "answer": "The distro agnostic generic repo is what you want. As root, add a couple of the jpackage-generic repos to yum (two snippets below). Then perform a yum update and finally yum install maven2.\ncat > /etc/yum.repos.d/jpackage-generic-free.repo << EOF\n[jpackage-generic-free]\nname=JPackage generic free\nbaseurl=http://mirrors.dotsrc.org/jpackage/6.0/generic/free/\nenabled=1\ngpgcheck=1\ngpgkey=http://www.jpackage.org/jpackage.asc\nEOF\n\ncat > /etc/yum.repos.d/jpackage-generic-devel.repo << EOF\n[jpackage-generic-devel]\nname=JPackage Generic Developer\nbaseurl=http://mirrors.dotsrc.org/jpackage/6.0/generic/devel/\nenabled=1\ngpgcheck=1\ngpgkey=http://www.jpackage.org/jpackage.asc\nEOF",
    "tag": "yum"
  },
  {
    "question": "How to install software in CentOS",
    "answer": "CentOS uses yum (Yellowdog Updater, Modified)\nyum install devscripts",
    "tag": "yum"
  },
  {
    "question": "How does one easily add posix support to PHP using yum?",
    "answer": "While the question was for centos, notice that for fedora the php-posix package is provided by php-process from fedora 11. I assume this change also will hit centos at some point.",
    "tag": "yum"
  },
  {
    "question": "Azure RedHat vm yum update fails with \"SSL peer rejected your certificate as expired.\"",
    "answer": "From MSDN, you can run this command to update the RHUI client certificate on the Azure RedHat VM:\nsudo yum update -y --disablerepo='*' --enablerepo='*microsoft*'\n\nAnd now you should be able to download/update packages packages without the SSL peer rejected your certificate as expired error. \nTested this on Azure RedHat Enterprise Linux 7.3 and it works fine for me.",
    "tag": "yum"
  },
  {
    "question": "Erase multiple packages using rpm or yum",
    "answer": "Using yum\nList and remove the indicated packages and all their dependencies, but with a y/N confirmation:\nyum remove 'php*'\n\nTo bypass the confirmation, replace yum with yum -y.\nUsing rpm\nThis section builds upon the answers by twalburg and Ricardo.\nList which RPMs are installed:\nrpm -qa 'php*'\nrpm -qa | grep '^php'  # Alternative listing.\n\nList which RPMs which will be erased, without actually erasing them:\nrpm -e --test -vv $(rpm -qa 'php*') 2>&1 | grep '^D:     erase:'\n\nOn Amazon Linux, you may need to use grep '^D: ========== ---' instead.\nIf the relevant RPMs are not listed by the command above, investigate errors:\nrpm -e --test -vv $(rpm -qa 'php*')\n\nErase these RPMs:\nrpm -e $(rpm -qa 'php*')\n\nConfirm the erasure:\nrpm -qa 'php*'",
    "tag": "yum"
  },
  {
    "question": "install sqlite3 dev and other packages in centos",
    "answer": "Late response, but perhaps this might help others who eventually stumble on this question looking for the same answer. \nThe sqlite3 development package can be found in the epel repo. EPEL\nEasy to install on CentOS -> yum install epel-release\n$ yum list | grep sqlite\nlibsqlite3x-devel.x86_64                20071018-20.el7                @epel\nSimilar list/grep can be done for the other libraries you are looking to install, although the names are most likely just slightly different (list edited for clarity).\n$ yum list | grep boost\nboost-devel.x86_64                      1.53.0-26.el7                  base",
    "tag": "yum"
  },
  {
    "question": "Linux - Bash - Get $releasever and $basearch values?",
    "answer": "Most distro uses the distroverpkg version to get the releasever and basearch.\nIf you look at /etc/yum.conf, you will see that distrover is set to redhat-release (for RHEL), enterpriselinux-release (for OEL), and others.\nTo get the package name:\ndistro=$(sed -n 's/^distroverpkg=//p' /etc/yum.conf)\n\nTo get the releasever:\nreleasever=$(rpm -q --qf \"%{version}\" -f /etc/$distro)\n\nTo get the basearch:\nbasearch=$(rpm -q --qf \"%{arch}\" -f /etc/$distro)\n\nThe new code above will try to get the package associated with a file /etc/$distro. Some Linux adds /etc/redhat-release to their package release. \nIf you get file not owned by any package then use the /etc/*-release file that came with your distro. It is probably /etc/centos-release.\nYou can check the appropriate /etc/*-release appropriate for this code by checking which file is packaged with centos.\nrpm -qf /etc/*-release\n\nThen use this file instead of the first line above.\ndistro=/etc/centos-release\n\nHere's an example from OEL where /etc/redhat-release is packaged as enterprise-release.\nrpm -q --qf \"%{name}\" -f /etc/redhat-release\n\nOutput:\nenterprise-release",
    "tag": "yum"
  },
  {
    "question": "What is the equivalent of apt-key in yum?",
    "answer": "Below is from an article on Baeldung  which I think answers this questions properly:\nAdding a repository in YUM is a manual operation, which consists in creating a file with the .repo extension under the folder /etc/yum.repos.d.\nThe file must contain all the information about the custom repository that we are connecting to.\nLet’s try adding the AdoptOpenJDK repository:\n# /etc/yum.repos.d/adoptopenjdk.repo\n[AdoptOpenJDK]\nname=AdoptOpenJDK\nbaseurl=http://adoptopenjdk.jfrog.io/adoptopenjdk/rpm/centos/7/$(uname -m)\nenabled=1\ngpgcheck=1\ngpgkey=https://adoptopenjdk.jfrog.io/adoptopenjdk/api/gpg/key/public\n\nIn APT, though, things are quite different. The GPG key of the repository must be downloaded and added to the APT keyring with apt-key add:\nwget -qO - https://adoptopenjdk.jfrog.io/adoptopenjdk/api/gpg/key/public | sudo apt-key add -\n\nThen, at this point, the repository can be added through add-apt-repository –yes followed by the URL:\nadd-apt-repository --yes https://adoptopenjdk.jfrog.io/adoptopenjdk/deb/\n\nContrary to YUM, all the repositories are saved in a single file,\n/etc/apt/sources.list.",
    "tag": "yum"
  },
  {
    "question": "can't install php-devel on centos",
    "answer": "Rather than running yum install php-devel you needed to run yum --enablerepo=remi,remi-php54 install php-devel .\nIn short, just specifying which php-devel version you wanted from the remi repo. In your case you had php54 installed from remi so you needed to add in \"remi-php54\" . That would have successfully installed php-devel without the downtime.",
    "tag": "yum"
  },
  {
    "question": "centos 7.1 - error repository",
    "answer": "The problem is config of dhcp, try typing \ndhclient",
    "tag": "yum"
  },
  {
    "question": "How do I install byobu in ec2 ami",
    "answer": "Not sure why, but the EPEL repo is installed, but disabled by default.  You can enable it permanently by changing the setting \"enabled=1\" in the [epel] stanza of /etc/yum.repos.d/epel.repo\nOr you can leave it disabled and still install byobu:\nsudo yum install --enablerepo=epel byobu\n\nYou can have a look at what packages are available in the epel repo with:\nsudo yum list --disablerepo=\\* --enablerepo=epel",
    "tag": "yum"
  },
  {
    "question": "Installing nodejs on Red Hat",
    "answer": "NodeJS provides a setup script that must run before you install it with yum\ncurl -sL https://rpm.nodesource.com/setup | bash -\n\nThen the yum command should work\nyum install -y nodejs\n\nhttps://github.com/joyent/node/wiki/installing-node.js-via-package-manager#enterprise-linux-and-fedora",
    "tag": "yum"
  },
  {
    "question": "Ansible Yum Module pending transactions error",
    "answer": "It seems there are unfinished / pending transactions on the target host.\nTry installing yum-utils package to run yum-complete-transaction to the target hosts giving the error.\n# yum-complete-transaction --cleanup-only\n\nLook at Fixing There are unfinished transactions remaining for more details.\n\nyum-complete-transaction is a program which finds incomplete or\n  aborted yum transactions on a system and attempts to complete them. It\n  looks at the transaction-all* and transaction-done* files which can\n  normally be found in /var/lib/yum if a yum transaction aborted in the\n  middle of execution.\nIf it finds more than one unfinished transaction it will attempt to\n  complete the most recent one first. You can run it more than once to\n  clean up all unfinished transactions.",
    "tag": "yum"
  },
  {
    "question": "Cannot install inotify on Amazon EC2",
    "answer": "I bumped into this issue as well. The solution below is a bit easier than grabbing an RPM or the source and compiling.\nThe Amazon Linux AMI comes with the EPEL repository source, but it's disabled. So you need to enable it.\nOriginal Amazon Linux (1) AMI:\nsudo yum-config-manager --enable epel\n\nAmazon Linux 2 AMI:\nsudo amazon-linux-extras install epel -y\n\nThen run a regular yum update and install the toolset:\nsudo yum update\nsudo yum install inotify-tools",
    "tag": "yum"
  },
  {
    "question": "Importing/adding a yum .repo file using Ansible",
    "answer": "Use the shell command with the creates flag. That will skip the step if the repo file exists.  You'll need to make sure you know what the repo file is called.\n- name: Add CentOS_o repository\n  shell: yum-config-manager --add-repo=http://example.net/mirror/centos_o.repo\n  args:\n    creates: /etc/yum.repos.d/centos_o.repo \n\nIf you need to add any architecture to the url use something like\n- name: Add CentOS_7_speciality repository\n  shell: yum-config-manager --add-repo=http://example.net/{{ ansible_distribution | lower }}/{{ ansible_distribution_major_version }}/{{ ansible_architecture }}/\ncentos_o.repo\n  args:\n    creates: /etc/yum.repos.d/centos_o.repo \n\nAnsible will replace the variables with\n{{ ansible_distribution | lower }} == centos\n{{ ansible_distribution_major_version }} == 7\n{{ ansible_architecture }} == x86_64",
    "tag": "yum"
  },
  {
    "question": "sudo yum install installs only JRE not JDK - Centos",
    "answer": "After going through this link, found out that sudo yum install java-1.8.0-openjdk installs only JRE.\nExecuted sudo yum install java-1.8.0-openjdk-devel to install JDK.",
    "tag": "yum"
  },
  {
    "question": "yum local install to install a package with its dependency",
    "answer": "Inside the local directory where you have all the downloaded RPMs, do this:\n sudo yum --disablerepo=* localinstall *.rpm\n\nOR\n sudo yum --disablerepo=* localinstall foo.rpm bar.rpm baz.rpm\n\nSince you have downloaded all the dependencies to a single directory, you can also use rpm to install those:\n sudo rpm -Uvvh *.rpm --test\n\n--test does a dry-run. Remove it to install on disk.",
    "tag": "yum"
  },
  {
    "question": "Aptitude: Show What Repo a Package is From, Listing Contents of a Repo",
    "answer": "You can use apt-cache policy to get that information (aptitude uses the same repositories as shown with apt-cache policy).\napt-cache policy fabric\n\nShows version and repository information about the fabric package.\nAs pointed out in another answer, you can also use\naptitude versions fabric\n\nto get more or less the same information (in a slightly different format).",
    "tag": "yum"
  },
  {
    "question": "Installing OpenJDK 11 on CentOS using yum",
    "answer": "As of November 2020\nyou can achieve this in 2 steps:\n\nInstall Java 11 using yum:\nyum install java-11-openjdk-devel\n\nGet all the Java configurations available in your machine:\nalternatives --config java\nRun the above command, select the version you want to set, I've set 1 here:\nThere are 2 programs which provide 'java'.\nSelection    Command\n\n-----------------------------------------------\n   1           java-11-openjdk.x86_64 (/usr/lib/jvm/java-11-openjdk-11.0.8.10-0.el7_8.x86_64/bin/java)\n*+ 2           java-1.8.0-openjdk.x86_64 (/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.262.b10-0.el7_8.x86_64/jre/bin/java)\n\nEnter to keep the current selection[+], or type selection number: 1\n\n\nCheck java -version:\nopenjdk version \"11.0.8\" 2020-07-14 LTS\nOpenJDK Runtime Environment 18.9 (build 11.0.8+10-LTS)\nOpenJDK 64-Bit Server VM 18.9 (build 11.0.8+10-LTS, mixed mode, sharing)\n\n\n\nJava 11 is now set to be used globally.",
    "tag": "yum"
  },
  {
    "question": "yum doesn't have enough cached data to continue. At this point the only\\n safe thing yum can do is fail",
    "answer": "This seems to be a problem with the EPEL repository. Try yum with \n--disablerepo=epel\\* \n\nuntil they get this fixed.\nif you don't have an immediate need to get packages from EPEL, use yum with \n--disablerepo=epel \n\nuntil your local mirror syncs. yum clean all may or may not help, depending on which mirror you hit.\nMy general suggestion is still valid, though -- unless you have an immediate need to get EPEL packages, I would suggest using \n--disablerepo=epel \n\nuntil your local mirror gets the fixed repodata. If you do have an immediate need for EPEL packages and \n yum clean all \n\ndoes not help, you will need to adjust your config to point to some specific mirror that has the fixed content, like by adding baseurl=http://ftp.funet.fi/pub/mirrors/fedora.redhat.com/pub/epel/7/\n$basearch    to epel.repo. Most mirrors should be updated by now, though.\n\nAlternatives >>>\nI happened to be on Freenode channel #epel when someone complained about this issue, but if this had happened to me, I could have enabled one repository at a time (with --disablerepo=* and --enablerepo=somerepo) to find which repo caused the trouble.\nskip_if_unavailable=1 for non-critical repos sounds like a good idea.\nMany people need only a few packages from EPEL. One option would be to import the EPEL packages you need to some locally controlled repository and use that instead. createrepo is related to this.\nTop",
    "tag": "yum"
  },
  {
    "question": "Yum repositories don't work unless there are exceptions in the AWS firewall. How do I make the exceptions based on a DNS name?",
    "answer": "On AWS Amazon Web Services, make sure you are the 'root' user and not ec2-user.\nType:\nsudo su - root\n\nThis fixed my problem.",
    "tag": "yum"
  },
  {
    "question": "How to install PostgeSQL 11 on AWS Amazon Linux AMI 2?",
    "answer": "Aman,\nIt may help you: https://stackoverflow.com/a/55883490/7541412\nMoreover, if you think PostgreSQL v10 can resolve your issues. You can try these commands:\n\nsudo yum update -y\nsudo amazon-linux-extras enable postgresql10\n\nAfter having PostgreSQL in your repository, now you can install:\n\nyum clean metadata\nyum install postgresql\n\nThanks!",
    "tag": "yum"
  },
  {
    "question": "What's the difference between yum -y install and yum install in CentOS",
    "answer": "If you supply -y it automatically chooses \"yes\" for future questions, i.e. are you sure you want to install squid? [Y/n]?.\nIt is handy if the installation takes a long time and asks multiple questions, which happens when you install multiple programs at once. In that case, having to type enter every now and again for the process to continue can be annoying.\nFor a full list of yum options and their definitions take a look at the help message for yum:\nyum -h",
    "tag": "yum"
  },
  {
    "question": "Unable to yum install anything on RHEL",
    "answer": "Centos has done it for you.\nCreate a repo file in /etc/yum.repos.d as \nvi /etc/yum.repos.d/myrepo.repo\n\nThen paste this in this file:\n[centos]\nname=CentOS-7\nbaseurl=http://ftp.heanet.ie/pub/centos/7/os/x86_64/\nenabled=1\ngpgcheck=1\ngpgkey=http://ftp.heanet.ie/pub/centos/7/os/x86_64/RPM-GPG-KEY-CentOS-7\n\nSaving it with wq! now run\nyum repolist\n\nCheck if you can install any package (say nmap)\nyum install nmap -y\n\nEnjoy!!!",
    "tag": "yum"
  },
  {
    "question": "No module named pip in venv but pip installed",
    "answer": "I ran into the exact same problem after installing Python 3.13 on WSL. Suddenly, all my existing virtual environments (created with Python 3.12) broke in VSCode. I was getting the \"Invalid Python interpreter\" error, Pylance couldn't resolve any imports, and pip appeared to be missing—even though I could see it in the venv/bin folder.\nHere’s what fixed it for me:\nFirst, check what your system python3 now points to:\npython3 --version\nwhich python3\n\nIn my case, it was now Python 3.13, which explains why stuff started breaking. Your virtual environment still points to the Python 3.12 binary internally, but VSCode (and maybe even pip) is trying to use 3.13 instead.\nYou can confirm that by looking at the pyvenv.cfg file inside your venv:\ncat venv/pyvenv.cfg\n\nYou should see something like:\nhome = /usr/bin/python3.12\n\nIf that's the case, then you just need to tell VSCode to use that exact interpreter. Open the command palette (Ctrl+Shift+P) in VSCode, choose “Python: Select Interpreter”, and manually select the path to your virtualenv’s Python binary:\n/path/to/your/venv/bin/python\n\nAlso, double-check the shebang in your pip script:\nhead -n 1 venv/bin/pip\n\nIf it says #!/usr/bin/python3, that might now point to Python 3.13, which breaks the venv. You can fix this by rebuilding the venv with the correct Python version:\npython3.12 -m venv --upgrade-deps venv\n\nOr, if that doesn’t work cleanly:\nrm -rf venv\npython3.12 -m venv venv\nsource venv/bin/activate\npip install -r requirements.txt\n\nAnd yeah, ensurepip being disabled for system Python is normal on Ubuntu. Just make sure you have the necessary packages installed:\nsudo apt install python3.12-venv python3.12-distutils\n\nOnce I manually selected the right interpreter in VSCode and fixed the pip shebang, everything worked again—IntelliSense, linting, imports, etc. Hope that helps.",
    "tag": "venv"
  },
  {
    "question": "python venv install skips component file \"pointer.png\"",
    "answer": "It's because it's not present in tool.setuptools.package-data in pyproject.toml file.\n[tool.setuptools.package-data]\n\"*\" = [\"*.fs\", \"*.vs\", \"*.inc\", \"*.gif\"]\n\nWith the previous configuration, you add all this extensions in your package as you can see in the next screenshot (content of the package uploaded on pypi).\n\nSo adding the png extension should work:\n[tool.setuptools.package-data]\n\"*\" = [\"*.fs\", \"*.vs\", \"*.inc\", \"*.gif\", \"*.png\"]",
    "tag": "venv"
  },
  {
    "question": "Why is my virtual environment not working?",
    "answer": "There is a very confusing amount of virtualenvironment-related programs around python which all sound very similarly.: pyenv, pyenv-virtualenv, virtualenv, pyvenv, venv.\npyenv allows you to switch between python versions.\nvirtualenv is a superset of venv (which is built-in as python -m venv).\npyenv-virtualenv combines the best of pyenv and virtualenv: switch between python versions and then use virtualenv on top of/using this python version.\npyvenv is just a wrapper script around venv, so one should avoid it and just use venv instead.\nBut they all have one thing in common: They don't pack an own Python version into their created virtualenvironment!\nEven poetry just uses a globally installed Python, but does not pack an own Python version into its virtual environment.\nThe virtualenvironment system which can do that is solely conda (or anaconda). It is however not any more license free since 2022.\nminiconda however - a stripped down minimalistic version of conda (anaconda has just all kind of packages with it so an overkill when one wants just some few python packages) - is license free. So install miniconda.\nAnd then you can create a new environment with it by:\nconda create --name myenv\n\n# enter it\nconda activate myenv\n\n# install python with version into it (always installs also pip)\nconda install -c conda-forge python=3.9.1\n\n# or install the newest python\nconda install -c conda-forge python\n\n# and from then on install all your python packages using pip\npip install pandas numpy\n\n# list all installed packages in the current environment\nconda list\n\n# list all pip-installed packages in the current enviornmet\npip list\n\n# exit the environment\nconda deactivate\n\nThe good thing of conda is, that it is a universal virtual environment manager. It is not python-specific. But you can use it for all kind of other programming languages too (R, Cpp, C, Rust, ...).\nJust create your conda environment. Install your programming language of choice - with the desired version. Install eventually their package managers into that environment. And then install with those package managers packages.\nIt is confusing with Python however, that anaconda contains already tons of python packages as conda packages. And some people think anaconda ist a Python virtual environment/package manager. But that's a very narrow view on (ana)conda. Anaconda ist just a repository of conda which contains most of Python's packages. conda install -c anaconda <pythonpackagename> mostly works.\nBut conda ist a universal package manager which helps you to make virtual environments reusable.\nHowever, Docker ist a more complete virtualization - because it also involves environment variables, the Operating System itself etc.\nConda is therefore much more lightweight virtualization than such a container.\nInstead of pip one should nowadays probably use uv, because it is much faster and with the lock file much more reproducible (pip is horrible in this aspect - upgrades packages without asking etc.). poetry is also a great virtualizer with a lock file - so you have full control over the package versions within the virtual environment.\nBut as I said, poetry doesn't come with its own Python version within the virtual environment.\nSo conda + pip + uv (use pip to do pip install uv and from then on use uv only and lock the versions) is currently the best version for a fully autonomous virtual environment which comes with its own python version!\nSo the better instruction is:\n# create a new environment\nconda create --name myenv\n\n# enter it\nconda activate myenv\n\n# install python with version into it (always installs also pip)\nconda install -c conda-forge python=3.9.1\n\n# or install the newest python\nconda install -c conda-forge python\n\n# using pip install uv\npip install uv\n\n# from now on install all your python packages using uv (uv pip):\nuv add pandas numpy\n# or uv has a quasi-pip interface with `uv pip`:\nuv pip install pandas numpy\n\n# list all installed packages in the current environment\nconda list\n\n# list all uv-installed packages in the current environment\nuv pip list\n\n# exit the environment\nconda deactivate",
    "tag": "venv"
  },
  {
    "question": "abjad.show() issues \"FileNotFoundError: [WinError 2] The system cannot find the file specified\" in Python",
    "answer": "Looking at the source of \"...\\abjad\\configuration.py\", line 388 (as pointed to by the error message), we have the following lines:\ncommand = [\"lilypond\", \"--version\"]\nproc = subprocess.run(command, stdout=subprocess.PIPE)\n\nSo the process is trying to run the command \"lilypond --version\", and failing because Windows cannot find an executable file or command script with that name. You should check where that command is installed and ensure that its full path is added to your PATH environment variable.",
    "tag": "venv"
  },
  {
    "question": "Pipenv on Windows &INI file location",
    "answer": "What you're seeing is related to how the Windows Store version of Python behaves — when you install Python from the Microsoft Store, it runs your scripts in a kind of sandboxed environment. That's why your .ini file ends up in that weird AppData\\Local\\Packages\\PythonSoftwareFoundation... path instead of your user home or working directory.\nHere’s what you can try:\n\nUse a \"normal\" Python install\nIf you haven’t already, download and install Python from python.org directly. Then make sure it's the version used by your terminal or VSCode. You can check this with:\nwhere python \n\nor in PowerShell:\nGet-Command python\n\n\nSet the working directory explicitly\nIf switching Python versions isn't an option, you can work around it in code. When writing your INI file, specify the exact path you want, like this:\nfrom pathlib import Path\n\nconfig_path = Path.home() / '.my_app_config.ini'  # or any other custom path\n\n\nCheck if Pipenv is actually the issue\nIn most cases, pipenv itself doesn’t affect the path of where files are written — unless your code uses os.getenv('HOME'), ~, or relies on os.path.expanduser() which might get messed up in that sandbox.",
    "tag": "venv"
  },
  {
    "question": "Check if a python module exists with specific venv path",
    "answer": "import sys\nimport importlib.util\nimport os\nfrom pathlib import Path\n\ndef is_module_installed_in_venv(module_name, venv_path):\n    venv_python_lib_path = Path(venv_path) / 'lib'\n    for python_dir in venv_python_lib_path.iterdir():\n        if python_dir.name.startswith('python'):\n            site_packages_path = python_dir / 'site-packages'\n            break\n    \n    if not site_packages_path.exists():\n        return False\n    \n    sys.path.insert(0, str(site_packages_path))\n\n    module_spec = importlib.util.find_spec(module_name)\n    \n    sys.path.pop(0)\n    \n    return module_spec is not None\n\nvenv_path = '/home/user/anaconda3/envs/env_name/' \nmodule_name = 'numpy'\n\nif is_module_installed_in_venv(module_name, venv_path):\n    print(\"do something\")\n\nthis works , make sure to include the full path",
    "tag": "venv"
  },
  {
    "question": "Can 2 VS Code windows for 2 git clones each work with their own venv?",
    "answer": "OK.  It was pretty simple.  I just had to:\n\nOn the command line: activate my venv: source .venv/bin/activate\ncode .\ncommand-shift-p\nClick \"Python: select interpreter\"\nClick \"Enter Interpreter path\"\nPaste and hit Enter\n\nI was mistakenly guessing that just activating the environment (either in the terminal from which I launch vscode or in vscode's terminal) was enough.  I probably did this for the primary clone long ago and just forgot.",
    "tag": "venv"
  },
  {
    "question": "Cannot import: `from serpapi import GoogleSearch`",
    "answer": "I want to share some possible steps to resolve your Import Error Issue. You can follow step by step, I hope it will fix the issue.\n1. Identify the Virtual Environment Issue\nThe root cause is often that PyCharm is using a different Python interpreter/virtual environment than the one where you installed the package.\n2. Check Current Package Installation\nIn your terminal or PyCharm terminal:\npip list | grep -E \"serpapi|google-search-results\"\n\nThis shows if and where the packages are installed.\n3. Install the Correct Package\nChoose ONE of these options:\n# Option A: Original package\npip install google-search-results\n\n# Option B: Newer package\npip install serpapi\n\n4. Configure PyCharm to Use the Correct Interpreter\n\nGo to File > Settings (or PyCharm > Preferences on macOS)\n\nNavigate to Project > Python Interpreter\n\nClick the gear icon ⚙️ and select Add...\n\nChoose the virtual environment where you installed the package:\n\nExisting: Select the path to your virtual environment\n\nNew: Create a new virtual environment in your project directory\n\n\n\n\n5. Verify the Package is Available\nAfter selecting the correct interpreter, check that the package appears in the list of installed packages.\n6. Use the Correct Import Statement\n# For google-search-results package\nfrom serpapi import GoogleSearch\n\n# For newer serpapi package (if needed)\nfrom serpapi.google_search import GoogleSearch\n# OR\nfrom serpapi.serp_api_client import GoogleSearch\n\n7. Invalidate Caches and Restart\n\nGo to File > Invalidate Caches / Restart\n\nSelect Invalidate and Restart\n\n\n8. Update Run Configuration\n\nGo to Run > Edit Configurations\n\nEnsure your run configuration is using the correct Python interpreter\n\nSave the configuration",
    "tag": "venv"
  },
  {
    "question": "How to upgrade all Python packages with pip",
    "answer": "There isn't a built-in flag yet. Starting with pip version 22.3, the --outdated and --format=freeze have become mutually exclusive. Use Python, to parse the JSON output:\npip --disable-pip-version-check list --outdated --format=json | python -c \"import json, sys; print('\\n'.join([x['name'] for x in json.load(sys.stdin)]))\" | xargs -n1 pip install -U\n\nIf you are using pip<22.3 you can use:\npip list --outdated --format=freeze | grep -v '^\\-e' | cut -d = -f 1  | xargs -n1 pip install -U\n\nFor older versions of pip:\npip freeze --local | grep -v '^\\-e' | cut -d = -f 1  | xargs -n1 pip install -U\n\n\n\nThe grep is to skip editable (\"-e\") package definitions, as suggested by @jawache. (Yes, you could replace grep+cut with sed or awk or perl or...).\n\nThe -n1 flag for xargs prevents stopping everything if updating one package fails (thanks @andsens).\n\n\n\nNote: there are infinite potential variations for this. I'm trying to keep this answer short and simple, but please do suggest variations in the comments!",
    "tag": "pip"
  },
  {
    "question": "How do I install pip on Windows?",
    "answer": "Python 3.4+ and 2.7.9+\nGood news! Python 3.4 (released March 2014) and Python 2.7.9 (released December 2014) ship with Pip. This is the best feature of any Python release. It makes the community's wealth of libraries accessible to everyone. Newbies are no longer excluded from using community libraries by the prohibitive difficulty of setup. In shipping with a package manager, Python joins Ruby, Node.js, Haskell, Perl, Go—almost every other contemporary language with a majority open-source community. Thank you, Python.\nIf you do find that pip is not available, simply run ensurepip.\n\nOn Windows:\npy -3 -m ensurepip\n\n\nOtherwise:\npython3 -m ensurepip\n\n\n\nOf course, that doesn't mean Python packaging is problem solved. The experience remains frustrating. I discuss this in the Stack Overflow question Does Python have a package/module management system?.\nPython 3 ≤ 3.3 and 2 ≤ 2.7.8\nFlying in the face of its 'batteries included' motto, Python ships without a package manager. To make matters worse, Pip was—until recently—ironically difficult to install.\nOfficial instructions\nPer https://pip.pypa.io/en/stable/installing/#do-i-need-to-install-pip:\nDownload get-pip.py, being careful to save it as a .py file rather than .txt. Then, run it from the command prompt:\npython get-pip.py\n\nYou possibly need an administrator command prompt to do this. Follow Start a Command Prompt as an Administrator (Microsoft TechNet).\nThis installs the pip package, which (in Windows) contains ...\\Scripts\\pip.exe that path must be in PATH environment variable to use pip from the command line (see the second part of 'Alternative Instructions' for adding it to your PATH,\nAlternative instructions\nThe official documentation tells users to install Pip and each of its dependencies from source. That's tedious for the experienced and prohibitively difficult for newbies.\nFor our sake, Christoph Gohlke prepares Windows installers (.msi) for popular Python packages. He builds installers for all Python versions, both 32 and 64 bit. You need to:\n\nInstall setuptools\nInstall pip\n\nFor me, this installed Pip at C:\\Python27\\Scripts\\pip.exe. Find pip.exe on your computer, then add its folder (for example, C:\\Python27\\Scripts) to your path (Start / Edit environment variables). Now you should be able to run pip from the command line. Try installing a package:\npip install httpie\n\nThere you go (hopefully)! Solutions for common problems are given below:\nProxy problems\nIf you work in an office, you might be behind an HTTP proxy. If so, set the environment variables http_proxy and https_proxy. Most Python applications (and other free software) respect these. Example syntax:\nhttp://proxy_url:port\nhttp://username:password@proxy_url:port\n\nIf you're really unlucky, your proxy might be a Microsoft NTLM proxy. Free software can't cope. The only solution is to install a free software friendly proxy that forwards to the nasty proxy. http://cntlm.sourceforge.net/\nUnable to find vcvarsall.bat\nPython modules can be partly written in C or C++. Pip tries to compile from source. If you don't have a C/C++ compiler installed and configured, you'll see this cryptic error message.\n\nError: Unable to find vcvarsall.bat\n\nYou can fix that by installing a C++ compiler such as MinGW or Visual C++. Microsoft actually ships one specifically for use with Python. Or try Microsoft Visual C++ Compiler for Python 2.7.\nOften though it's easier to check Christoph's site for your package.",
    "tag": "pip"
  },
  {
    "question": "Installing specific package version with pip",
    "answer": "TL;DR:\nUpdate as of 2022-12-28:\npip install --force-reinstall -v\nFor example: pip install --force-reinstall -v \"MySQL_python==1.2.2\"\nWhat these options mean:\n\n--force-reinstall is an option to reinstall all packages even if they are already up-to-date.\n-v is for verbose. You can combine for even more verbosity (i.e. -vv) up to 3 times (e.g. --force-reinstall -vvv).\n\nThanks to @Peter for highlighting this (and it seems that the context of the question has broadened given the time when the question was first asked!), the documentation for Python discusses a caveat with using -I, in that it can break your installation if it was installed with a different package manager or if if your package is/was a different version.\n\nOriginal answer:\n\npip install -Iv (i.e. pip install -Iv MySQL_python==1.2.2)\n\n\nWhat these options mean:\n\n-I stands for --ignore-installed which will ignore the installed packages, overwriting them.\n-v is for verbose. You can combine for even more verbosity (i.e. -vv) up to 3 times (e.g. -Ivvv).\n\nFor more information, see pip install --help\nFirst, I see two issues with what you're trying to do. Since you already have an installed version, you should either uninstall the current existing driver or use pip install -I MySQL_python==1.2.2\nHowever, you'll soon find out that this doesn't work. If you look at pip's installation log, or if you do a pip install -Iv MySQL_python==1.2.2 you'll find that the PyPI URL link does not work for MySQL_python v1.2.2. You can verify this here: http://pypi.python.org/pypi/MySQL-python/1.2.2\nThe download link 404s and the fallback URL links are re-directing infinitely due to sourceforge.net's recent upgrade and PyPI's stale URL.\nSo to properly install the driver, you can follow these steps:\npip uninstall MySQL_python\npip install -Iv http://sourceforge.net/projects/mysql-python/files/mysql-python/1.2.2/MySQL-python-1.2.2.tar.gz/download",
    "tag": "pip"
  },
  {
    "question": "How can I install packages using pip according to the requirements.txt file from a local directory?",
    "answer": "This works for everyone:\npip install -r /path/to/requirements.txt\n\nExplanation:\n\n-r, --requirement < filename >\n\nInstall from the given requirements file. This option can be used multiple times.",
    "tag": "pip"
  },
  {
    "question": "How do I install pip on macOS or OS X?",
    "answer": "On Linux or MacOS:\npython -m ensurepip --upgrade\n\nIf you want to install pip for Python 3, replace python with python3.\nSee https://pip.pypa.io/en/stable/installation/ for more details.",
    "tag": "pip"
  },
  {
    "question": "pg_config executable not found",
    "answer": "pg_config is in postgresql-devel (libpq-dev in Debian/Ubuntu, libpq-devel on Centos/Fedora/Cygwin/Babun.)",
    "tag": "pip"
  },
  {
    "question": "How do I remove all packages installed by pip?",
    "answer": "I've found this snippet as an alternative solution. It's a more graceful removal of libraries than remaking the virtualenv:\npip freeze | xargs pip uninstall -y\n\n\nIn case you have packages installed via VCS, you need to exclude those lines and remove the packages manually (elevated from the comments below):\npip freeze --exclude-editable | xargs pip uninstall -y\n\n\nIf you have packages installed directly from github/gitlab, those will have @.\nLike:\ndjango @ git+https://github.com/django.git@<sha>\nYou can add cut -d \"@\" -f1 to get just the package name that is required to uninstall it.\npip freeze | cut -d \"@\" -f1 | xargs pip uninstall -y",
    "tag": "pip"
  },
  {
    "question": "How do I get a list of locally installed Python modules?",
    "answer": "help('modules')\n\nin a Python shell/prompt.",
    "tag": "pip"
  },
  {
    "question": "How do I install a Python package with a .whl file?",
    "answer": "I just used the following which was quite simple. First open a console then cd to where you've downloaded your file like some-package.whl and use\npip install some-package.whl\n\nNote: if pip.exe is not recognized, you may find it in the \"Scripts\" directory from where python has been installed. If pip is not installed, this page can help:\nHow do I install pip on Windows?\nNote: for clarification\nIf you copy the *.whl file to your local drive (ex. C:\\some-dir\\some-file.whl) use the following command line parameters --  \npip install C:/some-dir/some-file.whl",
    "tag": "pip"
  },
  {
    "question": "Find which version of package is installed with pip",
    "answer": "As of pip 1.3, there is a pip show command.\n$ pip show Jinja2\n---\nName: Jinja2\nVersion: 2.7.3\nLocation: /path/to/virtualenv/lib/python2.7/site-packages\nRequires: markupsafe\n\nIn older versions, pip freeze and grep should do the job nicely.\n$ pip freeze | grep Jinja2\nJinja2==2.7.3",
    "tag": "pip"
  },
  {
    "question": "pip install from git repo branch",
    "answer": "Prepend the url prefix git+ (See VCS Support):\npip install git+https://github.com/tangentlabs/django-oscar-paypal.git@issue/34/oscar-0.6\n\nAnd specify the branch name without the leading /.",
    "tag": "pip"
  },
  {
    "question": "What is the difference between pip and Conda?",
    "answer": "Quoting from the Conda blog:\n\nHaving been involved in the python world for so long, we are all aware of pip, easy_install, and virtualenv, but these tools did not meet all of our specific requirements. The main problem is that they are focused around Python, neglecting non-Python library dependencies, such as HDF5, MKL, LLVM, etc., which do not have a setup.py in their source code and also do not install files into Python’s site-packages directory.\n\nSo Conda is a packaging tool and installer that aims to do more than what pip does; handle library dependencies outside of the Python packages as well as the Python packages themselves. Conda also creates a virtual environment, like virtualenv does.\nAs such, Conda should be compared to Buildout perhaps, another tool that lets you handle both Python and non-Python installation tasks.\nBecause Conda introduces a new packaging format, you cannot use pip and Conda interchangeably;  pip cannot install the Conda package format. You can use the two tools side by side (by installing pip with conda install pip) but they do not interoperate either.\nSince writing this answer, Anaconda has published a new page on Understanding Conda and Pip, which echoes this as well:\n\nThis highlights a key difference between conda and pip. Pip installs Python packages whereas conda installs packages which may contain software written in any language. For example, before using pip, a Python interpreter must be installed via a system package manager or by downloading and running an installer. Conda on the other hand can install Python packages as well as the Python interpreter directly.\n\nand further on\n\nOccasionally a package is needed which is not available as a conda package but is available on PyPI and can be installed with pip. In these cases, it makes sense to try to use both conda and pip.",
    "tag": "pip"
  },
  {
    "question": "How do I update/upgrade pip itself from inside my virtual environment?",
    "answer": "pip is just a PyPI package like any other; you could use it to upgrade itself the same way you would upgrade any package:\npip install --upgrade pip\n\nOn Windows the recommended command is:\npy -m pip install --upgrade pip",
    "tag": "pip"
  },
  {
    "question": "Why use pip over easy_install?",
    "answer": "From Ian Bicking's own introduction to pip:\n\npip was originally written to improve on easy_install in the following ways\n\nAll packages are downloaded before installation. Partially-completed installation doesn’t occur as a result.\nCare is taken to present useful output on the console.\nThe reasons for actions are kept track of. For instance, if a package is being installed, pip keeps track of why that package was required.\nError messages should be useful.\nThe code is relatively concise and cohesive, making it easier to use programmatically.\nPackages don’t have to be installed as egg archives, they can be installed flat (while keeping the egg metadata).\nNative support for other version control systems (Git, Mercurial and Bazaar)\nUninstallation of packages.\nSimple to define fixed sets of requirements and reliably reproduce a set of packages.",
    "tag": "pip"
  },
  {
    "question": "pip install mysql-python fails with EnvironmentError: mysql_config not found",
    "answer": "It seems mysql_config is missing on your system or the installer could not find it.\nBe sure mysql_config is really installed.\nFor example on Debian/Ubuntu you must install the package:\nsudo apt-get install libmysqlclient-dev\n\nMaybe the mysql_config is not in your path, it will be the case when you compile by yourself\nthe mysql suite.\nUpdate: For recent versions of debian/ubuntu (as of 2018) it is \nsudo apt install default-libmysqlclient-dev",
    "tag": "pip"
  },
  {
    "question": "How do I solve \"error: externally-managed-environment\" every time I use pip 3?",
    "answer": "The proper way to install Python libraries and applications is to install them in a Python virtual environment whenever possible (the exceptions to this rule are quite rare).\nThe error message describes two common ways to accomplish this: either by creating a virtual environment yourself, or for applications, by using pipx—a tool which will create a virtual environment for you and install the application in that virtual environment.\npipx is strongly recommended for installing applications, i.e., when you will primarily use the installed code from the command line. On Debian systems and Debian-based systems such as Ubuntu, you can install pipx using apt, and then use pipx to install the application:\napt install pipx\npipx install some-python-application\n\nFor libraries, i.e., when you will use the code primarily by importing it in your own projects. Typically, you should create a virtual environment yourself. You can do this with venv from the standard library:\npython -m venv my-venv\nmy-venv/bin/pip install some-python-library\n\nSee also this answer on a duplicate question for more details.\n(Commonly, your own project may need several libraries. Make one virtual environment and install the libraries that your project needs side by side in that virtual environment.)\n\nIf you have considered your options carefully and are still sure that you want to install packages \"system-wide\" and risk breaking your system (for example, by overwriting libraries that were part of tools written in Python that came with your system), Pip needs to be given permission to do so.\nThere are a few ways to do this:\n\nFor a single use of pip, add the --break-system-packages argument to the command.\n\nAdd these lines to ~/.config/pip/pip.conf (this will enable every future run of Pip to break system packages:\n[global]\nbreak-system-packages = true\n\n\nUse Pip's config command to edit the above file (credit to The Matt from the comments):\npython3 -m pip config set global.break-system-packages true\n\n\n\nTheoretically, removing or renaming the \"marker\" file (/usr/lib/python3.x/EXTERNALLY-MANAGED) would also disable the block, but this is a bad idea. The file was put there for a reason, and it's at least as easy to use the intended mechanisms instead.",
    "tag": "pip"
  },
  {
    "question": "Can I force pip to reinstall the current version?",
    "answer": "pip install --upgrade --force-reinstall <package>\n\nWhen upgrading, reinstall all packages even if they are already up-to-date.\npip install -I <package>\npip install --ignore-installed <package>\n\nIgnore the installed packages (reinstalling instead).",
    "tag": "pip"
  },
  {
    "question": "How to list all available package versions with pip?",
    "answer": "For pip >= 21.2 use:\npip index versions pylibmc\n\nNote that this command is experimental, and might change in the future!\n\nBelow approach breaks with pip 24.1 released on 2024-06-21.\nFor pip >= 21.1 use:\npip install pylibmc==\n\nFor pip >= 20.3 use:\npip install --use-deprecated=legacy-resolver pylibmc==\n\nFor pip >= 9.0 use:\n$ pip install pylibmc==\nCollecting pylibmc==\n  Could not find a version that satisfies the requirement pylibmc== (from \n  versions: 0.2, 0.3, 0.4, 0.5.1, 0.5.2, 0.5.3, 0.5.4, 0.5.5, 0.5, 0.6.1, 0.6, \n  0.7.1, 0.7.2, 0.7.3, 0.7.4, 0.7, 0.8.1, 0.8.2, 0.8, 0.9.1, 0.9.2, 0.9, \n  1.0-alpha, 1.0-beta, 1.0, 1.1.1, 1.1, 1.2.0, 1.2.1, 1.2.2, 1.2.3, 1.3.0)\nNo matching distribution found for pylibmc==\n\nThe available versions will be printed without actually downloading or installing any packages.\nFor pip < 9.0 use:\npip install pylibmc==blork\n\nwhere blork can be any string that is not a valid version number.",
    "tag": "pip"
  },
  {
    "question": "error: Unable to find vcvarsall.bat",
    "answer": "Update: Comments point out that the instructions here may be dangerous. Consider using the Visual C++ 2008 Express edition or the purpose-built Microsoft Visual C++ Compiler for Python (details) and NOT using the original answer below. Original error message means the required version of Visual C++ is not installed.\n\nFor Windows installations:\nWhile running setup.py for package installations, Python 2.7 searches for an installed Visual Studio 2008. You can trick Python to use a newer Visual Studio by setting the correct path in VS90COMNTOOLS environment variable before calling setup.py.\nExecute the following command based on the version of Visual Studio installed:\n\nVisual Studio 2010 (VS10): SET VS90COMNTOOLS=%VS100COMNTOOLS%\nVisual Studio 2012 (VS11): SET VS90COMNTOOLS=%VS110COMNTOOLS%\nVisual Studio 2013 (VS12): SET VS90COMNTOOLS=%VS120COMNTOOLS%\nVisual Studio 2015 (VS14): SET VS90COMNTOOLS=%VS140COMNTOOLS%\n\n\nWARNING: As noted below, this answer is unlikely to work if you are trying to compile python modules.\nSee Building lxml for Python 2.7 on Windows for details.",
    "tag": "pip"
  },
  {
    "question": "How to update/upgrade a package using pip?",
    "answer": "This is the way\npip install <package_name> --upgrade\n\nor in short\npip install <package_name> -U\n\nUsing sudo will ask to enter your root password to confirm the action, but although common, is considered unsafe.\nIf you do not have a root password (if you are not the admin) you should probably work with virtualenv.\nYou can also use the user flag to install it on this user only.\npip install <package_name> --upgrade --user",
    "tag": "pip"
  },
  {
    "question": "How to state in requirements.txt a direct github source",
    "answer": "Normally your requirements.txt file would look something like this:\npackage-one==1.9.4\npackage-two==3.7.1\npackage-three==1.0.1\n...\n\nTo specify a Github repo, you do not need the package-name== convention.\nThe examples below update package-two using a GitHub repo. The text after @ denotes the specifics of the package.\nSpecify commit hash (41b95ec in the context of updated requirements.txt):\npackage-one==1.9.4\npackage-two @ git+https://github.com/owner/repo@41b95ec\npackage-three==1.0.1\n\nSpecify branch name (main):\npackage-two @ git+https://github.com/owner/repo@main\n\nSpecify tag (0.1):\npackage-two @ git+https://github.com/owner/repo@0.1\n\nSpecify release (3.7.1):\npackage-two @ git+https://github.com/owner/repo@releases/tag/v3.7.1\n\nNote that in certain versions of pip you will need to update the package version in the package's setup.py, or pip will assume the requirement is already satisfied and not install the new version. For instance, if you have 1.2.1 installed, and want to fork this package with your own version, you could use the above technique in your requirements.txt and then update setup.py to 1.2.1.1.\nSee also the pip documentation on VCS support.",
    "tag": "pip"
  },
  {
    "question": "Where does pip install its packages?",
    "answer": "pip show <package name> will provide the location for Windows and macOS, and I'm guessing any system. :)\nFor example:\n> pip show cvxopt\nName: cvxopt\nVersion: 1.2.0\n...\nLocation: /usr/local/lib/python2.7/site-packages",
    "tag": "pip"
  },
  {
    "question": "pip install fails with \"connection error: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:598)\"",
    "answer": "UPDATE: 2024-04: This solution is insecure and can lead to other issues like mitm. This was given as a solution where the issue is trying to access package repository behind corporate firewalls. Please evaluate before considering this as a solution.\nuse trusted-host in either pip.conf or command line argument\nYou can ignore SSL errors by setting pypi.org and files.pythonhosted.org as well as the older pypi.python.org as trusted hosts.\n$ pip install --trusted-host pypi.org --trusted-host pypi.python.org --trusted-host files.pythonhosted.org <package_name>\n\nNote: Sometime during April 2018, the Python Package Index was migrated from pypi.python.org to pypi.org. This means \"trusted-host\" commands using the old domain no longer work, but you can add both.\nPermanent Fix\nSince the release of pip 10.0, you should be able to fix this permanently just by upgrading pip itself:\n$ pip install --trusted-host pypi.org --trusted-host pypi.python.org --trusted-host files.pythonhosted.org pip setuptools\n\nOr by just reinstalling it to get the latest version:\n$ curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py\n\n(… and then running get-pip.py with the relevant Python interpreter).\npip install <otherpackage> should just work after this. If not, then you will need to do more, as explained below.\n\nYou may want to add the trusted hosts and proxy to your config file.\npip.ini (Windows) or pip.conf (unix)\n[global]\ntrusted-host = pypi.python.org\n               pypi.org\n               files.pythonhosted.org\n\n\nAlternate Solutions (Less secure)\nAll of these answers shared to this question have a security risk associated with them, whether it is to disable SSL verification, add trusted domain, use self signed certificates, etc. Use this solution only if you are behind a corporate firewall and you understand that the risk are handled.\nTwo of the workarounds that help in installing most of the python packages with ease would be:\n\nUsing easy_install: if you are really lazy and don't want to waste much time, use easy_install <package_name>. Note that some packages won't be found or will give small errors.\nUsing Wheel: download the Wheel of the python package and use the pip command pip install wheel_package_name.whl to install the package.",
    "tag": "pip"
  },
  {
    "question": "Dealing with multiple Python versions and PIP",
    "answer": "The current recommendation is to use python -m pip, where python is the version of Python you would like to use. This is the recommendation because it works across all versions of Python, and in all forms of virtualenv. For example:\n# The system default Python installation:\npython -m pip install fish\n\n# A virtualenv's Python installation:\n.env/bin/python -m pip install fish\n\n# A specific version of python:\npython-3.6 -m pip install fish\n\nPrevious answer, left for posterity:\nSince version 0.8, Pip supports pip-{version}. You can use it the same as easy_install-{version}:\npip-2.5 install myfoopackage\npip-2.6 install otherpackage\npip-2.7 install mybarpackage\n\n\npip changed its schema to use pipVERSION instead of pip-VERSION in version 1.5. You should use the following if you have pip >= 1.5:\npip2.6 install otherpackage\npip2.7 install mybarpackage\n\nCheck Versioned commands consistent with Python. #1053 for more details\n\nReferences:\n\nNeed pip-x.y scripts #200\nv0.8 changelog or News for pip, v0.8",
    "tag": "pip"
  },
  {
    "question": "How to install psycopg2 with \"pip\" on Python?",
    "answer": "Note: Since a while back, there are binary wheels for Windows in PyPI, so this should no longer be an issue for Windows users. Below are solutions for Linux, Mac users, since lots of them find this post through web searches.\n\nOption 1\nInstall the psycopg2-binary PyPI package instead, it has Python wheels for Linux and Mac OS.\npip install psycopg2-binary\n\n\nOption 2\nInstall the prerequsisites for building the psycopg2 package from source:\nDebian/Ubuntu\n\n\n\n\nPython version\nCommand\nNote\n\n\n\n\nDefault Python 3\nsudo apt install libpq-dev python3-dev\n\n\n\nPython 3.x\nsudo apt install libpq-dev python3.x-dev\nsubstitute x in command\n\n\nPython 2\nsudo apt install libpq-dev python-dev\n\n\n\n\n\nIf that's not enough, you might additionally need to install\nsudo apt install build-essential\n\nor\nsudo apt install postgresql-server-dev-all\n\nas well before installing psycopg2 again.\nCentOS 6\nSee Banjer's answer\nmacOS\nSee nichochar's answer",
    "tag": "pip"
  },
  {
    "question": "TensorFlow not found using pip",
    "answer": "I found this to finally work.\npython3 -m pip install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.12.0-py3-none-any.whl\n\nEdit 1: This was tested on Windows (8, 8.1, 10), Mac and Linux. Change python3 to python according to your configuration. Change py3 to py2 in the url if you are using Python 2.x.\nEdit 2: A list of different versions if someone needs: https://storage.googleapis.com/tensorflow\nEdit 3: A list of urls for the available wheel packages is available here:\nhttps://www.tensorflow.org/install/pip#package-location",
    "tag": "pip"
  },
  {
    "question": "No module named pkg_resources",
    "answer": "July 2018 Update \nMost people should now use pip install setuptools (possibly with sudo).\nSome may need to (re)install the python-setuptools package via their package manager (apt-get install, yum install, etc.).\nThis issue can be highly dependent on your OS and dev environment. See the legacy/other answers below if the above isn't working for you.\nExplanation\nThis error message is caused by a missing/broken Python setuptools package. Per Matt M.'s comment and setuptools issue #581, the bootstrap script referred to below is no longer the recommended installation method.\nThe bootstrap script instructions will remain below, in case it's still helpful to anyone.\nLegacy Answer\nI encountered the same ImportError today while trying to use pip. Somehow the setuptools package had been deleted in my Python environment.\nTo fix the issue, run the setup script for setuptools:\nwget https://bootstrap.pypa.io/ez_setup.py -O - | python\n\n(or if you don't have wget installed (e.g. OS X), try\ncurl https://bootstrap.pypa.io/ez_setup.py | python\n\npossibly with sudo prepended.)\nIf you have any version of distribute, or any setuptools below 0.6, you will have to uninstall it first.*\nSee Installation Instructions for further details.\n\n* If you already have a working distribute, upgrading it to the \"compatibility wrapper\" that switches you over to setuptools is easier. But if things are already broken, don't try that.",
    "tag": "pip"
  },
  {
    "question": "pip uses incorrect cached package version, instead of the user-specified version",
    "answer": "If using pip 6.0 or newer, try adding the --no-cache-dir option (source).\nIf using pip older than pip 6.0, upgrade it with pip install -U pip.",
    "tag": "pip"
  },
  {
    "question": "How to install pip with Python 3?",
    "answer": "edit: Manual installation and use of setuptools is not the standard process anymore.\nIf you're running Python 2.7.9+ or Python 3.4+\nCongrats, you should already have pip installed. If you do not, read onward.\nIf you're running a Unix-like System\nYou can usually install the package for pip through your package manager if your version of Python is older than 2.7.9 or 3.4, or if your system did not include it for whatever reason.\nInstructions for some of the more common distros follow.\nInstalling on Debian (Wheezy and newer) and Ubuntu (Trusty Tahr and newer) for Python 2.x\nRun the following command from a terminal:\nsudo apt-get install python-pip \n\nInstalling on Debian (Wheezy and newer) and Ubuntu (Trusty Tahr and newer) for Python 3.x\nRun the following command from a terminal:\nsudo apt-get install python3-pip\n\nNote:\nOn a fresh Debian/Ubuntu install, the package may not be found until you do:\nsudo apt-get update\n\nInstalling pip on CentOS 7 for Python 2.x\nOn CentOS 7, you have to install setup tools first, and then use that to install pip, as there is no direct package for it.\nsudo yum install python-setuptools\nsudo easy_install pip\n\nInstalling pip on CentOS 7 for Python 3.x\nAssuming you installed Python 3.4 from EPEL, you can install Python 3's setup tools and use it to install pip.\n# First command requires you to have enabled EPEL for CentOS7\nsudo yum install python34-setuptools\nsudo easy_install pip\n\nIf your Unix/Linux distro doesn't have it in package repos\nInstall using the manual way detailed below.\nThe manual way\nIf you want to do it the manual way, the now-recommended method is to install using the get-pip.py script from pip's installation instructions.\n\nInstall pip\nTo install pip, securely download get-pip.py\nThen run the following (which may require administrator access):\npython get-pip.py \n\nIf setuptools is not already installed, get-pip.py will install setuptools for you.",
    "tag": "pip"
  },
  {
    "question": "How can I upgrade specific packages using pip and a requirements file?",
    "answer": "I ran the following command and it upgraded from 1.2.3 to 1.4.0\npip install Django --upgrade\n\nShortcut for upgrade:\npip install Django -U\n\nNote: if the package you are upgrading has any requirements this command will additionally upgrade all the requirements to the latest versions available.  In recent versions of pip, you can prevent this behavior by specifying --upgrade-strategy only-if-needed.  With that flag, dependencies will not be upgraded unless the installed versions of the dependent packages no longer satisfy the requirements of the upgraded package.",
    "tag": "pip"
  },
  {
    "question": "bash: pip: command not found",
    "answer": "Why not just do sudo easy_install pip or if this is for python 2.6 sudo easy_install-2.6 pip?\nThis installs pip using the default python package installer system and saves you the hassle of manual set-up all at the same time.\nThis will allow you to then run the pip command for python package installation as it will be installed with the system python. I also recommend once you have pip using the virtualenv package and pattern. :)",
    "tag": "pip"
  },
  {
    "question": "Install a Python package into a different directory using pip?",
    "answer": "The --target switch is the thing you're looking for:\npip install --target d:\\somewhere\\other\\than\\the\\default package_name\n\nBut you still need to add d:\\somewhere\\other\\than\\the\\default to PYTHONPATH to actually use them from that location.\n\n-t, --target <dir>\nInstall packages into <dir>. By default this will not replace existing files/folders in <dir>.\nUse --upgrade to replace existing packages in <dir> with new versions.\n\n\nUpgrade pip if target switch is not available:\nOn Linux or OS X:\npip install -U pip\n\nOn Windows (this works around an issue):\npython -m pip install -U pip",
    "tag": "pip"
  },
  {
    "question": "'pip' is not recognized as an internal or external command",
    "answer": "You need to add the path of your pip installation to your PATH system variable. By default, pip is installed to C:\\Python34\\Scripts\\pip (pip now comes bundled with new versions of python), so the path \"C:\\Python34\\Scripts\" needs to be added to your PATH variable.\nTo check if it is already in your PATH variable, type echo %PATH% at the CMD prompt\nTo add the path of your pip installation to your PATH variable, you can use the Control Panel or the setx command. For example:\nsetx PATH \"%PATH%;C:\\Python34\\Scripts\"\n\n\nNote:\nAccording to the official documentation, \"[v]ariables set with setx variables are available in future command windows only, not in the current command window\". In particular, you will need to start a new cmd.exe instance after entering the above command in order to utilize the new environment variable. \nThanks to Scott Bartell for pointing this out.",
    "tag": "pip"
  },
  {
    "question": "Is it possible to use pip to install a package from a private GitHub repository?",
    "answer": "You can use the git+ssh URI scheme, but you must set a username. Notice the git@ part in the URI:\npip install git+ssh://git@github.com/echweb/echweb-utils.git\n\nAlso read about deploy keys.\nPS: In my installation, the \"git+ssh\" URI scheme works only with \"editable\" requirements:\npip install -e URI#egg=EggName\n\nRemember: Change the : character that git remote -v prints to a / character before using the remote's address in the pip command:\n$ git remote -v\norigin  git@github.com:echweb/echweb-utils.git (fetch)\n#                     ^ change this to a '/' character\n\nIf you forget, you will get this error:\nssh: Could not resolve hostname github.com:echweb:\n         nodename nor servname provided, or not known",
    "tag": "pip"
  },
  {
    "question": "Installing Python packages from local file system folder to virtualenv with pip",
    "answer": "What about::\npip install --help\n...\n  -e, --editable <path/url>   Install a project in editable mode (i.e. setuptools\n                              \"develop mode\") from a local project path or a VCS url.\n\neg, pip install -e /srv/pkg\nwhere /srv/pkg is the top-level directory where 'setup.py' can be found.",
    "tag": "pip"
  },
  {
    "question": "How can I Install a Python module with Pip programmatically (from my code)?",
    "answer": "The officially recommended way to install packages from a script is by calling pip's command-line interface via a subprocess. Most other answers presented here are not supported by pip. Furthermore since pip v10, all code has been moved to pip._internal precisely in order to make it clear to users that programmatic use of pip is not allowed.\nUse sys.executable to ensure that you will call the same pip associated with the current runtime.\nimport subprocess\nimport sys\n\ndef install(package):\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])",
    "tag": "pip"
  },
  {
    "question": "setup script exited with error: command 'x86_64-linux-gnu-gcc' failed with exit status 1",
    "answer": "I encountered the same problem in college having installed Linux Mint for the main project of my final year, the third solution below worked for me.\nWhen encountering this error please note before the error it may say you are missing a package or header file — you should find those and install them and verify if it works (e.g. ssl → libssl).\nFor Python 2.x use:\nsudo apt-get install python-dev\n\nFor Python 2.7 use:\nsudo apt-get install libffi-dev\n\nFor Python 3.x use:\nsudo apt-get install python3-dev\n\nor for a specific version of Python 3, replace x with the minor version in\nsudo apt-get install python3.x-dev",
    "tag": "pip"
  },
  {
    "question": "pip is configured with locations that require TLS/SSL, however the ssl module in Python is not available",
    "answer": "For Windows 10\nif you want use pip in normal cmd, not only in Anaconda prompt. you need add 3 environment paths.\nlike the followings:\nD:\\Anaconda3 \nD:\\Anaconda3\\Scripts\nD:\\Anaconda3\\Library\\bin \n\nmost people only add D:\\Anaconda3\\Scripts",
    "tag": "pip"
  },
  {
    "question": "How do I install the yaml package for Python?",
    "answer": "You could try the search the feature on https://pypi.org/search (via a browser) and look for packages in PyPI with yaml in the short description.  That reveals various packages, including PyYaml, yamltools, and PySyck, among others (Note that PySyck docs recommend using PyYaml, since syck is out of date).  Now you know a specific package name, you can install it:\n$ pip install pyyaml\n\nIf you want to install python yaml system-wide in linux, you can also use a package manager, like aptitude or yum:\n$ sudo apt-get install python-yaml\n$ sudo yum install python-yaml",
    "tag": "pip"
  },
  {
    "question": "Error after upgrading pip: cannot import name 'main'",
    "answer": "You must have inadvertently upgraded your system pip (probably through something like sudo pip install pip --upgrade)\npip 10.x adjusts where its internals are situated.  The pip3 command you're seeing is one provided by your package maintainer (presumably debian based here?) and is not a file managed by pip.\nYou can read more about this on pip's issue tracker\nYou'll probably want to not upgrade your system pip and instead use a virtualenv.\nTo recover the pip3 binary you'll need to sudo python3 -m pip uninstall pip && sudo apt install python3-pip --reinstall\nIf you want to continue in \"unsupported territory\" (upgrading a system package outside of the system package manager), you can probably get away with python3 -m pip ... instead of pip3.",
    "tag": "pip"
  },
  {
    "question": "Using Pip to install packages to an Anaconda environment",
    "answer": "For others who run into this situation, I found this to be the most straightforward solution:\n\nRun conda create -n venv_name and conda activate venv_name, where venv_name is the name of your virtual environment.\n\nRun conda install pip. This will install pip to your venv directory.\n\nFind your anaconda directory, and find the actual venv folder. It should be somewhere like /anaconda/envs/venv_name/; or, you could also run conda activate venv_name.\n\nInstall new packages by doing /anaconda/envs/venv_name/bin/pip install package_name; or, simply run pip install package_name.\n\n\nThis should now successfully install packages using that virtual environment's pip!",
    "tag": "pip"
  },
  {
    "question": "Error \"filename.whl is not a supported wheel on this platform\"",
    "answer": "cp33 means CPython 3.3.\nYou need scipy‑0.15.1‑cp27‑none‑win_amd64.whl instead.",
    "tag": "pip"
  },
  {
    "question": "Can I add comments to a pip requirements file?",
    "answer": "Sure, you can, just use #\npip docs:\n\nA line that begins with # is treated as a comment and ignored. Whitespace followed by a # causes the # and the remainder of the line to be treated as a comment.",
    "tag": "pip"
  },
  {
    "question": "How to pip install a package with min and max version range?",
    "answer": "You can do:\n$ pip install \"package>=0.2,<0.3\"\n\nAnd pip will look for the best match, assuming the version is at least 0.2, and less than 0.3.\nThis also applies to pip requirements files.  See the full details on version specifiers in PEP 440.",
    "tag": "pip"
  },
  {
    "question": "Reference requirements.txt for the install_requires kwarg in setuptools setup.py file",
    "answer": "On the face of it, it does seem that requirements.txt and setup.py are silly duplicates, but it's important to understand that while the form is similar, the intended function is very different.\nThe goal of a package author, when specifying dependencies, is to say \"wherever you install this package, these are the other packages you need, in order for this package to work.\"\nIn contrast, the deployment author (which may be the same person at a different time) has a different job, in that they say \"here's the list of packages that we've gathered together and tested and that I now need to install\".\nThe package author writes for a wide variety of scenarios, because they're putting their work out there to be used in ways they may not know about, and have no way of knowing what packages will be installed alongside their package.  In order to be a good neighbor and avoid dependency version conflicts with other packages, they need to specify as wide a range of dependency versions as can possibly work.  This is what install_requires in setup.py does.\nThe deployment author writes for a very different, very specific goal: a single instance of an installed application or service, installed on a particular computer.  In order to precisely control a deployment, and be sure that the right packages are tested and deployed, the deployment author must specify the exact version and source-location of every package to be installed, including dependencies and dependency's dependencies.  With this spec, a deployment can be repeatably applied to several machines, or tested on a test machine, and the deployment author can be confident that the same packages are deployed every time.  This is what a requirements.txt does.\nSo you can see that, while they both look like a big list of packages and versions, these two things have very different jobs.  And it's definitely easy to mix this up and get it wrong!  But the right way to think about this is that requirements.txt is an \"answer\" to the \"question\" posed by the requirements in all the various setup.py package files.  Rather than write it by hand, it's often generated by telling pip to look at all the setup.py files in a set of desired packages, find a set of packages that it thinks fits all the requirements, and then, after they're installed, \"freeze\" that list of packages into a text file (this is where the pip freeze name comes from).  \nSo the takeaway: \n\nsetup.py should declare the loosest possible dependency versions that are still workable.  Its job is to say what a particular package can work with.\nrequirements.txt is a deployment manifest that defines an entire installation job, and shouldn't be thought of as tied to any one package.  Its job is to declare an exhaustive list of all the necessary packages to make a deployment work.\nBecause these two things have such different content and reasons for existing, it's not feasible to simply copy one into the other.\n\nReferences:\n\ninstall_requires vs Requirements files from the Python packaging user guide.",
    "tag": "pip"
  },
  {
    "question": "Error running 'pip install': \"ImportError: No module named pip\"",
    "answer": "With macOS v10.15 (Catalina) and Homebrew 2.1.6, I was getting this error with Python 3.7. I just needed to run:\npython3 -m ensurepip\n\nNow python3 -m pip works for me.",
    "tag": "pip"
  },
  {
    "question": "How to install python3 version of package via pip on Ubuntu?",
    "answer": "Ubuntu 12.10+ and Fedora 13+ have a package called python3-pip which will install pip-3.2 (or pip-3.3, pip-3.4 or pip3 for newer versions) without needing this jumping through hoops.\n\nI came across this and fixed this without needing the likes of wget or virtualenvs (assuming Ubuntu 12.04):\n\nInstall package python3-setuptools: run sudo aptitude install python3-setuptools, this will give you the command easy_install3.\nInstall pip using Python 3's setuptools: run sudo easy_install3 pip, this will give you the command pip-3.2 like kev's solution.\nInstall your PyPI packages: run sudo pip-3.2 install <package> (installing python packages into your base system requires root, of course).\n…\nProfit!",
    "tag": "pip"
  },
  {
    "question": "What is the purpose of \"pip install --user ...\"?",
    "answer": "pip defaults to installing Python packages to a system directory (such as /usr/local/lib/python3.4). This requires root access.\n--user makes pip install packages in your home directory instead, which doesn't require any special privileges.",
    "tag": "pip"
  },
  {
    "question": "Find all packages installed with easy_install/pip?",
    "answer": "pip freeze will output a list of installed packages and their versions. It also allows you to write those packages to a file that can later be used to set up a new environment.\nhttps://pip.pypa.io/en/stable/reference/pip_freeze/#pip-freeze",
    "tag": "pip"
  },
  {
    "question": "What is pyproject.toml file for?",
    "answer": "Yes, pyproject.toml is the specified file format of PEP 518 which contains the build system requirements of Python projects.\nThis solves the build-tool dependency chicken and egg problem, i.e. pip can read pyproject.toml and what version of setuptools or wheel one may need.\nIf you need a setup.py for an editable install, you could use a shim in setup.py:\n#!/usr/bin/env python\n\nimport setuptools\n\nif __name__ == \"__main__\":\n    setuptools.setup()",
    "tag": "pip"
  },
  {
    "question": "\"pip install unroll\": \"python setup.py egg_info\" failed with error code 1",
    "answer": "About the error code\nAccording to the Python documentation:\n\nThis module makes available standard errno system symbols. The value of each symbol is the corresponding integer value. The names and descriptions are borrowed from linux/include/errno.h, which should be pretty all-inclusive.\n\nError code 1 is defined in errno.h and means Operation not permitted.\nAbout your error\nYour setuptools do not appear to be installed. Just follow the Installation Instructions from the PyPI website.\nIf it's already installed, try\npip install --upgrade setuptools\n\nIf it's already up to date, check that the module ez_setup is not missing. If it is, then\npip install ez_setup\n\nThen try again\npip install unroll\n\nIf it's still not working, maybe pip didn't install/upgrade setup_tools properly so you might want to try\neasy_install -U setuptools\n\nAnd again\npip install unroll",
    "tag": "pip"
  },
  {
    "question": "Could not find a version that satisfies the requirement tensorflow",
    "answer": "The latest requirements for running TensorFlow are documented in the installation documentation.\n\nTensorFlow only supports 64-bit Python\n\nTensorFlow only supports certain versions of Python (for example, Python 3.6 is not supported)\n\n\nSo, if you're using an out-of-range version of Python (older or newer) or a 32-bit version, then you'll need to use a different version.",
    "tag": "pip"
  },
  {
    "question": "Upgrade Python in a virtual environment",
    "answer": "If you happen to be using the venv module that comes with Python 3.3+, it supports an --upgrade option. \nPer the docs:\n\nUpgrade the environment directory to use this version of Python, assuming Python has been upgraded in-place\n\npython3 -m venv --upgrade ENV_DIR",
    "tag": "pip"
  },
  {
    "question": "What is pip's `--no-cache-dir` good for?",
    "answer": "Cached is: store away in hiding or for future use\nUsed for\n\n\nstore the installation files(.whl, etc) of the modules that you install through pip\nstore the source files (.tar.gz, etc) to avoid re-download when not expired\n\n\nPossible Reason you might want to disable cache:\n\n\nyou don't have space on your hard drive\npreviously run pip install with unexpected settings\n\neg:\n\npreviously run export PYCURL_SSL_LIBRARY=nss and pip install pycurl\nwant new run export PYCURL_SSL_LIBRARY=openssl and pip install pycurl --compile --no-cache-dir\n\n\n\n\nyou want to keep a Docker image as small as possible\n\nLinks to documentation\nhttps://pip.pypa.io/en/stable/topics/caching",
    "tag": "pip"
  },
  {
    "question": "How to install multiple python packages at once using pip",
    "answer": "For installing multiple packages on the command line, just pass them as a space-delimited list, e.g.:\npip install wsgiref boto\n\nFor installing from a text file, then, from pip install --help:\n\n-r FILENAME, --requirement=FILENAME\nInstall all the packages listed in the given requirements file.  This option can be used multiple times.\n\nTake a look at the pip documentation regarding requirements files for their general layout and syntax - note that you can generate one based on current environment / site-packages with pip freeze if you want a quick example - e.g. (based on having installed wsgiref and boto in a clean virtualenv):\n$ pip freeze\nboto==2.3.0\nwsgiref==0.1.2",
    "tag": "pip"
  },
  {
    "question": "How to install Python package from GitHub?",
    "answer": "You need to use the proper git URL:\npip install git+https://github.com/jkbr/httpie.git#egg=httpie\n\nAlso see the VCS Support section of the pip documentation.\nDon’t forget to include the egg=<projectname> part to explicitly name the project; this way pip can track metadata for it without having to have run the setup.py script.",
    "tag": "pip"
  },
  {
    "question": "Could not find a version that satisfies the requirement <package>",
    "answer": "Although it doesn't really answers this specific question. Others got the same error message with this mistake.\nFor those who like me initial forgot the -r: Use pip install -r requirements.txt the -r is essential for the command.\nThe original answer:\nhttps://stackoverflow.com/a/42876654/10093070",
    "tag": "pip"
  },
  {
    "question": "How to install Python MySQLdb module using pip?",
    "answer": "It's easy to do, but hard to remember the correct spelling:\npip install mysqlclient\n\nIf you need 1.2.x versions (legacy Python only), use pip install MySQL-python\nNote: Some dependencies might have to be in place when running the above command. Some hints on how to install these on various platforms:\nUbuntu 14, Ubuntu 16, Debian 8.6 (jessie)\nsudo apt-get install python-pip python-dev libmysqlclient-dev\n\nFedora 24:\nsudo dnf install python python-devel mysql-devel redhat-rpm-config gcc\n\nMac OS\nbrew install mysql-connector-c\n\nif that fails, try\nbrew install mysql",
    "tag": "pip"
  },
  {
    "question": "What is pip's equivalent of `npm install package --save-dev`?",
    "answer": "There isn't an equivalent with pip.\nBest way is to pip install package && pip freeze > requirements.txt\nYou can see all the available options on their documentation page.\nIf it really bothers you, it wouldn't be too difficult to write a custom bash script (pips) that takes a -s argument and freezes to your requirements.txt file automatically.\nEdit 1\nSince writing this there has been no change in providing an auto --save-dev option similar to NPM however Kenneth Reitz (author of requests and many more) has released some more info about a better pip workflow to better handle pip updates.\nEdit 2\nLinked from the \"better pip workflow\" article above it is now recommended to use pipenv to manage requirements and virtual environments. Having used this a lot recently I would like to summarise how simple the transition is:\nInstall pipenv (on Mac)\nbrew install pipenv\n\npipenv creates and manages it's own virtual environments so in a project with an existing requirements.txt, installing all requirements (I use Python3.7 but you can remove the --three if you do not) is as simple as:\npipenv --three install\n\nActivating the virtualenv to run commands is also easy\npipenv shell\n\nInstalling requirements will automatically update the Pipfile and Pipfile.lock\npipenv install <package>\n\nIt's also possible to update out-of-date packages\npipenv update\n\nI highly recommend checking it out especially if coming from a npm background as it has a similar feel to package.json and package-lock.json",
    "tag": "pip"
  },
  {
    "question": "Does uninstalling a package with \"pip\" also remove the dependent packages?",
    "answer": "You can install and use the pip3-autoremove utility to remove a package plus unused dependencies.\n# install pip3-autoremove\npip install pip3-autoremove\n# remove \"somepackage\" plus its dependencies:\npip-autoremove somepackage -y",
    "tag": "pip"
  },
  {
    "question": "How to install packages offline?",
    "answer": "On the system that has access to internet\nThe pip download command lets you download packages without installing them:\npip download -r requirements.txt\n\n(In previous versions of pip, this was spelled pip install --download -r requirements.txt.)\nOn the system that has no access to internet\nCopy over the downloaded packages to this system and then you can use\npip install --no-index --find-links /path/to/download/dir/ -r requirements.txt\n\nto install those downloaded modules, without accessing the network.",
    "tag": "pip"
  },
  {
    "question": "After installing with pip, \"jupyter: command not found\"",
    "answer": "Try \npython -m notebook\nOr, if you used pip3 to install the notebook:\npython3 -m notebook\nOn Mac OS Catalina and brewed Python3.7",
    "tag": "pip"
  },
  {
    "question": "How are Pipfile and Pipfile.lock used?",
    "answer": "The concept behind these files is simple and analogue to other already existing tools, if you have some familiarity with Ruby's Bundler or Node's Npm. Pipenv is both a package and virtual environment management tool that uses the Pipfile and Pipfile.lock files to achieve these goals.\nPipenv handles the virtual environment for you (no more activate and deactivate required). Below, some basics to get you started, see more at pipenv website.\nGetting Started\nStart using pipenv is easy, in your project folder type...\n$ pipenv install\n\n... and if it already has a requirements.txt file, it will generate a Pipfile file with the requirements and a virtual environment folder, otherwise, it will generate an empty Pipfile file. If you disliked or changed your mind about something that you have installed, just type...\n$ pipenv uninstall <package>\n\n... and you're good to go. To activate the virtual environment that pipenv already generated, go with...\n$ pipenv shell\n\n... and your virtual environment will be activated. To leave the environment...\n$ exit\n\n... and you will be back to your original terminal session.\nPipfile\nThe Pipfile file is intended to specify packages requirements for your Python application or library, both to development and execution. You can install a package by simply using...\n$ pipenv install flask\n\n... and it will be added as a dependency for deployment and execution or by using ...\n$ pipenv install --dev pytest\n\n... and it will be used as a dependency for development time. In both cases, if you need to be more specific about the package version, as stated in the documentation pipenv makes use of the same version specifiers used by pip. The file syntax is pretty straight forward, as follows.\n[[source]] # Here goes your package sources (where you are downloading your packages from).\nurl = \"https://pypi.python.org/simple\"\nverify_ssl = true\nname = \"pypi\"\n\n[packages] # Here goes your package requirements for running the application and its versions (which packages you will use when running the application).\nrequests = \"*\"\nflask = \"*\"\npandas = \"*\"\n\n[dev-packages] # Here goes your package requirements for developing the application and its versions (which packages you will use when developing the application)\npylint = \"*\"\nwheel = \"*\"\n\n[requires] # Here goes your required Python version.\npython_version = \"3.6\"\n\nPipfile.lock\nThe Pipfile.lock is intended to specify, based on the packages present in Pipfile, which specific version of those should be used, avoiding the risks of automatically upgrading packages that depend upon each other and breaking your project dependency tree.\nYou can lock your currently installed packages using...\n$ pipenv lock\n\n... and the tool will lookup your virtual environment folder to generate the lock file for you automatically, based on the currently installed versions. The file syntax is not as obvious as is for Pipfile , so for the sake of conciseness, it will not be displayed here.",
    "tag": "pip"
  },
  {
    "question": "Installing pip packages to $HOME folder",
    "answer": "While you can use a virtualenv, you don't need to.  The trick is passing the PEP370 --user argument to the setup.py script.  With the latest version of pip, one way to do it is:\npip install --user mercurial\n\nThis should result in the hg script being installed in $HOME/.local/bin/hg and the rest of the hg package in $HOME/.local/lib/pythonx.y/site-packages/.\nNote, that the above is true for Python 2.6.  There has been a bit of controversy among the Python core developers about what is the appropriate directory location on Mac OS X for PEP370-style user installations.  In Python 2.7 and 3.2, the location on Mac OS X was changed from $HOME/.local to $HOME/Library/Python.  This might change in a future release.  But, for now, on 2.7 (and 3.2, if hg were supported on Python 3), the above locations will be $HOME/Library/Python/x.y/bin/hg and $HOME/Library/Python/x.y/lib/python/site-packages.",
    "tag": "pip"
  },
  {
    "question": "pip: force install ignoring dependencies",
    "answer": "pip has a --no-dependencies switch. You should use that.\nFor more information, run pip install -h, where you'll see this line: \n--no-deps, --no-dependencies\n                        Ignore package dependencies",
    "tag": "pip"
  },
  {
    "question": "pip installs packages successfully, but executables are not found from the command line",
    "answer": "I know the question asks about macOS, but here is a solution for Linux users who arrive here via Google.\nI was having the issue described in this question, having installed the pdfx package via pip.\nWhen I ran it however, nothing...\npip list | grep pdfx\npdfx (1.3.0)\n\nYet:\nwhich pdfx\npdfx not found\n\nThe problem on Linux is that pip install ... drops scripts into ~/.local/bin and this is not on the default Debian/Ubuntu $PATH.\nHere's a GitHub issue going into more detail: https://github.com/pypa/pip/issues/3813\nTo fix, just add ~/.local/bin to your $PATH, for example by adding the following line to your .bashrc file:\nexport PATH=\"$HOME/.local/bin:$PATH\"\n\nAfter that, restart your shell and things should work as expected.",
    "tag": "pip"
  },
  {
    "question": "Why does \"pip install\" inside Python raise a SyntaxError?",
    "answer": "pip is run from the command line, not the Python interpreter. It is a program that installs modules, so you can use them from Python. Once you have installed the module, then you can open the Python shell and do import selenium.\nThe Python shell is not a command line, it is an interactive interpreter. You type Python code into it, not commands.",
    "tag": "pip"
  },
  {
    "question": "What's the difference between dist-packages and site-packages?",
    "answer": "dist-packages is a Debian-specific convention that is also present in its derivatives, like Ubuntu. Modules are installed to dist-packages when they come from the Debian package manager into this location:\n/usr/lib/python2.7/dist-packages\n\nSince easy_install and pip are installed from the package manager, they also use dist-packages, but they put packages here:\n/usr/local/lib/python2.7/dist-packages\n\nFrom the Debian Python Wiki:\n\ndist-packages instead of site-packages. Third party Python software\ninstalled from Debian packages goes into dist-packages, not\nsite-packages. This is to reduce conflict between the system Python,\nand any from-source Python build you might install manually.\n\nThis means that if you manually compile and install Python interpreter from source, it uses the site-packages directory. This allows you to keep the two installations separate, especially since Debian and Ubuntu rely on the system version of Python for many system utilities.",
    "tag": "pip"
  },
  {
    "question": "libxml install error using pip",
    "answer": "** make sure the development packages of libxml2 and libxslt are installed **\n\nFrom the lxml documentation, assuming you are running a Debian-based distribution :\nsudo apt-get install libxml2-dev libxslt-dev python-dev\n\nFor Debian based systems, it should be enough to install the known build dependencies of python-lxml or python3-lxml, e.g.\nsudo apt-get install build-dep python3-lxml",
    "tag": "pip"
  },
  {
    "question": "Failed to install Python Cryptography package with PIP and setup.py",
    "answer": "I had a similar issue, and found I was simply missing a dependency (libssl-dev, for me). As referenced in https://cryptography.io/en/latest/installation/, ensure that all dependencies are met:\nOn Windows\nIf you’re on Windows you’ll need to make sure you have OpenSSL installed. There are pre-compiled binaries available. If your installation is in an unusual location set the LIB and INCLUDE environment variables to include the\ncorresponding locations. For example:\nC:\\> \\path\\to\\vcvarsall.bat x86_amd64\nC:\\> set LIB=C:\\OpenSSL-1.0.1f-64bit\\lib;%LIB%\nC:\\> set INCLUDE=C:\\OpenSSL-1.0.1f-64bit\\include;%INCLUDE%\nC:\\> pip install cryptography\n\nBuilding cryptography on Linux\ncryptography should build very easily on Linux provided you have a C compiler, headers for Python (if you’re\nnot using pypy), and headers for the OpenSSL and libffi libraries available on your system.\nFor Debian and Ubuntu, the following command will ensure that the required dependencies are installed:\nsudo apt-get install build-essential libssl-dev libffi-dev python-dev\n\nFor Fedora and RHEL-derivatives, the following command will ensure that the required dependencies are installed:\nsudo yum install gcc libffi-devel python-devel openssl-devel\n\nYou should now be able to build and install cryptography with the usual.\npip install cryptography",
    "tag": "pip"
  },
  {
    "question": "How to uninstall a package installed with pip install --user",
    "answer": "Having tested this using Python 3.5 and pip 7.1.2 on Linux, the situation appears to be this:\n\npip install --user somepackage installs to $HOME/.local, and uninstalling it does work using pip uninstall somepackage.\nThis is true whether or not somepackage is also installed system-wide at the same time.\nIf the package is installed at both places, only the local one will be uninstalled. To uninstall the package system-wide using pip, first uninstall it locally, then run the same uninstall command again, with root privileges.\nIn addition to the predefined user install directory, pip install --target somedir somepackage will install the package into somedir. There is no way to uninstall a package from such a place using pip. (But there is a somewhat old unmerged pull request on Github that implements pip uninstall --target.)\nSince the only places pip will ever uninstall from are system-wide and predefined user-local, you need to run pip uninstall as the respective user to uninstall from a given user's local install directory.",
    "tag": "pip"
  },
  {
    "question": "Identifying the dependency relationship for python packages installed with pip",
    "answer": "You could try pipdeptree, which displays dependencies as a tree structure e.g.:\n$ pipdeptree\nLookupy==0.1\nwsgiref==0.1.2\nargparse==1.2.1\npsycopg2==2.5.2\nFlask-Script==0.6.6\n  - Flask [installed: 0.10.1]\n    - Werkzeug [required: >=0.7, installed: 0.9.4]\n    - Jinja2 [required: >=2.4, installed: 2.7.2]\n      - MarkupSafe [installed: 0.18]\n    - itsdangerous [required: >=0.21, installed: 0.23]\nalembic==0.6.2\n  - SQLAlchemy [required: >=0.7.3, installed: 0.9.1]\n  - Mako [installed: 0.9.1]\n    - MarkupSafe [required: >=0.9.2, installed: 0.18]\nipython==2.0.0\nslugify==0.0.1\nredis==2.9.1\n\nTo install it, run:\npip install pipdeptree\n\nAs noted by @Esteban in the comments you can also list the tree in reverse with -r or for a single package with -p <package_name>. So to find which module(s) Werkzeug is a dependency for, you could run:\n$ pipdeptree -r -p Werkzeug\nWerkzeug==0.11.15\n  - Flask==0.12 [requires: Werkzeug>=0.7]",
    "tag": "pip"
  },
  {
    "question": "Could not install packages due to an EnvironmentError: [WinError 5] Access is denied:",
    "answer": "Just type the command you want execute with the user permission, if you don't want to change the permission:\npip3 install --upgrade tensorflow-gpu --user",
    "tag": "pip"
  },
  {
    "question": "When would the -e, --editable option be useful with pip install?",
    "answer": "As the man page says it:\n-e,--editable <path/url>\n     Install a project in editable mode (i.e.  setuptools \"develop mode\") from a local project path or a VCS url.\n\nSo you would use this when trying to install a package locally, most often in the case when you are developing it on your system. It will just link the package to the original location, basically meaning any changes to the original package would reflect directly in your environment.\nSome nuggets around the same here and here.\nAn example run can be:\npip install -e .\n\nor\npip install -e ~/ultimate-utils/ultimate-utils-proj-src/\n\nnote the second is the full path to where the setup.py would be at.",
    "tag": "pip"
  },
  {
    "question": "Stop pip from failing on single package when installing with requirements.txt",
    "answer": "Running each line with pip install may be a workaround.\ncat requirements.txt | xargs -n 1 pip install\n\nNote: -a parameter is not available under MacOS, so old cat is more portable.",
    "tag": "pip"
  },
  {
    "question": "Install specific git commit with pip",
    "answer": "You can specify commit hash, branch name, tag.\nFor the branch name and the tag, you can also install a compressed distribution. This is faster and more efficient, as it does not require cloning the entire repository. GitHub creates those bundles automatically.\nhash:\n$ pip install git+https://github.com/aladagemre/django-notification.git@2927346f4c513a217ac8ad076e494dd1adbf70e1\n\nbranch-name\nWith git\n$ pip install git+https://github.com/aladagemre/django-notification.git@cool-feature-branch\n\nor from source bundle\n$ pip install https://github.com/aladagemre/django-notification/archive/cool-feature-branch.tar.gz\n\ntag\nwith git\n$ pip install git+https://github.com/aladagemre/django-notification.git@v2.1.0\n\nor from source bundle\n$ pip install https://github.com/aladagemre/django-notification/archive/v2.1.0.tar.gz\n\nIt is a not well-documented feature, but you can find more information at https://pip.pypa.io/en/latest/topics/vcs-support/",
    "tag": "pip"
  },
  {
    "question": "Upgrade python packages from requirements.txt using pip command",
    "answer": "I already answered this question here. Here's my solution:\nBecause there was no easy way for upgrading package by package, and updating the requirements.txt file, I wrote this pip-upgrader which also updates the versions in your requirements.txt file for the packages chosen (or all packages).\nInstallation\npip install pip-upgrader\n\nUsage\nActivate your virtualenv (important, because it will also install the new versions of upgraded packages in current virtualenv).\ncd into your project directory, then run:\npip-upgrade\n\nAdvanced usage\nIf the requirements are placed in a non-standard location, send them as arguments:\npip-upgrade path/to/requirements.txt\n\nIf you already know what package you want to upgrade, simply send them as arguments:\npip-upgrade -p django -p celery -p dateutil\n\nIf you need to upgrade to  pre-release / post-release version, add --prerelease argument to your command.\nFull disclosure: I wrote this package.",
    "tag": "pip"
  },
  {
    "question": "Using pip behind a proxy with CNTLM",
    "answer": "With Ubuntu I could not get the proxy option to work as advertised – so the following command did not work:\nsudo pip --proxy http://web-proxy.mydomain.com install somepackage\n\nBut exporting the https_proxy environment variable (note it's https_proxy not http_proxy) did the trick:\nexport https_proxy=http://web-proxy.mydomain.com\n\nThen:\nsudo -E pip install somepackage",
    "tag": "pip"
  },
  {
    "question": "What do square brackets mean in pip install?",
    "answer": "The syntax that you are using is:\npip install \"project[extra]\"\n\n\nIn your case, you are installing the splinter package which has the added support for django.\n• pip install splinter django would install two packages named splinter and django.\n• pip install splinter[django], on the other hand, installs splinter, but it also installs optional dependencies defined by splinter using the keyword in the brackets. In this case, as of 2024-05-15 it's Django, lxml and cssselect.\nNote that the keyword in brackets has nothing to do with the django package itself, but is just a string defined by the splinter package for a particular set of dependencies that also get installed. How the argument django is interpreted depends on the build system, but any setuptools-based build system (including most instances of setup.py) will likely just use them as a hook for optional dependencies.\nIt's worth noting that the syntax supports using multiple keywords, e.g.:\npip install \"splinter[django,flask,selenium]\"\n\nKudos to @chepner for adding context in the comments.",
    "tag": "pip"
  },
  {
    "question": "What is the meaning of \"Failed building wheel for X\" in pip install?",
    "answer": "(pip maintainer here!)\nUpdate: This is no longer necessary starting Python 3.12, where pip will automatically use isolated builds in new virtual environments.\nFor a quick copy paste:\npip install wheel\n\nDo that in every new virtual environment created with venv.\nRead on for the details and explaination.\n\nIf the package is not a wheel, pip tries to build a wheel for it (via setup.py bdist_wheel). If that fails for any reason (like, missing system level libraries, incompatibilities with your system, bad version string in the built wheel, etc), you get the \"Failed building wheel for {...}\" message.\nIn some of these cases, currently, pip falls back to installing via setup.py install, so it's possible that the installation still succeeds. That said, pip always tries to install packages via wheels as often as it can. This is because of various advantages of using wheels (like faster installs, cache-able, not executing code again etc) and the fact that it is a standardizd format; unlike the (deprecated) setup.py install interface.\n\nYour error message here is due to the wheel package being missing, which contains the logic required to build the wheels in setup.py bdist_wheel. (pip install wheel can fix that -- but it won't fix any build time issues due to system configuration)\n\nSometime in the future, we'll switch to a more modern build system by default (if you're a package author, you can opt-in by adding a pyproject.toml) that will solve this issue, through isolated build environments where you will have wheel installed. :)\n\nPEP 517: A build-system independent format for source trees\nA blog post on \"PEP 517 and 518 in Plain English\"",
    "tag": "pip"
  },
  {
    "question": "How do I increase the scrollback buffer size in tmux?",
    "answer": "The history limit is a pane attribute that is fixed at the time of pane creation and cannot be changed for existing panes. The value is taken from the history-limit session option (the default value is 2000).\nTo create a pane with a different value you will need to set the appropriate history-limit option before creating the pane.\nTo establish a different default, you can put a line like the following in your .tmux.conf file:\nset-option -g history-limit 3000\n\nNote: Be careful setting a very large default value, it can easily consume lots of RAM if you create many panes.\nFor a new pane (or the initial pane in a new window) in an existing session, you can set that session’s history-limit. You might use a command like this (from a shell):\ntmux set-option history-limit 5000 \\; new-window\n\nFor (the initial pane of the initial window in) a new session you will need to set the “global” history-limit before creating the session:\ntmux set-option -g history-limit 5000 \\; new-session\n\nNote: If you do not re-set the history-limit value, then the new value will be also used for other panes/windows/sessions created in the future; there is currently no direct way to create a single new pane/window/session with its own specific limit without (at least temporarily) changing history-limit (though show-option (especially in 1.7 and later) can help with retrieving the current value so that you restore it later).",
    "tag": "tmux"
  },
  {
    "question": "Is there any way to redraw tmux window when switching smaller monitor to bigger one?",
    "answer": "tmux limits the dimensions of a window to the smallest of each dimension across all the sessions to which the window is attached. If it did not do this there would be no sensible way to display the whole window area for all the attached clients.\nThe easiest thing to do is to detach any other clients from the sessions when you attach:\ntmux attach -d\n\nAlternately, you can move any other clients to a different session before attaching to the session:\ntakeover() {\n    # create a temporary session that displays the \"how to go back\" message\n    tmp='takeover temp session'\n    if ! tmux has-session -t \"$tmp\"; then\n        tmux new-session -d -s \"$tmp\"\n        tmux set-option -t \"$tmp\" set-remain-on-exit on\n        tmux new-window -kt \"$tmp\":0 \\\n            'echo \"Use Prefix + L (i.e. ^B L) to return to session.\"'\n    fi\n\n    # switch any clients attached to the target session to the temp session\n    session=\"$1\"\n    for client in $(tmux list-clients -t \"$session\" | cut -f 1 -d :); do\n        tmux switch-client -c \"$client\" -t \"$tmp\"\n    done\n\n    # attach to the target session\n    tmux attach -t \"$session\"\n}\ntakeover 'original session' # or the session number if you do not name sessions\n\nThe screen will shrink again if a smaller client switches to the session.\nThere is also a variation where you only \"take over\" the window (link the window into a new session, set aggressive-resize, and switch any other sessions that have that window active to some other window), but it is harder to script in the general case (and different to “exit” since you would want to unlink the window or kill the session instead of just detaching from the session).",
    "tag": "tmux"
  },
  {
    "question": "How do I set tmux to open specified windows at startup?",
    "answer": "You can write a small shell script that launches tmux with the required programs. I have the following in a shell script that I call dev-tmux. A dev environment:\n#!/bin/sh\ntmux new-session -d 'vim'\ntmux split-window -v 'ipython'\ntmux split-window -h\ntmux new-window 'mutt'\ntmux -2 attach-session -d\n\nSo everytime I want to launch my favorite dev environment I can just do\n$ dev-tmux",
    "tag": "tmux"
  },
  {
    "question": "\"tmux set -g mouse-mode on\" not scrolling",
    "answer": "So this option has been renamed in version 2.1 (18 October 2015)\nFrom the changelog:\n Mouse-mode has been rewritten.  There's now no longer options for:\n    - mouse-resize-pane\n    - mouse-select-pane\n    - mouse-select-window\n    - mode-mouse\n\n  Instead there is just one option:  'mouse' which turns on mouse support\n\nSo this is what I'm using now in my .tmux.conf file\nset -g mouse on",
    "tag": "tmux"
  },
  {
    "question": "How do I terminate a window in tmux?",
    "answer": "try   Prefix + &\nif you have\nbind q killp\n\nin your .tmux.conf, you can press Prefix + q to kill the window too, only if there is only one panel in that window.\nif you have multiple panes and want to kill the whole window at once use killw instead of killp in your config.\nthe default of Prefix above is Ctrl+b,\nso to terminate window by default you can use Ctrl+b &",
    "tag": "tmux"
  },
  {
    "question": "How to send a command to all panes in tmux?",
    "answer": "Have you tried following in tmux window with multiple panes\nCtrl-B :\n\nsetw synchronize-panes on\n\nclear history",
    "tag": "tmux"
  },
  {
    "question": "How do I clear the scrollback buffer in tmux?",
    "answer": "This same question has been plaguing me for quite some time.  Here's the best I've come up with.  Put this into your .tmux.conf file:\nbind -n C-k clear-history\n\nThis binds ctrl-k to the tmux clear-history command.  The -n after bind makes it so you don't have to issue the tmux command prefix (ctrl-b by default).  I use bash, so ctrl-l already does the equivalent of typing \"clear\" at the command line.  With these two keys I get a nice ctrl-l, ctrl-k combo, which moves all the scroll buffer off the screen (the \"clear\") and then deletes all that history (the tmux \"clear-history\" command).\nIt's not quite as nice as Terminal's, iTerm's, or Konsole's 1-key combos for clearing it out, but it's a world better than typing in clear-history all the time.",
    "tag": "tmux"
  },
  {
    "question": "lose vim colorscheme in tmux mode",
    "answer": "I had the same problem. Only difference was I am using solarize rather then molokai.\nTo fix the issue, I have set up an alias in ~/.bashrc:\nalias tmux=\"TERM=screen-256color-bce tmux\"\n\nAnd set up the default-terminal option in ~/.tmux.conf:\nset -g default-terminal \"xterm\"\n\nLastly, do $ source ~/.bashrc to load new alias.",
    "tag": "tmux"
  },
  {
    "question": "How do I disconnect all other users in tmux?",
    "answer": "You can use <prefix> D (where prefix is C-b by default), to chose which clients to detach; it will also list they col/lines as well as the last used time. Note the uppercase D, i.e. Shift+d.\nYou could also use tmux's detach-client option\n detach-client [-P] [-a] [-s target-session] [-t target-client]\n               (alias: detach)\n         Detach the current client if bound to a key, the client specified\n         with -t, or all clients currently attached to the session speci-\n         fied by -s.  The -a option kills all but the client given with\n         -t.  If -P is given, send SIGHUP to the parent process of the\n         client, typically causing it to exit.\n\neither from <prefix>:followed by detach [options] or on the command line inside tmux with tmux detach [options]",
    "tag": "tmux"
  },
  {
    "question": "How do I rename a pane in tmux?",
    "answer": "Renaming a window\nCtrl-b ,\nwhere Ctrl-b is the default prefix key.\nAlternatively, run:\ntmux rename-window <new name>\n\nOr type Ctrl-b : rename-window <new name>.\n\nRenaming a pane\nIn newer versions you can rename pane using:\ntmux select-pane -T <title>\n\nOr type Ctrl-b : select-pane -T <pane_name>.\nAlso, I have the following two lines in ~/.tmux.conf, in order to view the title of the pane in the top of the pane itself, and reformat the title.\nset -g pane-border-status top\nset -g pane-border-format \" [ ###P #T ] \"",
    "tag": "tmux"
  },
  {
    "question": "Keep the window's name fixed in tmux",
    "answer": "As shown in a comment to the main post: set-option -g allow-rename off in your .tmux.conf file",
    "tag": "tmux"
  },
  {
    "question": "Tmux: How do I find out the currently running version of tmux?",
    "answer": "As pointed out in a comment, tmux -V returns the version:\n$ tmux -V\ntmux 3.0a\n\nTested on Centos 7 and OSX 12.5.",
    "tag": "tmux"
  },
  {
    "question": "How to join two tmux windows into one, as panes?",
    "answer": "Actually I found the way to do that. Suppose the two windows are number 1 and 2. Use\njoin-pane -s 2 -t 1 \n\nThis will move the 2nd window as a pane to the 1st window. The opposite command is break-pane",
    "tag": "tmux"
  },
  {
    "question": "How to automatically start tmux on SSH session?",
    "answer": "Server-side configuration:\nTo automatically start tmux on your remote server when ordinarily logging in via SSH (and only SSH), edit the ~/.bashrc of your user or root (or both) on the remote server accordingly:\nif [[ $- =~ i ]] && [[ -z \"$TMUX\" ]] && [[ -n \"$SSH_TTY\" ]]; then\n  tmux attach-session -t ssh_tmux || tmux new-session -s ssh_tmux\nfi\n\nThis command creates a tmux session called ssh_tmux if none exists, or reattaches to a already existing session with that name. In case your connection dropped or when you forgot a session weeks ago, every SSH login automatically brings you back to the tmux-ssh session you left behind.\nConnect from your client:\nNothing special, just ssh user@hostname.\nWhat if I mess up and lock myself out somehow?\nJust skip the init file with --norc when you connect:\n$ ssh -t myname@myserver bash --norc\n\n(You might also need --noprofile, depending on your setup)\nOr just connect with sh:\n$ ssh -t myname@myserver sh\n\nThen fix your mistake and try again.",
    "tag": "tmux"
  },
  {
    "question": "How to send commands when opening a tmux session inside another tmux session?",
    "answer": "The send-prefix command can be used to send your prefix keystroke to (the process running in) the active pane. By default, the prefix is C-b and C-b is bound to send-prefix (so that hitting it twice sends a single C-b to the active pane). This is just what we need to access the bindings of the inner tmux instance.\nThe first C-b is captured by the “outer” tmux instance as its prefix key. The second one is captured by the “outer” tmux instance and triggers its C-b binding (send-prefix). This sends a C-b to the outer instance’s active pane. The process running in this pane is (ultimately, through an ssh instance) the “inner” tmux instance. It captures the C-b as its prefix key. Now your next keystroke will be passed through the outer tmux instance and captured by the inner one to trigger a binding.\nTo trigger the c binding (new-window) in a second-level instance of tmux, you would type C-b C-b c. For a third-level instance of tmux you would type C-b C-b C-b C-b c.\nThis doubling for each level can be annoying if you are commonly dealing with multiple layers of tmux. If you can spare some other key, you could make a non-prefixed binding to make things (possibly) easier to type:\nbind-key -n C-\\ send-prefix\nbind-key -n C-^ send-prefix \\; send-prefix\n\nCreate new window in second-level tmux: C-\\ c\nCreate new window in third-level tmux: C-^ c (or C-\\ C-\\ c)\n\nIf you have a limited number of tmux commands that you want to (easily) send to the lower-level tmux instances, you might instead use send-keys to create some specific bindings (possibly just in your top-level tmux instance):\nbind-key C-c  send-keys C-b c\nbind-key C    send-keys C-b C-b c\n\nCreate new window in second-level tmux: C-b C-c\nCreate new window in third-level tmux: C-b C",
    "tag": "tmux"
  },
  {
    "question": "How to create new tmux session if none exists",
    "answer": "I figured it out (and had it pointed out to me).  \ntmux attach || tmux new",
    "tag": "tmux"
  },
  {
    "question": "Create new tmux session from inside a tmux session",
    "answer": "The quickest way (assuming you use ctrl-b as your command prefix) is:\nctrl-b :new\n\nTo create a new session, then \nctrl-b s\n\nto interactively select and attach to the session.",
    "tag": "tmux"
  },
  {
    "question": "Getting back old copy paste behaviour in tmux, with mouse",
    "answer": "Copy the text: select the text and press mouse left-button with shift key press too.\nPaste the text with shift key + middle-button",
    "tag": "tmux"
  },
  {
    "question": "How can I reattach to tmux process",
    "answer": "You can not re-attach a process id. You need to reattach the corresponding tmux session.\nSo do tmux ls. Pick whatever session you want to re-attach. Then do tmux attach -d -t <session id> to re-attach it to a new tmux instance and release it from the old one.",
    "tag": "tmux"
  },
  {
    "question": "Pane Title in Tmux",
    "answer": "This functionality has been added to tmux in this commit. It is not in version 2.2, but it looks like it will be in 2.3.\nTo enable it:\ntmux set -g pane-border-status top\n\nor if you prefer:\ntmux set -g pane-border-status bottom\n\nTo set a custom text as your pane border status line you can make use of pane-border-format, e.g. like so:\ntmux set -g pane-border-format \"#{pane_index} #{pane_current_command}\"",
    "tag": "tmux"
  },
  {
    "question": "Terminal Multiplexer for Microsoft Windows - Installers for GNU Screen or tmux",
    "answer": "Look. This is way old, but on the off chance that someone from Google finds this, absolutely the best solution to this - (and it is AWESOME) - is to use ConEmu (or a package that includes and is built on top of ConEmu called cmder) and then either use plink or putty itself to connect to a specific machine, or, even better, set up a development environment as a local VM using Vagrant.\nThis is the only way I can ever see myself developing from a Windows box again.\nI am confident enough to say that every other answer - while not necessarily bad answers - offer garbage solutions compared to this.\nUpdate:  As Of 1/8/2020 not all other solutions are garbage - Windows Terminal is getting there and WSL exists.",
    "tag": "tmux"
  },
  {
    "question": "Tmux vs. iTerm2 split panes",
    "answer": "There is another advantage of tmux: what happens if you accidentally close iterm2? If you do it really by accident, you want to reopen everything again. With tmux it is normally as simple as reattaching session without losing anything. Most terminal emulators send SIGHUP to all children which terminates them by default and thus you lose unsaved data (at least, shell and vim command history and other data stored in viminfo) and running processes and thus reopening means rerunning everything.",
    "tag": "tmux"
  },
  {
    "question": "In tmux can I resize a pane to an absolute value",
    "answer": "What about Ctrl-B then (pressing Ctrl) + arrow?\nIf in tmux < 1.8, doing this by Ctrl-B then (Esc + arrow) * n, where n is the number of times you want to resize.",
    "tag": "tmux"
  },
  {
    "question": "Switch between sessions in tmux?",
    "answer": "ctrl+b, s\nNote ctrl+b  is my prefix, your prefix could be something else.",
    "tag": "tmux"
  },
  {
    "question": "List running Jupyter notebooks and tokens",
    "answer": "UPDATE \nYou can now just run jupyter notebook list in the terminal to get the running jupyter sessions with tokens.\n\nTake care that you are within the right environment (conda, virtualenv etc.) otherwise the sessions will list without the associated tokens. Eg: The above reference screenshot is from the conda environment.\nOld answer:\nRun ipython and enter the following:\n> ipython\n[1] : system(\"jupyter\" \"notebook\" \"list\")\nOut[1]: \n['Currently running servers:','http://localhost:8895/token=067470c5ddsadc54153ghfjd817d15b5d5f5341e56b0dsad78a :: /u/user/dir']\n\nIf the notebook is running on a remote server, you will have to login in to that server first before running ipython.",
    "tag": "tmux"
  },
  {
    "question": "How to copy to system clipboard from tmux output after mouse selection?",
    "answer": "If you are using iTerm2, you can copy text in Tmux session, holding down the Option key while dragging the mouse to make selection.\nThen it should be possible to paste text anywhere with Cmd + V as usual.\nFound it here: http://web.archive.org/web/20131226003700/http://ootput.wordpress.com/2013/08/02/copy-and-paste-in-tmux-with-mouse/",
    "tag": "tmux"
  },
  {
    "question": "How to write a shell script that starts tmux session, and then runs a ruby script",
    "answer": "#!/bin/bash\ntmux new-session -d -s my_session 'ruby run.rb'\n\n\nCreate a file named my_script.sh and give it the above contents.\nMake the file executable by running:\nchmod 755 my_script.sh\nor\nchmod +x my_script.sh\nThen run the shell script:\n./my_script.sh\n\nMaking the shell script executable\nWhen you perform the chmod 755 filename command you allow everyone to read and execute the file, and the file owner is allowed to write to the file as well. You may need this for Perl and other scripts that should be run via a webserver. If you apply 755 to a directory, it means that everyone can go to it and get its file listing.\nThese permissions are usually translated into textual representation of rwxr-xr-x.\nYou can alternatively use chmod +x file_name on a file to make it executable.",
    "tag": "tmux"
  },
  {
    "question": "Move window between tmux clients",
    "answer": "Yes, you can use the move-window command:\nmove-window [-d] [-s src-window] [-t dst-window]\n           (alias: movew)\n\nThis is similar to link-window, except the window at src-window is moved to dst-window.\nwhere src-window and dst-window have the form: session:window.pane (session and window can be either name or id).\nSo, supposing you have an 'chat' session with an 'irc' window and want to move it to the 'other_session' session you can do (in the tmux prompt):\nmove-window -s chat:irc -t other_session\nIf you are already in the chat:irc window you don't need to specify the source so\nmove-window -t other_session:\nwill do it. \nIn the same way, from the 'other_session' session you don't need to specify the target.\nmovew -d irc:irc_window\nIf you haven't named you windows/sessions, you have to use their ids.",
    "tag": "tmux"
  },
  {
    "question": "Specify pane percentage in tmuxinator project",
    "answer": "The layout should be specified in the layout: line. But you are not limited to the five preset layouts (such as main-vertical). From the man page:\nIn addition, select-layout may be used to apply a previously used layout - \nthe list-windows command displays the layout of each window in a form \nsuitable for use with select-layout.  For example:\n\n       $ tmux list-windows\n       0: ksh [159x48]\n           layout: bb62,159x48,0,0{79x48,0,0,79x48,80,0}\n       $ tmux select-layout bb62,159x48,0,0{79x48,0,0,79x48,80,0}\n\n tmux automatically adjusts the size of the layout for the current window\n size.  Note that a layout cannot be applied to a window with more panes\n than that from which the layout was originally defined.\n\nFirst set up your layout just how you like it - you can adjust widths with resize-pane until it is just right for you. Then run tmux list-windows. And then you should be able to use the layout: line from the output unaltered in tmuxinator.conf\nSo based on the output from your gist:\n0: tmux [208x73] [layout b147,208x73,0,0[208x62,0,0,208x10,0,63{104x10,0,63,103x10,105,63}]] (active)\n\nThe relevant section of the tmuxinator conf file should be:\n  - editor:\n       layout: b147,208x73,0,0[208x62,0,0,208x10,0,63{104x10,0,63,103x10,105,63}]\n       panes:\n         - vim\n         - #empty, will just run plain bash\n         - top",
    "tag": "tmux"
  },
  {
    "question": "How to auto-update SSH agent environment variables when attaching to existing tmux sessions?",
    "answer": "There's an excellent gist by Martijn Vermaat, which addresses your problem in great depth, although it is intended for screen users, so I'm adjusting it for tmux here.\nTo summarize:\n\ncreate ~/.ssh/rc if it doesn't exist yet, and add the following content:\n#!/bin/bash\n\n# Fix SSH auth socket location so agent forwarding works with tmux.\nif test \"$SSH_AUTH_SOCK\" ; then\n  ln -sf $SSH_AUTH_SOCK ~/.ssh/ssh_auth_sock\nfi\n\n\nMake it work in tmux, add this to your ~/.tmux.conf:\n# fix ssh agent when tmux is detached\nsetenv -g SSH_AUTH_SOCK $HOME/.ssh/ssh_auth_sock\n\n\n\nExtra work is required if you want to enable X11 forwarding, see the gist.",
    "tag": "tmux"
  },
  {
    "question": ".bashrc/.profile is not loaded on new tmux session (or window) -- why?",
    "answer": "Yes, at the end of your .bash_profile, put the line:\n. ~/.bashrc\n\nThis automatically sources the rc file under those circumstances where it would normally only process the profile.\nThe rules as to when bash runs certain files are complicated, and depend on the type of shell being started (login/non-login, interactive or not, and so forth), along with command line arguments and environment variables.\nYou can see them in the man bash output, just look for INVOCATION - you'll probably need some time to digest and decode it though :-)",
    "tag": "tmux"
  },
  {
    "question": "How to enable scrolling in tmux panels with mouse wheel?",
    "answer": "Origin Answer (deprecated)\nTry this:\nsetw -g mode-mouse on\n\nIt can be used with the mouse-select-pane on and mouse-select-window on options.\n\nUpdate\nAfter tmux 2.1, mode-mouse option is no longer available. You should now use:\nset -g mouse on\n\nto capture mouse event.",
    "tag": "tmux"
  },
  {
    "question": "How can you tell which pane in Tmux is focused?",
    "answer": "Here are the relevant settings:\npane-active-border-style fg=colour,bg=colour\n    Set the pane border colour for the currently active pane.\n\nSo, try adding something like this to your ~/.tmux.conf:\nset-option -g pane-active-border-style fg=blue\n\nThat will set a blue border around the active pane. The pane-active-border-style bg=colour option can be used for a more visible solution, as well.",
    "tag": "tmux"
  },
  {
    "question": "Home/End keys do not work in tmux",
    "answer": "Add the following to your .tmux.conf:\nbind-key -n Home send Escape \"OH\"\nbind-key -n End send Escape \"OF\"\n\nAnd you're done!\n\nExplanation\nAfter attempting each one of these, and several others I saw while perusing other answers and documentation, this finally worked for me in every scenario I threw at it. I can't promise the same for you, because everyone's scenarios are different, but this is what I ended up with.\nThis was discovered after introducing the same trial/error and logic from a somewhat relevant article that is no longer available. The key is where the translation is occurring; in my case, this happens within my .tmux.conf, rather than .bashrc or .zshrc (mainly because my home/end worked fine outside of tmux)\nDebugging\nYou can debug this issue by using cat -v.\nRun cat -v, then press the Home and End keys. Exit using Ctrl+C.\n$ cat -v\n\nHere's what my output looked like within tmux using zsh, zsh, and bash:\ntmux\n➜  ~ cat -v\n^[[1~^[[4~^C\n\nzsh\n➜  ~ cat -v\n^[[H^[[F\n\nbash\nbash-3.2$ cat -v\n^[[H^[[F\n\nSolutioning\nCompare the above examples to what we're expecting to see, by pairing tput with cat -v:\n$ tput khome | cat -v; echo\n^[OH\n$ tput kend | cat -v; echo\n^[OF\n\nConclusion\nBecause this problem exists solely within tmux, and not within the shells themselves, I opted to make the bind changes within the tmux configuration instead. By using bind-key paired with send, we can use the Escape keyword paired with the sequence we want to achieve our translation. Thus:\nbind-key -n NAME_OF_KEY send Escape SEQUENCE_GOES_HERE\n\nThis debugging and solutioning process can be applied to any other key translation issues. But, don't go too crazy. Some keys are mapped to certain escape sequences for a reason. Notice how bash and zsh received the ^[[H sequence for Home instead of ^[OH; it's probably not recommended we override this in our .zshrc unless we're having major issues with this in zsh.",
    "tag": "tmux"
  },
  {
    "question": "Bash scripts with tmux to launch a 4-paned window",
    "answer": "As others have mentioned, your commands are being run by the shell script before launching your $SHELL; there is no general way the instance of $SHELL can know what its parent ran before starting it.\nTo get the “initial command” into the shell history, you need to feed the command keystrokes directly to the instance of $SHELL itself (after it has been started, of course). In other contexts I might suggest using a small Expect program to spawn an instance of $SHELL, feed it the keystrokes, then use interact to tie the tty to the expect-spawned $SHELL.\nBut in the context of tmux, we can just use send-keys: \n#!/bin/sh\n\ntmux new-session -d -s foo 'exec pfoo'\ntmux send-keys 'bundle exec thin start' 'C-m'\ntmux rename-window 'Foo'\ntmux select-window -t foo:0\ntmux split-window -h 'exec pfoo'\ntmux send-keys 'bundle exec compass watch' 'C-m'\ntmux split-window -v -t 0 'exec pfoo'\ntmux send-keys 'rake ts:start' 'C-m'\ntmux split-window -v -t 1 'exec pfoo'\ntmux -2 attach-session -t foo",
    "tag": "tmux"
  },
  {
    "question": "How do I change the starting directory of a tmux session?",
    "answer": "The way to do this is to detach from the session (^b d with the default keybindings) and then specify a different directory when you reattach to it. When attaching to a session, use the -c flag to specify the working directory. Here's an example:\n$ tmux list-sessions\ntmuxwtfbbq: 3 windows (created Tue Apr  5 14:25:48 2016) [190x49]\n$ tmux attach-session -t tmuxwtfbbq -c /home/chuck/new_default_directory\n\nThis setting will be persisted - after you've reset the working directory, you won't need to keep specifying it every time you reattach to the session.\nFor the record, I'm on tmux version 2.0 (though I don't think it matters - I couldn't find anything about adding a -c option to the attach-session command in the change logs so I assume it's been there for quite a while).",
    "tag": "tmux"
  },
  {
    "question": "Split pane switching in tmux: switch once per command",
    "answer": "That happens because the default bindings for the arrow keys are setup with bind-key -r, specifying that they may be repeated. There are two ways that you can disable this.\nFirst, you can use set-option repeat-time 0 to disable repeating entirely. This will affect all bindings. I find that to be very annoying when resizing panes.\nSecondly, you can change the bindings for the arrow keys to use bind-key without the -r option:\nbind-key Up    select-pane -U\nbind-key Down  select-pane -D\nbind-key Left  select-pane -L\nbind-key Right select-pane -R",
    "tag": "tmux"
  },
  {
    "question": "If I set key bind of C-b to c-a in tmux how can I move the cursor to the beginning of the line?",
    "answer": "Maybe its an issue about the version I am using, but if the above code does not work for you, try this:\nset -g prefix C-a\nunbind-key C-b\nbind-key C-a send-prefix",
    "tag": "tmux"
  },
  {
    "question": "Tmux borders displayed as x q instead of lines?",
    "answer": "I had the same problem with PuTTY and Windows 8 when connecting to tmux running on a Debian Squeeze machine. Even when setting the charset to UTF-8 in PuTTY (in the settings under Window > Translation > Remote character set) I didn't get the correct line drawing.\nSetting the Remote character set to \"Use font encoding\" did the trick for me.",
    "tag": "tmux"
  },
  {
    "question": "How to automatically rename tmux windows to the current directory",
    "answer": "With tmux 2.3+, the b: format modifier shows the \"basename\" (or \"tail\") of a path.\nset-option -g status-interval 5\nset-option -g automatic-rename on\nset-option -g automatic-rename-format '#{b:pane_current_path}'\n\nThe FORMATS section of man tmux describes other modifiers, such as #{d:} and even #{s/foo/bar/:}.\n\nWith tmux 2.2 or older, the basename shell command can be used instead.\nset-option -g status-interval 5\nset-option -g automatic-rename on\nset-option -g automatic-rename-format '#(basename \"#{pane_current_path}\")'",
    "tag": "tmux"
  },
  {
    "question": "tmux: open terminal failed: missing or unsuitable terminal: xterm-256color",
    "answer": "Your system doesn't have xterm-256color. You could:\n\nSet TERM to something other than xterm-256color outside tmux (try just plain export TERM=xterm).\n\nSee if there is a package containing xterm-256color, perhaps a later version of ncurses or terminfo.\n\nInstall it manually from another system with something like:\ninfocmp -x xterm-256color > out\n\nThen transfer the \"out\" file to your Mac and try:\ntic out",
    "tag": "tmux"
  },
  {
    "question": "How to copy and paste between different tmux panes running vim instances",
    "answer": "Sorry, I'm trying to convince you to use vim built-in features.\n\nTo make the copy/paste easy, you can open files in another Tabpages:\n:tabe /path/to/another/file\n\nUse gt or gT to switch Tabpages.\n\nOr split the window to edit another file:\n:sp /path/to/another/file\n\nUse Ctrl-ww to switch Windows.\nTo split the window vertically, please use :vsp file\n\nUpdate:\nThis is my .tmux.conf file:\n# vim\nsetw -g mode-keys vi\nbind [ copy-mode\nbind -t vi-copy v begin-selection\nbind -t vi-copy y copy-selection\nbind -t vi-copy V rectangle-toggle\nbind ] paste-buffer\n\n# buffer\nbind Space choose-buffer\n\nI only use them when I need to copy terminal output.",
    "tag": "tmux"
  },
  {
    "question": "TMUX using HJKL to navigate panes",
    "answer": "You can do this as follows:\nbind h select-pane -L\nbind j select-pane -D\nbind k select-pane -U\nbind l select-pane -R\n\nNote that mode-keys refers to using vi-like navigation within a buffer and status-keys refers to using vi-like editing within the status bar, but neither refers to switching between panes.",
    "tag": "tmux"
  },
  {
    "question": "How do I jump to double-digit window number in tmux?",
    "answer": "There are two straightforward options (let C-b represent the prefix key):\n\nBring up a prompt in which to enter a specific window index with C-b ' (this is a default key binding). Press enter after inputting the desired index.\nInteractively select the window you want from a list with C-b w (also a default key binding). In this list, windows are assigned a character in order from 0 to 9 and then from a onward. Press a character to jump to the corresponding window or use the arrow keys to highlight one and press enter.\n\nAlthough option 2 is fewer keystrokes (using the characters), it's arguably less elegant than option 1. First, the window list completely obscures the current pane; second, indices are assigned to windows based on their order, meaning that the index in the list may sometimes differ from the index displayed in the window titles (such as when there are gaps in the window numbering).\nIf you're looking for the least intrusive and most analogous (compared to C-b N) solution, option 1 is probably your best bet.",
    "tag": "tmux"
  },
  {
    "question": "tmux send-keys syntax",
    "answer": "The key names used by send-keys are the same ones that bind-key uses.\nFrom the Key Bindings section of the tmux manpage:\n\nWhen specifying keys, most represent themselves (for example ‘A’ to\n       ‘Z’).  Ctrl keys may be prefixed with ‘C-’ or ‘^’, and Alt (meta) with\n       ‘M-’.  In addition, the following special key names are accepted: Up,\n       Down, Left, Right, BSpace, BTab, DC (Delete), End, Enter, Escape, F1 to\n       F20, Home, IC (Insert), NPage/PageDown/PgDn, PPage/PageUp/PgUp, Space,\n       and Tab.\n\nAlthough they are not listed in the man page, there are also special names for keypad-specific keys: KP0 through KP9, KP/, KP*, KP-, KP+, KP., and KPEnter.\nSeveral of the more cryptic key names (BTab, IC, DC, NPage, PPage) probably come from the terminfo library.\nEmacs shares the convention of using C- and M- prefixes to indicate modifiers (I would not be surprised if there were earlier uses of this convention).",
    "tag": "tmux"
  },
  {
    "question": "Why am I getting a \"failed to connect to server\" message from tmux when I try to list sessions?",
    "answer": "TL;DR: Try sending SIGUSR1 signal to the tmux server process.\nIn my case, after about 8 days of inactivity, I was not able to reattach:\n$ tmux attach\nno sessions\n\nHowever, a grep for tmux process got me this output:\n$ ps -aef | fgrep -i tmux\nhari     7139     1  1  2016 ?        2-20:32:31 tmux\nhari    25943 25113  0 22:00 pts/0    00:00:00 fgrep --color=auto -i tmux\n\nAs suggested by @7heo.tk, this indicates that tmux server is still running, but tmux ls was giving failed to connect to server: Connection refused error. I verified that the tmp directory that belonged to the tmux session existed and lsof -p 7139 (the pid of tmux server) showed that the socket file is open:\nCOMMAND  PID  USER   FD   TYPE             DEVICE SIZE/OFF       NODE NAME\ntmux    7139 hari    5u  unix 0x0000000000000000      0t0 1712879255 /tmp/tmux-50440/default\n\nI also tried explicitly specifying the -S /tmp/tmux-50440/default to tmux but it didn't help. However, I read in the tmux man page that sending SIGUSR1 would make tmux recreate the socket file, so I tried that and I was able to immediately find the session and reattach:\n$ kill -s USR1 7139\n$ tmux ls\n0: 12 windows (created Mon Apr 18 21:17:55 2016) [198x62]",
    "tag": "tmux"
  },
  {
    "question": "Howto go to beginning of line in tmux after remapping prefix to CTRL+A?",
    "answer": "You need to tell tmux that Prefix+a should send the prefix key to the program running in the current pane:\nbind a send-prefix",
    "tag": "tmux"
  },
  {
    "question": "Fix Vim + Tmux yank/paste on unnamed register",
    "answer": "From the error message (Nothing in register *), it appears that when you do a plain? p, your instance of Vim is using the * register instead of the unnamed register*. This is probably because your clipboard option includes the value unnamed. When configured this way, Vim will use the * register instead of the unnamed register for yank, delete, change, and put operations by default (i.e. unless you specify another register with a \" prefix; e.g. \"ap to put from the a register).\n*The unnamed register is actually named \" (double quote). It is only “unnamed” in the sense that you do not have to name it to use it (it is the default). I.e. you do not have to say \"\"p to put from the unnamed register, just p.\nThe default value of clipboard does not contain unnamed, so it is probably coming from some bit of your configuration (or a plugin). The command :verbose set clipboard? will show you the script that set the current value. If this is being done in your configuration file, then you might want to not do it when you are running under tmux. E.g:\nif $TMUX == ''\n    set clipboard+=unnamed\nendif\n\nAlternatively, there may be some way to let instances of Vim-inside-tmux access the GUI selection/clipboard (thus work with the * register and/or unnamed in clipboard). If you are running Mac OS X, you may want to look at my workaround wrapper that re-enables clipboard access for processes running inside a tmux session. If you are using some other OS or GUI, then you will need to find out how Vim would normally talk to the GUI and why it is not working (e.g. wrong DISPLAY value under X11, possibly due to attaching to an old session that is running a shell that has an out-of-date value).",
    "tag": "tmux"
  },
  {
    "question": "How to write if statement in .tmux.conf to set different options for different tmux versions?",
    "answer": "Based on @ericx's answer and @thiagowfx's answer I put the following together which covers many of the listed incompatibilties from version 2.0 onwards:\n# Version-specific commands [grumble, grumble]\n# See: https://github.com/tmux/tmux/blob/master/CHANGES\nrun-shell 'tmux setenv -g TMUX_VERSION $(tmux -V | \\\n                           sed -En \"s/^tmux[^0-9]*([.0-9]+).*/\\1/p\")'\n\nif-shell -b '[ \"$(echo \"$TMUX_VERSION < 2.1\" | bc)\" = 1 ]' {\n    set -g mouse-select-pane on; set -g mode-mouse on\n    set -g mouse-resize-pane on; set -g mouse-select-window on\n    set -g message-fg red\n    set -g message-bg black\n    set -g message-attr bright\n    set -g window-status-bg default\n    set -g window-status-fg default\n    set -g window-status-current-attr bold\n    set -g window-status-current-bg cyan\n    set -g window-status-current-fg default\n    set -g window-status-bell-fg red\n    set -g window-status-bell-bg black\n    set -g window-status-activity-fg white\n    set -g window-status-activity-bg black\n}\n\n# In version 2.1 \"mouse\" replaced the previous 4 mouse options\nif-shell -b '[ \"$(echo \"$TMUX_VERSION >= 2.1\" | bc)\" = 1 ]' {\n    set -g mouse on\n}\n\n# UTF8 is autodetected in 2.2 onwards, but errors if explicitly set\nif-shell -b '[ \"$(echo \"$TMUX_VERSION < 2.2\" | bc)\" = 1 ]' \\\n    set -g utf8 on\n    set -g status-utf8 on\n    set -g mouse-utf8 on\n}\n\n# bind-key syntax changed in 2.4 -- selection / copy / paste\nif-shell -b '[ \"$(echo \"$TMUX_VERSION < 2.4\" | bc)\" = 1 ]' {\n    bind-key -t vi-copy v   begin-selection\n    bind-key -t vi-copy V   send -X select-line\n    bind-key -t vi-copy C-v rectangle-toggle\n    bind-key -t vi-copy y   copy-pipe 'xclip -selection clipboard -in'\n}\n\n# Newer versions\nif-shell -b '[ \"$(echo \"$TMUX_VERSION < 2.9\" | bc)\" = 1 ]' {\n    bind-key -T copy-mode-vi v   send -X begin-selection\n    bind-key -T copy-mode-vi V   send -X select-line\n    bind-key -T copy-mode-vi C-v send -X rectangle-toggle\n    bind-key -T copy-mode-vi y   send -X copy-pipe-and-cancel 'xclip -selection clipboard -in'\n}\n\nif-shell -b '[ \"$(echo \"$TMUX_VERSION >= 2.9\" | bc)\" = 1 ]' {\n    set -g message-style fg=red,bg=black\n    set -g message-style bright\n    set -g window-status-style          fg=default,bg=default\n    set -g window-status-current-style  fg=default,bg=cyan,bold\n    set -g window-status-bell-style     fg=red,bg=black\n    set -g window-status-activity-style fg=white,bg=black\n}\n\nI raised an issue about the problems with tmux's non-backward-compatibility here. The summary is that the tmux devs will not support backward compatibility, nor will they adopt a version numbering scheme which highlights which versions contain breaking changes. 😢\nI raised an issue to support numeric comparators for %if which was implemented in v3.0.",
    "tag": "tmux"
  },
  {
    "question": "How can I make TMUX be active whenever I start a new shell session?",
    "answer": "warning this can now 'corrupt' (make it unable to open a terminal window - which is not good!) your Ubuntu logins.  Use with extreme caution and make sure you have a second admin account on the computer that you can log into in case you have the same problems I did.  See my other answer for more details and a different approach.\nGiven that warning, the simplest solution can be to append the tmux invocation to the end of your .bashrc, e.g.\nalias g=\"grep\"\nalias ls=\"ls --color=auto\"\n\n# ...other stuff...\n\nif [[ ! $TERM =~ screen ]]; then\n    exec tmux\nfi\n\nNote that the exec means that the bash process which starts when you open the terminal is replaced by tmux, so Ctrl-B D (i.e. disconnect from tmux) actually closes the window, instead of returning to the original bash process, which is probably the behaviour you want?\nAlso, the if statement is required (it detects if the current bash window is in a tmux process already) otherwise each time you start tmux, the contained bash process will attempt to start its own tmux session, leading to an infinite number of nested tmuxen which can be, err, quite annoying (that said, it looks cool).\n\nHowever, there is a very small risk this can make bash behave in a way that other programs don't expect, since running bash can possibly cause it to turn into a tmux process, so it might be better to modify how you start your terminal emulator.\nI use a small executable shell script ~/bin/terminal (with ~/bin in $PATH, so it is found automatically) that looks a bit like:\n#!/bin/sh\nexec gnome-terminal -e tmux\n\n(I don't use gnome-terminal, so you might have to remove the exec, I'm not sure.)\nNow whenever you run the terminal scipt you have a terminal with tmux. You can add this to your menu/desktop/keyboard shortcuts to replace the default terminal.\n(This approach also allows you to more easily customise other things about the terminal emulator later, if you ever desire.)",
    "tag": "tmux"
  },
  {
    "question": "tmux status bar configuration",
    "answer": "The man page has very detailed descriptions of all of the various options (the status bar is highly configurable). Your best bet is to read through man tmux and pay particular attention to those options that begin with status-.\nSo, for example, status-bg red would set the background colour of the bar.\nThe three components of the bar, the left and right sections and the window-list in the middle, can all be configured to suit your preferences. status-left and status-right, in addition to having their own variables (like #S to list the session name) can also call custom scripts to display, for example, system information like load average or battery time.\nThe option to rename windows or panes based on what is currently running in them is automatic-rename. You can set, or disable it globally with:\nsetw -g automatic-rename [on | off]\nThe most straightforward way to become comfortable with building your own status bar is to start with a vanilla one and then add changes incrementally, reloading the config as you go.1\nYou might also want to have a look around on github or bitbucket for other people's conf files to provide some inspiration. You can see mine here2.\n\n\n1 You can automate this by including this line in your .tmux.conf:\nbind R source-file ~/.tmux.conf \\; display-message \"Config reloaded...\"\nYou can then test your new functionality with Ctrlb,Shiftr. tmux will print a helpful error message—including a line number of the offending snippet—if you misconfigure an option.\n2 Note: I call a different status bar depending on whether I am in X or the console - I find this quite useful.",
    "tag": "tmux"
  },
  {
    "question": "tmux: how to toggle \"on\" and \"off\" options with the same key",
    "answer": "If you don't explicitly specify \"on\" or \"off\", the option will get toggled. The following would suffice:\nbind-key a set-window-option synchronize-panes\\; display-message \"synchronize-panes is now #{?pane_synchronized,on,off}\"",
    "tag": "tmux"
  },
  {
    "question": "Tmux: Switch the split style of two adjacent panes",
    "answer": "Prefix + Space is bound to next layout",
    "tag": "tmux"
  },
  {
    "question": "Using Emacs server and emacsclient on other machines as other users",
    "answer": "This should provide a starting point for what you want.\nFrom the info node (emacs) emacsclient Options \n`--server-file=SERVER-FILE'\n     Specify a \"server file\" for connecting to an Emacs server via TCP.\n\n     An Emacs server usually uses an operating system feature called a\n     \"local socket\" to listen for connections.  Some operating systems,\n     such as Microsoft Windows, do not support local sockets; in that\n     case, Emacs uses TCP instead.  When you start the Emacs server,\n     Emacs creates a server file containing some TCP information that\n     `emacsclient' needs for making the connection.  By default, the\n     server file is in `~/.emacs.d/server/'.  On Microsoft Windows, if\n     `emacsclient' does not find the server file there, it looks in the\n     `.emacs.d/server/' subdirectory of the directory pointed to by the\n     `APPDATA' environment variable.  You can tell `emacsclient' to use\n     a specific server file with the `-f' or `--server-file' option, or\n     by setting the `EMACS_SERVER_FILE' environment variable.\n\n     Even if local sockets are available, you can tell Emacs to use TCP\n     by setting the variable `server-use-tcp' to `t'.  One advantage of\n     TCP is that the server can accept connections from remote machines.\n     For this to work, you must (i) set the variable `server-host' to\n     the hostname or IP address of the machine on which the Emacs server\n     runs, and (ii) provide `emacsclient' with the server file.  (One\n     convenient way to do the latter is to put the server file on a\n     networked file system such as NFS.)\n\nYou also may want to look at variables server-auth-dir, server-auth-key and server-port",
    "tag": "tmux"
  },
  {
    "question": "\"libevent not found\" error in tmux",
    "answer": "If you're trying to build software then you need the development package. Install libevent-devel.\nOn Debian/Ubuntu based distributions you can install it with\nsudo apt install libevent-dev",
    "tag": "tmux"
  },
  {
    "question": "Ugrade tmux from 1.8 to 1.9 on Ubuntu 14.04",
    "answer": "Update: due to new tmux version and changes in package repository, this answer is updated to show how to install tmux 2.0 (which is better, no reason to use 1.9 anymore).\nHere are the steps to update \"blank\" ubuntu - version 14.04 only (see below for other ubuntu versions):\n\nsudo apt-get update\nsudo apt-get install -y python-software-properties software-properties-common\nsudo add-apt-repository -y ppa:pi-rho/dev\nsudo apt-get update\nsudo apt-get install -y tmux=2.0-1~ppa1~t\nnow if you do tmux -V it should show tmux 2.0 which is a good version for tmux plugins\n\nI verified the above steps on a new digitalocean droplet.\nBasically, it's adding the pi-rho/dev repository, updating and then installing tmux from there.\nIf you have another ubuntu version you might want to install a different tmux version from the same repo. So:\n\nubuntu 12.04 (Precise Pangolin) step 5: sudo apt-get install -y tmux=1.9a-1~ppa1~p (installs tmux 1.9, no package for tmux 2.0 yet)\nubuntu 13.10 (Saucy Salamander) step 5: sudo apt-get install -y tmux=1.9a-1~ppa1~s (installs tmux 1.9, no package for tmux 2.0 yet)\nubuntu 14.10 (Utopic Unicorn) step 5: sudo apt-get install -y tmux=2.0-1~ppa1~u\nubuntu 15.04 (Vivid Vervet) step 5: sudo apt-get install -y tmux=2.0-1~ppa1~v",
    "tag": "tmux"
  },
  {
    "question": "Bind Ctrl+Tab and Ctrl+Shift+Tab in tmux",
    "answer": "Recent “unreleased” versions of tmux do automatically recognize those xterm-style key sequences once you have your terminal sending them (no need to change your terminfo entry). The next release version (1.8?) should also have this support. With an appropriate build of tmux1, all you have to do is bind the keys in your tmux configuration:\nbind-key C-Tab next-window\nbind-key C-S-Tab previous-window\n\nYou will still need to type your prefix key before these keys.\n(Note: ~/.tmux.conf is only processed when the server starts. If you make changes to it, you will either need to exit all your sessions and restart the server, or use (e.g.) tmux source ~/.tmux.conf to have your existing server re-process the file.)\nAlso, if you want tmux to pass along these (and other) xterm-style key sequences to programs running inside tmux, then you will need to enable the xterm-keys window option.\nset-option -gw xterm-keys on\n\n(If you prefer, you can do this on a per-window basis by using -w instead of -gw.)\n\nIf you want to be able to use those keys without typing the prefix, then you can use “no prefix” bindings instead:\nbind-key -n C-Tab next-window\nbind-key -n C-S-Tab previous-window\n\nThis will more or less “dedicate” the keys to tmux, though. It will be difficult to type these keys to any program running inside tmux (e.g. you would have to use the tmux command send-keys C-Tab—as normal, xterm-keys must be enabled to send these xterm-style key sequences).\n\nThe problem with your terminfo entry editing is probably because each line after the one that names the terminal type needs to start with a Tab. Lines that do not start with a tab are the beginning of a new terminal entry. Technically, the NL TAB sequence is basically a line continuation in this file format; each entry is a single logical line.\nAlso, if you are redefining terminfo entries, be sure to use -x with infocmp and tic to preserve the user-defined capabilities (some of which are fairly standard).\n\n1 I.e. built from recent code in the tmux Git repository at sf.net (at the clone-able URL git://git.code.sf.net/p/tmux/tmux-code).",
    "tag": "tmux"
  },
  {
    "question": "Complete tmux reset",
    "answer": "Had forgotten to kill the existing sessions:\ntmux kill-server\n\nwas the solution (credits to: @Kent)",
    "tag": "tmux"
  },
  {
    "question": "Change background color of active or inactive pane in Tmux",
    "answer": "It seems that tmux-2.1 (released 18 October 2015) now allows the colours of individual panes to be specified. From the changelog:\n* 'select-pane' now understands '-P' to set window/pane background colours.\n\ne.g. [from the manual] to change pane 1's foreground (text) to blue and background to red use:\nselect-pane -t:.1 -P 'fg=blue,bg=red'\n\nTo mimic iTerm colour scheme:\nTo answer the original question, I use the following lines in my ~/.tmux.conf for setting the background/foreground colours to mimic the behaviour in iTerm:\n#set inactive/active window styles\nset -g window-style 'fg=colour247,bg=colour236'\nset -g window-active-style 'fg=colour250,bg=black'\n\n# set the pane border colors \nset -g pane-border-style 'fg=colour235,bg=colour238' \nset -g pane-active-border-style 'fg=colour51,bg=colour236'\n\nI hadn't seen the window-style and window-active-style commands before, but maybe they were available in previous tmux versions.\nAlso, these two lines are pretty useful for splitting panes easily:\nbind | split-window -h\nbind - split-window -v\n\nEDIT: as Jamie Schembri mentions in the comments, tmux version 2.1 (at least) will now be installed with:\nbrew install tmux\n\nEDIT (Oct 2017): brew now installs tmux 2.6, and the above still works.\nEDIT Vim panes: If you find that the \"inactive colouring\" does not work with a Vim pane, it might be due to the colourscheme you are using. Try with the pablo scheme; i.e. in the Vim pane:\n:colo pablo\n\nTo make it work with your own custom Vim colourscheme, make sure that the setting for Normal highlighting does not have ctermbg or guibg specified. As an example, the \"inactive colouring\" does not work with the murphy colourscheme, because in murphy.vim there is the line:\nhi Normal    ctermbg=Black   ctermfg=lightgreen   guibg=Black   guifg=lightgreen\n\nthat sets ctermbg or guibg to Black. However, changing this line to:\nhi Normal    ctermfg=lightgreen  guifg=lightgreen\n\nwill make the \"inactive colouring\" work.\nEDIT July 2019 Augusto provided a good suggestion for also changing the background colour for the line numbers. What I use in my vim colourscheme is the following (you need to find and edit the colourscheme file):\nhi Normal    guifg=#e6e1de ctermfg=none gui=none\nhi LineNr    guifg=#e6e1de ctermfg=none gui=none",
    "tag": "tmux"
  },
  {
    "question": "How can I move a window to another session in tmux?",
    "answer": "From my testing on tmux 2.6, you'll need two things for the command to move an entire window over:\n\nThe name of the session you want to move the window from (for future reference, $session_name)\nThe index of the window you want to move (in the session it's currently in, of course -- we'll call this $window_index). This is actually optional -- if you omit this, then it defaults to the window in focus in the session you're pulling the window from.\n\nFrom this point, you can just change to the session you want to move the window into, <tmux-escape>: into a command prompt, and type a command of this form:\nmove-window -s $session_name[:$window_index]\n\n...where, as noted before, the $window_index is optional (as indicated by the square brackets, which aren't actually part of the syntax\n). To use some concrete examples:\n# Moves from currently-focused window from session named `$session_name`\nmove-window -s $session_name \n\n# Moves from window with index `$window_index` from \n# session named `$session_name` into the current session\nmove-window -s $session_name:$window_index\n\nEt voilà! Your window got moved. :)",
    "tag": "tmux"
  },
  {
    "question": "What are the differences between set -g, set -ga and set-option -g in a .tmux.conf file?",
    "answer": "set is the alias of set-option.\nset -g is used to set global options and -ga appends values to existing settings.\nFrom Tmux's man page:\n\nWith -a, and if the option expects a string or a style, value is\n  appended to the existing setting.  For example:\n   set -g status-left \"foo\"\n   set -ag status-left \"bar\"\n\nWill result in ‘foobar’.  And:\n   set -g status-style \"bg=red\"\n   set -ag status-style \"fg=blue\"\n\nWill result in a red background and blue foreground.  Without -a, the\n  result would be the default background and a blue foreground.\n\nset-window-option (alias setw) is used to configure window options (allow-rename, mode-keys, synchronize-panes, etc.) and the same flag options are available.\nSee: \n\nhttps://linux.die.net/man/1/tmux\nhttps://superuser.com/questions/758843/difference-between-global-server-session-and-window-options",
    "tag": "tmux"
  },
  {
    "question": "keep Running Jupyter notebook with VSCode on remote server (SSH) after disconnecting",
    "answer": "Open a new tmux session on your server: e.g., tmux new -s my_sess\nGo in the folder with your notebook\nIf you use Anaconda, activate the environment with your libraries (including Jupyter)\nStart the jupyter notebook, optionally specifying the port where you want to forward it: e.g., jupyter notebook --no-browser --port=8080 &\nDisconnect from tmux\nOpen your notebook in VS Code\nOn the top-right, click the button to select your kernel and choose the option \"select another kernel\" -> \"Existing jupyter server\" -> \"Enter the URL of the running Jupyter server\"\nIf you did as I wrote in point 4, write \"http://localhost:8080/\"\nAt this point, you are asked to choose a Python kernel. I think you can choose whatever and it will use in any case the kernel running on the jupyter server in tmux",
    "tag": "tmux"
  },
  {
    "question": "tmux open terminal failed: not a terminal",
    "answer": "There is an answer already here, but this link I think summarises it better. In a nutshell, use the -t flag:\nssh -t host tmux attach\n\nIf you want to set it into your .ssh/config file, look in the ssh_config manpage for the RequestTTY option:\n RequestTTY\n         Specifies whether to request a pseudo-tty for the session.  The\n         argument may be one of: ``no'' (never request a TTY), ``yes''\n         (always request a TTY when standard input is a TTY), ``force''\n         (always request a TTY) or ``auto'' (request a TTY when opening a\n         login session).  This option mirrors the -t and -T flags for\n         ssh(1).",
    "tag": "tmux"
  },
  {
    "question": "give a hint when press prefix key in tmux",
    "answer": "The development version of tmux has support for this, so the next release (1.8?) should also support it.\nThere have been two changes that can be combined to indicate in your status line whether a prefix key has been pressed:\n\nYou can include the extended “format” replacements in the values of the “status” options. These replacements were first available in tmux 1.6, but they were not previously usable in the status options.\nThe client_prefix format replacement was added.\n\nYou could add a (conditional) highlighted <Prefix> string before the default status-right like this:\nset -g status-right ' #{?client_prefix,#[reverse]<Prefix>#[noreverse] ,}\"#{=21:pane_title}\" %H:%M %d-%b-%y'",
    "tag": "tmux"
  },
  {
    "question": "TMUX setting environment variables for sessions",
    "answer": "I figured out a way to do this. I'm using tmux 2.5.\nBackground\nIn the tmux man page, it states that there are two groups of environment variables: global and per-session. When you create a new tmux session, it will merge the two groups together and that becomes the set of environment variables available within the session. If the environment variables get added to the global group, it appears that they get shared between all open sessions. You want to add them to the per-session group.\nDo this\nStep 1: Create a tmux session.\ntmux new-session -s one\n\nStep 2: Add an environment variable to the per-session group.\ntmux setenv FOO foo-one\n\nThis adds the environment variable to per-session set of environment variables. If you type tmux showenv, you'll see it in the output. However, it isn't in the environment of the current session. Typing echo $FOO won't give you anything. There's probably a better way to do this, but I found it easiest to just export it manually:\nexport FOO='foo-one'\n\nStep 3: Create new windows/panes\nNow, every time you create a new window or pane in the current session, tmux will grab the FOO environment variable from the per-session group.\nAutomating it\nI use bash scripts to automatically create tmux sessions that make use of these environment variables. Here's an example of how I might automate the above:\n#!/bin/bash\nBAR='foo-one'\ntmux new-session -s one \\; \\\n  setenv FOO $BAR \\; \\\n  send-keys -t 0 \"export FOO=\"$BAR C-m \\; \\\n  split-window -v \\; \\\n  send-keys -t 0 'echo $FOO' C-m \\; \\\n  send-keys -t 1 'echo $FOO' C-m",
    "tag": "tmux"
  },
  {
    "question": "How can I copy text from a tmux window to the system clipboard?",
    "answer": "I have been using tmux-yank for copying text from the tmux buffer to the system clipboard. It can be set up with Tmux Plugin Manager or manually; see the tmux-yank instructions for details.",
    "tag": "tmux"
  },
  {
    "question": "tmux: hangs and do not load, and do not respond to any option command",
    "answer": "I had faced this problem for a long time and after a bit of searching I figured out that this was being caused because I accidently hit Ctrl+S (Ctrl+A+S is my shortcut for switching panes), and this turns off flow control in terminals and stops the terminal from accepting input. It can be reenabled by pressing Ctrl+Q.\nSource: https://superuser.com/a/553349/137226",
    "tag": "tmux"
  },
  {
    "question": "Vim: Difference between t_Co=256 and term=xterm-256color in conjunction with TMUX",
    "answer": "When you don't use tmux or screen, you only need to configure your terminal emulators to advertise themselves as \"capable of displaying 256 colors\" by setting their TERM to xterm-256color or any comparable value that works with your terminals and platforms. How you do it will depend on the terminal emulator and is outside of the scope of your question and this answer.\nYou don't need to do anything in Vim as it's perfectly capable to do the right thing by itself.\nWhen you use tmux or screen, those programs set their own default value for $TERM, usually screen, and Vim does what it has to do with the info it is given. \nIf you want a more uniform (and colorful) behavior, you must configure them to use a \"better\" value for $TERM:\n\ntmux\nAdd this line to ~/.tmux.conf: \nset -g default-terminal \"screen-256color\"\n\nscreen\nAdd this line to ~/.screenrc:\nterm \"screen-256color\"\n\n\nNow, both multiplexers will tell Vim they support 256 colors and Vim will do what you expect it to do.\nedit\nMy answer assumes that you are able to edit those configuration files but, since you are able to edit your ~/.vimrc, I don't think that I'm that far off the mark.\nedit 2\nThe value of the term option (retrieved with &term) is the name of the terminal as picked up by Vim upon startup. That name is what you are supposed to setup in your terminal emulator itself.\nThe value of the t_Co option (&t_Co) is what Vim considers to be the maximum number of colors that can be displayed by the host terminal. It is defined according to the entry corresponding to $TERM in terminfo:\n term            | t_Co\n-----------------+------ \n xterm           | 8\n xterm-256color  | 256\n screen          | 8\n screen-256color | 256\n\nWhen Vim starts up, it gets the value of the TERM environment variable, queries the terminfo database with that value and stores a number of informations on its environment in several t_… variables among which… the number of colors available in t_Co. Given a \"legal\" terminal type (one that Vim can lookup), Vim always assumes the correct number of colors.\nSetting t_Co to 256 while leaving term to its Vim-defined value — or, more generally, setting t_Co and/or term to values that don't match with the host terminal — makes no sense and will likely create troubles when Vim sends a signal that is not understood by the terminal or vice-versa.\nWhile it is entirely possible to do so, messing with t_Co and term in Vim is both totally useless and possibly harmful.\nAgain, just setup your terminal emulators and terminal multiplexers correctly. That's really all you need.\nIf you end up in a terminal multiplexer or a terminal emulator where you can't define a correct TERM, then and only then you can force Vim to assume 256 colors. To that end, changing the value of t_Co is the only thing that makes sense:\nif &term == \"screen\"\n  set t_Co=256\nendif\n\nSo… if you can configure each individual part:\n\nterminal emulator: xterm-256color\ntmux/screen: screen-256color\nvim: nothing\n\nand you are done.\nIf you can't control every part, use a simple conditional in your ~/.vimrc to set t_Co according to &term but don't change the value of term.\nBut if you can edit a ~/.vimrc there's no reason you can't edit a ~/.screenrc or ~/.tmux.conf or ~/.bashrc or whatever.",
    "tag": "tmux"
  },
  {
    "question": "tmux mouse copy-mode jumps to bottom",
    "answer": "As of tmux 2.5 you should use\nunbind -T copy-mode-vi MouseDragEnd1Pane",
    "tag": "tmux"
  },
  {
    "question": "How do I make tmux reorder windows when one is deleted?",
    "answer": "Let's do it more simply.\nIf you are using tmux below version 1.7, append next line to ~/.tmux.conf:\n bind-key C-s run \"for i in $(tmux lsw|awk -F: '{print $1}'); do tmux movew -s \\$i; done\"\n\nYou could sort all windows, by typing PREFIX-KEY, then Ctrl + s.\nElse, if you are using tmux version 1.7 or above, as already everybody says, append next line to ~/.tmux.conf:\n set-option -g renumber-windows on",
    "tag": "tmux"
  },
  {
    "question": "How to start tmux with several windows in different directories?",
    "answer": "Tmuxinator is also really good for this. Basically you create setup files like so:\n# ~/.tmuxinator/project_name.yml\n# you can make as many tabs as you wish...\n\nproject_name: Tmuxinator\nproject_root: ~/code/rails_project\nsocket_name: foo # Not needed. Remove to use default socket\nrvm: 1.9.2@rails_project\npre: sudo /etc/rc.d/mysqld start\ntabs:\n  - editor:\n      layout: main-vertical\n      panes:\n        - vim\n        - #empty, will just run plain bash\n        - top\n  - shell: git pull\n  - database: rails db\n  - server: rails s\n  - logs: tail -f logs/development.log\n  - console: rails c\n  - capistrano:\n  - server: ssh me@myhost\n\nThen you can start a new session with:\nmux project_name\n\nI've been using it for a while and have had a good experience for the most part.",
    "tag": "tmux"
  },
  {
    "question": "Tmux - Tmux true color is not working properly",
    "answer": "Perhaps you overlooked this in setting up (one can see that you overlooked Tc):\ncommit 427b8204268af5548d09b830e101c59daa095df9\nAuthor: nicm <nicm>\nDate:   Fri Jan 29 11:13:56 2016 +0000\n\n    Support for RGB colour, using the extended cell mechanism to avoid\n    wasting unnecessary space. The 'Tc' flag must be set in the external\n    TERM entry (using terminal-overrides or a custom terminfo entry), if not\n    tmux will map to the closest of the 256 or 16 colour palettes.\n\n    Mostly from Suraj N Kurapati, based on a diff originally by someone else.\n\nin tmux.conf:\n# Enable RGB colour if running in xterm(1)\nset-option -sa terminal-overrides \",xterm*:Tc\"\n\nin the manpage:\nTERMINFO EXTENSIONS\n     tmux understands some unofficial extensions to terminfo(5):\n...\n     Tc      Indicate that the terminal supports the ‘direct colour’ RGB\n             escape sequence (for example, \\e[38;2;255;255;255m).\n\n             If supported, this is used for the OSC initialize colour escape \n             sequence (which may be enabled by adding the ‘initc’ and ‘ccc’  \n             capabilities to the tmux terminfo(5) entry).\n\nRegarding -s versus -g, the manual page says:\n\nset-option [-agoqsuw] [-t target-session | target-window] option value\n                     (alias: set)\n  Set a window option with -w (equivalent to the\n  set-window-option command), a server option with -s, otherwise\n  a session option.  If -g is given, the global session or window\n  option is set.  The -u flag unsets an option, so a session\n  inherits the option from the global options (or with -g,\n  restores a global option to the default).\nThe -o flag prevents setting an option that is already set and\n  the -q flag suppresses errors about unknown or ambiguous\n  options.\nWith -a, and if the option expects a string or a style, value\n  is appended to the existing setting.\n\nAs I understand it,\nUsing -s means that new connections (created by the server) will get this setting, which is useful in shell initialization, while -g makes its changes too late for the shell initialization.\nFurther reading:\n\nAdd TrueColor Support #34\nTmux true color support. #622\nWhy only 16 (or 256) colors? (ncurses FAQ)",
    "tag": "tmux"
  },
  {
    "question": "How do I start tmux with my current environment?",
    "answer": "You should configure the tmux session option update-environment to include the variables you want to be updated when creating new sessions. The default value includes several common X11 and SSH variables:\nDISPLAY SSH_ASKPASS SSH_AUTH_SOCK SSH_AGENT_PID SSH_CONNECTION WINDOWID XAUTHORITY\n\nTo add your variables, use the set-option tmux command with its -g and -a flags (append to the existing “global” (default) value). In your ~/.tmux.conf:\nset-option -ga update-environment ' YOUR_VAR'\n\nBe sure to include the leading space so that your variable name is separated from the trailing name in the default value.",
    "tag": "tmux"
  },
  {
    "question": "Clipboard failure in tmux + vim after upgrading to MacOS Sierra",
    "answer": "This seem to be a regression on macOS Sierra. A solution that worked for me has been mentioned by Josh McGinnis https://github.com/tmux/tmux/issues/543:\nbrew install reattach-to-user-namespace\nEnsure the following is set in .tmux.conf:\nset -g default-shell $SHELL \nset -g default-command \"reattach-to-user-namespace -l ${SHELL}\"\n\nIn .vimrc or ~/.config/nvim/init.vim (for Neovim):\nset clipboard=unnamed\n\nNow all is well and I can copy/paste between system <-> vim sessions using vim keybindings and/or system ctrl+c / ctrl+p.",
    "tag": "tmux"
  },
  {
    "question": "Can I use double click to select and copy in tmux?",
    "answer": "I found a way to achieve that: hold the option key when double clicking.",
    "tag": "tmux"
  },
  {
    "question": "How to copy from tmux running in PuTTY to the Windows clipboard",
    "answer": "I use PuTTY v0.62 and tmux v1.8.\ntmux configuration: setw -g mode-mouse on\nI want to copy some text from tmux to system clipboard. I press and hold Shift, select the text by mouse, and then click the left-button of the mouse.\nI want to paste some text into tmux, press Shift and click the right-button of the mouse.",
    "tag": "tmux"
  },
  {
    "question": "Ubuntu - change tmux 1.8 to tmux-next 1.9",
    "answer": "Just have installed tmux 2.1 on Ubuntu Server and faced the same problem. The solution for me is to delete all those unknown options and add those lines instead:\n set -g mouse-utf8 on\n set -g mouse on\n\nNow in tmux 2.1 I can choose a pane, a window and resize everything with a mouse as it was in tmux 1.8\nMOUSE SUPPORT section of 'man tmux':\n\"The default key bindings allow the mouse to be used to select and resize panes, to copy text and to change window using the status line.  These take effect if the mouse option is turned on.\"\n\nUPDATE (since there are a lot of questions):\n0 Install:\nbrew cask install easysimbl\n\n1 Download .dmg file here: https://bitheap.org/mouseterm/\n2 Execute: \ncd /Volumes/MouseTerm && cp -r MouseTerm.bundle /Library/Application\\ Support/SIMBL/Plugins\n\n3 Add this to your .tmux.conf file:\nset -g mouse-utf8 on\nset -g mouse on\nbind -n WheelUpPane   select-pane -t= \\; copy-mode -e \\; send-keys -M\nbind -n WheelDownPane select-pane -t= \\;                 send-keys -M\n\n4 Reload tmux\n5 Scroll!!!1\nI hope this will help. Works for me on iTerm 2/OS X El Capitan/tmux-2.1",
    "tag": "tmux"
  },
  {
    "question": "Tmux borders are drawn with dashed lines; how can I change them to continuous lines?",
    "answer": "I found the origin of the problem. It's the font. I was using Monaco and it displays vertical dashes in a way that the vertical pane separator is dashed. With Menlo however it's solid.",
    "tag": "tmux"
  },
  {
    "question": "vim in tmux background color changes when paging",
    "answer": "As explained here, disable Background Color Erase (BCE) by clearing the t_ut terminal option (run :set t_ut= in Vim and then press Control+L to refresh the terminal's display) so that color schemes work properly when Vim is used inside tmux and GNU screen.\nPer the above link, BCE can be set in .vimrc by adding the following\nif &term =~ '256color'\n    \" disable Background Color Erase (BCE) so that color schemes\n    \" render properly when inside 256-color tmux and GNU screen.\n    \" see also http://snk.tuxfamily.org/log/vim-256color-bce.html\n    set t_ut=\nendif",
    "tag": "tmux"
  },
  {
    "question": "Change tmux default to zsh",
    "answer": "From man tmux:\n             default-shell path\n                     Specify the default shell.  This is used as the login shell for new windows when the default-command option is set to empty, and must\n                     be the full path of the executable.  When started tmux tries to set a default value from the first suitable of the SHELL environment\n                     variable, the shell returned by getpwuid(3), or /bin/sh.  This option should be configured when tmux is used as a login shell.\nSo, in your tmux.conf:\n# set shell\nset -g default-shell /bin/zsh\nand if you want you can add default command each time, when we start a new window:\n# Retach userspaces\nset -g default-command \"reattach-to-user-namespace -l zsh\"",
    "tag": "tmux"
  },
  {
    "question": "How to permanently set $PATH on Linux/Unix",
    "answer": "You need to add it to your ~/.profile or ~/.bashrc file. \nexport PATH=\"$PATH:/path/to/dir\"\n\nDepending on what you're doing, you also may want to symlink to binaries:\ncd /usr/bin\nsudo ln -s /path/to/binary binary-name\n\nNote that this will not automatically update your path for the remainder of the session. To do this, you should run:\nsource ~/.profile \nor\nsource ~/.bashrc",
    "tag": "zsh"
  },
  {
    "question": "zsh compinit: insecure directories",
    "answer": "Note: This answer is from 2012.\n\nThis fixed it for me:\n$ sudo chmod -R 755 /usr/local/share/zsh/site-functions\n\nCredit: a post on zsh mailing list\n\nEDIT: As pointed out by @biocyberman in the comments. You may need to update the owner of site-functions as well:\n$ sudo chown -R root:root /usr/local/share/zsh/site-functions\n\nOn my machine (OSX 10.9), I do not need to do this but YMMV.\nEDIT2: On OSX 10.11, only this worked:\n$ sudo chmod -R 755 /usr/local/share/zsh\n$ sudo chown -R root:staff /usr/local/share/zsh\n\nAlso user:staff is the correct default permission on OSX, where user is actually your username (i.e. whoami, or $USER).  In other words:\n$ sudo chown -R ${USER}:staff /usr/local/share/zsh",
    "tag": "zsh"
  },
  {
    "question": "How to add a progress bar to a shell script?",
    "answer": "You can implement this by overwriting a line.  Use \\r to go back to the beginning of the line without writing \\n to the terminal.\nWrite \\n when you're done to advance the line.\nUse echo -ne to:\n\nnot print \\n and\nto recognize escape sequences like \\r.\n\nHere's a demo:\necho -ne '#####                     (33%)\\r'\nsleep 1\necho -ne '#############             (66%)\\r'\nsleep 1\necho -ne '#######################   (100%)\\r'\necho -ne '\\n'\n\nIn a comment below, puk mentions this \"fails\" if you start with a long line and then want to write a short line: In this case, you'll need to overwrite the length of the long line (e.g., with spaces).",
    "tag": "zsh"
  },
  {
    "question": "Adding a new entry to the PATH variable in ZSH",
    "answer": "Actually, using ZSH allows you to use special mapping of environment variables. So you can simply do:\n# append\npath+=('/home/david/pear/bin')\n# or prepend\npath=('/home/david/pear/bin' $path)\n\n# and don't forget to export to make it inherited by child processes\nexport PATH\n\nFor me that's a very neat feature which can be propagated to other variables.\nExample:\ntypeset -T LD_LIBRARY_PATH ld_library_path\n\nThis is what man zshbuiltins reports about -T.\n-T [ scalar[=value] array[=(value ...)] [ sep ] ]\n    This flag has a different meaning when used with -f; see below.  Otherwise\n    the -T option requires zero, two,  or  three  argu ments  to  be  present.\n    With no arguments, the list of parameters created in this fashion is shown.\n    With two or three arguments, the first two are the name of a scalar and of\n    an array parameter (in that order) that will be tied together in the manner\n    of $PATH and $path. The optional third argument is a single-character\n    separator which will be used to join  the  elements of the array to form\n    the scalar; if absent, a colon is used, as with $PATH. Only the first\n    character of the separator is significant;\n    any remaining characters are ignored.\n    Multibyte characters are not yet supported.",
    "tag": "zsh"
  },
  {
    "question": "After installing Homebrew I get `zsh: command not found: brew`",
    "answer": "I had a similar issue on macOS Big Sur (11.0.1). In my case homebrew was saved in /opt/homebrew/, and not in /usr/local/....\nSo I added\nexport PATH=/opt/homebrew/bin:$PATH\nto .zshrc file in my home directory, and the ZSH shell was able to find the brew command.",
    "tag": "zsh"
  },
  {
    "question": "in mac always getting zsh: command not found:",
    "answer": "It's evident that you've managed to mess up your PATH variable.  (Your current PATH doesn't contain any location where common utilities are located.)\nTry:\nPATH=/bin:/usr/bin:/usr/local/bin:/sbin:${PATH}\nexport PATH\n\nAlternatively, for \"resetting\" zsh, specify the complete path to the shell:\nexec /bin/zsh\n\nor\nexec /usr/bin/zsh",
    "tag": "zsh"
  },
  {
    "question": "Switching from zsh to bash on OS X, and back again?",
    "answer": "You can just use exec to replace your current shell with a new shell:\nSwitch to bash:\nexec bash\n\nSwitch to zsh:\nexec zsh\n\nThis won't affect new terminal windows or anything, but it's convenient.",
    "tag": "zsh"
  },
  {
    "question": "Complex Git branch name broke all Git commands",
    "answer": "Problem\n\nCan anyone explain what happened? [...] I'd love to be able to delete that branch, but Git won't work for me.\n\nBy running\ngit branch SSLOC-201_Implement___str__()_of_ProductSearchQuery\n\nin zsh, you did not create any branch. Instead, you accidentally defined three shell functions, called git, branch, and SSLOC-201_Implement___str__, which ignore their parameters (if any) and whose body is _of_ProductSearchQuery. You can check for yourself that this is indeed what happened, by invoking the builtin zsh command called functions, which lists all existing shell functions:\n$ functions                                                     \nSSLOC-201_Implement___str__ () {\n    _of_ProductSearchQuery\n}\nbranch () {\n    _of_ProductSearchQuery\n}\ngit () {\n    _of_ProductSearchQuery\n}\n\nUnfortunately, although the other two shell functions are not problematic, the shell function called \"git\" now shadows the bona fide git command!\n$ which git\ngit () {\n    _of_ProductSearchQuery\n}\n# but the real \"git\" is a binary file that lives in /usr/local/bin/git (or some similar path)\n\nTherefore, you will subsequently get the error\ncommand not found: _of_ProductSearchQuery\n\nwhenever you attempt to run a Git command, e.g. git log, git status, etc. (assuming, of course, that no command called _of_ProductSearchQuery exists).\nSide note\n\n[...] I get the same error:\ngit:176: command not found: _of_ProductSearchQuery\n\n(with the number after git increasing every time I type a command)\n\nThat number simply corresponds to the value of HISTCMD, an environment variable that holds\n\n[t]he current history event number in an interactive shell, in other words the event number for the command that caused $HISTCMD to be read.\n\nSee the zsh manual for more details.\nSolution\n\nAnd how do I get back to normal?\n\nSimply delete the problematic shell function (and the other two you created by accident, while you're at it):\nunset -f git\nunset -f branch SSLOC-201_Implement___str__\n\nThen everything should be fine.\nWhat if unset is shadowed also?!\nGood question! I refer you to Wumpus W. Wumbley's excellent comment below.\n\nBranch-naming tips\nAvoid any special shell characters\nYes, as pointed out in the comments, parentheses are valid characters in Git branch names; you just need to quote the name appropriately, e.g.\n$ git branch 'foo()bar'\n$ git branch\n  foo()bar\n* master\n$ git checkout 'foo()bar'\nSwitched to branch 'foo()bar'\n\nHowever, the need for quoting such names every single time when used as command-line arguments should convince you to eschew parentheses in reference names. More generally, you should (as much as possible) avoid characters that have a special meaning in shells, to prevent surprises like this one.\nUse simple branch names\nYou should keep your branch names short and sweet anyway. Long descriptions like\n\nSSLOC-201_Implement___str__()_of_ProductSearchQuery\n\nbelong in commit messages, not in branch names.",
    "tag": "zsh"
  },
  {
    "question": "How do I update zsh to the latest version?",
    "answer": "If you're using  oh-my-zsh\n\nType omz update in the terminal\n\nNote: upgrade_oh_my_zsh is deprecated",
    "tag": "zsh"
  },
  {
    "question": "Looking for ALT+LeftArrowKey solution in zsh",
    "answer": "Run cat then press keys to see the codes your shortcut send.\n(Press Ctrl+C to kill the cat when you're done.)\nFor me, (ubuntu, konsole, xterm) pressing Alt+← sends ^[[1;3D, so i would put in my .zshrc\nbindkey \"^[[1;3C\" forward-word\nbindkey \"^[[1;3D\" backward-word\n\n(Actually I prefer to use Ctrl + arrow to move word by word, like in a normal textbox under windows or linux gui.)\nRelated question: Fix key settings (Home/End/Insert/Delete) in .zshrc when running Zsh in Terminator Terminal Emulator",
    "tag": "zsh"
  },
  {
    "question": "Git doesn't work on MacOS Catalina: \"xcrun: error: invalid active developer path (/Library/Developer/CommandLineTools), missing\"",
    "answer": "You'll need to reinstall the command line tools:\n$ xcode-select --install",
    "tag": "zsh"
  },
  {
    "question": "How can I change the color of my prompt in zsh (different from normal text)?",
    "answer": "Put this in ~/.zshrc:\nautoload -U colors && colors\nPS1=\"%{$fg[red]%}%n%{$reset_color%}@%{$fg[blue]%}%m %{$fg[yellow]%}%~ %{$reset_color%}%% \"\n\nSupported Colors:\nred, blue, green, cyan, yellow, magenta, black, & white (from this answer) although different computers may have different valid options.\nSurround color codes (and any other non-printable chars) with %{....%}. This is for the text wrapping to work correctly.\nAdditionally, here is how you can get this to work with the directory-trimming from here.\nPS1=\"%{$fg[red]%}%n%{$reset_color%}@%{$fg[blue]%}%m %{$fg[yellow]%}%(5~|%-1~/.../%3~|%4~) %{$reset_color%}%% \"",
    "tag": "zsh"
  },
  {
    "question": "What does [nyae] mean in Zsh?",
    "answer": "zsh has a powerful correction mechanism. If you type a command in the wrong way it suggests corrections.\nWhat happend here is that dir is an unknown command and zsh suggests gdir, while maybe ls was what you wanted.\n\nIf you want to execute gdir hit y (yes)\nIf you want to try to execute dir anyway hit n (no)\nIf you want to execute completely different spelt command like ls hit a (abort) and type your command\nIf you want to execute a similar spelt commant like udir hit e (edit) and edit your command.",
    "tag": "zsh"
  },
  {
    "question": "zsh problem: compinit:503: no such file or directory: /usr/local/share/zsh/site-functions/_brew",
    "answer": "I had a similar issue. I ran brew cleanup which fixed the symlinks.",
    "tag": "zsh"
  },
  {
    "question": "Conda command not found",
    "answer": "If you're using zsh and it has not been set up to read .bashrc, you need to add the Miniconda directory to the zsh shell PATH environment variable. Add this to your .zshrc:\nexport PATH=\"/home/username/miniconda/bin:$PATH\"\n\nMake sure to replace /home/username/miniconda with your actual path.\nSave, exit the terminal and then reopen the terminal. conda command should work.",
    "tag": "zsh"
  },
  {
    "question": "Git tab completion not working in zsh on mac",
    "answer": "TL;DR one-liner\necho 'autoload -Uz compinit && compinit' >> ~/.zshrc && . ~/.zshrc\n\nthis will enable completion in .zshrc and apply the setting to your current terminal session.\nExplanation:\nActually, ZSH does know how to do git completion out of the box, but you need to turn on the completion feature itself (which from the steps you described I guess you haven't done)\nAdding this to your .zshrc should be enough:\nautoload -Uz compinit && compinit\n\nAfter you put the line .zshrc file, don't forget to restart the shell for ZSH to pick up the new config (alternatively, you can execute the line in your current session, that'll enable autocompletion for that one session)\nthe zsh compinit: insecure directories warning\nThanks to @FranMorzoa for suggesting to use compinit -u to skip the security checks for completion scripts\nWhile this will get rid of the warning/confirmation, the warning is there for a reason and it shouldn't happen normally.\nIt is a sign that something is wrong with ownership of the completion scripts, and it can (and should) be fixed with one of these:\n\nbrew.sh version:\nchmod -R go-w \"$(brew --prefix)/share\"\n\nanother one, will probably work for non-brew zsh, credits to pvinis on GitHub:\ncompaudit | xargs chmod g-w\n\n\nMore info\n\nhttps://git-scm.com/book/en/v2/Appendix-A:-Git-in-Other-Environments-Git-in-Zsh\nhttps://docs.brew.sh/Shell-Completion#configuring-completions-in-zsh\n\nPS Another answer here suggests installing the hub tool instead: although the tool is handy, it's merely a 3rd party (github community) wrapper around git. Hence, it has nothing to do with the topic of \"Git completion in ZSH\"",
    "tag": "zsh"
  },
  {
    "question": "ZSH alias with parameter",
    "answer": "If you really need to use an alias with a parameter for some reason, you can hack it by embedding a function in your alias and immediately executing it:\nalias example='f() { echo Your arg was $1. };f'\n\nI see this approach used a lot in .gitconfig aliases.",
    "tag": "zsh"
  },
  {
    "question": "Is there anything in Zsh like .bash_profile?",
    "answer": "Yes, it's called ~/.zshenv.\nHere's how I have $JAVA_HOME set in ~/.zshenv:\nexport JAVA_HOME=\"$(/usr/libexec/java_home)\"\n\nKeep in mind, however, that zsh is not bash, so just 'cause you have to source your .bash_profile every time you open a terminal does not mean that you have to do that with zsh.  With zsh, I only have to re-source my ~/.zshenv when I make changes to it, and then only for terminals which are already open: new terminals should have already sourced my new and improved ~/.zshenv.\nNOTE\nI often find it helpful, when trying to determine which of my zsh startup files I should place things in to consult zsh startup files.\nA newer version of the documentation for startup files can be found here.",
    "tag": "zsh"
  },
  {
    "question": "zsh: no matches found: requests[security]",
    "answer": "zsh uses square brackets for globbing / pattern matching.\nThat means that if you need to pass literal square brackets as an argument to a command, you either need to escape them or quote the argument like this:\npip install 'requests[security]'\n\nIf you want to disable globbing for the pip command permanently, you can do so by adding this to your ~/.zshrc:\nalias pip='noglob pip'",
    "tag": "zsh"
  },
  {
    "question": "oh-my-zsh slow, but only for certain Git repo",
    "answer": "You can add this to your git config and zsh won't check the status anymore  \ngit config --add oh-my-zsh.hide-status 1\ngit config --add oh-my-zsh.hide-dirty 1\n\n\nExplanation\nThere are two central git functions in in lib/git.zsh:  \n\ngit_prompt_info()\nparse_git_dirty()\n\nEach Method has a git config switch to disable it:\n\noh-my-zsh.hide-status\noh-my-zsh.hide-dirty\n\nSome themes create their own git queries and sometimes ignore these flags.",
    "tag": "zsh"
  },
  {
    "question": "Where to place $PATH variable assertions in zsh?",
    "answer": "tl;dr version: use ~/.zshrc\nAnd read the man page to understand the differences between: \n\n~/.zshrc, ~/.zshenv and ~/.zprofile. \n\n\nRegarding my comment\nIn my comment attached to the answer kev gave, I said:\n\nThis seems to be incorrect - /etc/profile isn't listed in any zsh documentation I can find.\n\nThis turns out to be partially incorrect: /etc/profile may be sourced by zsh. However, this only occurs if zsh is \"invoked as sh or ksh\"; in these compatibility modes:\n\nThe usual zsh startup/shutdown scripts are not executed. Login shells source /etc/profile followed by $HOME/.profile. If the ENV environment variable is set on invocation, $ENV is sourced after the profile scripts. The value of ENV is subjected to parameter expansion, command substitution, and arithmetic expansion before being interpreted as a pathname. [man zshall, \"Compatibility\"]. \n\nThe ArchWiki ZSH link says:\n\nAt login, Zsh sources the following files in this order:\n  /etc/profile\n  This file is sourced by all Bourne-compatible shells upon login\n\nThis implys that /etc/profile is always read by zsh at login - I haven't got any experience with the Arch Linux project; the wiki may be correct for that distribution, but it is not generally correct. The information is incorrect compared to the zsh manual pages, and doesn't seem to apply to zsh on OS X (paths in $PATH set in /etc/profile do not make it to my zsh sessions). \n\n\nTo address the question:\n\nwhere exactly should I be placing my rvm, python, node etc additions to my $PATH?\n\nGenerally, I would export my $PATH from ~/.zshrc, but it's worth having a read of the zshall man page, specifically the \"STARTUP/SHUTDOWN FILES\" section - ~/.zshrc is read for interactive shells, which may or may not suit your needs - if you want the $PATH for every zsh shell invoked by you (both interactive and not, both login and not, etc), then ~/.zshenv is a better option. \n\nIs there a specific file I should be using (i.e. .zshenv which does not currently exist in my installation), one of the ones I am currently using, or does it even matter?\n\nThere's a bunch of files read on startup (check the linked man pages), and there's a reason for that - each file has it's particular place (settings for every user, settings for user-specific, settings for login shells, settings for every shell, etc).\nDon't worry about ~/.zshenv not existing - if you need it, make it, and it will be read.\n.bashrc and .bash_profile are not read by zsh, unless you explicitly source them from ~/.zshrc or similar; the syntax between bash and zsh is not always compatible. Both .bashrc and .bash_profile are designed for bash settings, not zsh settings.",
    "tag": "zsh"
  },
  {
    "question": "How do I delete/remove a shell function?",
    "answer": "unset -f z\n\nWill unset the function named z.  A couple people have answered with:\nunset z\n\nbut if you have a function and a variable named z only the variable will be unset, not the function.",
    "tag": "zsh"
  },
  {
    "question": "Comments in command-line Zsh",
    "answer": "Having just started trying out zsh, I ran into this problem too. You can do setopt interactivecomments (in your .zshrc file to make it permanent) to activate the bash-style comments.\nThe Z Shell Manual indicates that while this is default behavior for ksh (Korn shell) and sh (Bourne shell), and I am guessing also for bash (Bourne-again shell), it is not default for zsh (Z shell):\n\nIn the following list, options set by default in all emulations are marked <D>; those set by default only in csh, ksh, sh, or zsh emulations are marked <C>, <K>, <S>, <Z> as appropriate.\n\n\nINTERACTIVE_COMMENTS (-k) <K> <S>\nAllow comments even in interactive shells.",
    "tag": "zsh"
  },
  {
    "question": "Worth switching to zsh for casual use?",
    "answer": "Personally, I love zsh. \nGenerally, you probably won't notice the difference between it and bash, until you want to quickly do things like recursive globbing:\n\n**/*.c for example.\n\nOr use suffix aliases to associate specific progs with different suffixes, so that you can \"execute\" them directly. The below alias lets you \"run\" a C source file at the prompt by simply typing ./my_program.c – which will work exactly as if you typed vim ./my_program.c. (Sort of the equivalent to double clicking on the icon of a file.)\n\nalias -s c=vim\n\nOr print the names of files modified today:\n\nprint *(e:age today now:)\n\nYou can probably do all of these things in bash, but my experience with zsh is that if there's something I want to do, I can probably find it in zsh-lovers.\nI also find the book 'From Bash to Z-Shell' really useful.\nPlaying with the mind bogglingly large number of options is good fun too!",
    "tag": "zsh"
  },
  {
    "question": "Homebrew’s `git` not using completion",
    "answer": "You're looking for:\nbrew install git bash-completion\n\nAs warpc's comment states, you'll need to add the following to your ~/.bash_profile to get homebrew's bash-completion working:\nif [ -f $(brew --prefix)/etc/bash_completion ]; then\n    . $(brew --prefix)/etc/bash_completion\nfi\n\nThe above is mentioned in the caveats when you install the bash-completion formula.\n\nNote: if you are using Bash v4 or later (via brew install bash) then you're going to want to use brew install bash-completion@2, to enable tab completion add the following to ~/.bash_profile as described in the caveats:\nexport BASH_COMPLETION_COMPAT_DIR=\"/usr/local/etc/bash_completion.d\"\n[[ -r \"/usr/local/etc/profile.d/bash_completion.sh\" ]] && . \"/usr/local/etc/profile.d/bash_completion.sh\"\n\nThe additional export is necessary for git, docker, youtube-dl, and other completions which may be included in the $(brew --prefix)/etc/bash_completion.d/ directory.",
    "tag": "zsh"
  },
  {
    "question": "Command not found after npm install in zsh",
    "answer": "add source /home/YOUUSERNAME/.bash_profile at the beginning of ~/.zshrc\nAnd all missing commands will be detected. \nFor Mac users : add source /Users/YOUUSERNAME/.bash_profile",
    "tag": "zsh"
  },
  {
    "question": "ZSH iterm2 increase number of lines history",
    "answer": "It's not immediately obvious in the iTerm2 documentation on how to change it.\n\nopen the iTerm2 preferences ⌘ + ,\nselect the Profiles tab\nthen select the Terminal subtab\nBeware, changes to the Scrollback lines value take effect immediately so check Unlimited scrollback now if you don't want to delete your current buffer(s)\nchange the value of the Scrollback Lines to whatever you'd like\nUncheck the Unlimited scrollback option if you'd like to use your Scrollback lines value",
    "tag": "zsh"
  },
  {
    "question": "ZSH complains about RVM __rvm_cleanse_variables: function definition file not found",
    "answer": "Running the following solved the problem:\nrm ~/.zcompdump*\nNote: The * is incase there are multiple .zcompdump files.",
    "tag": "zsh"
  },
  {
    "question": "npm global path prefix",
    "answer": "Extending your PATH with: \nexport PATH=/usr/local/share/npm/bin:$PATH\n\n\nisn't a terrible idea. Having said that, you shouldn't have to do it.\nRun this:\nnpm config get prefix\n\nThe default on OS X is /usr/local, which means that npm will symlink binaries into /usr/local/bin, which should already be on your PATH (especially if you're using Homebrew).\nSo:\n\nnpm config set prefix /usr/local if it's something else, and\nDon't use sudo with npm! According to the jslint docs, you should just be able to npm install it.\n\nIf you installed npm as sudo (sudo brew install), try reinstalling it with plain ol' brew install. Homebrew is supposed to help keep you sudo-free.",
    "tag": "zsh"
  },
  {
    "question": "Zsh: Conda/Pip installs command not found",
    "answer": "I found an easy way. Just follow below steps:\n\nin terminal, enter vim ~/.zshrc\n\nadd source ~/.bash_profile into .zshrc file\n\nand then in terminal, enter source ~/.zshrc\n\n\nCongratulation for you!!! ㊗️ 🎉🎉🎉",
    "tag": "zsh"
  },
  {
    "question": "Exit zsh, but leave running jobs open?",
    "answer": "Start the program with &!:\ndolphin &!\n\nThe &! (or equivalently, &|) is a zsh-specific shortcut to both background and disown the process, such that exiting the shell will leave it running.",
    "tag": "zsh"
  },
  {
    "question": "Oh My Zsh - Disable 'Would you like to check for updates' prompt",
    "answer": "Set environment variable DISABLE_UPDATE_PROMPT=true to always reply Yes and automatically upgrade.\nSet environment variable DISABLE_AUTO_UPDATE=true to always reply No and never upgrade.\nSimply add one of these in your ~/.zshrc somewhere before calling source $ZSH/oh-my-zsh.sh.",
    "tag": "zsh"
  },
  {
    "question": "How to make zsh run as a login shell on Mac OS X (in iTerm)?",
    "answer": "chsh -s $(which zsh)\n\nYou'll be prompted for your password, but once you update your settings any new iTerm/Terminal sessions you start on that machine will default to zsh.",
    "tag": "zsh"
  },
  {
    "question": "macOS Catalina 10.15(beta) - Why is ~/.bash_profile not sourced by my shell?",
    "answer": "Apple has changed the default shell to zsh. Therefore you have to rename your configuration files. .bashrc is now .zshrc and .bash_profile is now .zprofile.",
    "tag": "zsh"
  },
  {
    "question": "What does \"export\" do in shell programming?",
    "answer": "Exported variables such as $HOME and $PATH are available to (inherited by) other programs run by the shell that exports them (and the programs run by those other programs, and so on) as environment variables.  Regular (non-exported) variables are not available to other programs.\n$ env | grep '^variable='\n$                                 # No environment variable called variable\n$ variable=Hello                  # Create local (non-exported) variable with value\n$ env | grep '^variable='\n$                                 # Still no environment variable called variable\n$ export variable                 # Mark variable for export to child processes\n$ env | grep '^variable='\nvariable=Hello\n$\n$ export other_variable=Goodbye   # create and initialize exported variable\n$ env | grep '^other_variable='\nother_variable=Goodbye\n$\n\nFor more information, see the entry for the export builtin in the GNU Bash manual, and also the sections on command execution environment and environment.\nNote that non-exported variables will be available to subshells run via ( ... ) and similar notations because those subshells are direct clones of the main shell:\n$ othervar=present\n$ (echo $othervar; echo $variable; variable=elephant; echo $variable)\npresent\nHello\nelephant\n$ echo $variable\nHello\n$\n\nThe subshell can change its own copy of any variable, exported or not, and may affect the values seen by the processes it runs, but the subshell's changes cannot affect the variable in the parent shell, of course.\nSome information about subshells can be found under command grouping and command execution environment in the Bash manual.",
    "tag": "zsh"
  },
  {
    "question": "After upgrade MacOS terminal showing: zsh: command not found: flutter",
    "answer": "You need to update the environment path.\n\nOpen terminal.\n\nvim $HOME/.zshrc\n\nPress \"I\" key for going to insert mode.\n\nadd the following line in the opened file:\nexport PATH=\"$PATH:/YOUR_FLUTTER_DIR/flutter/bin\"\n\nPress \"Esc\" then write :wq! in terminal and press enter to exit vim.\n\nReopen the terminal and check \"flutter doctor\"\n\n\nIf this solution did not work, remove the double quote from path or use the full path explicitly instead of ~.",
    "tag": "zsh"
  },
  {
    "question": "What does autoload do in zsh?",
    "answer": "autoload tells zsh to look for a file in $FPATH/$fpath containing a function definition, instead of a file in $PATH/$path containing an executable script or binary.\nScript\nA script is just a sequence of commands that get executed when the script is run. For example, suppose you have a file called hello like this:\necho \"Setting 'greeting'\"\ngreeting='Hello'\n\nIf the file is executable and located in one of the directories in your $PATH, then you can run it as a script by just typing its name.  But scripts get their own copy of the shell process, so anything they do can't affect the calling shell environment. The assignment to greeting above will be in effect only within the script; once the script exits, it won't have had any impact on your interactive shell session:\n$ hello\nSetting 'greeting'\n$ echo $greeting\n\n$ \n\nFunction\nA function is instead defined once and stays in the shell's memory; when you call it, it executes inside the current shell, and can therefore have side effects:\nhello() {\n  echo \"Setting 'greeting'\"\n  greeting='Hello'\n}\n\n$ hello\nSetting 'greeting'\n$ echo $greeting\nHello\n\nSo you use functions when you want to modify your shell environment. The Zsh Line Editor (ZLE) also uses functions - when you bind a key to some action, that action is defined as a shell function (which has to be added to ZLE with the zle -N command).\nIf you have a lot of functions, then you might not want to define all of them in your .zshrc every time you start a new shell; that slows down shell startup and uses memory to store functions that you might not wind up calling during the lifetime of that shell. So you can instead put the function definitions into their own files, named after the functions they define, put those files into a directory ( I use $HOME/lib/zsh/functions myself), and include the directory in the environment variable $FPATH – which, like $PATH, is a colon-separated list of directories. (Also like PATH, it's tied to a lowercase equivalent $fpath, which presents the same list of directories as an array instead of a single string.)\nZsh comes with a number of standard functions in its install tree, in a set of directories already included in the default value of $FPATH. But FPATH and the files in its directories are not part of zsh's normal command lookup; they're ignored unless you have first specified that a certain command is the name of a function whose definition can be found by way of FPATH.\nThat's what autoload does: it says \"Hey, Zsh, this command name here is a function, so when I try to run it, go look for its definition in my FPATH, instead of looking for an executable in my PATH.\"\nThe first time you run a command which Zsh determines is an autoloaded function, the shell sources the definition file. Then, if there's nothing in the file except the function definition, or if the shell option KSH_AUTOLOAD is set, it proceeds to call the function immediately with the arguments you supplied. But if that option is not set and the file contains any code outside the function definition (such as initialization of variables used by the function), the function is not called automatically. In that case it's up to you to call the function inside the file after defining it, so that first invocation will still happen.\nI have my .zshrc autoload all the files in my personal functions directory (not the whole fpath); that takes up much less memory and time than actually loading all of their definitions on startup, while still keeping them available at my prompt without any additional setup. Here is the relevant bit of my zshrc; I put the code into an anonymous function just to avoid adding an extraneous variable to my shell:\n() {\n    # add our local functions dir to the fpath\n    local funcs=$HOME/lib/zsh/functions\n\n    # FPATH is already tied to fpath, but this adds\n    # a uniqueness constraint to prevent duplicate entries\n    typeset -TUg +x FPATH=$funcs:$FPATH fpath\n    \n    # Now autoload them\n    if [[ -d $funcs ]]; then\n        autoload ${=$(cd \"$funcs\" && echo *)}\n    fi\n}\n\nThat last autoload line is not super robust; it assumes that none of my function file names has funny characters, and that there aren't too many of them to fit on a single command line. But under those constraints it works fine, and is a bit more efficient than a loop.",
    "tag": "zsh"
  },
  {
    "question": "How can you export your .bashrc to .zshrc?",
    "answer": "While lhunath's answer pushed me in the right direction, zsh does not seem to source .profile automatically. Lot's of good info on this topic can be found on this superuser post.\nThe adaption I'm using is putting common aliases and functions in .profile and manually sourcing them as follows:\nIn ~/.bashrc:\nsource ~/.profile\n\nIn ~/.zshrc:\n[[ -e ~/.profile ]] && emulate sh -c 'source ~/.profile'\n\nemulate is a zsh builtin command. With single argument set up zsh options to emulate the specified shell as much as possible.",
    "tag": "zsh"
  },
  {
    "question": "How do I reload ZSH config files without replacing the current shell?",
    "answer": "Usually a source ~/.zshrc should do it.",
    "tag": "zsh"
  },
  {
    "question": "Making ZSH default Shell in MacOSX",
    "answer": "The correct answer should've addressed your problem:\n\nchsh: /usr/bin/zsh: non-standard shell\n\nThe reason this is the case is because chsh will only accept shells that are defined in the file /etc/shells, as you can see by reading the manual for chsh:\n\nchsh  will  accept  the  full  pathname  of  any executable file on\n  the system.  However, it will issue a warning if the shell is not\n  listed in the\n         /etc/shells file.\n\nTo solve this problem and make zsh the default shell, you should thus:\n$ sudo echo \"$(which zsh)\" >> /etc/shells\n$ chsh -s $(which zsh)\n\nObviously, I assume that zsh is in your path here. This solution will also work if you, for example, choose to install the latest zsh with brew install zsh.\nEDIT (thanks for ThisIsFlorianK for the comment):\nDepending on your shell setup you may get a message saying /etc/shells: Permission denied. You can find information about this issue here. \nTo work around it, use the following instead:\n$ sudo sh -c \"echo $(which zsh) >> /etc/shells\"\n$ chsh -s $(which zsh)",
    "tag": "zsh"
  },
  {
    "question": "Rails keeps telling me that it's not currently installed",
    "answer": "If you're running a rails command immediately after installing rails, you will need to restart your terminal before your commands will be recognized.",
    "tag": "zsh"
  },
  {
    "question": "How to remove an entry from the history in ZSH",
    "answer": "*BSD/Darwin (macOS):\nLC_ALL=C sed -i '' '/porn/d' $HISTFILE\n\nLinux (GNU sed):\nLC_ALL=C sed -i '/porn/d' $HISTFILE\n\nThis will remove all lines matching \"porn\" from your $HISTFILE.\nWith setopt HIST_IGNORE_SPACE, you can prepend the above command with a space character to prevent it from being written to $HISTFILE.\nAs Tim pointed out in his comment below, setting LC_ALL=C prevents 'illegal byte sequence' failure.",
    "tag": "zsh"
  },
  {
    "question": "zsh history is too short",
    "answer": "NVaughan (the OP) has already stated the answer in an update to the question: history behaves differently in bash than it does in zsh:\nIn short:\n\nzsh:\n\n\nhistory lists only the 15 most recent history entries\nhistory 1 lists all - see below.\n\nbash: \n\n\nhistory lists all history entries.\n\n\n\nSadly, passing a numerical operand to history behaves differently, too:\n\nzsh: \n\n\nhistory <n> shows all entries starting with <n> - therefore, history 1 shows all entries.\n(history -<n> - note the - - shows the <n> most recent entries, so the default behavior is effectively history -15)\n\nbash:\n\n\nhistory <n> shows the <n> most recent entries.\n(bash's history doesn't support listing from an entry number; you can use fc -l <n>, but a specific entry <n> must exist, otherwise the command fails - see below.)\n\n\n\nOptional background info:\n\nIn zsh, history is effectively (not actually) an alias for fc -l: see man zshbuiltins\n\nFor the many history-related features, see man zshall\n\nIn bash, history is its own command whose syntax differs from fc -l\n\nSee: man bash\n\nBoth bash and zsh support fc -l <fromNum> [<toNum>] to list a given range of history entries:\n\n\nbash: specific entry <fromNum> must exist.\nzsh: command succeeds as long as least 1 entry falls in the (explicit or implied) range.\nThus, fc -l 1 works in zsh to return all history entries, whereas in bash it generally won't, given that entry #1 typically no longer exists (but, as stated, you can use history without arguments to list all entries in bash).",
    "tag": "zsh"
  },
  {
    "question": "ZSH: Hide computer name in terminal",
    "answer": "Try to add export DEFAULT_USER=$USER to your .zshrc file",
    "tag": "zsh"
  },
  {
    "question": "Can a Bash tab-completion script be used in zsh?",
    "answer": "autoload bashcompinit\nbashcompinit\nsource /path/to/your/bash_completion_file",
    "tag": "zsh"
  },
  {
    "question": "Make iTerm2 launch with Zsh",
    "answer": "Change your default shell to /bin/zsh by running the chsh -s /bin/zsh command.",
    "tag": "zsh"
  },
  {
    "question": "How to automatically activate virtualenvs when cd'ing into a directory",
    "answer": "Add following in your .bashrc or .zshrc\nfunction cd() {\n  builtin cd \"$@\"\n\n  if [[ -z \"$VIRTUAL_ENV\" ]] ; then\n    ## If env folder is found then activate the vitualenv\n      if [[ -d ./.env ]] ; then\n        source ./.env/bin/activate\n      fi\n  else\n    ## check the current folder belong to earlier VIRTUAL_ENV folder\n    # if yes then do nothing\n    # else deactivate\n      parentdir=\"$(dirname \"$VIRTUAL_ENV\")\"\n      if [[ \"$PWD\"/ != \"$parentdir\"/* ]] ; then\n        deactivate\n      fi\n  fi\n}\n\nThis code will not deactivate the virtualenv even if someone goes into subfolder. Inspired by answers of @agnul and @Gilles.\nIf the virtualenv is made by pipenv, then please consider this wiki page.\nFurthermore, for added security please consider direnv.",
    "tag": "zsh"
  },
  {
    "question": "Which shell I am using in mac",
    "answer": "To see what shell is currently running - which may or may not be your default shell - use:\n# Prints something like '/bin/ksh' or '-zsh'\n# See bottom section if you always need the full path.\nps -o comm= $$\n\nThe above assumes that the running shell is a POSIX-compatible shell. If the running shell is PowerShell, replace $$ with $PID, which will tell you the full path even if PowerShell is also the default shell. If you use (Get-Process -Id $PID).Path instead, you'll get the full path with symlinks resolved, if any.\nTo see what shell is your default shell, run:\necho $SHELL\n\nIf the currently running shell is PowerShell: $env:SHELL\n\nIf you need to know the full path of the currently running shell:\nIf the current shell was launched directly by Terminal.app (or iTerm2), it is a login shell launched via the login utility, which causes the current shell process to self-report its binary abstractly as -<binary-filename>, e.g. -zsh; that is, you don't get the full path of the binary underlying the shell process.\nIf always obtaining the full path is required - e.g. if you want to distinguish the system Bash  /bin/bash from a later version installed via Homebrew - you can use the following command line:\n(bin=\"$(ps -o comm= $$)\"; expr \"$bin\" : '\\(-\\)' >/dev/null && bin=\"$(ps -o command= $PPID | grep -Eo ' SHELL=[^ ]+' | cut -f 2- -d =)\"; [ -n \"$bin\" ] && echo \"$bin\" || echo \"$SHELL\")",
    "tag": "zsh"
  },
  {
    "question": "How can I read documentation about built in zsh commands?",
    "answer": "The key information for getting a more useful help utility is actually included with Zsh, it's just a matter of finding the critical—and poorly discoverable—man page: man zshcontrib (here on the web), which describes the run-help widget:\n\nBy default, run-help is an alias for the man command, so this often fails when the command word is a shell builtin or a user-defined function. By redefining the run-help alias, one can improve the on-line help provided by the shell.\n\nIt further explains how to replace it with a built-in improvement.\n# Remove the default of run-help being aliased to man\nunalias run-help\n# Use zsh's run-help, which will display information for zsh builtins.\nautoload run-help\n\nAfter setting this up, calling run-help for names of builtins, completion functions and so forth will now try to show you extracted documentation, or show you the right containing man page, etc. For example run-help bindkey outputs:\nbindkey\n   See the section `Zle Builtins' in zshzle(1).\n\nwhich could be better. For a better example, run-help history shows the Zsh man page section for fc, which is the command that underlies history.\nAlso handy to note: ESC-h will call run-help for the command on the current input line.\nI presume this setup isn't the default because extracting the granular help data and setting HELPDIR to point to it might be a packaging decision left to OS distributions. There's also a user choice: the autoload run-help util is useful without setting HELPDIR at all. It seems to be good at taking you to the right man page even if it can't jump to the exact section for one item. Some may prefer this to running into cases like the bindkey example above which just wastes time. (Why they default to alias run-help=man then, I cannot fathom).\nFor Zsh version 5.0.3 or newer\nThe helpfiles extractions are likely included with the Zsh distribution. It's just a matter of finding them on your system to set HELPDIR if you wish—likely candidates are in /usr/share/zsh or /usr/local/share/zsh, look for a help subdirectory.\nFor versions of Zsh before 5.0.3\nYou will likely need to follow the procedure detailed in man zshcontrib yourself to generate the help files. It's a little annoying to need to do this, but otherwise quick and painless.\nFind your installed version with zsh --version and obtain the corresponding source tarball from the sourceforge archive. Then run the helpfiles script as shown in the man page and set the target as HELPDIR in your ~/.zshrc.",
    "tag": "zsh"
  },
  {
    "question": "Command not found go — on Mac after installing Go",
    "answer": "Like bjhaid mentioned in the comments above:\nThis is happening because you must add your PATH to your ~/.zshrc file.\nin the ~/.zshrc you should add the line:\nexport PATH=$PATH:/usr/local/go/bin\nexport PATH=$PATH:$GOPATH/bin\n\nyou should then source you .zshrc file:\n. ~/.zshrc",
    "tag": "zsh"
  },
  {
    "question": "How to load ~/.bash_profile when entering bash from within zsh?",
    "answer": "Open ~/.zshrc, and at the very bottom of the file, add the following:\nif [ -f ~/.bash_profile ]; then \n    . ~/.bash_profile;\nfi\n\nEvery time you open the terminal, it will load whatever is defined in ~/.bash_profile (if the file exist). With that, you can keep your custom settings for zsh (colors, and etc). And you get to keep your custom shell settings in .bash_profile file.\nThis is much cleaner than using bash -l IMO.\nIf you prefer putting your settings in .bashrc, or .bash_login, or .profile , you can do the same for them.\n\nSimilarly, you could also move the common profile settings to separate file, i.e. .my_common_profile, and add the following to both .bash_profile and .zshrc:\nif [ -f ~/.my_common_profile ]; then \n    . ~/.my_common_profile;\nfi",
    "tag": "zsh"
  },
  {
    "question": "RVM is not working in ZSH",
    "answer": "Do you have this line in your ~/.zshrc?\n[[ -s \"$HOME/.rvm/scripts/rvm\" ]] && . \"$HOME/.rvm/scripts/rvm\"",
    "tag": "zsh"
  },
  {
    "question": "Fix key settings (Home/End/Insert/Delete) in .zshrc when running Zsh in Terminator Terminal Emulator",
    "answer": "To know the code of a key, execute cat, press enter, press the key, then Ctrl+C.\nFor me, Home sends ^[[H and End ^[[F, so i can put i my .zshrc in my home dir\nbindkey  \"^[[H\"   beginning-of-line\nbindkey  \"^[[F\"   end-of-line\nbindkey  \"^[[3~\"  delete-char\n\nThese codes could change with the terminal emulator you use.\nautoload zkbd ; zkbd will create a file with an array of keycodes to use, like bindkey  \"${key[Home]}\" beginning-of-line, and you can source a different file depending on the terminal.",
    "tag": "zsh"
  },
  {
    "question": "How to show zsh function definition (like bash \"type myfunc\")?",
    "answer": "The zsh idiom is whence, the -f flag prints function definitions:\nzsh$ whence -f foo\nfoo () {\n    echo hello\n}\nzsh$\n\nIn zsh, type is defined as equivalent to whence -v, so you can continue to use type, but you'll need to use the -f argument:\nzsh$ type -f foo\nfoo () {\n    echo hello\n}\nzsh$\n\nAnd, finally, in zsh which is defined as equivalent to whence -c - print results in csh-like format, so which foo will yield the same results.\nman zshbuiltins for all of this.",
    "tag": "zsh"
  },
  {
    "question": "To get a prompt which indicates Git-branch in Zsh",
    "answer": "__git_ps1 is from git-completion.bash. In zsh you probably have to provide your own function to determine the current directories git branch. There are quite a few blog posts about a git prompt for zsh.\nYou just need:\n\na function to provide the branch name\nenable prompt (command) substitution\nadd the function to your prompt\n\nFor example\ngit_prompt() {\n ref=$(git symbolic-ref HEAD | cut -d'/' -f3)\n echo $ref\n}\nsetopt prompt_subst\nPS1=$(git_prompt)%#\nautoload -U promptinit\npromptinit\n\nUpdate: use the zsh vcs_info module instead of git_prompt()\nsetopt prompt_subst\nautoload -Uz vcs_info\nzstyle ':vcs_info:*' actionformats \\\n    '%F{5}(%f%s%F{5})%F{3}-%F{5}[%F{2}%b%F{3}|%F{1}%a%F{5}]%f '\nzstyle ':vcs_info:*' formats       \\\n    '%F{5}(%f%s%F{5})%F{3}-%F{5}[%F{2}%b%F{5}]%f '\nzstyle ':vcs_info:(sv[nk]|bzr):*' branchformat '%b%F{1}:%F{3}%r'\n\nzstyle ':vcs_info:*' enable git cvs svn\n\n# or use pre_cmd, see man zshcontrib\nvcs_info_wrapper() {\n  vcs_info\n  if [ -n \"$vcs_info_msg_0_\" ]; then\n    echo \"%{$fg[grey]%}${vcs_info_msg_0_}%{$reset_color%}$del\"\n  fi\n}\nRPROMPT=$'$(vcs_info_wrapper)'",
    "tag": "zsh"
  },
  {
    "question": "${BASH_SOURCE[0]} equivalent in zsh?",
    "answer": "${(%):-%x} is the closest zsh equivalent to bash's $BASH_SOURCE (and ksh's ${.sh.file}) - not $0.\nTip of the hat to Hui Zheng for providing the crucial pointer and background information in his answer.\nIt returns the (potentially relative) path of the enclosing script,\n\nregardless of whether the script is being sourced or not.\n\n\nspecifically, it also works inside initialization/profiles files such as ~/.zshrc (unlike $0, which inexplicably returns the shell's path there).\n\nregardless of whether called from inside a function defined in the script or not (unlike $0, which returns the function name inside a function).\n\n\nThe only difference to $BASH_SOURCE I've found is in the following obscure scenario - which may even be a bug (observed in zsh 5.0.5): inside a function nested inside another function in a sourced script, ${(%):-%x} does not return the enclosing script path when that nested function is called (again) later, after having been sourced (returns either nothing or 'zsh').\n\n\nBackground information on ${(%):-%x}:\n\n(%):- in lieu of a variable name in a parameter (variable) expansion (${...}) makes escape sequences available that are normally used to represent environmental information in prompt strings, such as used in the PS1 variable to determine the string displayed as the primary interactive prompt.\n\n% is an instance of a parameter expansion flag, all of which are listed in man zshexpn under the heading Parameter Expansion Flags.\n\n%x is one of the escape sequences that can be used in prompt strings, and it functions as described above; there are many more, such as %d to represent the current dir.\n\nman zshmisc lists all available sequences under the heading SIMPLE PROMPT ESCAPES.",
    "tag": "zsh"
  },
  {
    "question": "brew installation for zsh?",
    "answer": "This worked for me on macOS ARM (Apple M1):\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n\nand then:\nexport PATH=\"/opt/homebrew/bin:$PATH\" >> ~/.zshrc",
    "tag": "zsh"
  },
  {
    "question": "oh my zsh showing weird character '?' on terminal",
    "answer": "This has fixed the issue. Just select Use built-in Powerline glyphs and use Inconsolata for Powerline font.",
    "tag": "zsh"
  },
  {
    "question": "Delete specific line from Zsh history",
    "answer": "You are looking in wrong File. Look at ~/.zsh_history not ~/.zhistory To view in which file your history is saved:\necho $HISTFILE\n\nAnd delete:\nrm $HISTFILE",
    "tag": "zsh"
  },
  {
    "question": "OSX 10.10 yosemite beta on git pull: git-sh-setup: No such file or directory",
    "answer": "I think the cleanest solution for this for now is to change the initial command in your iTerm session to be\n/usr/bin/login -f <your user name>\n\nThis fixes the issue for me.\nA further data point for analysis of the issue: It seems that in 10.10, multiple copies of the PATH environment variable exist and subshells seem to prefer the second copy. \nYou can reproduce this by launching any cocoa application on the console as launched by iTerm. You'll get a warning that looks like this:\n2014-06-04 19:23:09.859 gitx[14676:362580] *** -[NSProcessInfo environment]: Warning: duplicate definition for key 'PATH' found in environment -- subsequent definitions are ignored.  The first definition was '(the path I have configured in my shell)', the ignored definition is '/usr/bin:/bin:/usr/sbin:/sbin'.\n\nI believe this to be a problem in 10.10 and not iTerm, but something iTerm is doing is causing it to manifest itself (this doesn't happen in Terminal.app)\nUpdate: This is caused by iTerm doing \"interesting\" stuff to the environment. Update to the official release of iTerm 2.0 to make this problem go away.",
    "tag": "zsh"
  },
  {
    "question": "Which shortcut in Zsh does the same as Ctrl-U in Bash?",
    "answer": "It sounds like you'd like for Ctrl+U to be bound to backward-kill-line rather than kill-whole-line, so add this to your .zshrc:\nbindkey \\^U backward-kill-line\n\nThe bindkey builtin and the available editing commands (“widgets”) are documented in the zshzle man page.",
    "tag": "zsh"
  },
  {
    "question": "How to get shell to self-detect using zsh or bash",
    "answer": "If the shell is Zsh, the variable $ZSH_VERSION is defined.  Likewise for Bash and $BASH_VERSION.\nif [ -n \"$ZSH_VERSION\" ]; then\n   # assume Zsh\nelif [ -n \"$BASH_VERSION\" ]; then\n   # assume Bash\nelse\n   # assume something else\nfi\n\nHowever, these variables only tell you which shell is being used to run the above code.  So you would have to source this fragment in the user's shell.\nAs an alternative, you could use the $SHELL environment variable (which should contain absolute path to the user's preferred shell) and guess the shell from the value of that variable:\ncase $SHELL in\n*/zsh) \n   # assume Zsh\n   ;;\n*/bash)\n   # assume Bash\n   ;;\n*)\n   # assume something else\nesac\n\nOf course the above will fail when /bin/sh is a symlink to /bin/bash.\nIf you want to rely on $SHELL, it is safer to actually execute some code:\nif [ -n \"$($SHELL -c 'echo $ZSH_VERSION')\" ]; then\n   # assume Zsh\nelif [ -n \"$($SHELL -c 'echo $BASH_VERSION')\" ]; then\n   # assume Bash\nelse\n   # assume something else\nfi\n\nThis last suggestion can be run from a script regardless of which shell is used to run the script.",
    "tag": "zsh"
  },
  {
    "question": "how can I run the ssh-agent auto in the zsh environment?",
    "answer": "open .zshrc in a text editor:\nvim ~/.zshrc\n\nAdd ssh-agent to the plugins list and save:\nplugins=(git ssh-agent)\n\nYou may want to immediately reload your .zshrc settings:\nsource ~/.zshrc",
    "tag": "zsh"
  },
  {
    "question": "zsh: stop backward-kill-word on directory delimiter",
    "answer": "For recent versions of zsh, you can simply add:\nautoload -U select-word-style\nselect-word-style bash\n\nto your zshrc as described in the zsh manual (also man zshcontrib).",
    "tag": "zsh"
  },
  {
    "question": "Getting a weird percent sign in printf output in terminal with C",
    "answer": "When (non-null) output from a program doesn't include a trailing newline, zsh adds that color-inverted % to indicate that and moves to the next line before printing the prompt; it's generally more convenient than bash's behavior, just starting the command prompt where the output ended.",
    "tag": "zsh"
  },
  {
    "question": "How can I format the output of a bash command in neat columns",
    "answer": "column(1) is your friend.\n$ column -t <<< '\"option-y\"      yank-pop\n> \"option-z\"      execute-last-named-cmd\n> \"option-|\"      vi-goto-column\n> \"option-~\"      _bash_complete-word\n> \"option-control-?\"      backward-kill-word\n> \"control-_\"     undo\n> \"control-?\"     backward-delete-char\n> '\n\"option-y\"          yank-pop\n\"option-z\"          execute-last-named-cmd\n\"option-|\"          vi-goto-column\n\"option-~\"          _bash_complete-word\n\"option-control-?\"  backward-kill-word\n\"control-_\"         undo\n\"control-?\"         backward-delete-char",
    "tag": "zsh"
  },
  {
    "question": "How to repeat the last part of a previous command?",
    "answer": "Wether you are in bash or zsh, you can use the ! operator to recover arguments of your previous command:\nIf we take: echo a b c d as an example\n\n!$ - the last argument: d\n!:*- all the arguments: a b c d (can be shorten !*)\n!:1 - the first argument: a (same as !^)\n!:1-3 - arguments from first to third: a b c\n!:2-$ - arguments from the second to the last one: b c d\n\nThis last point answer you question, you can take the last part of your command.\nNote: !:0 is the last command executed, here it would be echo in our example\nThe corresponding documentation can be found on the gnu website, the whole context gives much more resources than this comment.",
    "tag": "zsh"
  },
  {
    "question": "Remove function definition (unalias equivalent)",
    "answer": "unset -f my_function\n\nwill remove (or unset) the function my_function",
    "tag": "zsh"
  },
  {
    "question": "How do I change my $PS1 on a Macbook for oh-my-zsh?",
    "answer": "Changing your Theme:\nTo edit your prompt in oh-my-zsh you need to edit a PROMPT variable in your theme instead of PS1. In your .zshrc file you will find a line that looks something like this:\nZSH_THEME=\"themename\"\n\noh-my-zsh stores these themes in the ~/.oh-my-zsh/themes folder. If you ls ~/.oh-my-zsh/themes you will see a list of themes that you can change. The above theme would be named  themename.zsh-theme in this directory. \nCustomizing your Theme:\nIf you want a simple way to customize your oh-my-zsh theme you can copy a file already in this theme folder and edit that. \nTo change your prompt simply edit the PROMPT variable. For example:\nPROMPT=\">>\"\n\nThis would make two >'s your prompt.\nI like editing the already existing simple theme. the simple.zsh-theme file looks like this:\nPROMPT='%{$fg[green]%}%~%{$fg_bold[blue]%}$(git_prompt_info)%{$reset_color%} '\n\nZSH_THEME_GIT_PROMPT_PREFIX=\"(\"\nZSH_THEME_GIT_PROMPT_SUFFIX=\")\"\nZSH_THEME_GIT_PROMPT_DIRTY=\" ✗\"\nZSH_THEME_GIT_PROMPT_CLEAN=\" ✔\"\n\nApplying the Changes:\nNow just change the theme in your in your .zshrc file:\nZSH_THEME=\"simple\"\n\nAnd reload oh-my-zsh with:\n. ~/.zshrc",
    "tag": "zsh"
  },
  {
    "question": "Zsh wants to autocorrect a command, with an _ before it",
    "answer": "This is command autocorrection, activated by the correct option. It has nothing to do with completion. You're seeing _ruby because zsh thinks there is no ruby command and it offers _ruby as the nearest existing match.\nIf you've just installed ruby, it's possible that zsh has memorized the list of available command earlier, and it won't always try to see if the command has appeared in between. In that case, run hash -rf. Future zsh sessions won't have this problem since the ruby command already existed when they started.\nSometimes, when you change your PATH, zsh forgets some hashed commands. The option hash_listall helps against this. As above, if you can force zsh to refresh its command cache with hash -rf.",
    "tag": "zsh"
  },
  {
    "question": "How to run a vim command from the shell command-line?",
    "answer": "Note, now the syntax has changed, and the line should read (As per @sheharyar):\nvim +PluginInstall +qall\n\nFor posterity, previously, the correct line was:\nvim +BundleInstall +qall\n\nShould anyone other than me be looking! Note: this is in the Github README for vundle.",
    "tag": "zsh"
  },
  {
    "question": "ZSH not recognizing my aliases?",
    "answer": "if you do a very simple alias in zsh, does it work?  open your .zshrc file, and add the following line:\nalias ls='ls -GpF'\n\nafter adding that line, type this line in your Terminal:\nsource ~/.zshrc\n\ntell us what happens.  Also, just for shiggles, make sure you are using single quotes vs. double quotes, I have seen that make a difference in the past on different versions of shells/OS/whatnot.",
    "tag": "zsh"
  },
  {
    "question": "virtualenv name not show in zsh prompt",
    "answer": "Do this in ~/.zshrc:\nplugins=(virtualenv)\n\nPOWERLEVEL9K_RIGHT_PROMPT_ELEMENTS=(status virtualenv)\n\nCaveats:\n1 -- add that plugin in addition to other plugins you have.\n2 -- I'm using the POWERLEVEL9K theme. Maybe you theme",
    "tag": "zsh"
  },
  {
    "question": "List of zsh bindkey commands",
    "answer": "bindkey -l will give you a list of existing keymap names.\n\nbindkey -M <keymap> will list all the bindings in a given keymap.\n\nIf you use the zsh command line in emacs mode, then the emacs keymap is likely to be most important for you.\n\nIf you use it in vi mode, then you’d be interested in viins and vicmd.\n\n\n(See the zshzle(1) man page for more details.)\nOnce you have a list of keybindings, you can search the official ZLE documentation for the name of the action (or “widget” in zsh parlance).\nEdit: Elsewhere in this thread, Dave Lee answers the original question more accurately than I do here!\nEdit: An ever-more-upvoted comment by Keith Hughitt states that just executing bindkey with no arguments prints \"all keybindings\". Just in case you've missed my reply to that, let me state it here too: What Keith says is true, but only for the current keymap. This may or may not be what you want!",
    "tag": "zsh"
  },
  {
    "question": "Zsh detects insecure completion-dependent directories",
    "answer": "This is an issue with ZSH, your shell, not Hyper, your terminal. I actually had the same issue earlier today. There are some solutions in this issue on Github, and I will quote some of them here but I recommend you follow the link and read the comments there.\nThe first solution is to change the ownership of the problematic directories.\nI will not recommend this without knowing more about your environment, but for most people this will fix the issue:\nchmod 755 /usr/local/share/zsh\nchmod 755 /usr/local/share/zsh/site-functions\n\nThe second solution is to set ZSH_DISABLE_COMPFIX=true (or \"true\" in quotes) in your .zshrc file, to tell ZSH to not check for insecure directories.\nThe third solution, and the solution that fixed the issue for me, is to initialise compinit with the -u flag. This will use all the directories found by compaudit without checking them for security issues. To do this, you will have to change your .zshrc file or wherever you are configuring autocomplete.",
    "tag": "zsh"
  },
  {
    "question": "Anaconda not found in ZSh?",
    "answer": "Altough I cannot test it on a Mac, (I have a Linux Zsh installed) this should work for you as well: Just execute\n/(your conda installation path)/bin/conda init zsh\n\nand restart your zsh shell. The init command will change your ~/.zshrc file accordingly, setting your PATH correctly and slightly change the PS1 (which is was most answers here do manually...).",
    "tag": "zsh"
  },
  {
    "question": "zsh theme for full path + display git changes",
    "answer": "Creating a copy of original theme file is the recommended way of tweaking original theme files. Oh-my-zsh docs\nCheck your existing theme:\n$ echo $ZSH_THEME\nrobbyrussell\n\nCreate a copy of original theme file in your $ZSH_CUSTOM/themes directory:\ncp $ZSH/themes/robbyrussell.zsh-theme $ZSH_CUSTOM/themes/ \n\nEdit your custom copy of theme file\nvim $ZSH_CUSTOM/themes/robbyrussell.zsh-theme\n\nWhich looks like this:\nPROMPT+=' %{$fg[cyan]%}%c%{$reset_color%} $(git_prompt_info)'\n#                       ^ replace c with ~\n\nThen source theme again:\nexec zsh\n\nIt will now show the path relative to your home directory (~). For example:\n# BEFORE\n➜  sqlboiler git:(master)\n# AFTER\n➜  ~/open-source/sqlboiler git:(master)",
    "tag": "zsh"
  },
  {
    "question": "Have zsh return case-insensitive auto-complete matches, but prefer exact matches",
    "answer": "Create a file ~/.oh-my-zsh/custom/better-completion.zsh (assuming you are using default paths for oh-my-zsh) with the following line\nzstyle ':completion:*' matcher-list '' 'm:{a-zA-Z}={A-Za-z}' 'r:|[._-]=* r:|=*' 'l:|=* r:|=*'\n\nExplanation:\nRules for matches in zsh completion in general are defined in the matcher-list style. For oh-my-zsh this is defined in ~/.oh-my-zsh/lib/completion.zsh (once for case-sensitive and once for case-insensitive). You could change it there but it would probably be gone if you updated your oh-my-zsh. ~/.oh-my-zsh/custom is specifially intended for customization and files with extension .zsh  are loaded from there by .oh-my-zsh/oh-my-zsh.sh at the end of the configuration.\nThe default (case-insensitive) settings for matcher-list in oh-my-zsh are:\nzstyle ':completion:*' matcher-list 'm:{a-zA-Z}={A-Za-z}' 'r:|[._-]=* r:|=*' 'l:|=* r:|=*'\n\nThe first of which tells to handle upper and lower case interchangeable.\nAs it is the first rule, it will be invariably used for every match.\nThe only change needed is to prepend '' for simple completion (it is even the first example in zshcompsys(1) for matcher-list)\nzstyle ':completion:*' matcher-list '' 'm:{a-zA-Z}={A-Za-z}' 'r:|[._-]=* r:|=*' 'l:|=* r:|=*'\n\nThis tries first to complete the current word exactly as its written, before trying case-insensitive or other matches.\nTo be complete:\n\nThe second (original) rule allows for partial completion before ., _ or -, e.g. f.b -> foo.bar.\nThe third rule allows for completing on the left side of the written text, e.g. bar -> foobar)",
    "tag": "zsh"
  },
  {
    "question": "How to change zsh-autosuggestions color",
    "answer": "You can edit your ~/.zshrc and change/add the variable: ZSH_AUTOSUGGEST_HIGHLIGHT_STYLE='fg=value'\nI have just tested the value from fg=8 to fg=5. I think fg stands for Foreground.\nZSH_AUTOSUGGEST_HIGHLIGHT_STYLE='fg=5'\n**OBS: Add the above line at the end of your zshrc (after loading the plugin) **\nI have found another reference here.",
    "tag": "zsh"
  },
  {
    "question": "Command not found - Oh-My-Zsh",
    "answer": "Question:\n➜  ~ mvn \nzsh: command not found: mvn\nAnswer:\nstep 1:\nvim ~/.zshrc\n\nstep 2:(Add at the end of the file)\nsource ~/.bash_profile;\n\nstep 3:(Execution shell)\n> source ~/.bash_profile\n\nYou can use mvn :\n➜  / mvn\n\n[INFO] Scanning for projects...\n.......",
    "tag": "zsh"
  },
  {
    "question": "How do I list all cron jobs for all users?",
    "answer": "You would have to run this as root, but:\nfor user in $(cut -f1 -d: /etc/passwd); do crontab -u $user -l; done\n\nwill loop over each user name listing out their crontab.  The crontabs are owned by the respective users so you won't be able to see another user's crontab with out being them or root.\n\nEdit\nif you want to know which user a crontab belongs to, use echo $user\nfor user in $(cut -f1 -d: /etc/passwd); do echo $user; crontab -u $user -l; done",
    "tag": "crontab"
  },
  {
    "question": "Restarting cron after changing crontab file?",
    "answer": "No.\nFrom the cron man page:\n\n...cron will then examine the modification time on all crontabs \n    and reload those which have changed.  Thus cron need not be restarted \n    whenever a crontab file is modified\n\nBut if you just want to make sure its done anyway,\nsudo service cron reload\n\nor\n/etc/init.d/cron reload",
    "tag": "crontab"
  },
  {
    "question": "How to run a cron job inside a docker container?",
    "answer": "You can copy your crontab into an image, in order for the container launched from said image to run the job.\n\nImportant: as noted in docker-cron issue 3: use LF, not CRLF for your cron file.\n\nSee \"Run a cron job with Docker\" from Julien Boulay in his Ekito/docker-cron:\n\nLet’s create a new file called \"hello-cron\" to describe our job.\n\n# must be ended with a new line \"LF\" (Unix) and not \"CRLF\" (Windows)\n* * * * * echo \"Hello world\" >> /var/log/cron.log 2>&1\n# An empty line is required at the end of this file for a valid cron file.\n\nIf you are wondering what is 2>&1, Ayman Hourieh explains.\n\nThe following Dockerfile describes all the steps to build your image\n\nFROM ubuntu:latest\nMAINTAINER docker@ekito.fr\n\nRUN apt-get update && apt-get -y install cron\n\n# Copy hello-cron file to the cron.d directory\nCOPY hello-cron /etc/cron.d/hello-cron\n \n# Give execution rights on the cron job\nRUN chmod 0644 /etc/cron.d/hello-cron\n\n# Apply cron job\nRUN crontab /etc/cron.d/hello-cron\n \n# Create the log file to be able to run tail\nRUN touch /var/log/cron.log\n \n# Run the command on container startup\nCMD cron && tail -f /var/log/cron.log\n\nBut: if cron dies, the container keeps running.\n(see Gaafar's comment and How do I make apt-get install less noisy?:\napt-get -y install -qq --force-yes cron can work too)\nAs noted by Nathan Lloyd in the comments:\n\nQuick note about a gotcha:\nIf you're adding a script file and telling cron to run it, remember to\nRUN chmod 0744 /the_script\nCron fails silently if you forget.\n\nWarning: as noted in the comments by user8046323\n\nThis config schedules tasks two times.\n\nOne time with crontab and\none time with cron.d\n\nPlease use only one of the ways to evade scheduling your tasks twice\n\nTrue: the problem is with those two lines in the above Dockerfile:\nCOPY hello-cron /etc/cron.d/hello-cron\nRUN crontab /etc/cron.d/hello-cron\n\n\nBy placing the hello-cron file in the /etc/cron.d directory, you automatically schedule the cron jobs contained in this file. The cron daemon checks this directory for any files containing cron schedules and automatically loads them.\n\nThe crontab command with /etc/cron.d/hello-cron takes the contents of the hello-cron file and loads them into the main crontab. This means the same jobs are now scheduled directly in the crontab as well, effectively duplicating them.\n\n\nyou should choose one method to manage your cron jobs, depending on your specific needs:\n\nIf you prefer using /etc/cron.d (often easier for managing multiple separate cron job files):\nCOPY hello-cron /etc/cron.d/hello-cron\nRUN chmod 0644 /etc/cron.d/hello-cron\n\n\nIf you prefer using crontab (gives you a consolidated view of all cron jobs and can be easier for a single or a few jobs):\nADD hello-cron /etc/cronjob\nRUN crontab /etc/cronjob\n\n\n\n\nOR, make sure your job itself redirect directly to stdout/stderr instead of a log file, as described in hugoShaka's answer:\n * * * * * root echo hello > /proc/1/fd/1 2>/proc/1/fd/2\n\nReplace the last Dockerfile line with\nCMD [\"cron\", \"-f\"]\n\nBut: it doesn't work if you want to run tasks as a non-root.\nSee also (about cron -f, which is to say cron \"foreground\") \"docker ubuntu cron -f is not working\"\n\nBuild and run it:\nsudo docker build --rm -t ekito/cron-example .\nsudo docker run -t -i ekito/cron-example\n\n\nBe patient, wait for 2 minutes and your command-line should display:\nHello world\nHello world\n\n\n\nEric adds in the comments:\n\nDo note that tail may not display the correct file if it is created during image build.\nIf that is the case, you need to create or touch the file during container runtime in order for tail to pick up the correct file.\n\nSee \"Output of tail -f at the end of a docker CMD is not showing\".\n\nSee more in \"Running Cron in Docker\" (Apr. 2021) from Jason Kulatunga, as he commented below\nSee Jason's image AnalogJ/docker-cron based on:\n\nDockerfile installing cronie/crond, depending on distribution.\n\nan entrypoint initializing /etc/environment and then calling\ncron -f -l 2",
    "tag": "crontab"
  },
  {
    "question": "How to create a cron job using Bash automatically without the interactive editor?",
    "answer": "You can add to the crontab as follows:\n#write out current crontab\ncrontab -l > mycron\n#echo new cron into cron file\necho \"00 09 * * 1-5 echo hello\" >> mycron\n#install new cron file\ncrontab mycron\nrm mycron\n\n\nCron line explaination\n* * * * * \"command to be executed\"\n- - - - -\n| | | | |\n| | | | ----- Day of week (0 - 7) (Sunday=0 or 7)\n| | | ------- Month (1 - 12)\n| | --------- Day of month (1 - 31)\n| ----------- Hour (0 - 23)\n------------- Minute (0 - 59)\n\nSource nixCraft.",
    "tag": "crontab"
  },
  {
    "question": "How to pass in password to pg_dump?",
    "answer": "Create a .pgpass file in the home directory of the account that pg_dump will run as.\nThe format is:\nhostname:port:database:username:password\n\nThen, set the file's mode to 0600. Otherwise, it will be ignored.\nchmod 600 ~/.pgpass\n\nSee the Postgresql documentation libpq-pgpass for more details.",
    "tag": "crontab"
  },
  {
    "question": "How do I get a Cron like scheduler in Python?",
    "answer": "If you're looking for something lightweight checkout schedule:\nimport schedule\nimport time\n\ndef job():\n    print(\"I'm working...\")\n\nschedule.every(10).minutes.do(job)\nschedule.every().hour.do(job)\nschedule.every().day.at(\"10:30\").do(job)\n\nwhile 1:\n    schedule.run_pending()\n    time.sleep(1)\n\nDisclosure: I'm the author of that library.",
    "tag": "crontab"
  },
  {
    "question": "Running a cron every 30 seconds",
    "answer": "You have */30 in the minutes specifier - that means every minute but with a step of 30 (in other words, every half hour). Since cron does not go down to sub-minute resolutions, you will need to find another way.\nOne possibility, though it's a bit of a kludge(a), is to have two jobs, one offset by 30 seconds:\n# Need these to run on 30-sec boundaries, keep commands in sync.\n* * * * *              /path/to/executable param1 param2\n* * * * * ( sleep 30 ; /path/to/executable param1 param2 )\n\nYou'll see I've added comments and formatted to ensure it's easy to keep them synchronised.\nBoth cron jobs actually run every minute but the latter one will wait half a minute before executing the \"meat\" of the job, /path/to/executable.\nFor other (non-cron-based) options, see the other answers here, particularly the ones mentioning fcron and systemd. These are probably preferable assuming your system has the ability to use them (such as installing fcron or having a distro with systemd in it).\n\nIf you don't want to use the kludgy solution, you can use a loop-based solution with a small modification. You'll still have to manage keeping your process running in some form but, once that's sorted, the following script should work:\n#!/bin/env bash\n\n# Debug code to start on minute boundary and to\n# gradually increase maximum payload duration to\n# see what happens when the payload exceeds 30 seconds.\n\n((maxtime = 20))\nwhile [[ \"$(date +%S)\" != \"00\" ]]; do true; done\n\nwhile true; do\n    # Start a background timer BEFORE the payload runs.\n\n    sleep 30 &\n\n    # Execute the payload, some random duration up to the limit.\n    # Extra blank line if excess payload.\n\n    ((delay = RANDOM % maxtime + 1))\n    ((maxtime += 1))\n    echo \"$(date) Sleeping for ${delay} seconds (max ${maxtime}).\"\n    [[ ${delay} -gt 30 ]] && echo\n    sleep ${delay}\n\n    # Wait for timer to finish before next cycle.\n\n    wait\ndone\n\nThe trick is to use a sleep 30 but to start it in the background before your payload runs. Then, after the payload is finished, just wait for the background sleep to finish.\nIf the payload takes n seconds (where n <= 30), the wait after the payload will then be 30 - n seconds. If it takes more than 30 seconds, then the next cycle will be delayed until the payload is finished, but no longer.\nYou'll see that I have debug code in there to start on a one-minute boundary to make the output initially easier to follow. I also gradually increase the maximum payload time so you'll eventually see the payload exceed the 30-second cycle time (an extra blank line is output so the effect is obvious).\nA sample run follows (where cycles normally start 30 seconds after the previous cycle):\nTue May 26 20:56:00 AWST 2020 Sleeping for 9 seconds (max 21).\nTue May 26 20:56:30 AWST 2020 Sleeping for 19 seconds (max 22).\nTue May 26 20:57:00 AWST 2020 Sleeping for 9 seconds (max 23).\nTue May 26 20:57:30 AWST 2020 Sleeping for 7 seconds (max 24).\nTue May 26 20:58:00 AWST 2020 Sleeping for 2 seconds (max 25).\nTue May 26 20:58:30 AWST 2020 Sleeping for 8 seconds (max 26).\nTue May 26 20:59:00 AWST 2020 Sleeping for 20 seconds (max 27).\nTue May 26 20:59:30 AWST 2020 Sleeping for 25 seconds (max 28).\nTue May 26 21:00:00 AWST 2020 Sleeping for 5 seconds (max 29).\nTue May 26 21:00:30 AWST 2020 Sleeping for 6 seconds (max 30).\nTue May 26 21:01:00 AWST 2020 Sleeping for 27 seconds (max 31).\nTue May 26 21:01:30 AWST 2020 Sleeping for 25 seconds (max 32).\nTue May 26 21:02:00 AWST 2020 Sleeping for 15 seconds (max 33).\nTue May 26 21:02:30 AWST 2020 Sleeping for 10 seconds (max 34).\nTue May 26 21:03:00 AWST 2020 Sleeping for 5 seconds (max 35).\nTue May 26 21:03:30 AWST 2020 Sleeping for 35 seconds (max 36).\n\nTue May 26 21:04:05 AWST 2020 Sleeping for 2 seconds (max 37).\nTue May 26 21:04:35 AWST 2020 Sleeping for 20 seconds (max 38).\nTue May 26 21:05:05 AWST 2020 Sleeping for 22 seconds (max 39).\nTue May 26 21:05:35 AWST 2020 Sleeping for 18 seconds (max 40).\nTue May 26 21:06:05 AWST 2020 Sleeping for 33 seconds (max 41).\n\nTue May 26 21:06:38 AWST 2020 Sleeping for 31 seconds (max 42).\n\nTue May 26 21:07:09 AWST 2020 Sleeping for 6 seconds (max 43).\n\nIf you want to avoid the kludgy solution, this is probably better. You'll still need a cron job (or equivalent) to periodically detect if this script is running and, if not, start it. But the script itself then handles the timing.\n\n(a) Some of my workmates would say that kludges are my specialty :-)",
    "tag": "crontab"
  },
  {
    "question": "Where can I set environment variables that crontab will use?",
    "answer": "You can define environment variables in the crontab itself when running crontab -e from the command line.\nLANG=nb_NO.UTF-8\nLC_ALL=nb_NO.UTF-8\n\n# m h dom mon dow   command\n* * * * *           sleep 5s && echo \"yo\"\n\nThis feature is only available to certain implementations of cron.  Ubuntu and Debian currently use vixie-cron which allows these to be declared in the crontab file (also GNU mcron).\nArchlinux and RedHat use cronie which does not allow environment variables to be declared and will throw syntax errors in the cron.log.  Workaround can be done per-entry:\n# m h dom mon dow   command\n* * * * *           export LC_ALL=nb_NO.UTF-8; sleep 5s && echo \"yo\"",
    "tag": "crontab"
  },
  {
    "question": "Running a cron job at 2:30 AM every day",
    "answer": "crontab -e\n\nadd:\n30 2 * * * /your/command",
    "tag": "crontab"
  },
  {
    "question": "How do I schedule jobs in Jenkins?",
    "answer": "By setting the schedule period to 15 13 * * * you tell Jenkins to schedule the build every day of every month of every year at the 15th minute of the 13th hour of the day.\nJenkins used a cron expression (official documentation), and the different fields are:\n\nMINUTES   Minutes in one hour (0-59)\nHOURS     Hours in one day (0-23)\nDAYMONTH  Day in a month (1-31)\nMONTH     Month in a year (1-12)\nDAYWEEK   Day of the week (0-7) where 0 and 7 are sunday\n\nIf you want to schedule your build every 5 minutes, this will do the job : */5 * * * *\nIf you want to schedule your build every day at 8h00, this will do the job : 0 8 * * *\nFor the past few versions (2014), Jenkins have a new parameter, H (extract from the Jenkins code documentation):\n\nTo allow periodically scheduled tasks to produce even load on the system, the symbol H (for “hash”) should be used wherever possible.\nFor example, using 0 0 * * * for a dozen daily jobs will cause a large spike at midnight. In contrast, using H H * * * would still execute each job once a day, but not all at the same time, better using limited resources.\n\nNote also that:\n\nThe H symbol can be thought of as a random value over a range, but it actually is a hash of the job name, not a random function, so that the value remains stable for any given project.\n\nMore example of using 'H'",
    "tag": "crontab"
  },
  {
    "question": "Run Cron job every N minutes plus offset",
    "answer": "To run a task every 20 minutes starting at 5 past the hour, try this:\n 5-59/20 * * * *\n\nExplanation\nAn * in the minute field is the same as 0-59/1 where 0-59 is the range and 1 is the step. The command will run at the first minute in the range (0), then at all successive minutes that are distant from the first by step (1), until the last (59).\nWhich is why */20 * * * * will run at 0 minutes, 20 minutes after, and 40 minutes after -- which is the same as every 20 minutes. However, */25 * * * * will run at 0 minutes, 25 minutes after, and 50 minutes after -- which is not the same as every 25 minutes. That's why it's usually desirable to use a step value in the minute field that divides evenly into 60.\nSo to offset the start time, specify the range explicitly and set the first value to the amount of the offset.\nExamples\n5-59/20 * * * * will run at 5 minutes after, 25 minutes after, and 45 minutes after.\n10-59/25 * * * * will run at 10 minutes after and 35 minutes after.\n1-59/2 * * * * will run every odd minute.",
    "tag": "crontab"
  },
  {
    "question": "How to run crontab job every week on Sunday",
    "answer": "Here is an explanation of the crontab format.\n# 1. Entry: Minute when the process will be started [0-60]\n# 2. Entry: Hour when the process will be started [0-23]\n# 3. Entry: Day of the month when the process will be started [1-28/29/30/31]\n# 4. Entry: Month of the year when the process will be started [1-12]\n# 5. Entry: Weekday when the process will be started [0-6] [0 is Sunday]\n#\n# all x min = */x\n\nSo according to this your 5 8 * * 0 would run 8:05 every Sunday.",
    "tag": "crontab"
  },
  {
    "question": "Cron and virtualenv",
    "answer": "You should be able to do this by using the python in your virtual environment:\n/home/my/virtual/bin/python /home/my/project/manage.py command arg\n\nEDIT: If your django project isn't in the PYTHONPATH, then you'll need to switch to the right directory:\ncd /home/my/project && /home/my/virtual/bin/python ...\n\nYou can also try to log the failure from cron:\ncd /home/my/project && /home/my/virtual/bin/python /home/my/project/manage.py > /tmp/cronlog.txt 2>&1\n\nAnother thing to try is to make the same change in your manage.py script at the very top:\n#!/home/my/virtual/bin/python",
    "tag": "crontab"
  },
  {
    "question": "How do I write a bash script to restart a process if it dies?",
    "answer": "Avoid PID-files, crons, or anything else that tries to evaluate processes that aren't their children.\nThere is a very good reason why in UNIX, you can ONLY wait on your children.  Any method (ps parsing, pgrep, storing a PID, ...) that tries to work around that is flawed and has gaping holes in it.  Just say no.\nInstead you need the process that monitors your process to be the process' parent.  What does this mean?  It means only the process that starts your process can reliably wait for it to end.  In bash, this is absolutely trivial.\nuntil myserver; do\n    echo \"Server 'myserver' crashed with exit code $?.  Respawning..\" >&2\n    sleep 1\ndone\n\nOr to be able to stop it:\ntrap 'kill $(jobs -p)' EXIT; until myserver & wait; do\n    echo \"ldap proxy crashed with exit code $?. Respawning..\" >&2\n    sleep 1\ndone\n\nThe above piece of bash code runs myserver in an until loop.  The first line starts myserver and waits for it to end.  When it ends, until checks its exit status.  If the exit status is 0, it means it ended gracefully (which means you asked it to shut down somehow, and it did so successfully).  In that case we don't want to restart it (we just asked it to shut down!).  If the exit status is not 0, until will run the loop body, which emits an error message on STDERR and restarts the loop (back to line 1) after 1 second.\nWhy do we wait a second?  Because if something's wrong with the startup sequence of myserver and it crashes immediately, you'll have a very intensive loop of constant restarting and crashing on your hands.  The sleep 1 takes away the strain from that.\nNow all you need to do is start this bash script (asynchronously, probably), and it will monitor myserver and restart it as necessary.  If you want to start the monitor on boot (making the server \"survive\" reboots), you can schedule it in your user's cron(1) with an @reboot rule.  Open your cron rules with crontab:\ncrontab -e\n\nThen add a rule to start your monitor script:\n@reboot /usr/local/bin/myservermonitor\n\n\nAlternatively; look at inittab(5) and /etc/inittab.  You can add a line in there to have myserver start at a certain init level and be respawned automatically.\n\nEdit.\nLet me add some information on why not to use PID files.  While they are very popular; they are also very flawed and there's no reason why you wouldn't just do it the correct way.\nConsider this:\n\nPID recycling (killing the wrong process):\n\n/etc/init.d/foo start: start foo, write foo's PID to /var/run/foo.pid\nA while later: foo dies somehow.\nA while later: any random process that starts (call it bar) takes a random PID, imagine it taking foo's old PID.\nYou notice foo's gone: /etc/init.d/foo/restart reads /var/run/foo.pid, checks to see if it's still alive, finds bar, thinks it's foo, kills it, starts a new foo.\n\n\nPID files go stale.  You need over-complicated (or should I say, non-trivial) logic to check whether the PID file is stale, and any such logic is again vulnerable to 1..\n\nWhat if you don't even have write access or are in a read-only environment?\n\nIt's pointless overcomplication; see how simple my example above is.  No need to complicate that, at all.\n\n\nSee also: Are PID-files still flawed when doing it 'right'?\nBy the way; even worse than PID files is parsing ps!  Don't ever do this.\n\nps is very unportable.  While you find it on almost every UNIX system; its arguments vary greatly if you want non-standard output.  And standard output is ONLY for human consumption, not for scripted parsing!\nParsing ps leads to a LOT of false positives.  Take the ps aux | grep PID example, and now imagine someone starting a process with a number somewhere as argument that happens to be the same as the PID you stared your daemon with!  Imagine two people starting an X session and you grepping for X to kill yours.  It's just all kinds of bad.\n\nIf you don't want to manage the process yourself; there are some perfectly good systems out there that will act as monitor for your processes.  Look into runit, for example.",
    "tag": "crontab"
  },
  {
    "question": "Using crontab to execute script every minute and another every 24 hours",
    "answer": "every minute:\n* * * * * /path/to/php /var/www/html/a.php\nevery 24hours (every midnight):\n0 0 * * * /path/to/php /var/www/html/reset.php\nSee this reference for how crontab works: http://adminschoice.com/crontab-quick-reference, and this handy tool to build cron jobx: http://www.htmlbasix.com/crontab.shtml",
    "tag": "crontab"
  },
  {
    "question": "How to log cron jobs?",
    "answer": "* * * * * myjob.sh >> /var/log/myjob.log 2>&1\n\nwill log all output from the cron job to /var/log/myjob.log\nYou might use mail to send emails. Most systems will send unhandled cron job output by email to root or the corresponding user.",
    "tag": "crontab"
  },
  {
    "question": "Crontab Day of the Week syntax",
    "answer": "0 and 7 both stand for Sunday, you can use the one you want, so writing 0-6 or 1-7 has the same result.\nAlso, as suggested by @Henrik, it is possible to replace numbers by shortened name of days, such as MON, THU, etc:\n0 - Sun      Sunday\n1 - Mon      Monday\n2 - Tue      Tuesday\n3 - Wed      Wednesday\n4 - Thu      Thursday\n5 - Fri      Friday\n6 - Sat      Saturday\n7 - Sun      Sunday\n\nGraphically, * * * * * command to be executed stands for:\n\n\n\nminute\nhour\nday of month\nmonth\nday of week\n\n\n\n\n\n(0-59)\n(0-23)\n(1-31)\n(1-12)\n(1-7)\n\n\n\n*\n*\n*\n*\n*\ncommand to be executed\n\n\n\nOr using the old style:\n ┌────────── minute (0 - 59)\n │ ┌──────── hour (0 - 23)\n │ │ ┌────── day of month (1 - 31)\n │ │ │ ┌──── month (1 - 12)\n │ │ │ │ ┌── day of week (0 - 6 => Sunday - Saturday, or\n │ │ │ │ │                1 - 7 => Monday - Sunday)\n ↓ ↓ ↓ ↓ ↓\n * * * * * command to be executed\n\nFinally, if you want to specify day by day, you can separate days with commas, for example SUN,MON,THU will exectute the command only on sundays, mondays on thursdays.\nYou can read further details in Wikipedia's article about Cron and check a cron expression online with crontab.guru.",
    "tag": "crontab"
  },
  {
    "question": "Spring cron expression for every day 1:01:am",
    "answer": "Try with:\n@Scheduled(cron = \"0 1 1 * * ?\")\n\nBelow you can find the example patterns from the spring forum:\n* \"0 0 * * * *\" = the top of every hour of every day.\n* \"*/10 * * * * *\" = every ten seconds.\n* \"0 0 8-10 * * *\" = 8, 9 and 10 o'clock of every day.\n* \"0 0 8,10 * * *\" = 8 and 10 o'clock of every day.\n* \"0 0/30 8-10 * * *\" = 8:00, 8:30, 9:00, 9:30 and 10 o'clock every day.\n* \"0 0 9-17 * * MON-FRI\" = on the hour nine-to-five weekdays\n* \"0 0 0 25 12 ?\" = every Christmas Day at midnight\n\nCron expression is represented by six fields:\nsecond, minute, hour, day of month, month, day(s) of week\n\n(*) means match any\n*/X means \"every X\"\n? (\"no specific value\") - useful when you need to specify something in one of the two fields in which the character is allowed, but not the other. For example, if I want my trigger to fire on a particular day of the month (say, the 10th), but I don't care what day of the week that happens to be, I would put \"10\" in the day-of-month field and \"?\" in the day-of-week field.\nPS: In order to make it work, remember to enable it in your application context: https://docs.spring.io/spring/docs/3.2.x/spring-framework-reference/html/scheduling.html#scheduling-annotation-support",
    "tag": "crontab"
  },
  {
    "question": "A cron job for rails: best practices?",
    "answer": "I've used the extremely popular Whenever on projects that rely heavily on scheduled tasks, and it's great.  It gives you a nice DSL to define your scheduled tasks instead of having to deal with crontab format.  From the README:\n\nWhenever is a Ruby gem that provides a\n  clear syntax for writing and deploying\n  cron jobs.\n\nExample from the README:\nevery 3.hours do\n  runner \"MyModel.some_process\"       \n  rake \"my:rake:task\"                 \n  command \"/usr/bin/my_great_command\"\nend\n\nevery 1.day, :at => '4:30 am' do \n  runner \"MyModel.task_to_run_at_four_thirty_in_the_morning\"\nend",
    "tag": "crontab"
  },
  {
    "question": "What is the Windows version of cron?",
    "answer": "For newer Microsoft OS versions, Windows Server 2012 / Windows 8, look at the schtasks command line utility.\nIf using PowerShell, the Scheduled Tasks Cmdlets in Windows PowerShell are made for scripting.\nFor command-line usage before Windows 8, you can schedule with the AT command.\nFor the original question, asking about Windows XP (and Windows 7): Windows Task Scheduler",
    "tag": "crontab"
  },
  {
    "question": "What is the Windows version of cron?",
    "answer": "For newer Microsoft OS versions, Windows Server 2012 / Windows 8, look at the schtasks command line utility.\nIf using PowerShell, the Scheduled Tasks Cmdlets in Windows PowerShell are made for scripting.\nFor command-line usage before Windows 8, you can schedule with the AT command.\nFor the original question, asking about Windows XP (and Windows 7): Windows Task Scheduler",
    "tag": "crontab"
  },
  {
    "question": "How to run cron job every 2 hours?",
    "answer": "Just do:\n0 */2 * * *  /home/username/test.sh \n\nThe 0 at the beginning means to run at the 0th minute. (If it were an *, the script would run every minute during every second hour.)\nDon't forget, you can check syslog to see if it ever actually ran!",
    "tag": "crontab"
  },
  {
    "question": "How would I get a cron job to run every 30 minutes?",
    "answer": "Do:\n0,30 * * * * your_command",
    "tag": "crontab"
  },
  {
    "question": "How to simulate the environment cron executes a script with?",
    "answer": "Add this to your crontab (temporarily):\n* * * * * env > ~/cronenv\n\nAfter it runs, do this:\nenv - `cat ~/cronenv` /bin/sh\n\nThis assumes that your cron runs /bin/sh, which is the default regardless of the user's default shell.\nFootnote: if env contains more advanced config, eg PS1=$(__git_ps1 \" (%s)\")$, it will error cryptically env: \": No such file or directory.",
    "tag": "crontab"
  },
  {
    "question": "Test a weekly cron job",
    "answer": "Just do what cron does, run the following as root:\nrun-parts -v /etc/cron.weekly\n\n... or the next one if you receive the \"Not a directory: -v\" error:\nrun-parts /etc/cron.weekly -v\n\nOption -v prints the script names before they are run.",
    "tag": "crontab"
  },
  {
    "question": "Crontab - Run in directory",
    "answer": "All jobs are executed by a shell, so start that shell snippet by a command to change the directory.\ncd /path/to/directory && ./bin/myapp\n\nConcerning the use of && instead of ;: normally it doesn't make a difference, but if the cd command fails (e.g. because the directory doesn't exist) with && the application isn't executed, whereas with ; it's executed (but not in the intended directory).",
    "tag": "crontab"
  },
  {
    "question": "How do I create a crontab through a script",
    "answer": "Here's a one-liner that doesn't use/require the new job to be in a file:\n(crontab -l 2>/dev/null; echo \"*/5 * * * * /path/to/job -with args\") | crontab -\n\nThe 2>/dev/null is important so that you don't get the no crontab for username message that some *nixes produce if there are currently no crontab entries.",
    "tag": "crontab"
  },
  {
    "question": "How to run cron once, daily at 10pm",
    "answer": "It's running every minute of the hour 22 I guess. Try the following to run it every first minute of the hour 22:\n0 22 * * * ....",
    "tag": "crontab"
  },
  {
    "question": "What is the curl error 52 \"empty reply from server\"?",
    "answer": "This can happen if curl is asked to do plain HTTP on a server that does HTTPS.\nExample:\n$ curl http://google.com:443\ncurl: (52) Empty reply from server",
    "tag": "crontab"
  },
  {
    "question": "How to specify in crontab by what user to run script?",
    "answer": "Instead of creating a crontab to run as the root user, create a crontab for the user that you want to run the script.  In your case, crontab -u www-data -e will edit the crontab for the www-data user.  Just put your full command in there and remove it from the root user's crontab.",
    "tag": "crontab"
  },
  {
    "question": "Run cron job only if it isn't already running",
    "answer": "Use flock.  It's new.  It's better.  \nNow you don't have to write the code yourself.  Check out more reasons here: https://serverfault.com/a/82863\n/usr/bin/flock -n /tmp/my.lockfile /usr/local/bin/my_script",
    "tag": "crontab"
  },
  {
    "question": "How to get CRON to call in the correct PATHs",
    "answer": "I used /etc/crontab. I used vi and entered in the PATHs I needed into this file and ran it as root. The normal crontab overwrites PATHs that you have set up. A good tutorial on how to do this.\nThe systemwide cron file looks like this:\nThis has the username field, as used by /etc/crontab.\n# /etc/crontab: system-wide crontab\n# Unlike any other crontab you don't have to run the `crontab'\n# command to install the new version when you edit this file.\n# This file also has a username field, that none of the other crontabs do.\n\nSHELL=/bin/sh\nPATH=/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin\n\n# m h dom mon dow user   command\n42 6 * * *   root    run-parts --report /etc/cron.daily\n47 6 * * 7   root    run-parts --report /etc/cron.weekly\n52 6 1 * *   root    run-parts --report /etc/cron.monthly\n01 01 * * 1-5 root python /path/to/file.py",
    "tag": "crontab"
  },
  {
    "question": "How to automatically remove completed Kubernetes Jobs created by a CronJob?",
    "answer": "You can now set history limits, or disable history altogether, so that failed or successful CronJobs are not kept around indefinitely.  See my answer here. Documentation is here.\nTo set the history limits:\n\nThe .spec.successfulJobsHistoryLimit and .spec.failedJobsHistoryLimit fields are optional. These fields specify how many completed and failed jobs should be kept. By default, they are set to 3 and 1 respectively. Setting a limit to 0 corresponds to keeping none of the corresponding kind of jobs after they finish.\n\nThe config with 0 limits would look like:\napiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: hello\nspec:\n  schedule: \"*/1 * * * *\"\n  successfulJobsHistoryLimit: 0\n  failedJobsHistoryLimit: 0\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: hello\n            image: busybox\n            args:\n            - /bin/sh\n            - -c\n            - date; echo Hello from the Kubernetes cluster\n          restartPolicy: OnFailure",
    "tag": "crontab"
  },
  {
    "question": "How to run a script in the background even after I logout SSH?",
    "answer": "Run nohup python bgservice.py & to get the script to ignore the hangup signal and keep running. Output will be put in nohup.out.\nIdeally, you'd run your script with something like supervise so that it can be restarted if (when) it dies.",
    "tag": "crontab"
  },
  {
    "question": "How to write a cron that will run a script every day at midnight?",
    "answer": "Here's a good tutorial on what crontab is and how to use it on Ubuntu. Your crontab line will look something like this:\n00 00 * * * ruby path/to/your/script.rb\n\n(00 00 indicates midnight--0 minutes and 0 hours--and the *s mean every day of every month.)\n\nSyntax: \n  mm hh dd mt wd  command\n\n  mm minute 0-59\n  hh hour 0-23\n  dd day of month 1-31\n  mt month 1-12\n  wd day of week 0-7 (Sunday = 0 or 7)\n  command: what you want to run\n  all numeric values can be replaced by * which means all",
    "tag": "crontab"
  },
  {
    "question": "How to schedule a function to run every hour on Flask?",
    "answer": "You can use BackgroundScheduler() from APScheduler package (v3.5.3):\nimport time\nimport atexit\n\nfrom apscheduler.schedulers.background import BackgroundScheduler\n\n\ndef print_date_time():\n    print(time.strftime(\"%A, %d. %B %Y %I:%M:%S %p\"))\n\n\nscheduler = BackgroundScheduler()\nscheduler.add_job(func=print_date_time, trigger=\"interval\", seconds=60)\nscheduler.start()\n\n# Shut down the scheduler when exiting the app\natexit.register(lambda: scheduler.shutdown())\n\nNote that two of these schedulers will be launched when Flask is in debug mode. For more information, check out this question.",
    "tag": "crontab"
  },
  {
    "question": "CRON job to run on the last day of the month",
    "answer": "Possibly the easiest way is to simply do three separate jobs:\n55 23 30 4,6,9,11        * myjob.sh\n55 23 31 1,3,5,7,8,10,12 * myjob.sh\n55 23 28 2               * myjob.sh\n\nThat will run on the 28th of February though, even on leap years so, if that's a problem, you'll need to find another way.\n\nHowever, it's usually both substantially easier and correct to run the job as soon as possible on the first day of each month, with something like:\n0 0 1 * * myjob.sh\n\nand modify the script to process the previous month's data.\nThis removes any hassles you may encounter with figuring out which day is the last of the month, and also ensures that all data for that month is available, assuming you're processing data. Running at five minutes to midnight on the last day of the month may see you missing anything that happens between then and midnight.\nThis is the usual way to do it anyway, for most end-of-month jobs.\n\nIf you still really want to run it on the last day of the month, one option is to simply detect if tomorrow is the first (either as part of your script, or in the crontab itself).\nSo, something like:\n55 23 28-31 * * [[ \"$(date --date=tomorrow +\\%d)\" == \"01\" ]] && myjob.sh\n\nshould be a good start, assuming you have a relatively intelligent date program.\nIf your date program isn't quite advanced enough to give you relative dates, you can just put together a very simple program to give you tomorrow's day of the month (you don't need the full power of date), such as:\n#include <stdio.h>\n#include <time.h>\n\nint main (void) {\n    // Get today, somewhere around midday (no DST issues).\n\n    time_t noonish = time (0);\n    struct tm *localtm = localtime (&noonish);\n    localtm->tm_hour = 12;\n\n    // Add one day (86,400 seconds).\n\n    noonish = mktime (localtm) + 86400;\n    localtm = localtime (&noonish);\n\n    // Output just day of month.\n\n    printf (\"%d\\n\", localtm->tm_mday);\n\n    return 0;\n}\n\nand then use (assuming you've called it tomdom for \"tomorrow's day of month\"):\n55 23 28-31 * * [[ \"$(tomdom)\" == \"1\" ]] && myjob.sh\n\nThough you may want to consider adding error checking since both time() and mktime() can return -1 if something goes wrong. The code above, for reasons of simplicity, does not take that into account.",
    "tag": "crontab"
  },
  {
    "question": "Spring cron vs normal cron?",
    "answer": "Spring Scheduled tasks are not in the same format as cron expressions.\nThey don't follow the same format as UNIX cron expressions.\nThere are only 6 fields:\n\nsecond,\nminute,\nhour,\nday of month,\nmonth,\nday(s) of week.\n\nAsterisk (*) means match any.\n*/X means \"every X\" (see examples).\nNumeric days of the week do not work for me.  Besides, \"MON-FRI\" is much easier to read.\nHere are some example expressions:\n\"0 0 18 * * MON-FRI\" means every weekday at 6:00 PM. \n\n\"0 0 */1 * * *\" means every hour on the hour.\n\n\"0 0 */8 * * *\" means every 8 hours on the hour.\n\n\"0 0 12 1 * *\" means 12:00 PM on the first day of every month. \n\nHere you can find some additional information.\nAlso you may find the spring documentation useful.",
    "tag": "crontab"
  },
  {
    "question": "How can I programmatically create a new cron job?",
    "answer": "The best way if you're running as root, is to drop a file into /etc/cron.d\nif you use a package manager to package your software, you can simply lay down files in that directory and they are interpreted as if they were crontabs, but with an extra field for the username, e.g.:\nFilename: /etc/cron.d/per_minute\nContent:\n* * * * * root /bin/sh /home/root/script.sh",
    "tag": "crontab"
  },
  {
    "question": "Cron jobs and random times, within given hours",
    "answer": "How to cron something at a random offset 20 times a day between 9am and 11pm? That's kinda tricky within cron, because you are dividing 14 hours by 20 execution times. I don't like the other answers very much because they require writing a bash wrapper script for your php script.\nHowever, if you'll allow me the liberty to ease the timing and frequency restriction to 13 times between 8:30am and 11:09pm, this might do the trick, and all within the confines of your crontab:\n30 8-21/* * * * sleep ${RANDOM:0:2}m ; /path/to/script.php\n\n${RANDOM:3:2} uses bash's $RANDOM that other people have mentioned above, but adds bash array slicing. Since bash variables are untyped, the pseudo-random signed 16-bit number gets truncated to the first 2 of its 5 decimal digits, giving you a succinct one-liner for delaying your cronjob between 10 and 99 minutes (though the distribution is biased towards 10 to 32).\nThe following might also work for you, but I found it do be \"less random\" for some reason (perhaps Benford's Law is triggered by modulating pseudo-random numbers. Hey, I don't know, I flunked math... Blame it on bash!):\n30 8-21/* * * * sleep $[RANDOM\\%90]m ; /path/to/script.php\n\nYou need to render modulus as \\% above because cron (well, at least Linux 'vixie-cron') terminates the line when it encounters an unescaped %.\nMaybe you could get the remaining 7 script executions in there by adding another line with another 7-hour range. Or relax your restriction to run between 3am and 11pm.",
    "tag": "crontab"
  },
  {
    "question": "Running a cron job on Linux every six hours",
    "answer": "You forgot a *, and you've too many fields. It's the hour you need to care about\n0 */6 * * * /path/to/mycommand\n\nThis means every sixth hour starting from 0, i.e. at hour 0, 6, 12 and 18 which you could write as\n0 0,6,12,18 * * * /path/to/mycommand",
    "tag": "crontab"
  },
  {
    "question": "Disabling cronjob in Kubernetes",
    "answer": "If you want to suspend cronjob via patch, use:\nkubectl patch cronjobs <job-name> -p '{\"spec\" : {\"suspend\" : true }}'\n\nYou may need to escape \" and/or specify namespace\n kubectl patch cronjob <job-name> [-n namespace] -p '{\\\"spec\\\" : {\\\"suspend\\\" : true }}'",
    "tag": "crontab"
  },
  {
    "question": "Cron job every three days",
    "answer": "Run it every three days - or less at the end of the month. (It'll run 2 days in a row if the previous month had 31 days.)\n0 0 */3 * *\n\nHow about that?\nIf you want it to run on specific days of the month, like the 1st, 4th, 7th, etc... then you can just have a conditional in your script that checks for the current day of the month.\nif (((date('j') - 1) % 3))\n   exit();\n\nor, as @mario points out, you can use date('k') to get the day of the year instead of doing it based on the day of the month.",
    "tag": "crontab"
  },
  {
    "question": "A cron job that will never execute",
    "answer": "If you're still looking for something robust even in the far future, try https://stackoverflow.com/a/13938099/1601531, where I suggest the use of February 31st in crontab entries which are never intended to execute.\n0 0 5 31 2 ?",
    "tag": "crontab"
  },
  {
    "question": "I need a Nodejs scheduler that allows for tasks at different intervals",
    "answer": "I would recommend node-cron. It allows to run tasks using Cron patterns e.g.\n'* * * * * *' - runs every second\n'*/5 * * * * *' - runs every 5 seconds\n'10,20,30 * * * * *' - run at 10th, 20th and 30th second of every minute\n'0 * * * * *' - runs every minute\n'0 0 * * * *' - runs every hour (at 0 minutes and 0 seconds)\n\nBut also more complex schedules e.g.\n'00 30 11 * * 1-5' - Runs every weekday (Monday through Friday) at 11:30:00 AM. It does not run on Saturday or Sunday.\n\nSample code: running job every 10 minutes:\n\nvar cron = require('cron');\nvar cronJob = cron.job(\"0 */10 * * * *\", function(){\n    // perform operation e.g. GET request http.get() etc.\n    console.info('cron job completed');\n}); \ncronJob.start();\n\nYou can find more examples in node-cron wiki\nMore on cron configuration can be found on cron wiki\nI've been using that library in many projects and it does the job. I hope that will help.",
    "tag": "crontab"
  },
  {
    "question": "Setting up a cron job in Windows",
    "answer": "The windows equivalent to a cron job is a scheduled task. \nA scheduled task can be created as described by Alex and Rudu, but it can also be done command line with schtasks (if you for instance need to script it or add it to version control). \nAn example:\nschtasks /create /tn calculate /tr calc /sc weekly /d MON /st 06:05 /ru \"System\"\n\nCreates the task calculate, which starts the calculator(calc) every monday at 6:05 (should you ever need that.)\nAll available commands can be found here: http://technet.microsoft.com/en-us/library/cc772785%28WS.10%29.aspx \nIt works on windows server 2008 as well as windows server 2003.",
    "tag": "crontab"
  },
  {
    "question": "How do I set a task to run every so often?",
    "answer": "Just use launchd. It is a very powerful launcher system and meanwhile it is the standard launcher system for Mac OS X (current OS X version wouldn't even boot without it). For those who are not familiar with launchd (or with OS X in general), it is like a crossbreed between init, cron, at, SysVinit (init.d), inetd, upstart and systemd. Borrowing concepts of all these projects, yet also offering things you may not find elsewhere.\nEvery service/task is a file. The location of the file depends on the questions: \"When is this service supposed to run?\" and \"Which privileges will the service require?\"\nSystem tasks go to\n/Library/LaunchDaemons/\n\nif they shall run no matter if any user is logged in to the system or not. They will be started with \"root\" privileges.\nIf they shall only run if any user is logged in, they go to\n/Library/LaunchAgents/\n\nand will be executed with the privileges of the user that just logged in.\nIf they shall run only if you are logged in, they go to\n~/Library/LaunchAgents/\n\nwhere ~ is your HOME directory. These task will run with your privileges, just as if you had started them yourself by command line or by double clicking a file in Finder.\nNote that there also exists /System/Library/LaunchDaemons and /System/Library/LaunchAgents, but as usual, everything under /System is managed by OS X. You shall not place any files there, you shall not change any files there, unless you really know what you are doing. Messing around in the Systems folder can make your system unusable (get it into a state where it will even refuse to boot up again). These are the directories where Apple places the launchd tasks that get your system up and running during boot, automatically start services as required, perform system maintenance tasks, and so on.\nEvery launchd task is a file in PLIST format. It should have reverse domain name notation. E.g. you can name your task\ncom.example.my-fancy-task.plist\n\nThis plist can have various options and settings. Writing one per hand is not for beginners, so you may want to get a tool like LaunchControl (commercial, $18) or Lingon (commercial, $14.99) to create your tasks.\nJust as an example, it could look like this\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<!DOCTYPE plist PUBLIC \"-//Apple Computer//DTD PLIST 1.0//EN\" \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\">\n<plist version=\"1.0\">\n<dict>\n    <key>Label</key>\n    <string>com.example.my-fancy-task</string>\n    <key>OnDemand</key>\n    <true/>\n    <key>ProgramArguments</key>\n    <array>\n        <string>/bin/sh</string>\n        <string>/usr/local/bin/my-script.sh</string>\n    </array>\n    <key>StartInterval</key>\n    <integer>1800</integer>\n</dict>\n</plist>\n\nThis agent will run the shell script /usr/local/bin/my-script.sh every 1800 seconds (every 30 minutes). You can also have task run on certain dates/times (basically launchd can do everything cron can do) or you can even disable \"OnDemand\" causing launchd to keep the process permanently running (if it quits or crashes, launchd will immediately restart it). You can even limit how much resources a process may use.\nUpdate: Even though OnDemand is still supported, it is deprecated. The new setting is named KeepAlive, which makes much more sense. It can have a boolean value, in which case it is the exact opposite of OnDemand (setting it to false behaves as if OnDemand is true and the other way round). The great new feature is, that it can also have a dictionary value instead of a boolean one. If it has a dictionary value, you have a couple of extra options that give you more fine grain control under which circumstances the task shall be kept alive. E.g. it is only kept alive as long as the program terminated with an exit code of zero, only as long as a certain file/directory on disk exists, only if another task is also alive, or only if the network is currently up.\nAlso you can manually enable/disable tasks via command line:\nlaunchctl <command> <parameter>\n\ncommand can be load or unload, to load a plist or unload it again, in which case parameter is the path to the file. Or command can be start or stop, to just start or stop such a task, in which case parameter is the label (com.example.my-fancy-task). Other commands and options exist as well.\nUpdate: Even though load, unload, start, and stop do still work, they are legacy now. The new commands are bootstrap, bootout, enable, and disable with slightly different syntax and options. One big difference is that disable is persistent, so once a service has been disabled, it will stay disabled, even across reboots until you enable it again. Also you can use kickstart to run a task immediately, regardless how it has been configured to run.\nThe main difference between the new and the old commands is that they separate tasks by \"domain\". The system has domain and so has every user. So equally labeled tasks may exist in different domains and launchctl can still distinguish them. Even different login and different UI sessions of the same user have their own domain (e.g. the same user may once be logged locally and once remote via SSH and different tasks may run for either session) and so does every single running processes. Thus instead of com.example.my-fancy-task, you now would use system/com.example.my-fancy-task or user/501/com.example.my-fancy-task to identify a task, with 501 being the user ID of a specific user.\nSee documentation of the plist format and of the launchctl command line tool.",
    "tag": "crontab"
  },
  {
    "question": "How to run a cronjob every X minutes?",
    "answer": "In a crontab file, the fields are:\n\nminute of the hour.\nhour of the day.\nday of the month.\nmonth of the year.\nday of the week.\n\nSo:\n10 * * * * blah\n\nmeans execute blah at 10 minutes past every hour.\nIf you want every five minutes, use either:\n*/5 * * * * blah\n\nmeaning every minute but only every fifth one, or:\n0,5,10,15,20,25,30,35,40,45,50,55 * * * * blah\n\nfor older cron executables that don't understand the */x notation.\nIf it still seems to be not working after that, change the command to something like:\ndate >>/tmp/debug_cron_pax.txt\n\nand monitor that file to ensure something's being written every five minutes. If so, there's something wrong with your PHP scripts. If not, there's something wrong with your cron daemon.",
    "tag": "crontab"
  },
  {
    "question": "How to conditionally enable or disable scheduled jobs in Spring?",
    "answer": "The most efficient way to disable @Scheduled in Spring is to set cron expression to -\n@Scheduled(cron = \"-\")\npublic void autoEvictAllCache() {\n    LOGGER.info(\"Refresing the Cache Start :: \" + new Date());\n    activeMQUtility.sendToTopicCacheEviction(\"ALL\");\n    LOGGER.info(\"Refresing the Cache Complete :: \" + new Date());\n}\n\nFrom the docs:\n\nCRON_DISABLED\npublic static final String CRON_DISABLED\nA special cron\nexpression value that indicates a disabled trigger: \"-\". This is\nprimarily meant for use with ${...} placeholders, allowing for\nexternal disabling of corresponding scheduled methods.\nSince:\n5.1 See Also: ScheduledTaskRegistrar.CRON_DISABLED",
    "tag": "crontab"
  },
  {
    "question": "mysqldump & gzip commands to properly create a compressed file of a MySQL database using crontab",
    "answer": "First the mysqldump command is executed and the output generated is redirected using the pipe. The pipe is sending the standard output into the gzip command as standard input. Following the filename.gz, is the output redirection operator (>) which is going to continue redirecting the data until the last filename, which is where the data will be saved.\nFor example, this command will dump the database and run it through gzip and the data will finally land in three.gz\nmysqldump -u user -pupasswd my-database | gzip > one.gz > two.gz > three.gz\n\n$> ls -l\n-rw-r--r--  1 uname  grp     0 Mar  9 00:37 one.gz\n-rw-r--r--  1 uname  grp  1246 Mar  9 00:37 three.gz\n-rw-r--r--  1 uname  grp     0 Mar  9 00:37 two.gz\n\nMy original answer is an example of redirecting the database dump to many compressed files (without double compressing). (Since I scanned the question and seriously missed - sorry about that)\nThis is an example of recompressing files:\nmysqldump -u user -pupasswd my-database | gzip -c > one.gz; gzip -c one.gz > two.gz; gzip -c two.gz > three.gz\n\n$> ls -l\n-rw-r--r--  1 uname  grp  1246 Mar  9 00:44 one.gz\n-rw-r--r--  1 uname  grp  1306 Mar  9 00:44 three.gz\n-rw-r--r--  1 uname  grp  1276 Mar  9 00:44 two.gz\n\nThis is a good resource explaining I/O redirection: http://www.codecoffee.com/tipsforlinux/articles2/042.html",
    "tag": "crontab"
  },
  {
    "question": "CronJob not running",
    "answer": "What?! My cronjob doesn't run?!\nHere's a checklist guide to debug not running cronjobs:\n\nIs the Cron daemon running?\n\n\nRun ps ax | grep cron and look for cron.\nDebian: service cron start or service cron restart\n\n\nIs cron working?\n\n\n* * * * * /bin/echo \"cron works\" >> /tmp/file\nSyntax correct? See below.\nYou obviously need to have write access to the file you are redirecting the output to. A unique file name in /tmp which does not currently exist should always be writable.\nProbably also add 2>&1 to include standard error as well as standard output, or separately output standard error to another file with 2>>/tmp/errors\n\n\nIs the command working standalone?\n\n\nCheck if the script has an error, by doing a dry run on the CLI\nWhen testing your command, test as the user whose crontab you are editing, which might not be your login or root\n\n\nCan cron run your job?\n\n\nCheck /var/log/cron.log or /var/log/messages for errors.\nUbuntu: grep CRON /var/log/syslog\nRedhat: /var/log/cron\n\n\nCheck permissions\n\n\nSet executable flag on the command: chmod +x /var/www/app/cron/do-stuff.php\nIf you redirect the output of your command to a file, verify you have permission to write to that file/directory\n\n\nCheck paths\n\n\nCheck she-bangs / hashbangs line\nDo not rely on environment variables like PATH, as their value will likely not be the same under cron as under an interactive session. See How to get CRON to call in the correct PATHs\n\n\nDon't suppress output while debugging\n\n\nCommonly used is this suppression: 30 1 * * * command > /dev/null 2>&1\nRe-enable the standard output or standard error message output by removing >/dev/null 2>&1 altogether; or perhaps redirect to a file in a location where you have write access: >>cron.out 2>&1 will append standard output and standard error to cron.out in the invoking user's home directory.\nIf you don't redirect output from a cron job, the daemon will try to send you any output or error messages by email. Check your inbox (maybe simply more $MAIL if you don't have a mail client). If mail is not available, maybe check for a file named dead.letter in your home directory, or system log entries saying that the output was discarded. Especially in the latter case, probably edit the job to add redirection to a file, then wait for the job to run, and examine the log file for error messages or other useful feedback.\nIf you are trying to figure out why something failed, the error messages will be visible in this file. Read it and understand it.\n\nStill not working? Yikes!\n\nRaise the cron debug level\n\n\nDebian\n\nin /etc/default/cron\nset EXTRA_OPTS=\"-L 2\"\nservice cron restart\ntail -f /var/log/syslog to see the scripts executed\n\n\nUbuntu\n\nin /etc/rsyslog.d/50-default.conf\nadd or comment out line cron.* /var/log/cron.log\nreload logger sudo /etc/init.d/rsyslog restart\nre-run cron\nopen /var/log/cron.log  and look for detailed error output\n\n\nReminder: deactivate log level, when you are done with debugging\n\n\nRun cron and check log files again\n\nCronjob Syntax\n# Minute  Hour  Day of Month      Month         Day of Week    User Command    \n# (0-59) (0-23)   (1-31)    (1-12 or Jan-Dec) (0-6 or Sun-Sat)  \n         \n    0       2       *             *                *          root /usr/bin/find\n\nThis syntax is only correct for the root user. Regular user crontab syntax doesn't have the User field (regular users aren't allowed to run code as any other user);\n# Minute  Hour  Day of Month      Month         Day of Week    Command    \n# (0-59) (0-23)   (1-31)    (1-12 or Jan-Dec) (0-6 or Sun-Sat)  \n         \n    0       2       *             *                *          /usr/bin/find\n\nCrontab Commands\n\ncrontab -l\n\nLists all the user's cron tasks.\n\n\ncrontab -e, for a specific user: crontab -e -u agentsmith\n\nStarts edit session of your crontab file.\nWhen you exit the editor, the modified crontab is installed automatically.\n\n\ncrontab -r\n\nRemoves your crontab entry from the cron spooler, but not from crontab file.",
    "tag": "crontab"
  },
  {
    "question": "How to create cron job using PHP?",
    "answer": "This is the best explanation with code in PHP I have found so far:\nhttp://code.tutsplus.com/tutorials/managing-cron-jobs-with-php--net-19428\nIn short:\nAlthough the syntax of scheduling a new job may seem daunting at first glance, it's actually relatively simple to understand once you break it down. A cron job will always have five columns each of which represent a chronological 'operator' followed by the full path and command to execute:\n* * * * * home/path/to/command/the_command.sh\nEach of the chronological columns has a specific relevance to the schedule of the task. They are as follows:\nMinutes represents the minutes of a given hour, 0-59 respectively.\nHours represents the hours of a given day, 0-23 respectively.\nDays represents the days of a given month, 1-31 respectively.\nMonths represents the months of a given year, 1-12 respectively.\nDay of the Week represents the day of the week, Sunday through Saturday, numerically, as 0-6 respectively.\n\n\nSo, for example, if one wanted to schedule a task for 12am on the first day of every month it would look something like this:\n0 0 1 * * home/path/to/command/the_command.sh\nIf we wanted to schedule a task to run every Saturday at 8:30am we'd write it as follows:\n30 8 * * 6 home/path/to/command/the_command.sh\nThere are also a number of operators which can be used to customize the schedule even further:\nCommas is used to create a comma separated list of values for any of the cron columns.\nDashes is used to specify a range of values.\nAsterisksis used to specify 'all' or 'every' value\n\nVisit the link for the full article, it explains:\n\nWhat is the format of the cronjob if you want to enter/edit it manually.\nHow to use PHP with SSH2 library to authenticate as the user, which crontab you are going to edit.\nFull PHP class with all necessary methods for authentication, editing  and deleting crontab entries.",
    "tag": "crontab"
  },
  {
    "question": "Use PHP to create, edit and delete crontab jobs?",
    "answer": "crontab command usage \nusage:  crontab [-u user] file\n        crontab [-u user] [ -e | -l | -r ]\n                (default operation is replace, per 1003.2)\n        -e      (edit user's crontab)\n        -l      (list user's crontab)\n        -r      (delete user's crontab)\n        -i      (prompt before deleting user's crontab)\n\nSo,\n$output = shell_exec('crontab -l');\nfile_put_contents('/tmp/crontab.txt', $output.'* * * * * NEW_CRON'.PHP_EOL);\necho exec('crontab /tmp/crontab.txt');\n\nThe above can be used for both create and edit/append provided the user has the adequate file write permission.\nTo delete jobs:\necho exec('crontab -r');\n\nAlso, take note that apache is running as a particular user and that's usually not root, which means the cron jobs can only be changed for the apache user unless given crontab -u privilege to the apache user.",
    "tag": "crontab"
  },
  {
    "question": "How to convert Linux cron jobs to \"the Amazon way\"?",
    "answer": "I signed up for Amazon Gold support to ask them this question, this was their response:\n\nTom\nI did a quick poll of some of my colleagues and came up empty on the\n  cron, but after sleeping on it I realised the important step may be\n  limited to locking.  So I looked for \"distributed cron job locking\"\n  and found a reference to Zookeeper, an Apache project.\nhttp://zookeeper.apache.org/doc/r3.2.2/recipes.html\nhttp://highscalability.com/blog/2010/3/22/7-secrets-to-successfully-scaling-with-scalr-on-amazon-by-se.html\nAlso I have seen reference to using memcached or a similar caching\n  mechanism as a way to create locks with a TTL.  In this way you set a\n  flag, with a TTL of 300 seconds and no other cron worker will execute\n  the job. The lock will automatically be released after the TTL has\n  expired.  This is conceptually very similar to the SQS option we\n  discussed yesterday.\nAlso see; Google's chubby\n  http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/chubby-osdi06.pdf\nLet me know if this helps, and feel free to ask questions, we are very\n  aware that our services can be complex and daunting to both beginners\n  and seasoned developers alike. We are always happy to offer\n  architecture and best practice advice.\nBest regards,\nRonan G. Amazon Web Services",
    "tag": "crontab"
  },
  {
    "question": "How to keep Laravel Queue system running on server",
    "answer": "Running\nnohup php artisan queue:work --daemon &\n\nWill prevent the command exiting when you log out.\nThe trailing ampersand (&) causes process start in the background, so you can continue to use the shell and do not have to wait until the script is finished.\nSee nohup\n\nnohup - run a command immune to hangups, with output to a non-tty\n\nThis will output information to a file entitled nohup.out in the directory where you run the command. If you have no interest in the output, you can redirect stdout and stderr to /dev/null; similarly, you could output it into your normal Laravel log. For example\nnohup php artisan queue:work --daemon > /dev/null 2>&1 &\n\nnohup php artisan queue:work --daemon >> storage/logs/laravel.log &\n\nBut you should also use something like Supervisord to ensure that the service remains running and is restarted after crashes/failures.\nUse >> to append to laravel.log. Using a single > will replace the file each time.",
    "tag": "crontab"
  },
  {
    "question": "Execute Python script via crontab",
    "answer": "Just use crontab -e and follow the tutorial here.\nLook at point 3 for a guide on how to specify the frequency.\nBased on your requirement, it should effectively be:\n*/10 * * * * /usr/bin/python script.py",
    "tag": "crontab"
  },
  {
    "question": "How to run a cron job on every Monday, Wednesday and Friday?",
    "answer": "Here's my example crontab I always use as a template:\n# Use the hash sign to prefix a comment\n# +---------------- minute (0 - 59)\n# |  +------------- hour (0 - 23)\n# |  |  +---------- day of month (1 - 31)\n# |  |  |  +------- month (1 - 12)\n# |  |  |  |  +---- day of week (0 - 7) (Sunday=0 or 7)\n# |  |  |  |  |\n# *  *  *  *  *  command to be executed\n#--------------------------------------------------------------------------\n\nTo run my cron job every Monday, Wednesday and Friday at 7:00PM, the result will be:\n0 19 * * 1,3,5 nohup /home/lathonez/script.sh > /tmp/script.log 2>&1\n\nsource",
    "tag": "crontab"
  },
  {
    "question": "Run a PHP file in a cron job using CPanel",
    "answer": "I used this command to activate cron job for this.\n/usr/bin/php -q /home/username/public_html/yourfilename.php\n\non godaddy server, and its working fine.",
    "tag": "crontab"
  },
  {
    "question": "Does WGET timeout?",
    "answer": "According to the man page of wget, there are a couple of options related to timeouts -- and there is a default read timeout of 900s -- so I say that, yes, it could timeout.\n\nHere are the options in question :\n-T seconds\n--timeout=seconds\n\n\nSet the network timeout to seconds\n  seconds.  This is equivalent to\n  specifying --dns-timeout,\n  --connect-timeout, and\n  --read-timeout, all at the same\n  time.\n\n\nAnd for those three options :\n--dns-timeout=seconds\n\n\nSet the DNS lookup timeout to seconds\n  seconds.  DNS lookups that don't\n  complete within the specified time\n  will fail. By default, there is no\n  timeout on DNS lookups, other than\n  that implemented by system libraries.\n\n--connect-timeout=seconds\n\n\nSet the connect timeout to seconds\n  seconds. TCP connections that take\n  longer to establish will be aborted.\n  By default, there is no connect\n  timeout, other than that implemented\n  by system libraries.\n\n--read-timeout=seconds\n\n\nSet the read (and write) timeout to\n  seconds seconds. The \"time\" of\n  this timeout refers to idle time: if,\n  at any point in the download, no data\n  is received for more than the\n  specified number of seconds, reading\n  fails and the download is restarted.\n  This option does not directly\n  affect the duration of the entire\n  download.\n\n\nI suppose using something like \nwget -O - -q -t 1 --timeout=600 http://www.example.com/cron/run\n\nshould make sure there is no timeout before longer than the duration of your script.\n(Yeah, that's probably the most brutal solution possible ^^ )",
    "tag": "crontab"
  },
  {
    "question": "How do you run a crontab in Cygwin on Windows?",
    "answer": "You need to also install cygrunsrv so you can set cron up as a windows service, then run cron-config.\nIf you want the cron jobs to send email of any output you'll also need to install either exim or ssmtp (before running cron-config.)\nSee /usr/share/doc/Cygwin/cron-*.README for more details.\nRegarding programs without a .exe extension, they are probably shell scripts of some type.  If you look at the first line of the file you could see what program you need to use to run them (e.g., \"#!/bin/sh\"), so you could perhaps execute them from the windows scheduler by calling the shell program (e.g., \"C:\\cygwin\\bin\\sh.exe -l /my/cygwin/path/to/prog\".)",
    "tag": "crontab"
  },
  {
    "question": "Scheduling Python Script to run every hour accurately",
    "answer": "Maybe this can help: Advanced Python Scheduler\nHere's a small piece of code from their documentation:\nfrom apscheduler.schedulers.blocking import BlockingScheduler\n\ndef some_job():\n    print \"Decorated job\"\n\nscheduler = BlockingScheduler()\nscheduler.add_job(some_job, 'interval', hours=1)\nscheduler.start()",
    "tag": "crontab"
  },
  {
    "question": "find -mtime files older than 1 hour",
    "answer": "What about -mmin?\nfind /var/www/html/audio -daystart -maxdepth 1 -mmin +59 -type f -name \"*.mp3\" \\\n    -exec rm -f {} \\;\n\nFrom man find:\n\n-mmin n\n        File's data was last modified n minutes ago.\n\nAlso, make sure to test this first!\n\n... -exec echo rm -f '{}' \\;\n          ^^^^ Add the 'echo' so you just see the commands that are going to get\n               run instead of actual trying them first.",
    "tag": "crontab"
  },
  {
    "question": "How can I access Docker set Environment Variables From a Cron Job",
    "answer": "I ran into this same problem. I have a docker container that runs cron to execute some shell scripts periodically. I too had a hard time finding out why my scripts would run fine when I manually executed them inside the container. I tried all the tricks of creating a shell script that would run first to set the environment, but they never worked for me (most likely I did something wrong). But I continued looking and found this and it does work.\n\nSetup a start or entry point shell script for your cron container\nMake this the first line to execute printenv | grep -v \"no_proxy\" >> /etc/environment\n\nThe trick here is the /etc/environment file. When the container is built that file is empty, I think on purpose. I found a reference to this file in the man pages for cron(8). After looking at all the versions of cron they all elude to an /etc/? file that you can use to feed environment variables to child processes.\nAlso, note that I created my docker container to run cron in the foreground, cron -f. This helped me avoid other tricks with tail running to keep the container up.\nHere is my entrypoint.sh file for reference and my container is a debian:jessie base image.\nprintenv | grep -v \"no_proxy\" >> /etc/environment\n\ncron -f\n\nAlso, this trick worked even with environment variables that are set during, docker run commands.",
    "tag": "crontab"
  },
  {
    "question": "How to set up a cron job to run an executable every hour?",
    "answer": "0 * * * * cd folder_containing_exe && ./exe_name\n\nshould work unless there is something else that needs to be setup for the program to run.",
    "tag": "crontab"
  },
  {
    "question": "Spring cron expression for every after 30 minutes",
    "answer": "According to the Quartz-Scheduler Tutorial\nIt should be value=\"0 0/30 * * * ?\"\nThe field order of the cronExpression is\n\nSeconds\nMinutes\nHours\nDay-of-Month\nMonth\nDay-of-Week\nYear (optional field)\n\nEnsure you have at least 6 parameters or you will get an error (year is optional).",
    "tag": "crontab"
  },
  {
    "question": "Parameter ScheduleExpression is not valid",
    "answer": "Could you try : cron(5,15,25,35,45,55 * * * ? *)\nCron expressions have six required fields here.\nAWS documentation\n\nEDIT: Also, don't miss this important wildcard note...\n\nYou cannot use * in both the Day-of-month and Day-of-week fields. If you use it in one, you must use ? in the other.",
    "tag": "crontab"
  },
  {
    "question": "AWS Elastic Beanstalk, running a cronjob",
    "answer": "This is how I added a cron job to Elastic Beanstalk:\nCreate a folder at the root of your application called .ebextensions if it doesn't exist already.  Then create a config file inside the .ebextensions folder.  I'll use example.config for illustration purposes.  Then add this to example.config\ncontainer_commands:\n  01_some_cron_job:\n    command: \"cat .ebextensions/some_cron_job.txt > /etc/cron.d/some_cron_job && chmod 644 /etc/cron.d/some_cron_job\"\n    leader_only: true\n\nThis is a YAML configuration file for Elastic Beanstalk.  Make sure when you copy this into your text editor that your text editor uses spaces instead of tabs.  Otherwise you'll get a YAML error when you push this to EB.\nSo what this does is create a command called 01_some_cron_job.  Commands are run in alphabetical order so the 01 makes sure it's run as the first command.\nThe command then takes the contents of a file called some_cron_job.txt and adds it to a file called some_cron_job in /etc/cron.d.\nThe command then changes the permissions on the /etc/cron.d/some_cron_job file.\nThe leader_only key ensures the command is only run on the ec2 instance that is considered the leader.  Rather than running on every ec2 instance you may have running.\nThen create a file called some_cron_job.txt inside the .ebextensions folder.  You will place your cron jobs in this file.\nSo for example:\n# The newline at the end of this file is extremely important.  Cron won't run without it.\n* * * * * root /usr/bin/php some-php-script-here > /dev/null\n\nSo this cron job will run every minute of every hour of every day as the root user and discard the output to /dev/null.  /usr/bin/php is the path to php.  Then replace some-php-script-here with the path to your php file.  This is obviously assuming your cron job needs to run a PHP file.\nAlso, make sure the some_cron_job.txt file has a newline at the end of the file just like the comment says.  Otherwise cron won't run.\nUpdate:\nThere is an issue with this solution when Elastic Beanstalk scales up your instances.  For example, lets say you have one instance with the cron job running.  You get an increase in traffic so Elastic Beanstalk scales you up to two instances.  The leader_only will ensure you only have one cron job running between the two instances.  Your traffic decreases and Elastic Beanstalk scales you down to one instance.  But instead of terminating the second instance, Elastic Beanstalk terminates the first instance that was the leader.  You now don't have any cron jobs running since they were only running on the first instance that was terminated.  See the comments below.\nUpdate 2:\nJust making this clear from the comments below:\nAWS has now protection against automatic instance termination. Just enable it on your leader instance and you're good to go. – Nicolás Arévalo Oct 28 '16 at 9:23",
    "tag": "crontab"
  },
  {
    "question": "How can I run a cron job every 5 minutes starting from a time other than 0 minutes?",
    "answer": "Syntax 1\n*/5+2 * * * * 1st-script\n*/5+4 * * * * 2nd-script\n\nFor future reference take a look at this online Cron Job Generator.\nSyntax 2\nSince there are several reports that the + syntax is not working on Ubuntu 14.04, here's a variation:\n2-59/5 * * * * 1st-script\n4-59/5 * * * * 2nd-script\n\nThis will result in the 1st script to run every 5 minutes starting with an offset of 2 minutes at the beginning of each hour and the 2nd script to behave the same with an offset of 4 minutes.",
    "tag": "crontab"
  },
  {
    "question": "Crontab run every 15 minutes except at 3AM?",
    "answer": "With one cron line, no. With three, yes:\n# Every 15 minutes except for 3:00-3:59\n*/15 0-2,4-23 * * * thejob\n# 3:15, 3:30, 3:45\n15-45/15 3 * * * thejob\n# 3:00 dead\n0 3 * * * otherjob",
    "tag": "crontab"
  },
  {
    "question": "Silence tqdm's output while running tests or running the code via cron",
    "answer": "Example using the 'disable' parameter: \nfrom tqdm import tqdm\nimport time\n\nfor i in tqdm(range(10), disable=True):\n    time.sleep(1)",
    "tag": "crontab"
  },
  {
    "question": "Debugging crontab jobs",
    "answer": "You can enable logging for cron jobs in order to track problems. You need to edit the /etc/rsyslog.conf or /etc/rsyslog.d/50-default.conf (on Ubuntu) file and make sure you have the following line uncommented or add it if it is missing:\ncron.*                         /var/log/cron.log\n\nThen restart rsyslog and cron:\nsudo service rsyslog restart\nsudo service cron restart\n\nCron jobs will log to /var/log/cron.log.",
    "tag": "crontab"
  },
  {
    "question": "Run CRON job every day at specific time",
    "answer": "Cron utility is an effective way to schedule a routine background job at a specific time and/or day on an on-going basis.\nLinux Crontab Format\n\nMIN HOUR DOM MON DOW CMD\n\n\nExample::Scheduling a Job For a Specific Time\nThe basic usage of cron is to execute a job in a specific time as shown below. This will execute the Full backup shell script (full-backup) on 10th June 08:30 AM.\n\nPlease note that the time field uses 24 hours format. So, for 8 AM use\n8, and for 8 PM use 20.\n\n30 08 10 06 * /home/yourname/full-backup\n\n\n30 – 30th Minute\n08 – 08 AM\n10 – 10th Day\n06 – 6th Month (June)\n*– Every day of the week\n\nIn your case, for 2.30PM,\n30 10,14 * * * YOURCMD\n\n\n30 – 30th Minute\n10,14 – 10AM & 2PM\n*– Every day\n*– Every month\n*– Every day of the week\n\nTo know more about cron, visit this website.",
    "tag": "crontab"
  },
  {
    "question": "How to setup CRON job to run every 10 seconds in Linux?",
    "answer": "To elaborate on Sougata Bose's answer, I think the OP wants a command to be run every 10 seconds from a start time; not 10 seconds after the first minute and every subsequent minute.\ncron only has a resolution of 1 minute (there are other tools I think that may have finer resolutions but they are not standard on unix).\nTherefore, to resolve your issue you need 60 seconds / 10 seconds = 6 cron jobs, each with a sleep.\ne.g. run crontab -e and add the following lines to your chosen editor:\n* * * * * ( /usr/bin/wget http://api.us/application/ )  \n* * * * * ( sleep 10 ; /usr/bin/wget http://api.us/application/ )  \n* * * * * ( sleep 20 ; /usr/bin/wget http://api.us/application/ )  \n* * * * * ( sleep 30 ; /usr/bin/wget http://api.us/application/ )  \n* * * * * ( sleep 40 ; /usr/bin/wget http://api.us/application/ )  \n* * * * * ( sleep 50 ; /usr/bin/wget http://api.us/application/ )",
    "tag": "crontab"
  },
  {
    "question": "setup cron tab to specific time of during weekdays",
    "answer": "Same as you did for hours:\n*/2 09-18 * * 1-5 /path_to_script\n\n0 and 7 stand for Sunday\n6 stands for Saturday\nso, 1-5 means from Monday to Friday",
    "tag": "crontab"
  },
  {
    "question": "Export crontab to a file",
    "answer": "Create the backup (export):\ncrontab -l > /some/shared/location/crontab.bak\n\nImport it from the new user:\ncrontab /some/shared/location/crontab.bak",
    "tag": "crontab"
  },
  {
    "question": "Should linux cron jobs be specified with an \"&\" to indicate to run in background?",
    "answer": "Every job that's run by systemd-cron (and most other implementations?) is run in the background automatically, so no need for the &.",
    "tag": "crontab"
  },
  {
    "question": "How can you execute a Node.js script via a cron job?",
    "answer": "just provide the full path to node /usr/local/bin/node in your cron job like:\n30 6 1 * * /usr/local/bin/node /home/steve/example/script.js",
    "tag": "crontab"
  },
  {
    "question": "gpg encrypt file without keyboard interaction",
    "answer": "As David intimated, the problem here is that gpg doesn't trust the public key you're using to encrypt. You could sign the key as he explained.\nAn alternative--especially if the key might be changing occasionally--would be to tack on --trust-model always to your gpg command.\nHere's the relevant bit from the man page:\n\n--trust-model pgp|classic|direct|always|auto\n\n     Set what trust model GnuPG should follow. The models are:\n\n     pgp    This is the Web of Trust combined with trust signatures as used in\n            PGP 5.x and later. This is the default trust model when creating a\n            new trust database.\n\n     classic\n            This is the standard Web of Trust as used in PGP 2.x and earlier.\n\n     direct Key validity is set directly by the user and  not  calculated  via\n            the Web of Trust.\n\n     always Skip  key  validation  and  assume that used keys are always fully\n            trusted. You generally won't use this unless you  are  using  some\n            external  validation  scheme.  This  option  also  suppresses  the\n            \"[uncertain]\" tag printed with signature checks when there  is  no\n            evidence that the user ID is bound to the key.\n\n     auto   Select  the  trust  model depending on whatever the internal trust\n            database says. This is  the  default  model  if  such  a  database\n            already exists.",
    "tag": "crontab"
  },
  {
    "question": "How to instruct cron to execute a job every second week?",
    "answer": "Answer\nModify your Tuesday cron logic to execute every other week since the epoch.\nKnowing that there are 604800 seconds in a week (ignoring DST changes and leap seconds, thank you), and using GNU date:\n0 6 * * Tue expr `date +\\%s` / 604800 \\% 2 >/dev/null || /scripts/fortnightly.sh\n\nAside\nCalendar arithmetic is frustrating.\n@xahtep's answer is terrific but, as @Doppelganger noted in comments, it will fail on certain year boundaries.  None of the date utility's \"week of year\" specifiers can help here.  Some Tuesday in early January will inevitably repeat the week parity of the final Tuesday in the preceding year:  2016-01-05 (%V), 2018-01-02 (%U), and 2019-01-01 (%W).",
    "tag": "crontab"
  },
  {
    "question": "CRON command to run URL address every 5 minutes",
    "answer": "Based on the comments try\n*/5 * * * * wget http://example.com/check\n\n[Edit: 10 Apr 2017]\nThis answer still seems to be getting a few hits so I thought I'd add a link to a new page I stumbled across which may help create cron commands: https://crontab.guru",
    "tag": "crontab"
  }
]